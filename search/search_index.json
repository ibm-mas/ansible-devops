{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MAS DevOps Ansible Collection \u00a4 The ibm.mas_devops Ansible Collection is published on Ansible Galaxy and works with all supported releases of IBM Maximo Application Suite. Release information for the collection can be found in GitHub . Overview \u00a4 Many users ask what is the difference between the MAS Ansible collection and the MAS CLI , the best way we have come up with so far to explain the difference is as below: The ansible collection is a toolbox The cli is a solution built using that toolbox Both are viable ways to install, but anyone using the ansible collection needs to understand what they are using; it is a means to create a solution, it's not a solution in it's own right. The MAS CLI is the reference solution that we (IBM) offer, based on the tools provided in the ansible collection. Using the CLI is the right answer for 95% of users; if you are unsure what is right for you, start here . Usage \u00a4 Run a Playbook \u00a4 The collection includes a number of playbooks that string together multiple roles, you can directly invoke them after installing the collection: ansible-playbook ibm.mas_devops.mas_install_core Run a Role \u00a4 If you only want to perform a single action, you can directly invoke one of our roles from the command line without the need to build a playbook: ansible localhost -m include_role -a name=ibm.mas_devops.ocp_verify You can also use the run_role playbook: ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role Running in a Container Image \u00a4 The easiest way to use this collection is to take advantage of the ibmmas/cli container image, this negates the need to install anything on your local machine (other than docker - or podman if you prefer). # Run with docker docker run -ti --rm --pull always quay.io/ibmmas/cli # Run with podman podman run -ti --rm --pull always quay.io/ibmmas/cli Local Install \u00a4 Install the collection direct from Ansible Galaxy , you must also install the mas-devops Python package. Python 3.11 is recommended as it is the most widely used version of Python within our development team, but any in-support version of Python should work. ansible-galaxy collection install ibm.mas_devops python3 -m pip install mas-devops Optionally, you can also pin the version of the collection that you install, allowing you to control exactly what version of the collection is in use in your solution: ansible-galaxy collection install ibm.mas_devops:18.10.4 python3 -m pip install mas-devops The ansible collection makes use of many dependencies, you can find install scripts showing how we install these dependencies in our own container image in the ibm-mas/cli-base repository, the dependencies you need will be determined by the roles that you intend to use, refer to the roles documentation for dependency infomation. Tip Many systems contain more than one installation of Python, when you install the mas-devops package you must install it to the Python that Ansible is configured to use. You can check the version being used by Ansible by reviewing the output of ansible --version . If you see the error message ERROR! Unexpected Exception, this is probably a bug: No module named 'mas' it almost certainly means that you have not installed the mas-devops package, or have added it to the wrong instance of Python. Ansible Automation Platform \u00a4 If you wish to use Red Hat Ansible Automation Platform then a Automation Execution Environment image is available at quay.io/ibmmas/ansible-devops-ee that contains the ibm.mas_devops collection at the same release level, plus required client packages and access to the automation content collections supported by Red Hat. More details on how to use the ansible-devops execution environment can be found here Action Groups \u00a4 The collection provide a new action group ibm.mas_devops.k8s which can be used to set the default Kubernetes target cluster as an alternative to authenticating with the cluster prior to running our ansible playbooks/roles/actions, see the example below which would return the default storage classes that would be used in this collection for the specified cluster: --- - hosts: localhost any_errors_fatal: true collections: - ibm.mas_devops module_defaults: group/ibm.mas_devops.k8s: host: \"<your host url>\" api_key: \"<your api key>\" tasks: - name: \"Lookup default storage classes\" ibm.mas_devops.get_default_storage_classes: register: classes - debug: msg: \"{{classes}}\" Support \u00a4 This Ansible collection is developed by the IBM Maximo Application Suite development team, customers may raise support tickets via the same routes they would an issue with the product itself, or raise an issue directly in the GitHub repository .","title":"Home"},{"location":"#mas-devops-ansible-collection","text":"The ibm.mas_devops Ansible Collection is published on Ansible Galaxy and works with all supported releases of IBM Maximo Application Suite. Release information for the collection can be found in GitHub .","title":"MAS DevOps Ansible Collection"},{"location":"#overview","text":"Many users ask what is the difference between the MAS Ansible collection and the MAS CLI , the best way we have come up with so far to explain the difference is as below: The ansible collection is a toolbox The cli is a solution built using that toolbox Both are viable ways to install, but anyone using the ansible collection needs to understand what they are using; it is a means to create a solution, it's not a solution in it's own right. The MAS CLI is the reference solution that we (IBM) offer, based on the tools provided in the ansible collection. Using the CLI is the right answer for 95% of users; if you are unsure what is right for you, start here .","title":"Overview"},{"location":"#usage","text":"","title":"Usage"},{"location":"#run-a-playbook","text":"The collection includes a number of playbooks that string together multiple roles, you can directly invoke them after installing the collection: ansible-playbook ibm.mas_devops.mas_install_core","title":"Run a Playbook"},{"location":"#run-a-role","text":"If you only want to perform a single action, you can directly invoke one of our roles from the command line without the need to build a playbook: ansible localhost -m include_role -a name=ibm.mas_devops.ocp_verify You can also use the run_role playbook: ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role","title":"Run a Role"},{"location":"#running-in-a-container-image","text":"The easiest way to use this collection is to take advantage of the ibmmas/cli container image, this negates the need to install anything on your local machine (other than docker - or podman if you prefer). # Run with docker docker run -ti --rm --pull always quay.io/ibmmas/cli # Run with podman podman run -ti --rm --pull always quay.io/ibmmas/cli","title":"Running in a Container Image"},{"location":"#local-install","text":"Install the collection direct from Ansible Galaxy , you must also install the mas-devops Python package. Python 3.11 is recommended as it is the most widely used version of Python within our development team, but any in-support version of Python should work. ansible-galaxy collection install ibm.mas_devops python3 -m pip install mas-devops Optionally, you can also pin the version of the collection that you install, allowing you to control exactly what version of the collection is in use in your solution: ansible-galaxy collection install ibm.mas_devops:18.10.4 python3 -m pip install mas-devops The ansible collection makes use of many dependencies, you can find install scripts showing how we install these dependencies in our own container image in the ibm-mas/cli-base repository, the dependencies you need will be determined by the roles that you intend to use, refer to the roles documentation for dependency infomation. Tip Many systems contain more than one installation of Python, when you install the mas-devops package you must install it to the Python that Ansible is configured to use. You can check the version being used by Ansible by reviewing the output of ansible --version . If you see the error message ERROR! Unexpected Exception, this is probably a bug: No module named 'mas' it almost certainly means that you have not installed the mas-devops package, or have added it to the wrong instance of Python.","title":"Local Install"},{"location":"#ansible-automation-platform","text":"If you wish to use Red Hat Ansible Automation Platform then a Automation Execution Environment image is available at quay.io/ibmmas/ansible-devops-ee that contains the ibm.mas_devops collection at the same release level, plus required client packages and access to the automation content collections supported by Red Hat. More details on how to use the ansible-devops execution environment can be found here","title":"Ansible Automation Platform"},{"location":"#action-groups","text":"The collection provide a new action group ibm.mas_devops.k8s which can be used to set the default Kubernetes target cluster as an alternative to authenticating with the cluster prior to running our ansible playbooks/roles/actions, see the example below which would return the default storage classes that would be used in this collection for the specified cluster: --- - hosts: localhost any_errors_fatal: true collections: - ibm.mas_devops module_defaults: group/ibm.mas_devops.k8s: host: \"<your host url>\" api_key: \"<your api key>\" tasks: - name: \"Lookup default storage classes\" ibm.mas_devops.get_default_storage_classes: register: classes - debug: msg: \"{{classes}}\"","title":"Action Groups"},{"location":"#support","text":"This Ansible collection is developed by the IBM Maximo Application Suite development team, customers may raise support tickets via the same routes they would an issue with the product itself, or raise an issue directly in the GitHub repository .","title":"Support"},{"location":"execution-environment/","text":"Execution Environment \u00a4 Details on the Red Hat Ansible Automation Platform Execution Environment for the ibm.mas_devops Ansible Collection. Execution Environment Image \u00a4 The execution environment image for ansible-devops builds on the latest ansible-automation-platform-24/ee-supported-rhel9 image from Red Hat that provides the ansible-core and Red Hat supported collections. The ansible-devops-ee image includes the ibm.mas_devops collection and all required client libraries to function. The image is uploaded to quay.io at quay.io/ibmmas/ansible-devops-ee How to setup Anisble Automation Platform \u00a4 Organization \u00a4 An Organization is a logical collection of Users, Teams, Projects, and Inventories, and is the highest level in the automation controller object hierarchy. Create an organization if you don't already have one: Inventory \u00a4 An Inventory is a collection of hosts against which jobs may be launched, the same as an Ansible inventory file. The ibm.mas_devops collection runs against localhost so an Inventory of hosts just requires the one host of localhost to be added but this might be important for any other roles you might want to execute outside of this collection but within this organization. Create an initial inventory if one doesn't exist yet: Ensure that the host entry has the following variables set to ensure a local connection: Execution Environment \u00a4 In Ansible Automation Platform (AAP) you can setup a new Execution Environment by specifying the image and tag. You can use either a versioned tag or latest such as quay.io/ibmmas/ansible-devops-ee:latest or quay.io/ibmmas/ansible-devops-ee:24.0.0 Credentials \u00a4 To use your own playbooks that are in source control management (SCM) you will need to setup credentials in AAP to fetch these (unless the repo is public). An example of this is providing a GitHub fine-grained access token which will just allow the token to read the contents of the repo. \u00a1 creds-2 Depending on if you have an exiting cluster or not, then you will need to setup the OpenShift or Kubernetes API Bearer Token credentials to access the Openshift cluster, which will be required on any Jobs that interect with the cluster. Project \u00a4 Once you have the execution environment setup you can now create a Project that will point to the source of your playbooks. It is recommended that you use your own source of playbooks rather than the playbooks contained in this repo, to give you full control over what is run to suit your needs. Create the Project and set the Execution Environment to be the one created eariler, and set the source control details and credentials needed for AAP to read your playbooks. Job Templates \u00a4 The Job Templates are what each Job is launched from and contains the details of what Playbook to execute. The Job is the executed instance of a Template that contains any specific values required. To create the Template add the Inventory, Project and Execution Environment setup previously. The playbook dropdown should contain all the playbooks that are found in your SCM (if no playbooks are shown then check the Troubleshooting section). At this point you can choose any other settings that you might want to use, and it is recommended to check the AAP documentation on this. Once the Job template is created then you can click on the created template and navigate to Survey . This is where you can provide any secure variables for your playbook that you don't want to keep in source control. You can choose the questions to ask and what type the variable should be. The variable name should make the variable name that the corresponding role is expecting. Finally, ensure that the Survey is Enabled: Workflow Templates \u00a4 The Workflow Templates allow you to configure a number of job templates (or other workflow templates) together. This can be useful when you want to use other roles from other collections (see the AAP Certified Content here ) as well as the ansible-devops collection. For example, you might want to run some AWS or VMWare roles to configure resources after or before the ansible-devops collection roles. How to run a Job \u00a4 Now you have the Job Template setup, you can launch the Job from that Template. Navigate to the Templates view and click the launch icon: Enter any survey questions you have configured: The Job now launches and you can see the output. The list of executing and executed jobs can be seen from the Jobs seciton Examples of playbooks \u00a4 You can use the ansbile-devops playbooks as a reference on how to setup your own playbooks. Defining your own playbooks gives you full control over what roles can be run and also include your own roles or roles from the supported Red Hat collection. If you are using the playbooks as a starting point then please note the following changes that would be required: Remove any pre_tasks related to environment variables Remove any lookups of environment variables from the playbook mas_install_core.yml \u00a4 An example of this is taking the mas_install_core.yml playbook: --- - hosts: localhost any_errors_fatal: true vars: # Install SLS # Note: We need to create some intermediate variables to construct sls_mongodb_cfg_file, # This is the only reason they feature here, all the roles that use these variables would also # load them directly from the same environment variables if they were not defined here. mas_config_dir: \"{{ lookup('env', 'MAS_CONFIG_DIR') }}\" mongodb_namespace: \"{{ lookup('env', 'MONGODB_NAMESPACE') | default('mongoce', True) }}\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" # Core Services Configuration mas_channel: \"{{ lookup('env', 'MAS_CHANNEL') | default('9.1.x', true) }}\" # Workspace Configuration mas_workspace_name: \"{{ lookup('env', 'MAS_WORKSPACE_NAME') | default('MAS Development', true) }}\" mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') | default('masdev', true) }}\" pre_tasks: # For the full set of supported environment variables refer to the playbook documentation - name: Check for required environment variables assert: that: # IBM - lookup('env', 'IBM_ENTITLEMENT_KEY') != \"\" # MAS - lookup('env', 'MAS_INSTANCE_ID') != \"\" - lookup('env', 'MAS_CONFIG_DIR') != \"\" # SLS - (lookup('env', 'SLS_LICENSE_ID') != \"\" and lookup('env', 'SLS_LICENSE_FILE') != \"\") or (lookup('env', 'SLS_ENTITLEMENT_FILE') != \"\") # DRO - lookup('env', 'DRO_CONTACT_EMAIL') != \"\" - lookup('env', 'DRO_CONTACT_FIRSTNAME') != \"\" - lookup('env', 'DRO_CONTACT_LASTNAME') != \"\" fail_msg: \"One or more required environment variables are not defined\" roles: # 1. Install cluster-scoped dependencies (e.g. Cert-Manager, Operator Catalogs) & Grafana - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.cert_manager - ibm.mas_devops.grafana # 2. Install MongoDb - ibm.mas_devops.mongodb # 3. Install SLS # Set sls_url, sls_tls_crt_local_file_path, sls_registration_key variables to skip install and set up SLSCfg for # an existing installation of SLS - ibm.mas_devops.sls # 4 Install DRO - ibm.mas_devops.dro # 5. Generate a Workspace - ibm.mas_devops.gencfg_workspace # 6. Install & configure MAS - ibm.mas_devops.suite_dns - ibm.mas_devops.suite_certs - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify and changing it to look like the following: --- - name: \"mas-core\" hosts: localhost any_errors_fatal: true vars: mas_config_dir: \"/tmp\" mongodb_namespace: \"mongoce\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" # Core Services Configuration mas_channel: \"9.1.x\" mas_instance_id: \"aap1\" # Workspace Configuration mas_workspace_name: \"MAS Development\" mas_workspace_id: \"masdev\" entitlement_file: \"license_file/entitlement.lic\" dro_contact: email: \"whitfiea@uk.ibm.com\" first_name: \"Andrew\" last_name: \"Whitfield\" roles: # 1. Install cluster-scoped dependencies (e.g. Cert-Manager, Operator Catalogs) & Grafana - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.cert_manager # 2. Install MongoDb - ibm.mas_devops.mongodb # 3. Install SLS - ibm.mas_devops.sls # 4 Install DRO - ibm.mas_devops.dro # 5. Generate a Workspace - ibm.mas_devops.gencfg_workspace # 6. Install & configure MAS - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify The above has removed any environment variable lookups in the playbook itself, and also modified the roles so it doesn't run the grafana role. ocp-provision \u00a4 If you need to set an environment variable then you can do this in the playbook as well, this would be needed in certain cases such as setting the AWS cli details. The aws_access_key_id and aws_secret_access_key are variables set by AAP as a result of the Survey questions: --- - name: \"ocp-provision\" hosts: localhost vars: cluster_name: testcluster cluster_type: rosa ocp_version: 4.19.14 rosa_compute_nodes: 3 environment: AWS_DEFAULT_REGION: us-east-1 AWS_ACCESS_KEY_ID: \"{{ aws_access_key_id }}\" AWS_SECRET_ACCESS_KEY: \"{{ aws_secret_access_key }}\" roles: # 1. Provision the ROSA cluster - ibm.mas_devops.ocp_provision # 2. Login and verify the cluster is ready - ibm.mas_devops.ocp_login - ibm.mas_devops.ocp_verify # 3. Set up storage classes - ibm.mas_devops.ocp_efs Troubleshooting \u00a4 Helpful Links \u00a4 Red Hat AAP documentation Installing AAP in OCP Other install methods of AAP AAP Certified Content Can't see all output in log file \u00a4 The output for the job doesn't output all the data in the standard view. To see the expanded data you must click on the entry to see the details and then select JSON: My playbook or updated playbook can't be found \u00a4 If you have updated your Playbook in your SCM but it is not reflected in AAP then you can sync the Project. Navigate to the Project view and click the sync icon so the Project gets the latest revision. Job failed can I re-launch it? \u00a4 If the job fails and you want to relaunch with the same variables (from both hosts and survey), then you can re-launch the job from the Job page or the Jobs list page How to provide supporting files? \u00a4 If you have a file that is not a playbook but you want to reference it, then you can include this in the same repo as your playbooks as AAP will be syncing the whole repo. You can then reference these files from your playbook or roles, using a pth relative to the playbook being run. An exmaple of this is the entitlement_file which can be contained in your repo: and then referenced in your playbook in the path \"license_file/entitlement.lic\" : --- - hosts: localhost any_errors_fatal: true vars: mas_config_dir: \"/tmp\" mongodb_namespace: \"mongoce\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" entitlement_file: \"license_file/entitlement.lic\" roles: - ibm.mas_devops.mongodb - ibm.mas_devops.sls How do I set environment variables? \u00a4 Running in AAP means you can't set environment variables on the ansible controller before a playbook is executed. If a role in the ibm.mas_devops collection has an environment variable to be set there is normally a corresponding ansible variable. For exmaple, in the documentation for (kafka_action)[https://ibm-mas.github.io/ansible-devops/roles/kafka/#kafka_action] in the kafka role it says to set the environment variable KAFKA_VERSION , this is just setting the ansible role varible called kafka_version which you can set in your playbook. With ansible predence in place, the play vars or role vars will override the default vars that use the environment variable. Example setting play vars: --- - hosts: localhost any_errors_fatal: true vars: kafka_version: 3.7.0 roles: - ibm.mas_devops.kafka Example setting role vars: --- - hosts: localhost any_errors_fatal: true roles: - role: ibm.mas_devops.kafka vars: kafka_version: 3.7.0","title":"Ansible Automation Platform"},{"location":"execution-environment/#execution-environment","text":"Details on the Red Hat Ansible Automation Platform Execution Environment for the ibm.mas_devops Ansible Collection.","title":"Execution Environment"},{"location":"execution-environment/#execution-environment-image","text":"The execution environment image for ansible-devops builds on the latest ansible-automation-platform-24/ee-supported-rhel9 image from Red Hat that provides the ansible-core and Red Hat supported collections. The ansible-devops-ee image includes the ibm.mas_devops collection and all required client libraries to function. The image is uploaded to quay.io at quay.io/ibmmas/ansible-devops-ee","title":"Execution Environment Image"},{"location":"execution-environment/#how-to-setup-anisble-automation-platform","text":"","title":"How to setup Anisble Automation Platform"},{"location":"execution-environment/#organization","text":"An Organization is a logical collection of Users, Teams, Projects, and Inventories, and is the highest level in the automation controller object hierarchy. Create an organization if you don't already have one:","title":"Organization"},{"location":"execution-environment/#inventory","text":"An Inventory is a collection of hosts against which jobs may be launched, the same as an Ansible inventory file. The ibm.mas_devops collection runs against localhost so an Inventory of hosts just requires the one host of localhost to be added but this might be important for any other roles you might want to execute outside of this collection but within this organization. Create an initial inventory if one doesn't exist yet: Ensure that the host entry has the following variables set to ensure a local connection:","title":"Inventory"},{"location":"execution-environment/#execution-environment_1","text":"In Ansible Automation Platform (AAP) you can setup a new Execution Environment by specifying the image and tag. You can use either a versioned tag or latest such as quay.io/ibmmas/ansible-devops-ee:latest or quay.io/ibmmas/ansible-devops-ee:24.0.0","title":"Execution Environment"},{"location":"execution-environment/#credentials","text":"To use your own playbooks that are in source control management (SCM) you will need to setup credentials in AAP to fetch these (unless the repo is public). An example of this is providing a GitHub fine-grained access token which will just allow the token to read the contents of the repo. \u00a1 creds-2 Depending on if you have an exiting cluster or not, then you will need to setup the OpenShift or Kubernetes API Bearer Token credentials to access the Openshift cluster, which will be required on any Jobs that interect with the cluster.","title":"Credentials"},{"location":"execution-environment/#project","text":"Once you have the execution environment setup you can now create a Project that will point to the source of your playbooks. It is recommended that you use your own source of playbooks rather than the playbooks contained in this repo, to give you full control over what is run to suit your needs. Create the Project and set the Execution Environment to be the one created eariler, and set the source control details and credentials needed for AAP to read your playbooks.","title":"Project"},{"location":"execution-environment/#job-templates","text":"The Job Templates are what each Job is launched from and contains the details of what Playbook to execute. The Job is the executed instance of a Template that contains any specific values required. To create the Template add the Inventory, Project and Execution Environment setup previously. The playbook dropdown should contain all the playbooks that are found in your SCM (if no playbooks are shown then check the Troubleshooting section). At this point you can choose any other settings that you might want to use, and it is recommended to check the AAP documentation on this. Once the Job template is created then you can click on the created template and navigate to Survey . This is where you can provide any secure variables for your playbook that you don't want to keep in source control. You can choose the questions to ask and what type the variable should be. The variable name should make the variable name that the corresponding role is expecting. Finally, ensure that the Survey is Enabled:","title":"Job Templates"},{"location":"execution-environment/#workflow-templates","text":"The Workflow Templates allow you to configure a number of job templates (or other workflow templates) together. This can be useful when you want to use other roles from other collections (see the AAP Certified Content here ) as well as the ansible-devops collection. For example, you might want to run some AWS or VMWare roles to configure resources after or before the ansible-devops collection roles.","title":"Workflow Templates"},{"location":"execution-environment/#how-to-run-a-job","text":"Now you have the Job Template setup, you can launch the Job from that Template. Navigate to the Templates view and click the launch icon: Enter any survey questions you have configured: The Job now launches and you can see the output. The list of executing and executed jobs can be seen from the Jobs seciton","title":"How to run a Job"},{"location":"execution-environment/#examples-of-playbooks","text":"You can use the ansbile-devops playbooks as a reference on how to setup your own playbooks. Defining your own playbooks gives you full control over what roles can be run and also include your own roles or roles from the supported Red Hat collection. If you are using the playbooks as a starting point then please note the following changes that would be required: Remove any pre_tasks related to environment variables Remove any lookups of environment variables from the playbook","title":"Examples of playbooks"},{"location":"execution-environment/#mas_install_coreyml","text":"An example of this is taking the mas_install_core.yml playbook: --- - hosts: localhost any_errors_fatal: true vars: # Install SLS # Note: We need to create some intermediate variables to construct sls_mongodb_cfg_file, # This is the only reason they feature here, all the roles that use these variables would also # load them directly from the same environment variables if they were not defined here. mas_config_dir: \"{{ lookup('env', 'MAS_CONFIG_DIR') }}\" mongodb_namespace: \"{{ lookup('env', 'MONGODB_NAMESPACE') | default('mongoce', True) }}\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" # Core Services Configuration mas_channel: \"{{ lookup('env', 'MAS_CHANNEL') | default('9.1.x', true) }}\" # Workspace Configuration mas_workspace_name: \"{{ lookup('env', 'MAS_WORKSPACE_NAME') | default('MAS Development', true) }}\" mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') | default('masdev', true) }}\" pre_tasks: # For the full set of supported environment variables refer to the playbook documentation - name: Check for required environment variables assert: that: # IBM - lookup('env', 'IBM_ENTITLEMENT_KEY') != \"\" # MAS - lookup('env', 'MAS_INSTANCE_ID') != \"\" - lookup('env', 'MAS_CONFIG_DIR') != \"\" # SLS - (lookup('env', 'SLS_LICENSE_ID') != \"\" and lookup('env', 'SLS_LICENSE_FILE') != \"\") or (lookup('env', 'SLS_ENTITLEMENT_FILE') != \"\") # DRO - lookup('env', 'DRO_CONTACT_EMAIL') != \"\" - lookup('env', 'DRO_CONTACT_FIRSTNAME') != \"\" - lookup('env', 'DRO_CONTACT_LASTNAME') != \"\" fail_msg: \"One or more required environment variables are not defined\" roles: # 1. Install cluster-scoped dependencies (e.g. Cert-Manager, Operator Catalogs) & Grafana - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.cert_manager - ibm.mas_devops.grafana # 2. Install MongoDb - ibm.mas_devops.mongodb # 3. Install SLS # Set sls_url, sls_tls_crt_local_file_path, sls_registration_key variables to skip install and set up SLSCfg for # an existing installation of SLS - ibm.mas_devops.sls # 4 Install DRO - ibm.mas_devops.dro # 5. Generate a Workspace - ibm.mas_devops.gencfg_workspace # 6. Install & configure MAS - ibm.mas_devops.suite_dns - ibm.mas_devops.suite_certs - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify and changing it to look like the following: --- - name: \"mas-core\" hosts: localhost any_errors_fatal: true vars: mas_config_dir: \"/tmp\" mongodb_namespace: \"mongoce\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" # Core Services Configuration mas_channel: \"9.1.x\" mas_instance_id: \"aap1\" # Workspace Configuration mas_workspace_name: \"MAS Development\" mas_workspace_id: \"masdev\" entitlement_file: \"license_file/entitlement.lic\" dro_contact: email: \"whitfiea@uk.ibm.com\" first_name: \"Andrew\" last_name: \"Whitfield\" roles: # 1. Install cluster-scoped dependencies (e.g. Cert-Manager, Operator Catalogs) & Grafana - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.cert_manager # 2. Install MongoDb - ibm.mas_devops.mongodb # 3. Install SLS - ibm.mas_devops.sls # 4 Install DRO - ibm.mas_devops.dro # 5. Generate a Workspace - ibm.mas_devops.gencfg_workspace # 6. Install & configure MAS - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify The above has removed any environment variable lookups in the playbook itself, and also modified the roles so it doesn't run the grafana role.","title":"mas_install_core.yml"},{"location":"execution-environment/#ocp-provision","text":"If you need to set an environment variable then you can do this in the playbook as well, this would be needed in certain cases such as setting the AWS cli details. The aws_access_key_id and aws_secret_access_key are variables set by AAP as a result of the Survey questions: --- - name: \"ocp-provision\" hosts: localhost vars: cluster_name: testcluster cluster_type: rosa ocp_version: 4.19.14 rosa_compute_nodes: 3 environment: AWS_DEFAULT_REGION: us-east-1 AWS_ACCESS_KEY_ID: \"{{ aws_access_key_id }}\" AWS_SECRET_ACCESS_KEY: \"{{ aws_secret_access_key }}\" roles: # 1. Provision the ROSA cluster - ibm.mas_devops.ocp_provision # 2. Login and verify the cluster is ready - ibm.mas_devops.ocp_login - ibm.mas_devops.ocp_verify # 3. Set up storage classes - ibm.mas_devops.ocp_efs","title":"ocp-provision"},{"location":"execution-environment/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"execution-environment/#helpful-links","text":"Red Hat AAP documentation Installing AAP in OCP Other install methods of AAP AAP Certified Content","title":"Helpful Links"},{"location":"execution-environment/#cant-see-all-output-in-log-file","text":"The output for the job doesn't output all the data in the standard view. To see the expanded data you must click on the entry to see the details and then select JSON:","title":"Can't see all output in log file"},{"location":"execution-environment/#my-playbook-or-updated-playbook-cant-be-found","text":"If you have updated your Playbook in your SCM but it is not reflected in AAP then you can sync the Project. Navigate to the Project view and click the sync icon so the Project gets the latest revision.","title":"My playbook or updated playbook can't be found"},{"location":"execution-environment/#job-failed-can-i-re-launch-it","text":"If the job fails and you want to relaunch with the same variables (from both hosts and survey), then you can re-launch the job from the Job page or the Jobs list page","title":"Job failed can I re-launch it?"},{"location":"execution-environment/#how-to-provide-supporting-files","text":"If you have a file that is not a playbook but you want to reference it, then you can include this in the same repo as your playbooks as AAP will be syncing the whole repo. You can then reference these files from your playbook or roles, using a pth relative to the playbook being run. An exmaple of this is the entitlement_file which can be contained in your repo: and then referenced in your playbook in the path \"license_file/entitlement.lic\" : --- - hosts: localhost any_errors_fatal: true vars: mas_config_dir: \"/tmp\" mongodb_namespace: \"mongoce\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" entitlement_file: \"license_file/entitlement.lic\" roles: - ibm.mas_devops.mongodb - ibm.mas_devops.sls","title":"How to provide supporting files?"},{"location":"execution-environment/#how-do-i-set-environment-variables","text":"Running in AAP means you can't set environment variables on the ansible controller before a playbook is executed. If a role in the ibm.mas_devops collection has an environment variable to be set there is normally a corresponding ansible variable. For exmaple, in the documentation for (kafka_action)[https://ibm-mas.github.io/ansible-devops/roles/kafka/#kafka_action] in the kafka role it says to set the environment variable KAFKA_VERSION , this is just setting the ansible role varible called kafka_version which you can set in your playbook. With ansible predence in place, the play vars or role vars will override the default vars that use the environment variable. Example setting play vars: --- - hosts: localhost any_errors_fatal: true vars: kafka_version: 3.7.0 roles: - ibm.mas_devops.kafka Example setting role vars: --- - hosts: localhost any_errors_fatal: true roles: - role: ibm.mas_devops.kafka vars: kafka_version: 3.7.0","title":"How do I set environment variables?"},{"location":"playbooks/aiservice/","text":"Install AI Service \u00a4 Important These playbooks are samples to demonstrate how to use the roles in this collection. They are not intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install AI Service is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance. Dependencies \u00a4 IBM Suite License Service installed on OCP cluster or external instance or details from external instance IBM Data Reporter Operator installed on OCP cluster or external instance or details from external instance Object Storage Minio (installed on the same cluster as aiservice) AWS S3 (Provide details to connect to AWS S3. Bucket names must be unique globally. Ensure to use a unique bucket prefix when using AWS S3) Overview \u00a4 This playbook will add AI Service v9.1.x to OCP cluster. This playbook can be ran against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: IBM Maximo Operator Catalog RedHat Certificate Manager MongoDb optional if SLS is not being installed on same cluster IBM Suite License Service (~10 Minutes) optional if customer chooses to use SLS from external cluster IBM Data Reporter Operator (~10 Minutes) optional if customer chooses to use DRO from external cluster IBM Db2 Minio (~5 minutes) optional if customer choose to use external object storage Install ODH: Install Red Hat OpenShift Serverless Operator Required when using serverless deployment mode for ODH Install Red Hat OpenShift Service Mesh Operator Required when using serverless deployment mode for ODH Install Authorino Operator Install Open Data Hub Operator Create DSCInitialization instance Create DataScienceCluster instance Install AI Service (using playbook): Install application (~20 Minutes) Configure AI Service (kmodels, tenant, etc) (~20 Minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing application, just customize the application install and configure stages at the end of the playbook. Required environment variables \u00a4 AISERVICE_INSTANCE_ID Declare the instance ID for the AI service install MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry MAS_ENTITLEMENT_USERNAME Your IBM Entitlement user to access the IBM Container Registry AISERVICE_CHANNEL Aiservice application channel AISERVICE_S3_ACCESSKEY Access Key for object storage provider AISERVICE_S3_SECRETKEY Secret key for object storage provider AISERVICE_S3_HOST Your object storage provider host AISERVICE_S3_REGION Your object storage provider region - only when use AWS S3 instance AISERVICE_S3_SSL Is your object storage SSL enabled? (true/false) AISERVICE_S3_TENANTS_BUCKET Bucket name to store tenants info AISERVICE_S3_TEMPLATES_BUCKET Bucket name to store templates info AISERVICE_S3_BUCKET_PREFIX Bucket prefix configured with object storage provider AISERVICE_WATSONXAI_APIKEY You WatsonX AI api key AISERVICE_WATSONXAI_URL You WatsonX AI url AISERVICE_WATSONXAI_PROJECT_ID You WatsonX projedt Id Tip AI service supports AWS and Minio storage providers. Required environment variables (SaaS) \u00a4 AISERVICE_SAAS specify if saas deployment (default value is: false) MAS_CONFIG_DIR specify config location, mandatory when AISERVICE_SAAS=true AISERVICE_DOMAIN specify cluster domain, mandatory when AISERVICE_SAAS=true AISERVICE_SLS_URL specify SLS url, mandatory when AISERVICE_SAAS=true AISERVICE_SLS_REGISTRATION_KEY specify sls registration key, mandatory when AISERVICE_SAAS=true , to get value: look in ibm-sls namespace, pod sls-api-licensing-xxx and in Environment tab check REGISTRATION_KEY value AISERVICE_DRO_URL specify DRO url, mandatory when AISERVICE_SAAS=true AISERVICE_DRO_TOKEN specify DRO token, mandatory when AISERVICE_SAAS=true to get value: go to mas-{{ instance_id }}-core and look in secret dro-apikey DB2_INSTANCE_NAME specify DB2 instance name (default value is: aiservice), mandatory when AISERVICE_SAAS=true IBM_ENTITLEMENT_KEY specify IBM Entitlement key, mandatory when AISERVICE_SAAS=true Optional environment variables \u00a4 MAS_ICR_CP Provide custom registry for AI service applications MAS_ICR_CPOPEN Provide custom registry for AI service operator MAS_CATALOG_VERSION Your custom AI service catalog version ARTIFACTORY_USERNAME Your artifactory user name to access - this is needed if user deploy from custom registry for example docker-na-public.artifactory.swg-devops.com ARTIFACTORY_TOKEN Your artifactory token for user to access - this is needed if user deploy from custom registry for example docker-na-public.artifactory.swg-devops.com AISERVICE_TENANT_ACTION Whether to install or remove tenant (default value is: install) AISERVICE_APIKEY_ACTION Whether to install or remove or update apikey (default value is: install) AISERVICE_WATSONX_ACTION Whether to install or remove watsonx secret (default value is: install) AISERVICE_S3_ACTION Whether to install or remove s3 (default value is: install) INSTALL_DB2 Whether to install DB2 (default value is: false) INSTALL_MINIO Whether to install minio (default value is: false) INSTALL_SLS Whether to install IBM Suite License Service (default value is: false) INSTALL_DRO Whether to install IBM Data Reporter Operator (default value is: false) AISERVICE_DB2_USERNAME The username to use for authentication with the database AISERVICE_DB2_PASSWORD The password to use for authentication with the database AISERVICE_DB2_JDBC_URL The JDBC URL specifying the host and port of the database, typically in the format jdbc:db2://host:port/ AISERVICE_DB2_SSL_ENABLED A flag indicating whether to enable SSL encryption for the database connection (default value is: true) USE_AWS_DB2 A flag indicating whether to use an AWS-hosted DB2 instance (default value is: false) AISERVICE_DOMAIN Provide custom domain (default value is: empty) AISERVICE_WATSONXAI_CA_CRT provide WatsonX AI CA certificate AISERVICE_WATSONXAI_FULL optional on prem to define if WatsonX AI engine is full or light (true/false) AISERVICE_WATSONXAI_DEPLOYMENT_ID optional on prem define deployment Id AISERVICE_WATSONXAI_SPACE_ID optional on prem define space Id AISERVICE_WATSONXAI_INSTANCE_ID optional on prem define instance id (default: openshift) AISERVICE_WATSONXAI_USERNAME optional on prem define user name AISERVICE_WATSONXAI_VERSION optional on prem define version of CPD AISERVICE_CERTIFICATE_ISSUER Optional to specify pre-configured certificate issuer to use for AI Service public certificates AISERVICE_CERTIFICATE_DURATION Optional to specify expiration duration for public certificates (default: 8760h0m0s) AISERVICE_CERTIFICATE_RENEW_BEFORE Optional to specify when to renew public certificates before they expire (default: 72h0m0s) Usage \u00a4 AI service deployment steps \u00a4 Tip For S3 manage please make sure you have deployed dependencies Install boto3 python module (use python environment): python3 -m venv /tmp/venv source /tmp/venv/bin/activate python3 -m pip install boto3 Run playbooks for deploy AI service: AISERVICE_SLS_REGISTRATION_KEY - value can be found in ibm-sls namespace, in pod sls-api-licensing-85699fb57-9lmrq please look in environments tab, then value REGISTRATION_KEY AISERVICE_DRO_TOKEN - go to mas-instance_id-core namespace and in secrets find dro-apikey In AWS for AISERVICE_S3_TENANTS_BUCKET , AISERVICE_S3_TEMPLATES_BUCKET user need to create S3 buckets with unique name export ARTIFACTORY_USERNAME=\"\" export ARTIFACTORY_TOKEN=\"\" export MAS_ICR_CP=\"\" export MAS_ICR_CPOPEN=\"\" export MAS_ENTITLEMENT_USERNAME=\"\" export MAS_ENTITLEMENT_KEY=\"\" export MAS_INSTANCE_ID=\"\" export MAS_APP_CHANNEL=\"\" export MAS_CATALOG_VERSION=\"\" export IBM_ENTITLEMENT_KEY=${MAS_ENTITLEMENT_KEY} export MAS_CONFIG_DIR=\"\" export DRO_CONTACT_EMAIL=\"\" export DRO_CONTACT_FIRSTNAME=\"\" export DRO_CONTACT_LASTNAME=\"\" export SLS_MONGODB_CFG_FILE=${MAS_CONFIG_DIR}/mongo-mongoce.yml export SLS_LICENSE_ID=\"\" export SLS_LICENSE_FILE=\"\" export INSTALL_DB2=\"\" export INSTALL_MINIO=\"\" export INSTALL_MONGO=\"\" export INSTALL_SLS=\"\" export INSTALL_DRO=\"\" export AISERVICE_S3_BUCKET_PREFIX=\"\" export AISERVICE_S3_REGION=\"\" export AISERVICE_S3_ENDPOINT_URL=\"\" export AISERVICE_TENANT_S3_REGION=\"\" export AISERVICE_TENANT_S3_ENDPOINT_URL=\"\" export AISERVICE_TENANT_S3_BUCKET_PREFIX=\"\" export AISERVICE_TENANT_S3_ACCESS_KEY=\"\" export AISERVICE_TENANT_S3_SECRET_KEY=\"\" export RSL_URL=\"\" export RSL_ORG_ID=\"\" export RSL_TOKEN=\"\" export MINIO_ROOT_PASSWORD=\"\" export AISERVICE_S3_ACCESSKEY=\"\" export AISERVICE_S3_SECRETKEY=${MINIO_ROOT_PASSWORD} export AISERVICE_S3_HOST=\"\" export AISERVICE_S3_SSL=\"\" export AISERVICE_S3_PROVIDER=\"\" export AISERVICE_S3_PORT=\"\" export AISERVICE_S3_REGION=\"\" export AISERVICE_S3_TENANTS_BUCKET=\"\" export AISERVICE_S3_TEMPLATES_BUCKET=\"\" export AISERVICE_WATSONXAI_APIKEY=\"\" export AISERVICE_WATSONXAI_URL=\"\" export AISERVICE_WATSONXAI_PROJECT_ID=\"\" export AISERVICE_SUBSCRIPTION_ID=\"\" export AISERVICE_DRO_TENANT_ID=\"\" export AISERVICE_TENANT_ENTITLEMENT_START_DATE=\"YYYY-MM-DD\" export AISERVICE_TENANT_ENTITLEMENT_END_DATE=\"YYYY-MM-DD\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/aiservice.yml Create S3 \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_S3_ACCESSKEY=\"<storage provider access key>\" export AISERVICE_S3_SECRETKEY=\"<storage provider secret key>\" export AISERVICE_S3_HOST=\"<storage provider host>\" export AISERVICE_S3_REGION=\"<storage provider region>\" export AISERVICE_S3_ACTION=\"install\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Delete S3 \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_S3_ACCESSKEY=\"<storage provider access key>\" export AISERVICE_S3_SECRETKEY=\"<storage provider secret key>\" export AISERVICE_S3_HOST=\"<storage provider host>\" export AISERVICE_S3_REGION=\"<storage provider region>\" export AISERVICE_S3_ACTION=\"remove\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Create API Key \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_APIKEY_ACTION=\"install\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Delete API Key \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_APIKEY_ACTION=\"remove\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Create WatsonX API Key \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_WATSONX_ACTION=\"install\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Delete WatsonX API Key \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_WATSONX_ACTION=\"remove\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Create Tenant \u00a4 The AISERVICE_SLS_REGISTRATION_KEY value can be found in ibm-sls namespace, in pod sls-api-licensing-85699fb57-9lmrq please look in environments tab, then value REGISTRATION_KEY . To obtain the AISERVICE_DRO_TOKEN go to mas-instance_id-core namespace and in secrets find dro-apikey export AISERVICE_TENANT_NAME=\"user7\" export AISERVICE_SLS_SUBSCRIPTION_ID=\"007\" export TENANT_ACTION=\"install\" export ROLE_NAME=\"aiservice_tenant\" export AISERVICE_SAAS=\"true\" export AISERVICE_DOMAIN=\"\" export AISERVICE_SLS_URL=\"https://sls.ibm-sls.ibm-sls.\"${AISERVICE_DOMAIN} export AISERVICE_SLS_REGISTRATION_KEY=\"\" export AISERVICE_DRO_URL=\"https://ibm-data-reporter-redhat-marketplace.\"${AISERVICE_DOMAIN} export AISERVICE_DRO_TOKEN=\"\" export AISERVICE_SLS_CACERT=\"\" export AISERVICE_DRO_CACERT=\"\" export AISERVICE_WATSONXAI_APIKEY=\"\" export AISERVICE_WATSONXAI_URL=\"\" export AISERVICE_WATSONXAI_PROJECT_ID=\"\" export AISERVICE_S3_ACCESSKEY=\"\" export AISERVICE_S3_SECRETKEY=\"\" export AISERVICE_S3_HOST=\"\" export AISERVICE_S3_SSL=\"\" export AISERVICE_S3_PROVIDER=\"\" export AISERVICE_S3_PORT=\"\" export AISERVICE_S3_REGION=\"\" export AISERVICE_S3_TENANTS_BUCKET=\"\" export AISERVICE_S3_TEMPLATES_BUCKET=\"\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Tip To create addidional tenants we don't need to specify buckets","title":"Install AI Service"},{"location":"playbooks/aiservice/#install-ai-service","text":"Important These playbooks are samples to demonstrate how to use the roles in this collection. They are not intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install AI Service is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance.","title":"Install AI Service"},{"location":"playbooks/aiservice/#dependencies","text":"IBM Suite License Service installed on OCP cluster or external instance or details from external instance IBM Data Reporter Operator installed on OCP cluster or external instance or details from external instance Object Storage Minio (installed on the same cluster as aiservice) AWS S3 (Provide details to connect to AWS S3. Bucket names must be unique globally. Ensure to use a unique bucket prefix when using AWS S3)","title":"Dependencies"},{"location":"playbooks/aiservice/#overview","text":"This playbook will add AI Service v9.1.x to OCP cluster. This playbook can be ran against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: IBM Maximo Operator Catalog RedHat Certificate Manager MongoDb optional if SLS is not being installed on same cluster IBM Suite License Service (~10 Minutes) optional if customer chooses to use SLS from external cluster IBM Data Reporter Operator (~10 Minutes) optional if customer chooses to use DRO from external cluster IBM Db2 Minio (~5 minutes) optional if customer choose to use external object storage Install ODH: Install Red Hat OpenShift Serverless Operator Required when using serverless deployment mode for ODH Install Red Hat OpenShift Service Mesh Operator Required when using serverless deployment mode for ODH Install Authorino Operator Install Open Data Hub Operator Create DSCInitialization instance Create DataScienceCluster instance Install AI Service (using playbook): Install application (~20 Minutes) Configure AI Service (kmodels, tenant, etc) (~20 Minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/aiservice/#required-environment-variables","text":"AISERVICE_INSTANCE_ID Declare the instance ID for the AI service install MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry MAS_ENTITLEMENT_USERNAME Your IBM Entitlement user to access the IBM Container Registry AISERVICE_CHANNEL Aiservice application channel AISERVICE_S3_ACCESSKEY Access Key for object storage provider AISERVICE_S3_SECRETKEY Secret key for object storage provider AISERVICE_S3_HOST Your object storage provider host AISERVICE_S3_REGION Your object storage provider region - only when use AWS S3 instance AISERVICE_S3_SSL Is your object storage SSL enabled? (true/false) AISERVICE_S3_TENANTS_BUCKET Bucket name to store tenants info AISERVICE_S3_TEMPLATES_BUCKET Bucket name to store templates info AISERVICE_S3_BUCKET_PREFIX Bucket prefix configured with object storage provider AISERVICE_WATSONXAI_APIKEY You WatsonX AI api key AISERVICE_WATSONXAI_URL You WatsonX AI url AISERVICE_WATSONXAI_PROJECT_ID You WatsonX projedt Id Tip AI service supports AWS and Minio storage providers.","title":"Required environment variables"},{"location":"playbooks/aiservice/#required-environment-variables-saas","text":"AISERVICE_SAAS specify if saas deployment (default value is: false) MAS_CONFIG_DIR specify config location, mandatory when AISERVICE_SAAS=true AISERVICE_DOMAIN specify cluster domain, mandatory when AISERVICE_SAAS=true AISERVICE_SLS_URL specify SLS url, mandatory when AISERVICE_SAAS=true AISERVICE_SLS_REGISTRATION_KEY specify sls registration key, mandatory when AISERVICE_SAAS=true , to get value: look in ibm-sls namespace, pod sls-api-licensing-xxx and in Environment tab check REGISTRATION_KEY value AISERVICE_DRO_URL specify DRO url, mandatory when AISERVICE_SAAS=true AISERVICE_DRO_TOKEN specify DRO token, mandatory when AISERVICE_SAAS=true to get value: go to mas-{{ instance_id }}-core and look in secret dro-apikey DB2_INSTANCE_NAME specify DB2 instance name (default value is: aiservice), mandatory when AISERVICE_SAAS=true IBM_ENTITLEMENT_KEY specify IBM Entitlement key, mandatory when AISERVICE_SAAS=true","title":"Required environment variables (SaaS)"},{"location":"playbooks/aiservice/#optional-environment-variables","text":"MAS_ICR_CP Provide custom registry for AI service applications MAS_ICR_CPOPEN Provide custom registry for AI service operator MAS_CATALOG_VERSION Your custom AI service catalog version ARTIFACTORY_USERNAME Your artifactory user name to access - this is needed if user deploy from custom registry for example docker-na-public.artifactory.swg-devops.com ARTIFACTORY_TOKEN Your artifactory token for user to access - this is needed if user deploy from custom registry for example docker-na-public.artifactory.swg-devops.com AISERVICE_TENANT_ACTION Whether to install or remove tenant (default value is: install) AISERVICE_APIKEY_ACTION Whether to install or remove or update apikey (default value is: install) AISERVICE_WATSONX_ACTION Whether to install or remove watsonx secret (default value is: install) AISERVICE_S3_ACTION Whether to install or remove s3 (default value is: install) INSTALL_DB2 Whether to install DB2 (default value is: false) INSTALL_MINIO Whether to install minio (default value is: false) INSTALL_SLS Whether to install IBM Suite License Service (default value is: false) INSTALL_DRO Whether to install IBM Data Reporter Operator (default value is: false) AISERVICE_DB2_USERNAME The username to use for authentication with the database AISERVICE_DB2_PASSWORD The password to use for authentication with the database AISERVICE_DB2_JDBC_URL The JDBC URL specifying the host and port of the database, typically in the format jdbc:db2://host:port/ AISERVICE_DB2_SSL_ENABLED A flag indicating whether to enable SSL encryption for the database connection (default value is: true) USE_AWS_DB2 A flag indicating whether to use an AWS-hosted DB2 instance (default value is: false) AISERVICE_DOMAIN Provide custom domain (default value is: empty) AISERVICE_WATSONXAI_CA_CRT provide WatsonX AI CA certificate AISERVICE_WATSONXAI_FULL optional on prem to define if WatsonX AI engine is full or light (true/false) AISERVICE_WATSONXAI_DEPLOYMENT_ID optional on prem define deployment Id AISERVICE_WATSONXAI_SPACE_ID optional on prem define space Id AISERVICE_WATSONXAI_INSTANCE_ID optional on prem define instance id (default: openshift) AISERVICE_WATSONXAI_USERNAME optional on prem define user name AISERVICE_WATSONXAI_VERSION optional on prem define version of CPD AISERVICE_CERTIFICATE_ISSUER Optional to specify pre-configured certificate issuer to use for AI Service public certificates AISERVICE_CERTIFICATE_DURATION Optional to specify expiration duration for public certificates (default: 8760h0m0s) AISERVICE_CERTIFICATE_RENEW_BEFORE Optional to specify when to renew public certificates before they expire (default: 72h0m0s)","title":"Optional environment variables"},{"location":"playbooks/aiservice/#usage","text":"","title":"Usage"},{"location":"playbooks/aiservice/#ai-service-deployment-steps","text":"Tip For S3 manage please make sure you have deployed dependencies Install boto3 python module (use python environment): python3 -m venv /tmp/venv source /tmp/venv/bin/activate python3 -m pip install boto3 Run playbooks for deploy AI service: AISERVICE_SLS_REGISTRATION_KEY - value can be found in ibm-sls namespace, in pod sls-api-licensing-85699fb57-9lmrq please look in environments tab, then value REGISTRATION_KEY AISERVICE_DRO_TOKEN - go to mas-instance_id-core namespace and in secrets find dro-apikey In AWS for AISERVICE_S3_TENANTS_BUCKET , AISERVICE_S3_TEMPLATES_BUCKET user need to create S3 buckets with unique name export ARTIFACTORY_USERNAME=\"\" export ARTIFACTORY_TOKEN=\"\" export MAS_ICR_CP=\"\" export MAS_ICR_CPOPEN=\"\" export MAS_ENTITLEMENT_USERNAME=\"\" export MAS_ENTITLEMENT_KEY=\"\" export MAS_INSTANCE_ID=\"\" export MAS_APP_CHANNEL=\"\" export MAS_CATALOG_VERSION=\"\" export IBM_ENTITLEMENT_KEY=${MAS_ENTITLEMENT_KEY} export MAS_CONFIG_DIR=\"\" export DRO_CONTACT_EMAIL=\"\" export DRO_CONTACT_FIRSTNAME=\"\" export DRO_CONTACT_LASTNAME=\"\" export SLS_MONGODB_CFG_FILE=${MAS_CONFIG_DIR}/mongo-mongoce.yml export SLS_LICENSE_ID=\"\" export SLS_LICENSE_FILE=\"\" export INSTALL_DB2=\"\" export INSTALL_MINIO=\"\" export INSTALL_MONGO=\"\" export INSTALL_SLS=\"\" export INSTALL_DRO=\"\" export AISERVICE_S3_BUCKET_PREFIX=\"\" export AISERVICE_S3_REGION=\"\" export AISERVICE_S3_ENDPOINT_URL=\"\" export AISERVICE_TENANT_S3_REGION=\"\" export AISERVICE_TENANT_S3_ENDPOINT_URL=\"\" export AISERVICE_TENANT_S3_BUCKET_PREFIX=\"\" export AISERVICE_TENANT_S3_ACCESS_KEY=\"\" export AISERVICE_TENANT_S3_SECRET_KEY=\"\" export RSL_URL=\"\" export RSL_ORG_ID=\"\" export RSL_TOKEN=\"\" export MINIO_ROOT_PASSWORD=\"\" export AISERVICE_S3_ACCESSKEY=\"\" export AISERVICE_S3_SECRETKEY=${MINIO_ROOT_PASSWORD} export AISERVICE_S3_HOST=\"\" export AISERVICE_S3_SSL=\"\" export AISERVICE_S3_PROVIDER=\"\" export AISERVICE_S3_PORT=\"\" export AISERVICE_S3_REGION=\"\" export AISERVICE_S3_TENANTS_BUCKET=\"\" export AISERVICE_S3_TEMPLATES_BUCKET=\"\" export AISERVICE_WATSONXAI_APIKEY=\"\" export AISERVICE_WATSONXAI_URL=\"\" export AISERVICE_WATSONXAI_PROJECT_ID=\"\" export AISERVICE_SUBSCRIPTION_ID=\"\" export AISERVICE_DRO_TENANT_ID=\"\" export AISERVICE_TENANT_ENTITLEMENT_START_DATE=\"YYYY-MM-DD\" export AISERVICE_TENANT_ENTITLEMENT_END_DATE=\"YYYY-MM-DD\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/aiservice.yml","title":"AI service deployment steps"},{"location":"playbooks/aiservice/#create-s3","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_S3_ACCESSKEY=\"<storage provider access key>\" export AISERVICE_S3_SECRETKEY=\"<storage provider secret key>\" export AISERVICE_S3_HOST=\"<storage provider host>\" export AISERVICE_S3_REGION=\"<storage provider region>\" export AISERVICE_S3_ACTION=\"install\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Create S3"},{"location":"playbooks/aiservice/#delete-s3","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_S3_ACCESSKEY=\"<storage provider access key>\" export AISERVICE_S3_SECRETKEY=\"<storage provider secret key>\" export AISERVICE_S3_HOST=\"<storage provider host>\" export AISERVICE_S3_REGION=\"<storage provider region>\" export AISERVICE_S3_ACTION=\"remove\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Delete S3"},{"location":"playbooks/aiservice/#create-api-key","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_APIKEY_ACTION=\"install\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Create API Key"},{"location":"playbooks/aiservice/#delete-api-key","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_APIKEY_ACTION=\"remove\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Delete API Key"},{"location":"playbooks/aiservice/#create-watsonx-api-key","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_WATSONX_ACTION=\"install\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Create WatsonX API Key"},{"location":"playbooks/aiservice/#delete-watsonx-api-key","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_WATSONX_ACTION=\"remove\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Delete WatsonX API Key"},{"location":"playbooks/aiservice/#create-tenant","text":"The AISERVICE_SLS_REGISTRATION_KEY value can be found in ibm-sls namespace, in pod sls-api-licensing-85699fb57-9lmrq please look in environments tab, then value REGISTRATION_KEY . To obtain the AISERVICE_DRO_TOKEN go to mas-instance_id-core namespace and in secrets find dro-apikey export AISERVICE_TENANT_NAME=\"user7\" export AISERVICE_SLS_SUBSCRIPTION_ID=\"007\" export TENANT_ACTION=\"install\" export ROLE_NAME=\"aiservice_tenant\" export AISERVICE_SAAS=\"true\" export AISERVICE_DOMAIN=\"\" export AISERVICE_SLS_URL=\"https://sls.ibm-sls.ibm-sls.\"${AISERVICE_DOMAIN} export AISERVICE_SLS_REGISTRATION_KEY=\"\" export AISERVICE_DRO_URL=\"https://ibm-data-reporter-redhat-marketplace.\"${AISERVICE_DOMAIN} export AISERVICE_DRO_TOKEN=\"\" export AISERVICE_SLS_CACERT=\"\" export AISERVICE_DRO_CACERT=\"\" export AISERVICE_WATSONXAI_APIKEY=\"\" export AISERVICE_WATSONXAI_URL=\"\" export AISERVICE_WATSONXAI_PROJECT_ID=\"\" export AISERVICE_S3_ACCESSKEY=\"\" export AISERVICE_S3_SECRETKEY=\"\" export AISERVICE_S3_HOST=\"\" export AISERVICE_S3_SSL=\"\" export AISERVICE_S3_PROVIDER=\"\" export AISERVICE_S3_PORT=\"\" export AISERVICE_S3_REGION=\"\" export AISERVICE_S3_TENANTS_BUCKET=\"\" export AISERVICE_S3_TEMPLATES_BUCKET=\"\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Tip To create addidional tenants we don't need to specify buckets","title":"Create Tenant"},{"location":"playbooks/backup-restore/","text":"Backup and Restore \u00a4 Overview \u00a4 MAS Devops Collection includes playbooks for backing up and restoring of the following MAS components and their dependencies: MongoDB Db2 MAS Core Manage IoT Monitor Health Optimizer Visual Inspection Creation of both full and incremental backups are supported. The backup and restore Ansible roles can also be used individually, allowing you to build your own customized backup and restore playbook covering exactly what you need. For example, you can only backup/restore Manage attachments . Important The backup and restore playbooks in this collection are still work in progress, they are not suitable for production use at this time. You may track development progress using the Backup & Restore label in the Github repository. Production-ready backup and restore options are detailed in the Backup and restore topic in the product documentation. Configuration - Storage \u00a4 You can save the backup files to a folder on your local file system by setting the following environment variables: Envrionment variable Required (Default Value) Description MASBR_STORAGE_LOCAL_FOLDER Yes The local path to save the backup files MASBR_LOCAL_TEMP_FOLDER No ( /tmp/masbr ) Local folder for saving the temporary backup/restore data, the data in this folder will be deleted after the backup/restore job completed. Configuration - Backup \u00a4 Envrionment variable Required (Default Value) Description MASBR_ACTION Yes Whether to run the playbook to perform a backup or a restore MASBR_BACKUP_TYPE No ( full ) Set full or incr to indicate the playbook to create a full backup or incremental backup. MASBR_BACKUP_FROM_VERSION No Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). The playbooks are switched to backup mode by setting MASBR_ACTION to backup . Full Backups \u00a4 If you set environment variable MASBR_BACKUP_TYPE=full or do not specify a value for this variable, the playbook will take a full backup. Incremental Backups \u00a4 You can set environment variable MASBR_BACKUP_TYPE=incr to indicate the playbook to take an incremental backup. Important Only supports creating incremental backup for MonogDB, Db2 and persistent volume data. The playbook will always create a full backup for other type of data regardless of whether this variable be set to incr . The environment variable MASBR_BACKUP_FROM_VERSION is only valid if MASBR_BACKUP_TYPE=incr . It indicates which backup version that the incremental backup to based on. If you do not set a value for this variable, the playbook will try to find the latest Completed Full backup from the specified storage location, and then take an incremental backup based on it. Important The backup files you specified by MASBR_BACKUP_FROM_VERSION must be a Full backup. And the component name and data types in the specified Full backup file must be same as the current incremental backup job. Configuration - Restore \u00a4 Envrionment variable Required (Default Value) Description MASBR_ACTION Yes Whether to run the playbook to perform a backup or a restore MASBR_RESTORE_FROM_VERSION Yes Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) MASBR_RESTORE_OVERWRITE Yes Set whether the restore should overwrite any existing data or if we should stop and FAIL if there is data detected in the directory. WARNING: This will overwrite all data when restoring! The playbooks are switched to restore mode by setting MASBR_ACTION to restore . You must specify the MASBR_RESTORE_FROM_VERSION environment variable to indicate which version of the backup files to use. In the case of restoring from an incremental backup, the corresponding full backup will be restored first before continuing to restore the incremental backup. Backup/Restore for MongoDB \u00a4 This playbook ibm.mas_devops.br_mongodb will invoke the role mongodb to backup/restore the MongoDB databases. This playbook supports backing up and restoring databases for an in-cluster MongoDB CE instance. If you are using other MongoDB venders, such as IBM Cloud Databases for MongoDB, Amazon DocumentDB or MongoDB Altas Database, please refer to the corresponding vender's documentation for more information about their provided backup/restore service. Environment Variables \u00a4 MONGODB_NAMESPACE : By default the backup and restore processes will use a namespace of mongoce , if you have customized the install of MongoDb CE you must set this environment variable to the appropriate namespace you wish to backup from/restore to. MAS_INSTANCE_ID : Required . This playbook supports backup/restore MongoDB databases that belong to a specific MAS instance, call the playbook multiple times with different values for MAS_INSTANCE_ID if you wish to back up multiple MAS instances that use the same MongoDB CE instance. MAS_APP_ID : Optional . By default, this playbook will backup all databases belonging to the specified MAS instance. You can backup the databases only belong to a specific MAS application by setting this environment variable to a supported MAS application id core , manage , iot , monitor , health , optimizer or visualinspection . Examples \u00a4 # Full backup all MongoDB data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_mongodb # Incremental backup all MongoDB data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_mongodb # Restore all MongoDB data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_mongodb # Backup just the IoT MongoDB data for the dev2 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev2 export MAS_APP_ID=iot ansible-playbook ibm.mas_devops.br_mongodb Backup/Restore for Db2 \u00a4 This playbook ibm.mas_devops.br_db2 will invoke the role db2 to backup/restore a single Db2 instance. Environment Variables \u00a4 DB2_INSTANCE_NAME : Required This playbook only supports backing up specific Db2 instance at a time. If you want to backup all Db2 instances in the Db2 cluster, you need to run this playbook multiple times with different value of this environment variable. MAS_INSTANCE_ID : Required Set the instance ID for the MAS install. MASBR_ACTION : Required Set the action to be performed, backup or restore . MASBR_STORAGE_LOCAL_FOLDER : Required Set the local path to the directory to be used for backup and restore. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u Examples \u00a4 # Full backup for the db2w-shared Db2 instance export MAS_INSTANCE_ID=dev export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 # Incremental backup for the db2w-shared Db2 instance export MAS_INSTANCE_ID=dev export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 # Restore for the db2w-shared Db2 instance export MAS_INSTANCE_ID=dev export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 Backup/Restore for MAS Core \u00a4 This playbook ibm.mas_devops.br_core will backup the following components that MAS Core depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core core suite_backup_restore MAS Core namespace resources Environment Variables \u00a4 MAS_INSTANCE_ID Required . The MAS instance ID to perform a backup for. Examples \u00a4 # Full backup all core data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_core # Incremental backup all core data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_core # Restore all core data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_core Backup/Restore for Manage \u00a4 This playbook ibm.mas_devops.br_manage will backup the following components that Manage depends on in order: Component Role Data included mongodb mongodb MongoDB databases used by MAS Core db2 db2 Db2 instance used by Manage core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Optional . When defined, this playbook will backup the Db2 instance used by Manage. DB2 role is skipped when environment variable is not defined.. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u Examples \u00a4 # Full backup all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage # Incremental backup all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage # Restore all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage Backup/Restore for IoT \u00a4 This playbook ibm.mas_devops.br_iot will backup the following components that IoT depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and IoT db2 db2 Db2 instance used by IoT core suite_backup_restore MAS Core namespace resources iot suite_app_backup_restore IoT namespace resources Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by IoT, you need to set the correct Db2 instance name for this environment variable. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u Examples \u00a4 # Full backup all iot data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot # Incremental backup all iot data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot # Restore all iot data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot Backup/Restore for Monitor \u00a4 This playbook ibm.mas_devops.br_monitor will backup the following components that Monitor depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core, IoT and Monitor db2 db2 Db2 instance used by IoT and Monitor core suite_backup_restore MAS Core namespace resources iot suite_app_backup_restore IoT namespace resources monitor suite_app_backup_restore Monitor namespace resources Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by IoT and Monitor, you need to set the correct Db2 instance name for this environment variable. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u Examples \u00a4 # Full backup all monitor data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor # Incremental backup all monitor data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor # Restore all monitor data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor Backup/Restore for Health \u00a4 This playbook ibm.mas_devops.br_health will backup the following components that Health depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core db2 db2 Db2 instance used by Manage and Health core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments health suite_backup_restore Health namespace resources Watson Studio project assets Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by Manage and Health, you need to set the correct Db2 instance name for this environment variable. Examples \u00a4 # Full backup all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health # Incremental backup all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health # Restore all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health Backup/Restore for Optimizer \u00a4 This playbook ibm.mas_devops.br_optimizer will backup the following components that Optimizer depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and Optimizer db2 db2 Db2 instance used by Manage core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments optimizer suite_backup_restore Optimizer namespace resources Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by Manage, you need to set the correct Db2 instance name for this environment variable. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u Examples \u00a4 # Full backup all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer # Incremental backup all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer # Restore all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer Backup/Restore for Visual Inspection \u00a4 This playbook ibm.mas_devops.br_visualinspection will backup the following components that Visual Inspection depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and Visual Inspection core suite_backup_restore MAS Core namespace resources visualinspection suite_app_backup_restore Visual Inspection namespace resources Persistent volume data, such as images and models Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. Examples \u00a4 # Full backup all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection # Incremental backup all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection # Restore all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection Reference \u00a4 Directory Structure \u00a4 No matter what kind of storage systems you choose, the folder structure created in the storage system is same. Below is the sample folder structure for saving backup jobs: <root_folder>/backups/mongodb-main-full-20240621122530 \u251c\u2500\u2500 backup.yml \u251c\u2500\u2500 database \u2502 \u251c\u2500\u2500 mongodb-main-full-20240621122530.tar.gz \u2502 \u2514\u2500\u2500 query.json \u2514\u2500\u2500 log \u251c\u2500\u2500 mongodb-main-full-20240621122530-backup-log.tar.gz \u2514\u2500\u2500 mongodb-main-full-20240621122530-ansible-log.tar.gz <root_folder>/backups/core-main-full-20240621122530 \u251c\u2500\u2500 backup.yml \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 core-main-full-20240621122530-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 core-main-full-20240621122530-namespace-log.tar.gz \u2514\u2500\u2500 namespace \u2514\u2500\u2500 core-main-full-20240621122530-namespace.tar.gz <root_folder> : the root folder is specified by MASBR_STORAGE_LOCAL_FOLDER or MASBR_STORAGE_CLOUD_BUCKET The backup playbooks will create a seperated backup job folder under the backups folder for each component. The backup job folder is named by following this format: {BACKUP COMPONENT}-{INSTANCE ID}-{BACKUP TYPE}-{BACKUP VERSION} . When using playbook to backup multiple components at once, all backup job folders will be assigned to the same backup version. In above example, the same backup version 20240621122530 for backing up mongodb and core components. backup.yml : keep the backup job information database : data type for database. This folder save the backup files of MongoDB database, Db2 database. namespace : data type for namespace resources. This folder save the exported namespace resources. pv : data type for persistent volume. This folder save the persistent volume data, e.g. the Manage attachments, VI images and models. log : this folder save all job running log files In addition to the backup jobs, we also save restore jobs in the specified storage location. For example: <root_folder>/restores/mongodb-main-incr-20240622040201-20240622075501 \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 mongodb-main-incr-20240622040201-20240622075501-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 mongodb-main-incr-20240622040201-20240622075501-restore-log.tar.gz \u2514\u2500\u2500 restore.yml <root_folder>/restores/core-main-incr-20240622040201-20240622075501 \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 core-main-incr-20240622040201-20240622075501-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 core-main-incr-20240622040201-20240622075501-namespace-log.tar.gz \u2514\u2500\u2500 restore.yml The restore playbooks will create a seperated restore job folder under the restores folder for each component. The restore job folder is named by following this format: {BACKUP JOB NAME}-{RESTORE VERSION} . restore.yml : keep the restore job information log : this folder save all job running log files Data Model \u00a4 backup.yml \u00a4 kind: Backup name: \"core-main-incr-20240622040201\" version: \"20240622040201\" type: \"incr\" from: \"core-main-full-20240621122530\" source: domain: \"source-cluster.mydomain.com\" suite: \"8.11.11\" instance: \"main\" workspace: \"\" component: name: \"core\" instance: \"main\" namespace: \"mas-main-core\" data: - seq: \"1\" type: \"namespace\" phase: \"Completed\" status: phase: \"Completed\" startTimestamp: \"2024-06-22T04:05:22\" completionTimestamp: \"2024-06-22T04:06:04\" sentNotifications: - type: \"Slack\" channel: \"#ansible-slack-dev\" timestamp: \"2024-06-22T04:05:34\" phase: \"InProgress\" - type: \"Slack\" channel: \"#ansible-slack-dev\" timestamp: \"2024-06-22T04:06:10\" phase: \"Completed\" restore.yml \u00a4 kind: Restore name: \"core-main-incr-20240622040201-20240622075501\" version: \"20240622075501\" from: \"core-main-incr-20240622040201\" target: domain: \"target-cluster.mydomain.com\" component: name: \"core\" instance: \"main\" namespace: \"mas-main-core\" data: - seq: 1 type: \"namespace\" phase: \"Completed\" status: phase: \"Completed\" startTimestamp: \"2024-06-22T08:04:19\" completionTimestamp: \"2024-06-22T08:04:33\"","title":"Backup & Restore"},{"location":"playbooks/backup-restore/#backup-and-restore","text":"","title":"Backup and Restore"},{"location":"playbooks/backup-restore/#overview","text":"MAS Devops Collection includes playbooks for backing up and restoring of the following MAS components and their dependencies: MongoDB Db2 MAS Core Manage IoT Monitor Health Optimizer Visual Inspection Creation of both full and incremental backups are supported. The backup and restore Ansible roles can also be used individually, allowing you to build your own customized backup and restore playbook covering exactly what you need. For example, you can only backup/restore Manage attachments . Important The backup and restore playbooks in this collection are still work in progress, they are not suitable for production use at this time. You may track development progress using the Backup & Restore label in the Github repository. Production-ready backup and restore options are detailed in the Backup and restore topic in the product documentation.","title":"Overview"},{"location":"playbooks/backup-restore/#configuration-storage","text":"You can save the backup files to a folder on your local file system by setting the following environment variables: Envrionment variable Required (Default Value) Description MASBR_STORAGE_LOCAL_FOLDER Yes The local path to save the backup files MASBR_LOCAL_TEMP_FOLDER No ( /tmp/masbr ) Local folder for saving the temporary backup/restore data, the data in this folder will be deleted after the backup/restore job completed.","title":"Configuration - Storage"},{"location":"playbooks/backup-restore/#configuration-backup","text":"Envrionment variable Required (Default Value) Description MASBR_ACTION Yes Whether to run the playbook to perform a backup or a restore MASBR_BACKUP_TYPE No ( full ) Set full or incr to indicate the playbook to create a full backup or incremental backup. MASBR_BACKUP_FROM_VERSION No Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). The playbooks are switched to backup mode by setting MASBR_ACTION to backup .","title":"Configuration - Backup"},{"location":"playbooks/backup-restore/#full-backups","text":"If you set environment variable MASBR_BACKUP_TYPE=full or do not specify a value for this variable, the playbook will take a full backup.","title":"Full Backups"},{"location":"playbooks/backup-restore/#incremental-backups","text":"You can set environment variable MASBR_BACKUP_TYPE=incr to indicate the playbook to take an incremental backup. Important Only supports creating incremental backup for MonogDB, Db2 and persistent volume data. The playbook will always create a full backup for other type of data regardless of whether this variable be set to incr . The environment variable MASBR_BACKUP_FROM_VERSION is only valid if MASBR_BACKUP_TYPE=incr . It indicates which backup version that the incremental backup to based on. If you do not set a value for this variable, the playbook will try to find the latest Completed Full backup from the specified storage location, and then take an incremental backup based on it. Important The backup files you specified by MASBR_BACKUP_FROM_VERSION must be a Full backup. And the component name and data types in the specified Full backup file must be same as the current incremental backup job.","title":"Incremental Backups"},{"location":"playbooks/backup-restore/#configuration-restore","text":"Envrionment variable Required (Default Value) Description MASBR_ACTION Yes Whether to run the playbook to perform a backup or a restore MASBR_RESTORE_FROM_VERSION Yes Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) MASBR_RESTORE_OVERWRITE Yes Set whether the restore should overwrite any existing data or if we should stop and FAIL if there is data detected in the directory. WARNING: This will overwrite all data when restoring! The playbooks are switched to restore mode by setting MASBR_ACTION to restore . You must specify the MASBR_RESTORE_FROM_VERSION environment variable to indicate which version of the backup files to use. In the case of restoring from an incremental backup, the corresponding full backup will be restored first before continuing to restore the incremental backup.","title":"Configuration - Restore"},{"location":"playbooks/backup-restore/#backuprestore-for-mongodb","text":"This playbook ibm.mas_devops.br_mongodb will invoke the role mongodb to backup/restore the MongoDB databases. This playbook supports backing up and restoring databases for an in-cluster MongoDB CE instance. If you are using other MongoDB venders, such as IBM Cloud Databases for MongoDB, Amazon DocumentDB or MongoDB Altas Database, please refer to the corresponding vender's documentation for more information about their provided backup/restore service.","title":"Backup/Restore for MongoDB"},{"location":"playbooks/backup-restore/#environment-variables","text":"MONGODB_NAMESPACE : By default the backup and restore processes will use a namespace of mongoce , if you have customized the install of MongoDb CE you must set this environment variable to the appropriate namespace you wish to backup from/restore to. MAS_INSTANCE_ID : Required . This playbook supports backup/restore MongoDB databases that belong to a specific MAS instance, call the playbook multiple times with different values for MAS_INSTANCE_ID if you wish to back up multiple MAS instances that use the same MongoDB CE instance. MAS_APP_ID : Optional . By default, this playbook will backup all databases belonging to the specified MAS instance. You can backup the databases only belong to a specific MAS application by setting this environment variable to a supported MAS application id core , manage , iot , monitor , health , optimizer or visualinspection .","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples","text":"# Full backup all MongoDB data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_mongodb # Incremental backup all MongoDB data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_mongodb # Restore all MongoDB data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_mongodb # Backup just the IoT MongoDB data for the dev2 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev2 export MAS_APP_ID=iot ansible-playbook ibm.mas_devops.br_mongodb","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-db2","text":"This playbook ibm.mas_devops.br_db2 will invoke the role db2 to backup/restore a single Db2 instance.","title":"Backup/Restore for Db2"},{"location":"playbooks/backup-restore/#environment-variables_1","text":"DB2_INSTANCE_NAME : Required This playbook only supports backing up specific Db2 instance at a time. If you want to backup all Db2 instances in the Db2 cluster, you need to run this playbook multiple times with different value of this environment variable. MAS_INSTANCE_ID : Required Set the instance ID for the MAS install. MASBR_ACTION : Required Set the action to be performed, backup or restore . MASBR_STORAGE_LOCAL_FOLDER : Required Set the local path to the directory to be used for backup and restore. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_1","text":"# Full backup for the db2w-shared Db2 instance export MAS_INSTANCE_ID=dev export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 # Incremental backup for the db2w-shared Db2 instance export MAS_INSTANCE_ID=dev export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 # Restore for the db2w-shared Db2 instance export MAS_INSTANCE_ID=dev export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-mas-core","text":"This playbook ibm.mas_devops.br_core will backup the following components that MAS Core depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core core suite_backup_restore MAS Core namespace resources","title":"Backup/Restore for MAS Core"},{"location":"playbooks/backup-restore/#environment-variables_2","text":"MAS_INSTANCE_ID Required . The MAS instance ID to perform a backup for.","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_2","text":"# Full backup all core data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_core # Incremental backup all core data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_core # Restore all core data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_core","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-manage","text":"This playbook ibm.mas_devops.br_manage will backup the following components that Manage depends on in order: Component Role Data included mongodb mongodb MongoDB databases used by MAS Core db2 db2 Db2 instance used by Manage core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments","title":"Backup/Restore for Manage"},{"location":"playbooks/backup-restore/#environment-variables_3","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Optional . When defined, this playbook will backup the Db2 instance used by Manage. DB2 role is skipped when environment variable is not defined.. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_3","text":"# Full backup all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage # Incremental backup all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage # Restore all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-iot","text":"This playbook ibm.mas_devops.br_iot will backup the following components that IoT depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and IoT db2 db2 Db2 instance used by IoT core suite_backup_restore MAS Core namespace resources iot suite_app_backup_restore IoT namespace resources","title":"Backup/Restore for IoT"},{"location":"playbooks/backup-restore/#environment-variables_4","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by IoT, you need to set the correct Db2 instance name for this environment variable. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_4","text":"# Full backup all iot data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot # Incremental backup all iot data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot # Restore all iot data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-monitor","text":"This playbook ibm.mas_devops.br_monitor will backup the following components that Monitor depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core, IoT and Monitor db2 db2 Db2 instance used by IoT and Monitor core suite_backup_restore MAS Core namespace resources iot suite_app_backup_restore IoT namespace resources monitor suite_app_backup_restore Monitor namespace resources","title":"Backup/Restore for Monitor"},{"location":"playbooks/backup-restore/#environment-variables_5","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by IoT and Monitor, you need to set the correct Db2 instance name for this environment variable. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_5","text":"# Full backup all monitor data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor # Incremental backup all monitor data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor # Restore all monitor data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-health","text":"This playbook ibm.mas_devops.br_health will backup the following components that Health depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core db2 db2 Db2 instance used by Manage and Health core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments health suite_backup_restore Health namespace resources Watson Studio project assets","title":"Backup/Restore for Health"},{"location":"playbooks/backup-restore/#environment-variables_6","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by Manage and Health, you need to set the correct Db2 instance name for this environment variable.","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_6","text":"# Full backup all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health # Incremental backup all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health # Restore all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-optimizer","text":"This playbook ibm.mas_devops.br_optimizer will backup the following components that Optimizer depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and Optimizer db2 db2 Db2 instance used by Manage core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments optimizer suite_backup_restore Optimizer namespace resources","title":"Backup/Restore for Optimizer"},{"location":"playbooks/backup-restore/#environment-variables_7","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by Manage, you need to set the correct Db2 instance name for this environment variable. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_7","text":"# Full backup all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer # Incremental backup all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer # Restore all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-visual-inspection","text":"This playbook ibm.mas_devops.br_visualinspection will backup the following components that Visual Inspection depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and Visual Inspection core suite_backup_restore MAS Core namespace resources visualinspection suite_app_backup_restore Visual Inspection namespace resources Persistent volume data, such as images and models","title":"Backup/Restore for Visual Inspection"},{"location":"playbooks/backup-restore/#environment-variables_8","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable.","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_8","text":"# Full backup all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection # Incremental backup all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection # Restore all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection","title":"Examples"},{"location":"playbooks/backup-restore/#reference","text":"","title":"Reference"},{"location":"playbooks/backup-restore/#directory-structure","text":"No matter what kind of storage systems you choose, the folder structure created in the storage system is same. Below is the sample folder structure for saving backup jobs: <root_folder>/backups/mongodb-main-full-20240621122530 \u251c\u2500\u2500 backup.yml \u251c\u2500\u2500 database \u2502 \u251c\u2500\u2500 mongodb-main-full-20240621122530.tar.gz \u2502 \u2514\u2500\u2500 query.json \u2514\u2500\u2500 log \u251c\u2500\u2500 mongodb-main-full-20240621122530-backup-log.tar.gz \u2514\u2500\u2500 mongodb-main-full-20240621122530-ansible-log.tar.gz <root_folder>/backups/core-main-full-20240621122530 \u251c\u2500\u2500 backup.yml \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 core-main-full-20240621122530-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 core-main-full-20240621122530-namespace-log.tar.gz \u2514\u2500\u2500 namespace \u2514\u2500\u2500 core-main-full-20240621122530-namespace.tar.gz <root_folder> : the root folder is specified by MASBR_STORAGE_LOCAL_FOLDER or MASBR_STORAGE_CLOUD_BUCKET The backup playbooks will create a seperated backup job folder under the backups folder for each component. The backup job folder is named by following this format: {BACKUP COMPONENT}-{INSTANCE ID}-{BACKUP TYPE}-{BACKUP VERSION} . When using playbook to backup multiple components at once, all backup job folders will be assigned to the same backup version. In above example, the same backup version 20240621122530 for backing up mongodb and core components. backup.yml : keep the backup job information database : data type for database. This folder save the backup files of MongoDB database, Db2 database. namespace : data type for namespace resources. This folder save the exported namespace resources. pv : data type for persistent volume. This folder save the persistent volume data, e.g. the Manage attachments, VI images and models. log : this folder save all job running log files In addition to the backup jobs, we also save restore jobs in the specified storage location. For example: <root_folder>/restores/mongodb-main-incr-20240622040201-20240622075501 \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 mongodb-main-incr-20240622040201-20240622075501-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 mongodb-main-incr-20240622040201-20240622075501-restore-log.tar.gz \u2514\u2500\u2500 restore.yml <root_folder>/restores/core-main-incr-20240622040201-20240622075501 \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 core-main-incr-20240622040201-20240622075501-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 core-main-incr-20240622040201-20240622075501-namespace-log.tar.gz \u2514\u2500\u2500 restore.yml The restore playbooks will create a seperated restore job folder under the restores folder for each component. The restore job folder is named by following this format: {BACKUP JOB NAME}-{RESTORE VERSION} . restore.yml : keep the restore job information log : this folder save all job running log files","title":"Directory Structure"},{"location":"playbooks/backup-restore/#data-model","text":"","title":"Data Model"},{"location":"playbooks/backup-restore/#backupyml","text":"kind: Backup name: \"core-main-incr-20240622040201\" version: \"20240622040201\" type: \"incr\" from: \"core-main-full-20240621122530\" source: domain: \"source-cluster.mydomain.com\" suite: \"8.11.11\" instance: \"main\" workspace: \"\" component: name: \"core\" instance: \"main\" namespace: \"mas-main-core\" data: - seq: \"1\" type: \"namespace\" phase: \"Completed\" status: phase: \"Completed\" startTimestamp: \"2024-06-22T04:05:22\" completionTimestamp: \"2024-06-22T04:06:04\" sentNotifications: - type: \"Slack\" channel: \"#ansible-slack-dev\" timestamp: \"2024-06-22T04:05:34\" phase: \"InProgress\" - type: \"Slack\" channel: \"#ansible-slack-dev\" timestamp: \"2024-06-22T04:06:10\" phase: \"Completed\"","title":"backup.yml"},{"location":"playbooks/backup-restore/#restoreyml","text":"kind: Restore name: \"core-main-incr-20240622040201-20240622075501\" version: \"20240622075501\" from: \"core-main-incr-20240622040201\" target: domain: \"target-cluster.mydomain.com\" component: name: \"core\" instance: \"main\" namespace: \"mas-main-core\" data: - seq: 1 type: \"namespace\" phase: \"Completed\" status: phase: \"Completed\" startTimestamp: \"2024-06-22T08:04:19\" completionTimestamp: \"2024-06-22T08:04:33\"","title":"restore.yml"},{"location":"playbooks/cp4d/","text":"Install Cloud Pak for Data \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift v4.10 cluster with IBM Maximo Application Suite Core v8.10 already installed, the mas_install_core playbook can be used to set this up. Overview \u00a4 This playbook will add Cloud Pak for Data 4.x to an existing cluster installation. It could also install additional CP4D services. This playbook can be run against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install CP4D ControlPlane (~1 hour) Install CP4D Services (~30 Minutes - 1 hour for each service) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \u00a4 MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) CPD_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry CPD_ENTITLEMENT_USERNAME Your user name to access the IBM Container Registr CPD_INSTALL_PLATFORM True/False - If you HAVE CP4D already installed in your cluster, then set it to \"false\" CPD_PRODUCT_VERSION (Required) Cloud Pak for Data version installed in the cluster in 4.X format. These variables are required only if you set CP4D_INSTALL_WSL to false in optional varibles, otherwise don't set it. \u00a4 CPD_ADMIN_USERNAME CP4D Username CPD_ADMIN_PASSWORD CP4D Password CPD_ADMIN_URL CP4D Base URL Optional environment variables \u00a4 CPD_INSTALL_COGNOS True/False - Set to true to install Cognos Analytics CPD_INSTALL_WSL True/False - Set to true to install Watson Studio CPD_INSTALL_WML True/False - Set to true to install Watson Machine Learning CPD_INSTALL_SPARK True/False - Set to true to install Analytics Engine \"Spark\" CPD_INSTALL_OPENSCALE True/False - Set to true to install AI Openscale CPD_INSTALL_DISCOVERY True/False - Set to true to install Watson Discovery CPD_INSTALL_SPSS True/False - Set to true to install SPSS Modeler Usage when you already HAVE CP4D installed \u00a4 export MAS_CONFIG_DIR=~/masconfig export CPD_INSTALL_COGNOS=\"true\" export CPD_ENTITLEMENT_KEY=xxx export CPD_ENTITLEMENT_USERNAME=xxx export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_URL=\"https://mycp4durl\" export CPD_PRODUCT_VERSION=\"4.6.6\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.cp4d Usage when you DON'T HAVE CP4D installed \u00a4 export MAS_CONFIG_DIR=~/masconfig export CPD_ENTITLEMENT_KEY=xxx export CPD_ENTITLEMENT_USERNAME=xxx export CPD_INSTALL_PLATFORM=\"true\" export CPD_PRODUCT_VERSION=\"4.6.6\" ## To install additional CP4D services, add one or many of these environment variables: export CPD_INSTALL_COGNOS=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_INSTALL_WML=\"true\" export CPD_INSTALL_SPARK=\"true\" export CPD_INSTALL_OPENSCALE=\"true\" export CPD_INSTALL_DISCOVERY=\"true\" export CPD_INSTALL_SPSS=\"true\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.cp4d Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Install Cloud Pak For Data"},{"location":"playbooks/cp4d/#install-cloud-pak-for-data","text":"","title":"Install Cloud Pak for Data"},{"location":"playbooks/cp4d/#prerequisites","text":"You will need a RedHat OpenShift v4.10 cluster with IBM Maximo Application Suite Core v8.10 already installed, the mas_install_core playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/cp4d/#overview","text":"This playbook will add Cloud Pak for Data 4.x to an existing cluster installation. It could also install additional CP4D services. This playbook can be run against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install CP4D ControlPlane (~1 hour) Install CP4D Services (~30 Minutes - 1 hour for each service) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/cp4d/#required-environment-variables","text":"MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) CPD_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry CPD_ENTITLEMENT_USERNAME Your user name to access the IBM Container Registr CPD_INSTALL_PLATFORM True/False - If you HAVE CP4D already installed in your cluster, then set it to \"false\" CPD_PRODUCT_VERSION (Required) Cloud Pak for Data version installed in the cluster in 4.X format.","title":"Required environment variables"},{"location":"playbooks/cp4d/#these-variables-are-required-only-if-you-set-cp4d_install_wsl-to-false-in-optional-varibles-otherwise-dont-set-it","text":"CPD_ADMIN_USERNAME CP4D Username CPD_ADMIN_PASSWORD CP4D Password CPD_ADMIN_URL CP4D Base URL","title":"These variables are required only if you set CP4D_INSTALL_WSL to false in optional varibles, otherwise don't set it."},{"location":"playbooks/cp4d/#optional-environment-variables","text":"CPD_INSTALL_COGNOS True/False - Set to true to install Cognos Analytics CPD_INSTALL_WSL True/False - Set to true to install Watson Studio CPD_INSTALL_WML True/False - Set to true to install Watson Machine Learning CPD_INSTALL_SPARK True/False - Set to true to install Analytics Engine \"Spark\" CPD_INSTALL_OPENSCALE True/False - Set to true to install AI Openscale CPD_INSTALL_DISCOVERY True/False - Set to true to install Watson Discovery CPD_INSTALL_SPSS True/False - Set to true to install SPSS Modeler","title":"Optional environment variables"},{"location":"playbooks/cp4d/#usage-when-you-already-have-cp4d-installed","text":"export MAS_CONFIG_DIR=~/masconfig export CPD_INSTALL_COGNOS=\"true\" export CPD_ENTITLEMENT_KEY=xxx export CPD_ENTITLEMENT_USERNAME=xxx export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_URL=\"https://mycp4durl\" export CPD_PRODUCT_VERSION=\"4.6.6\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.cp4d","title":"Usage when you already HAVE CP4D installed"},{"location":"playbooks/cp4d/#usage-when-you-dont-have-cp4d-installed","text":"export MAS_CONFIG_DIR=~/masconfig export CPD_ENTITLEMENT_KEY=xxx export CPD_ENTITLEMENT_USERNAME=xxx export CPD_INSTALL_PLATFORM=\"true\" export CPD_PRODUCT_VERSION=\"4.6.6\" ## To install additional CP4D services, add one or many of these environment variables: export CPD_INSTALL_COGNOS=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_INSTALL_WML=\"true\" export CPD_INSTALL_SPARK=\"true\" export CPD_INSTALL_OPENSCALE=\"true\" export CPD_INSTALL_DISCOVERY=\"true\" export CPD_INSTALL_SPSS=\"true\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.cp4d Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage when you DON'T HAVE CP4D installed"},{"location":"playbooks/mas-core/","text":"OneClick Install for MAS Core \u00a4 Important These playbooks are samples to demonstrate how to use the roles in this collection. They are not intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install MAS is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance. Overview \u00a4 This playbook will install and configure IBM Maximo Application Suite Core along with all necessary dependencies. This can be ran against any OCP cluster regardless of its type, whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. It will take approximately 90 minutes to set up MAS core services and all of its dependencies, at the end of the process you will be able to login to the MAS admin dashboard to install any applications that you wish to use, or you can use our other playbooks to automate the installation of those applications (including any additional dependencies) Playbook Content \u00a4 Install IBM Operator Catalogs (1 minute) Install Certificate Manager Operator (3 minutes) Install Mongodb Operator and Create a Cluster (10 minutes, skipped if SKIP_MONGO set to TRUE) Install and bootstrap IBM Suite License Service (10 minutes) Generate a MAS Workspace Configuration (1 minute) Configure Cloud Internet Services Integration for Maximo Application Suite (Optional, 1 minute) Install Maximo Application Suite Core Services (1 minute) Configure Maximo Application Suite (1 minute) Verify the Install and Configuration of Maximo Application Suite (25 minutes) All timings are estimates, see the individual pages for each of these roles for more information and full details of all configuration options available in this playbook. Preparation \u00a4 1. IBM Entitlement key \u00a4 Access Container Software Library using your IBMId to access your entitlement key 2. MAS License File \u00a4 Access IBM License Key Center , on the Get Keys menu select IBM AppPoint Suites . Select IBM MAXIMO APPLICATION SUITE AppPOINT LIC and on the next page fill in the information as below: Field Content Number of Keys How many AppPoints to assign to the license file Host ID Type Set to Ethernet Address Host ID Enter any 12 digit hexadecimal string Hostname Set to the hostname of your OCP instance Port Set to 27000 The other values can be left at their defaults. Finally, click Generate and download the license file to your home directory as entitlement.lic , set SLS_LICENSE_FILE to point to this location. Usage \u00a4 Required environment variables \u00a4 IBM_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in SLS_LICENSE_FILE SLS_LICENSE_FILE The path to the location of the license file. DRO_CONTACT_EMAIL Primary contact e-mail address DRO_CONTACT_FIRSTNAME Primary contact first name DRO_CONTACT_LASTNAME Primary contact last name Optional environment variables (for this playbook only) \u00a4 SKIP_MONGO Controls whether to use an custom MongoDB configuration instead of deploying a new MongoDB cluster. If set to \"true\" , the playbook will skip the mongo role and expect that MongoDB connection details are provided via a YAML configuration file in the directory specified by MAS_CONFIG_DIR . If set to \"false\" (or left unset), the playbook will deploy MongoDB using the ibm.mas_devops.mongodb role. Storage Class Configuraton \u00a4 Storage class configuration is built into the collection and the playbook will auto-select the appropriate storage classes when it detects the presence of certain storage classes in your cluster (IBM Cloud Storage or OpenShift Container Storage). If you are running the install on a cluster that does not have these storage classes then you will also must configure the following environment variables: ReadWriteMany Access Mode \u00a4 Usually fulfilled by file storage classes: PROMETHEUS_ALERTMGR_STORAGE_CLASS ReadWriteOnce Access Mode \u00a4 Usually fulfilled by block storage classes: PROMETHEUS_STORAGE_CLASS PROMETHEUS_USERWORKLOAD_STORAGE_CLASS GRAFANA_INSTANCE_STORAGE_CLASS MONGODB_STORAGE_CLASS DRO_STORAGE_CLASS Examples \u00a4 Release build \u00a4 The simplest configuration to deploy a release build of IBM Maximo Application Suite (core only) with dependencies is: export IBM_ENTITLEMENT_KEY=xxx export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_install_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli Pre-release build \u00a4 To deploy a pre-release build of IBM Maximo Application Suite (core only) with dependencies a number of additional parameters are required, note that pre-release builds are only available to IBM employees: export IBM_ENTITLEMENT_KEY=xxx export ARTIFACTORY_USERNAME=$W3_USERNAME_LOWERCASE export ARTIFACTORY_TOKEN=xxx export MAS_ICR_CP=docker-na-public.artifactory.swg-devops.com/wiotp-docker-local export MAS_ICR_CPOPEN=docker-na-public.artifactory.swg-devops.com/wiotp-docker-local export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_TOKEN export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export MAS_CATALOG_SOURCE=ibm-operator-catalog export MAS_CHANNEL=rp1dev88 export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export SKIP_MONGO=TRUE oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_install_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli Tip Enable login for Maximo Application Suite self-signed certificates. If you are using self-signed certificates in a development or test environment, you must manually enable login by using either of the following methods: Download the certificates from the cluster and add them to your local certificate manager. In your browser, go to the Maximo Application Suite API URL: \"https://api.mas_domain/\" and then accept the certificate security risks. After you accept the risks, an AIUC01999E error is displayed. This message is expected. You can now continue with the setup process.","title":"Install Core"},{"location":"playbooks/mas-core/#oneclick-install-for-mas-core","text":"Important These playbooks are samples to demonstrate how to use the roles in this collection. They are not intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install MAS is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance.","title":"OneClick Install for MAS Core"},{"location":"playbooks/mas-core/#overview","text":"This playbook will install and configure IBM Maximo Application Suite Core along with all necessary dependencies. This can be ran against any OCP cluster regardless of its type, whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. It will take approximately 90 minutes to set up MAS core services and all of its dependencies, at the end of the process you will be able to login to the MAS admin dashboard to install any applications that you wish to use, or you can use our other playbooks to automate the installation of those applications (including any additional dependencies)","title":"Overview"},{"location":"playbooks/mas-core/#playbook-content","text":"Install IBM Operator Catalogs (1 minute) Install Certificate Manager Operator (3 minutes) Install Mongodb Operator and Create a Cluster (10 minutes, skipped if SKIP_MONGO set to TRUE) Install and bootstrap IBM Suite License Service (10 minutes) Generate a MAS Workspace Configuration (1 minute) Configure Cloud Internet Services Integration for Maximo Application Suite (Optional, 1 minute) Install Maximo Application Suite Core Services (1 minute) Configure Maximo Application Suite (1 minute) Verify the Install and Configuration of Maximo Application Suite (25 minutes) All timings are estimates, see the individual pages for each of these roles for more information and full details of all configuration options available in this playbook.","title":"Playbook Content"},{"location":"playbooks/mas-core/#preparation","text":"","title":"Preparation"},{"location":"playbooks/mas-core/#1-ibm-entitlement-key","text":"Access Container Software Library using your IBMId to access your entitlement key","title":"1. IBM Entitlement key"},{"location":"playbooks/mas-core/#2-mas-license-file","text":"Access IBM License Key Center , on the Get Keys menu select IBM AppPoint Suites . Select IBM MAXIMO APPLICATION SUITE AppPOINT LIC and on the next page fill in the information as below: Field Content Number of Keys How many AppPoints to assign to the license file Host ID Type Set to Ethernet Address Host ID Enter any 12 digit hexadecimal string Hostname Set to the hostname of your OCP instance Port Set to 27000 The other values can be left at their defaults. Finally, click Generate and download the license file to your home directory as entitlement.lic , set SLS_LICENSE_FILE to point to this location.","title":"2. MAS License File"},{"location":"playbooks/mas-core/#usage","text":"","title":"Usage"},{"location":"playbooks/mas-core/#required-environment-variables","text":"IBM_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in SLS_LICENSE_FILE SLS_LICENSE_FILE The path to the location of the license file. DRO_CONTACT_EMAIL Primary contact e-mail address DRO_CONTACT_FIRSTNAME Primary contact first name DRO_CONTACT_LASTNAME Primary contact last name","title":"Required environment variables"},{"location":"playbooks/mas-core/#optional-environment-variables-for-this-playbook-only","text":"SKIP_MONGO Controls whether to use an custom MongoDB configuration instead of deploying a new MongoDB cluster. If set to \"true\" , the playbook will skip the mongo role and expect that MongoDB connection details are provided via a YAML configuration file in the directory specified by MAS_CONFIG_DIR . If set to \"false\" (or left unset), the playbook will deploy MongoDB using the ibm.mas_devops.mongodb role.","title":"Optional environment variables (for this playbook only)"},{"location":"playbooks/mas-core/#storage-class-configuraton","text":"Storage class configuration is built into the collection and the playbook will auto-select the appropriate storage classes when it detects the presence of certain storage classes in your cluster (IBM Cloud Storage or OpenShift Container Storage). If you are running the install on a cluster that does not have these storage classes then you will also must configure the following environment variables:","title":"Storage Class Configuraton"},{"location":"playbooks/mas-core/#readwritemany-access-mode","text":"Usually fulfilled by file storage classes: PROMETHEUS_ALERTMGR_STORAGE_CLASS","title":"ReadWriteMany Access Mode"},{"location":"playbooks/mas-core/#readwriteonce-access-mode","text":"Usually fulfilled by block storage classes: PROMETHEUS_STORAGE_CLASS PROMETHEUS_USERWORKLOAD_STORAGE_CLASS GRAFANA_INSTANCE_STORAGE_CLASS MONGODB_STORAGE_CLASS DRO_STORAGE_CLASS","title":"ReadWriteOnce Access Mode"},{"location":"playbooks/mas-core/#examples","text":"","title":"Examples"},{"location":"playbooks/mas-core/#release-build","text":"The simplest configuration to deploy a release build of IBM Maximo Application Suite (core only) with dependencies is: export IBM_ENTITLEMENT_KEY=xxx export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_install_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Release build"},{"location":"playbooks/mas-core/#pre-release-build","text":"To deploy a pre-release build of IBM Maximo Application Suite (core only) with dependencies a number of additional parameters are required, note that pre-release builds are only available to IBM employees: export IBM_ENTITLEMENT_KEY=xxx export ARTIFACTORY_USERNAME=$W3_USERNAME_LOWERCASE export ARTIFACTORY_TOKEN=xxx export MAS_ICR_CP=docker-na-public.artifactory.swg-devops.com/wiotp-docker-local export MAS_ICR_CPOPEN=docker-na-public.artifactory.swg-devops.com/wiotp-docker-local export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_TOKEN export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export MAS_CATALOG_SOURCE=ibm-operator-catalog export MAS_CHANNEL=rp1dev88 export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export SKIP_MONGO=TRUE oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_install_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli Tip Enable login for Maximo Application Suite self-signed certificates. If you are using self-signed certificates in a development or test environment, you must manually enable login by using either of the following methods: Download the certificates from the cluster and add them to your local certificate manager. In your browser, go to the Maximo Application Suite API URL: \"https://api.mas_domain/\" and then accept the certificate security risks. After you accept the risks, an AIUC01999E error is displayed. This message is expected. You can now continue with the setup process.","title":"Pre-release build"},{"location":"playbooks/mas-facilities/","text":"Install Real Estate and Facilities Application \u00a4 Important These playbooks are samples to demonstrate how to use the roles in this collection. They are note intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install MAS is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance. Overview \u00a4 This playbook will add Maximo Real Estate and Facilities to an existing IBM Maximo Application Suite Instance. Refer to the mas_install_core playbook to set up the MAS Core Platform before running this playbook. This playbook will create an in-cluster Db2 instance using the IBM Db2 Universal Operator, which will be automatically set up as the workspace-application level JDBC configuration in MAS. Playbook Content \u00a4 Create and initialize Db2 Instance for Maximo Real Estate and Facilities Configure MAS to use BYO database (optional, set CONFIGURE_EXTERNAL_DB ) Configure MAS to use the new Db2 Instance Install Maximo Real Estate and Facilities Application Configure Maximo Real Estate and Facilities Workspace See the individual pages for each of these roles for more information and full details of all configuration options available in this playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry In-Cluster Db2 \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_facilities Bring Your Own Database \u00a4 If you do not want to use the Db2 Universal Operator to provide the database for MREF then you can configure the playbook as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CONFIGURE_EXTERNAL_DB=true export DB_INSTANCE_ID=maxdbxx export MAS_JDBC_USER=user1 export MAS_JDBC_PASSWORD=xxx export MAS_JDBC_URL=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_facilities For full details of configuration options available refer to the gencfg_jdbc role documentation.","title":"Add Real Estate and Facilities"},{"location":"playbooks/mas-facilities/#install-real-estate-and-facilities-application","text":"Important These playbooks are samples to demonstrate how to use the roles in this collection. They are note intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install MAS is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance.","title":"Install Real Estate and Facilities Application"},{"location":"playbooks/mas-facilities/#overview","text":"This playbook will add Maximo Real Estate and Facilities to an existing IBM Maximo Application Suite Instance. Refer to the mas_install_core playbook to set up the MAS Core Platform before running this playbook. This playbook will create an in-cluster Db2 instance using the IBM Db2 Universal Operator, which will be automatically set up as the workspace-application level JDBC configuration in MAS.","title":"Overview"},{"location":"playbooks/mas-facilities/#playbook-content","text":"Create and initialize Db2 Instance for Maximo Real Estate and Facilities Configure MAS to use BYO database (optional, set CONFIGURE_EXTERNAL_DB ) Configure MAS to use the new Db2 Instance Install Maximo Real Estate and Facilities Application Configure Maximo Real Estate and Facilities Workspace See the individual pages for each of these roles for more information and full details of all configuration options available in this playbook.","title":"Playbook Content"},{"location":"playbooks/mas-facilities/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/mas-facilities/#in-cluster-db2","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_facilities","title":"In-Cluster Db2"},{"location":"playbooks/mas-facilities/#bring-your-own-database","text":"If you do not want to use the Db2 Universal Operator to provide the database for MREF then you can configure the playbook as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CONFIGURE_EXTERNAL_DB=true export DB_INSTANCE_ID=maxdbxx export MAS_JDBC_USER=user1 export MAS_JDBC_PASSWORD=xxx export MAS_JDBC_URL=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_facilities For full details of configuration options available refer to the gencfg_jdbc role documentation.","title":"Bring Your Own Database"},{"location":"playbooks/mas-iot/","text":"Install IoT Application \u00a4 Important These playbooks are samples to demonstrate how to use the roles in this collection. They are note intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install MAS is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance. Prerequisites \u00a4 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 already be installed, the mas_install_core playbook can be used to set this up. Overview \u00a4 This playbook will add Maximo IoT v8.4 to an existing IBM Maximo Application Suite Core installation. It will also creatie an in-cluster Db2 instance and Kafka cluster, both of which will be automatically set up as system-level configurations in MAS. IoT will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install IBM Db2 Universal Operator (2 minutes) Create Db2 Warehouse Instance (45 minutes) Install RedHat AMQ Streams Operator (2 minutes) Create Apache Kafka Cluster (15 minutes) Configure Maximo Application Suite: Set up Db2 instance as the system-level JDBC datasource Set up Kafka cluster as the system-level Kafka Install Maximo IoT application: Install application (90 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Storage Class Configuraton \u00a4 A persistent volume storage class is required for the FPL component. Storage class configuration is built into the collection and the playbook will auto-select the appropriate storage classes when it detects the presence of certain storage classes in your cluster (IBM Cloud Storage or OpenShift Container Storage). If you are running the install on a cluster that does not have these storage classes then you will also must configure the following environment variables: ReadWriteOnce Access Mode \u00a4 Usually fulfilled by block storage classes: MAS_APP_SETTINGS_IOT_FPL_PVC_STORAGE_CLASS Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Optional environment variables \u00a4 MAS_APP_SETTINGS_IOT_DEPLOYMENT_SIZE Define the IoT deployment size, one of dev , small or large . Defaults to small . Usage \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_iot Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Add IoT"},{"location":"playbooks/mas-iot/#install-iot-application","text":"Important These playbooks are samples to demonstrate how to use the roles in this collection. They are note intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install MAS is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance.","title":"Install IoT Application"},{"location":"playbooks/mas-iot/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 already be installed, the mas_install_core playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/mas-iot/#overview","text":"This playbook will add Maximo IoT v8.4 to an existing IBM Maximo Application Suite Core installation. It will also creatie an in-cluster Db2 instance and Kafka cluster, both of which will be automatically set up as system-level configurations in MAS. IoT will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install IBM Db2 Universal Operator (2 minutes) Create Db2 Warehouse Instance (45 minutes) Install RedHat AMQ Streams Operator (2 minutes) Create Apache Kafka Cluster (15 minutes) Configure Maximo Application Suite: Set up Db2 instance as the system-level JDBC datasource Set up Kafka cluster as the system-level Kafka Install Maximo IoT application: Install application (90 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/mas-iot/#storage-class-configuraton","text":"A persistent volume storage class is required for the FPL component. Storage class configuration is built into the collection and the playbook will auto-select the appropriate storage classes when it detects the presence of certain storage classes in your cluster (IBM Cloud Storage or OpenShift Container Storage). If you are running the install on a cluster that does not have these storage classes then you will also must configure the following environment variables:","title":"Storage Class Configuraton"},{"location":"playbooks/mas-iot/#readwriteonce-access-mode","text":"Usually fulfilled by block storage classes: MAS_APP_SETTINGS_IOT_FPL_PVC_STORAGE_CLASS","title":"ReadWriteOnce Access Mode"},{"location":"playbooks/mas-iot/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/mas-iot/#optional-environment-variables","text":"MAS_APP_SETTINGS_IOT_DEPLOYMENT_SIZE Define the IoT deployment size, one of dev , small or large . Defaults to small .","title":"Optional environment variables"},{"location":"playbooks/mas-iot/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_iot Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/mas-manage/","text":"Install Manage Application \u00a4 This playbook will add Maximo Manage to an existing IBM Maximo Application Suite Instance. Refer to the mas_install_core playbook to set up the MAS Core Platform before running this playbook. The playbook will also create an in-cluster Db2 instance using the IBM Db2 Universal Operator, which will be automatically set up as the system-level JDBC configuration in MAS. Playbook Content \u00a4 Install Cloud Pak for Data (optional, set CPD_INSTALL_PLATFORM ) Add Cognos to CP4D (optional, set CPD_INSTALL_COGNOS ) Add Watson Studio Local to CP4D (optional, set CPD_INSTALL_WSL ) Create Db2 Instance using IBM Db2 Universal Operator Initialize Db2 Instance for Maximo Manage Configure MAS to use the new Db2 Instance Configure MAS to use BYO database (optional, set CONFIGURE_EXTERNAL_DB ) Install Maximo Manage Application Configure Maximo Manage Workspace Configure Manage Attachments (optional, set CONFIGURE_MANAGE_ATTACHMENTS ) Configure Manage Building Information Models (optional, set CONFIGURE_MANAGE_BIM ) See the individual pages for each of these roles for more information and full details of all configuration options available in this playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Tip Manage requires the user to select one or more application components to enable in the workspace. By default the base component at the latest version will be installed if no MAS_APPWS_COMPONENTS is set. To customise the components that are enabled use the MAS_APPWS_COMPONENTS environment variable, for example to enable Manage(base) and Health set it to the following: export MAS_APPWS_COMPONENTS=\"base=latest,health=latest\" To disable Asset Investment Optimizer, optional feature of health, set MAS_APP_SETTINGS_AIO_FLAG to false . By default this flag is set to true . This feature is only avalaible on Manage with health as a addon or on Health as a Standalone install. This feature is disabled on MAS Core 9.1 and later. export MAS_APP_SETTINGS_AIO_FLAG=false Note : To install Manage Foundation only that is available on MAS Core 9.1 or later, export the following environment variable: MAS_APPWS_COMPONENTS environment variable must be empty: export MAS_APPWS_COMPONENTS=\"\" Optional Cloud Pak for Data Installation \u00a4 Optional integration with Cloud Pak for Data is supported in Maximo Manage. This can be enabled in the playbook as below: export CPD_INSTALL_PLATFORM=true export CPD_INSTALL_COGNOS=true export CPD_INSTALL_WSL=true export CPD_PRODUCT_VERSION=x.y.z Usage \u00a4 Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli In-Cluster Db2 \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage Bring Your Own Database \u00a4 If you do not want to use the Db2 Universal Operator to provide the datbase for Maximo Manage then you can configure the playbook as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CONFIGURE_EXTERNAL_DB=true export DB_INSTANCE_ID=maxdbxx export MAS_JDBC_USER=maximo export MAS_JDBC_PASSWORD=xxx export MAS_JDBC_URL=xxx export MAS_APP_SETTINGS_DB_SCHEMA=maximo export MAS_APP_SETTINGS_TABLESPACE=maxdata export MAS_APP_SETTINGS_INDEXSPACE=maxindex oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage For full details of configuration options available refer to the gencfg_jdbc role documentation. Cloud Pak For Data Integration \u00a4 To install CP4D with Cognos and/or Watson Studio Local optional dependencies: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CPD_INSTALL_PLATFORM=\"true\" export CPD_INSTALL_COGNOS=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_PRODUCT_VERSION=\"4.6.6\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage For full details of configuration options available refer to the cp4d & cp4d_service role documentation. Health Standalone Install \u00a4 To install Health as a Standalone application, set MAS_APP_ID and MAS_APPWS_COMPONENTS as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_ID=health export MAS_APPWS_COMPONENTS=\"health=latest\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage Warning Note that installing Health standalone will prevent the use of all other Manage components. It is recommended to install Manage normally and just enable the Health component in the Manage application.","title":"Add Manage"},{"location":"playbooks/mas-manage/#install-manage-application","text":"This playbook will add Maximo Manage to an existing IBM Maximo Application Suite Instance. Refer to the mas_install_core playbook to set up the MAS Core Platform before running this playbook. The playbook will also create an in-cluster Db2 instance using the IBM Db2 Universal Operator, which will be automatically set up as the system-level JDBC configuration in MAS.","title":"Install Manage Application"},{"location":"playbooks/mas-manage/#playbook-content","text":"Install Cloud Pak for Data (optional, set CPD_INSTALL_PLATFORM ) Add Cognos to CP4D (optional, set CPD_INSTALL_COGNOS ) Add Watson Studio Local to CP4D (optional, set CPD_INSTALL_WSL ) Create Db2 Instance using IBM Db2 Universal Operator Initialize Db2 Instance for Maximo Manage Configure MAS to use the new Db2 Instance Configure MAS to use BYO database (optional, set CONFIGURE_EXTERNAL_DB ) Install Maximo Manage Application Configure Maximo Manage Workspace Configure Manage Attachments (optional, set CONFIGURE_MANAGE_ATTACHMENTS ) Configure Manage Building Information Models (optional, set CONFIGURE_MANAGE_BIM ) See the individual pages for each of these roles for more information and full details of all configuration options available in this playbook.","title":"Playbook Content"},{"location":"playbooks/mas-manage/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Tip Manage requires the user to select one or more application components to enable in the workspace. By default the base component at the latest version will be installed if no MAS_APPWS_COMPONENTS is set. To customise the components that are enabled use the MAS_APPWS_COMPONENTS environment variable, for example to enable Manage(base) and Health set it to the following: export MAS_APPWS_COMPONENTS=\"base=latest,health=latest\" To disable Asset Investment Optimizer, optional feature of health, set MAS_APP_SETTINGS_AIO_FLAG to false . By default this flag is set to true . This feature is only avalaible on Manage with health as a addon or on Health as a Standalone install. This feature is disabled on MAS Core 9.1 and later. export MAS_APP_SETTINGS_AIO_FLAG=false Note : To install Manage Foundation only that is available on MAS Core 9.1 or later, export the following environment variable: MAS_APPWS_COMPONENTS environment variable must be empty: export MAS_APPWS_COMPONENTS=\"\"","title":"Required environment variables"},{"location":"playbooks/mas-manage/#optional-cloud-pak-for-data-installation","text":"Optional integration with Cloud Pak for Data is supported in Maximo Manage. This can be enabled in the playbook as below: export CPD_INSTALL_PLATFORM=true export CPD_INSTALL_COGNOS=true export CPD_INSTALL_WSL=true export CPD_PRODUCT_VERSION=x.y.z","title":"Optional Cloud Pak for Data Installation"},{"location":"playbooks/mas-manage/#usage","text":"Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/mas-manage/#in-cluster-db2","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage","title":"In-Cluster Db2"},{"location":"playbooks/mas-manage/#bring-your-own-database","text":"If you do not want to use the Db2 Universal Operator to provide the datbase for Maximo Manage then you can configure the playbook as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CONFIGURE_EXTERNAL_DB=true export DB_INSTANCE_ID=maxdbxx export MAS_JDBC_USER=maximo export MAS_JDBC_PASSWORD=xxx export MAS_JDBC_URL=xxx export MAS_APP_SETTINGS_DB_SCHEMA=maximo export MAS_APP_SETTINGS_TABLESPACE=maxdata export MAS_APP_SETTINGS_INDEXSPACE=maxindex oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage For full details of configuration options available refer to the gencfg_jdbc role documentation.","title":"Bring Your Own Database"},{"location":"playbooks/mas-manage/#cloud-pak-for-data-integration","text":"To install CP4D with Cognos and/or Watson Studio Local optional dependencies: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CPD_INSTALL_PLATFORM=\"true\" export CPD_INSTALL_COGNOS=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_PRODUCT_VERSION=\"4.6.6\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage For full details of configuration options available refer to the cp4d & cp4d_service role documentation.","title":"Cloud Pak For Data Integration"},{"location":"playbooks/mas-manage/#health-standalone-install","text":"To install Health as a Standalone application, set MAS_APP_ID and MAS_APPWS_COMPONENTS as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_ID=health export MAS_APPWS_COMPONENTS=\"health=latest\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage Warning Note that installing Health standalone will prevent the use of all other Manage components. It is recommended to install Manage normally and just enable the Health component in the Manage application.","title":"Health Standalone Install"},{"location":"playbooks/mas-monitor/","text":"Install Monitor Application \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 and Maximo IoT v8.4 already be installed, the mas_install_core and mas-iot playbooks can be used to set this up. Overview \u00a4 This playbook will add Maximo Monitor v8.7 to an existing IBM Maximo Application Suite Core installation. Monitor will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Monitor application: Install application (60 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Optional environment variables \u00a4 MAS_APP_SETTINGS_MONITOR_DEPLOYMENT_SIZE Define the Monitor deployment size, one of dev , small or large . Defaults to dev . Usage \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_monitor Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Add Monitor"},{"location":"playbooks/mas-monitor/#install-monitor-application","text":"","title":"Install Monitor Application"},{"location":"playbooks/mas-monitor/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 and Maximo IoT v8.4 already be installed, the mas_install_core and mas-iot playbooks can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/mas-monitor/#overview","text":"This playbook will add Maximo Monitor v8.7 to an existing IBM Maximo Application Suite Core installation. Monitor will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Monitor application: Install application (60 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/mas-monitor/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/mas-monitor/#optional-environment-variables","text":"MAS_APP_SETTINGS_MONITOR_DEPLOYMENT_SIZE Define the Monitor deployment size, one of dev , small or large . Defaults to dev .","title":"Optional environment variables"},{"location":"playbooks/mas-monitor/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_monitor Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/mas-optimizer/","text":"Install Optimizer Application \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.9. The [mas_install_core] (mas-core.md) playbook can be used to set this up. Overview \u00a4 This playbook will add Maximo Optimizer v8.3 to an existing IBM Maximo Application Suite Core installation. Optimizer will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Optimizer application: Install application (10 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Usage \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_optimizer Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Add Optimizer"},{"location":"playbooks/mas-optimizer/#install-optimizer-application","text":"","title":"Install Optimizer Application"},{"location":"playbooks/mas-optimizer/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.9. The [mas_install_core] (mas-core.md) playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/mas-optimizer/#overview","text":"This playbook will add Maximo Optimizer v8.3 to an existing IBM Maximo Application Suite Core installation. Optimizer will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Optimizer application: Install application (10 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/mas-optimizer/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/mas-optimizer/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_optimizer Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/mas-predict/","text":"Install Predict Application \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift cluster with IBM Maximo Application Suite Core v8.11 already be installed, the mas_install_core playbook can be used to set this up. Overview \u00a4 This playbook will add Predict v8.9 to an existing IBM Maximo Application Suite Core installation. It will also install CloudPak for Data + CP4D services. This playbook can be ran against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install CP4D (~1 1/2 hours) Install Watson Studio (~3 hours) Install Watson Machine Learning (~2 1/2 hours) Install Spark (~30 minutes) Install Openscale (~1 hour) Install SPSS Install Predict application: Install application (~15 Minutes) Configure workspace (~30 Minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. As of MAS 8.10, predict 8.8.0 will start to support SPSS Modeler, to install SPSS as part of CP4D set CPD_INSTALL_SPSS=true in your environment variables before running the playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry CPD_WSL_PROJECT_ID - Ensure a Project ID Text box has a valid Watson Studio project ID. To obtain the project ID, Navigate to Cp4d/Watson Studio and create/Reuse a project. Open the project and look into the Browser URL, obtain the project ID from the URL and update Project ID settings. CPD_WML_INSTANCE_ID Set Default value to \"openshift\" CPD_WML_URL Set Default value to \"https://internal-nginx-svc.ibm-cpd.svc:12443\" . ibm-cpd in the URL corresponds to the project name (namespace) of cp4d installation CPD_PRODUCT_VERSION (Required if WML_VERSION is not informed) Cloud Pak for Data version installed in the cluster in 4.X format, it will be used to obtain the correct WML version to be installed WML_VERSION (Required if CPD_PRODUCT_VERSION is not informed) The wml_version for cp4d 4.0.x will be 4.0, if cp4d is 4.5.x , wml_version should change to 4.5, if cp4d is 4.6.x , wml_version should change to 4.6 These variables are required only if you set CP4D_INSTALL_WSL to false in optional varibles: CPD_ADMIN_USERNAME CP4D Username CPD_ADMIN_PASSWORD CP4D Password CPD_ADMIN_URL CP4D Base URL Warning When not using this playbook to install Cloud Pak for Data it is important to ensure that your existing instance already has all the required services enabled. Optional environment variables \u00a4 CPD_INSTALL_PLATFORM True/False - If you HAVE CP4D already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_WSL True/False - If you HAVE Watson Studio already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_WML True/False - If you HAVE Watson Machine Learning already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_SPARK True/False - If you HAVE Spark already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_OPENSCALE True/False - If you HAVE Openscale already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_SPSS True/False - If you HAVE SPSS Modeler already installed in your cluster you can skip this variable as False is set default Usage \u00a4 Tip If you do not want to set up all the dependencies on your local system, you can run the playbook from inside the CLI container image: docker run -ti --pull always quay.io/ibmmas/cli Cloud Pak for Data is already installed \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_CHANNEL=\"8.9.x\" export CPD_PRODUCT_VERSION=\"4.6.4\" export CPD_WSL_PROJECT_ID=\"xxxx\" export CPD_WML_INSTANCE_ID=\"openshift\" export CPD_WML_URL=\"https://internal-nginx-svc.ibm-cpd.svc:12443\" export WML_VERSION=\"4.6\" export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_ADMIN_URL=\"https://mycp4durl\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_predict Cloud Pak for Data is not installed \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_CHANNEL=\"8.9.x\" export CPD_PRODUCT_VERSION=\"4.6.4\" export CPD_INSTALL_PLATFORM=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_INSTALL_WML=\"true\" export CPD_INSTALL_SPARK=\"true\" export CPD_INSTALL_OPENSCALE=\"true\" export CPD_INSTALL_DISCOVERY=\"true\" export CPD_INSTALL_SPSS=\"true\" export CPD_WSL_PROJECT_ID=\"xxxx\" export CPD_WML_INSTANCE_ID=\"openshift\" export CPD_WML_URL=\"https://internal-nginx-svc.ibm-cpd.svc:12443\" export WML_VERSION=\"4.6\" export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_ADMIN_URL=\"https://mycp4durl\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_predict","title":"Add Predict"},{"location":"playbooks/mas-predict/#install-predict-application","text":"","title":"Install Predict Application"},{"location":"playbooks/mas-predict/#prerequisites","text":"You will need a RedHat OpenShift cluster with IBM Maximo Application Suite Core v8.11 already be installed, the mas_install_core playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/mas-predict/#overview","text":"This playbook will add Predict v8.9 to an existing IBM Maximo Application Suite Core installation. It will also install CloudPak for Data + CP4D services. This playbook can be ran against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install CP4D (~1 1/2 hours) Install Watson Studio (~3 hours) Install Watson Machine Learning (~2 1/2 hours) Install Spark (~30 minutes) Install Openscale (~1 hour) Install SPSS Install Predict application: Install application (~15 Minutes) Configure workspace (~30 Minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. As of MAS 8.10, predict 8.8.0 will start to support SPSS Modeler, to install SPSS as part of CP4D set CPD_INSTALL_SPSS=true in your environment variables before running the playbook.","title":"Overview"},{"location":"playbooks/mas-predict/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry CPD_WSL_PROJECT_ID - Ensure a Project ID Text box has a valid Watson Studio project ID. To obtain the project ID, Navigate to Cp4d/Watson Studio and create/Reuse a project. Open the project and look into the Browser URL, obtain the project ID from the URL and update Project ID settings. CPD_WML_INSTANCE_ID Set Default value to \"openshift\" CPD_WML_URL Set Default value to \"https://internal-nginx-svc.ibm-cpd.svc:12443\" . ibm-cpd in the URL corresponds to the project name (namespace) of cp4d installation CPD_PRODUCT_VERSION (Required if WML_VERSION is not informed) Cloud Pak for Data version installed in the cluster in 4.X format, it will be used to obtain the correct WML version to be installed WML_VERSION (Required if CPD_PRODUCT_VERSION is not informed) The wml_version for cp4d 4.0.x will be 4.0, if cp4d is 4.5.x , wml_version should change to 4.5, if cp4d is 4.6.x , wml_version should change to 4.6 These variables are required only if you set CP4D_INSTALL_WSL to false in optional varibles: CPD_ADMIN_USERNAME CP4D Username CPD_ADMIN_PASSWORD CP4D Password CPD_ADMIN_URL CP4D Base URL Warning When not using this playbook to install Cloud Pak for Data it is important to ensure that your existing instance already has all the required services enabled.","title":"Required environment variables"},{"location":"playbooks/mas-predict/#optional-environment-variables","text":"CPD_INSTALL_PLATFORM True/False - If you HAVE CP4D already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_WSL True/False - If you HAVE Watson Studio already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_WML True/False - If you HAVE Watson Machine Learning already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_SPARK True/False - If you HAVE Spark already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_OPENSCALE True/False - If you HAVE Openscale already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_SPSS True/False - If you HAVE SPSS Modeler already installed in your cluster you can skip this variable as False is set default","title":"Optional environment variables"},{"location":"playbooks/mas-predict/#usage","text":"Tip If you do not want to set up all the dependencies on your local system, you can run the playbook from inside the CLI container image: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/mas-predict/#cloud-pak-for-data-is-already-installed","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_CHANNEL=\"8.9.x\" export CPD_PRODUCT_VERSION=\"4.6.4\" export CPD_WSL_PROJECT_ID=\"xxxx\" export CPD_WML_INSTANCE_ID=\"openshift\" export CPD_WML_URL=\"https://internal-nginx-svc.ibm-cpd.svc:12443\" export WML_VERSION=\"4.6\" export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_ADMIN_URL=\"https://mycp4durl\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_predict","title":"Cloud Pak for Data is already installed"},{"location":"playbooks/mas-predict/#cloud-pak-for-data-is-not-installed","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_CHANNEL=\"8.9.x\" export CPD_PRODUCT_VERSION=\"4.6.4\" export CPD_INSTALL_PLATFORM=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_INSTALL_WML=\"true\" export CPD_INSTALL_SPARK=\"true\" export CPD_INSTALL_OPENSCALE=\"true\" export CPD_INSTALL_DISCOVERY=\"true\" export CPD_INSTALL_SPSS=\"true\" export CPD_WSL_PROJECT_ID=\"xxxx\" export CPD_WML_INSTANCE_ID=\"openshift\" export CPD_WML_URL=\"https://internal-nginx-svc.ibm-cpd.svc:12443\" export WML_VERSION=\"4.6\" export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_ADMIN_URL=\"https://mycp4durl\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_predict","title":"Cloud Pak for Data is not installed"},{"location":"playbooks/mas-update/","text":"OneClick Update \u00a4 This playbook will update the IBM Maximo Operator Catalog on your OpenShift cluster. This will make available new operator updates, which will be automatically applied across the cluster. These updates will not change the functionality of the software in your cluster, they will only carry fixes for security vulnerabilities and bugs. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) this playbook can be ignored, as you will recieve catalog updates in your cluster as soon as they are released. This playbook is specifically for customers who choose to use static catalogs to control the consumption of updates in their cluster. This is distinct from an upgrade , which will modify the operator subscriptions on your cluster to deliver new features. Performing an updating may make new upgrades available in the cluster, but it will never initiate the upgrade, you must choose when to upgrade. Playbook Content \u00a4 Install IBM Operator Catalog (1 minute) Preparation \u00a4 You will need to determine the version of the IBM Maximo Operator Catalog that you wish to update to. Generally speaking, you should update the most recent catalog available. Important If you are using a private/mirror registry it is critical that you mirror the images from the updated catalog before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the catalog has been updated. You do not need to worry about translating the image tags to digests to make these catalogs compatible with image mirroring on OpenShift, the role will automatically usse the image digest when it installs any static operator catalog. Usage \u00a4 Required environment variables \u00a4 MAS_CATALOG_VERSION Example \u00a4 Only one parameter is required, the new tag of the IBM Maximo Operator Catalog that you wish to use: export MAS_CATALOG_VERSION=@@MAS_LATEST_CATALOG@@ oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_update Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm --pull always quay.io/ibmmas/cli","title":"Update"},{"location":"playbooks/mas-update/#oneclick-update","text":"This playbook will update the IBM Maximo Operator Catalog on your OpenShift cluster. This will make available new operator updates, which will be automatically applied across the cluster. These updates will not change the functionality of the software in your cluster, they will only carry fixes for security vulnerabilities and bugs. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) this playbook can be ignored, as you will recieve catalog updates in your cluster as soon as they are released. This playbook is specifically for customers who choose to use static catalogs to control the consumption of updates in their cluster. This is distinct from an upgrade , which will modify the operator subscriptions on your cluster to deliver new features. Performing an updating may make new upgrades available in the cluster, but it will never initiate the upgrade, you must choose when to upgrade.","title":"OneClick Update"},{"location":"playbooks/mas-update/#playbook-content","text":"Install IBM Operator Catalog (1 minute)","title":"Playbook Content"},{"location":"playbooks/mas-update/#preparation","text":"You will need to determine the version of the IBM Maximo Operator Catalog that you wish to update to. Generally speaking, you should update the most recent catalog available. Important If you are using a private/mirror registry it is critical that you mirror the images from the updated catalog before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the catalog has been updated. You do not need to worry about translating the image tags to digests to make these catalogs compatible with image mirroring on OpenShift, the role will automatically usse the image digest when it installs any static operator catalog.","title":"Preparation"},{"location":"playbooks/mas-update/#usage","text":"","title":"Usage"},{"location":"playbooks/mas-update/#required-environment-variables","text":"MAS_CATALOG_VERSION","title":"Required environment variables"},{"location":"playbooks/mas-update/#example","text":"Only one parameter is required, the new tag of the IBM Maximo Operator Catalog that you wish to use: export MAS_CATALOG_VERSION=@@MAS_LATEST_CATALOG@@ oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_update Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm --pull always quay.io/ibmmas/cli","title":"Example"},{"location":"playbooks/mas-upgrade/","text":"OneClick Upgrade \u00a4 This playbook will upgrade the channel subscriptions for IBM Maximo Application Suite on your OpenShift cluster. Upgrades can only be performed to releases available in the version of the IBM Maximo Pperator Catalog that is installed in your cluster. To update to a newer version of the operator catalog refer to the mas-update playbook documentation. The playbook will attempt to upgrade MAS Core and all installed applications. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) you will always have access to the latest MAS releases, as you will recieve catalog updates in your cluster as soon as they are released. Customers using the static catalogs to control the consumption of updates in their cluster will need to ensure that the version of the catalog they have installed supports the version of MAS that they wish to upgrade to. Playbook Content \u00a4 Upgrade MAS Core Verify MAS Core Upgrade MAS Application (Assist) Upgrade MAS Application (IoT) Upgrade MAS Application (Manage) Upgrade MAS Application (Monitor) Upgrade MAS Application (Optimizer) Upgrade MAS Application (Predict) Upgrade MAS Application (Visual Inspection) Preparation \u00a4 If you are using a private/mirror registry it is critical that you mirror the images for the new release before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the subscription has been changed, if you have not mirrored the new images the subscription change itself may fail if the operator bundle is not on your private registry. Usage \u00a4 Required Parameters \u00a4 MAS_INSTANCE_ID Set the instance ID of the MAS installation to upgrade Optional Parameters \u00a4 If you provide no values for MAS Core or the individual applications, the roles will attempt to upgrade to the next level of MAS and upgrade applications to the latest version supported by the installed version of MAS Core (after upgrading MAS Core). MAS_CHANNEL Set the target subscription channel for MAS Core MAS_APP_CHANNEL_ASSIST Set the target subscription channel for Assist MAS_APP_CHANNEL_IOT Set the target subscription channel for IoT MAS_APP_CHANNEL_MONITOR Set the target subscription channel for Monitor MAS_APP_CHANNEL_OPTIMIZER Set the target subscription channel for Optimizer MAS_APP_CHANNEL_PREDICT Set the target subscription channel for Predict MAS_APP_CHANNEL_VISUALINSPECTION Set the target subscription channel for Visual Inspection Example \u00a4 The simplest way to upgrade MAS is to provide only the instance ID that you wish to upgrade, allowing the roles to determine the correct target version each application. export MAS_INSTANCE_ID=instance1 oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_upgrade You can also explicitly specify the target upgrade: export MAS_INSTANCE_ID=instance1 export MAS_CHANNEL=8.8.x export MAS_APP_CHANNEL_IOT=8.5.x oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_upgrade Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm --pull always quay.io/ibmmas/cli","title":"Upgrade"},{"location":"playbooks/mas-upgrade/#oneclick-upgrade","text":"This playbook will upgrade the channel subscriptions for IBM Maximo Application Suite on your OpenShift cluster. Upgrades can only be performed to releases available in the version of the IBM Maximo Pperator Catalog that is installed in your cluster. To update to a newer version of the operator catalog refer to the mas-update playbook documentation. The playbook will attempt to upgrade MAS Core and all installed applications. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) you will always have access to the latest MAS releases, as you will recieve catalog updates in your cluster as soon as they are released. Customers using the static catalogs to control the consumption of updates in their cluster will need to ensure that the version of the catalog they have installed supports the version of MAS that they wish to upgrade to.","title":"OneClick Upgrade"},{"location":"playbooks/mas-upgrade/#playbook-content","text":"Upgrade MAS Core Verify MAS Core Upgrade MAS Application (Assist) Upgrade MAS Application (IoT) Upgrade MAS Application (Manage) Upgrade MAS Application (Monitor) Upgrade MAS Application (Optimizer) Upgrade MAS Application (Predict) Upgrade MAS Application (Visual Inspection)","title":"Playbook Content"},{"location":"playbooks/mas-upgrade/#preparation","text":"If you are using a private/mirror registry it is critical that you mirror the images for the new release before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the subscription has been changed, if you have not mirrored the new images the subscription change itself may fail if the operator bundle is not on your private registry.","title":"Preparation"},{"location":"playbooks/mas-upgrade/#usage","text":"","title":"Usage"},{"location":"playbooks/mas-upgrade/#required-parameters","text":"MAS_INSTANCE_ID Set the instance ID of the MAS installation to upgrade","title":"Required Parameters"},{"location":"playbooks/mas-upgrade/#optional-parameters","text":"If you provide no values for MAS Core or the individual applications, the roles will attempt to upgrade to the next level of MAS and upgrade applications to the latest version supported by the installed version of MAS Core (after upgrading MAS Core). MAS_CHANNEL Set the target subscription channel for MAS Core MAS_APP_CHANNEL_ASSIST Set the target subscription channel for Assist MAS_APP_CHANNEL_IOT Set the target subscription channel for IoT MAS_APP_CHANNEL_MONITOR Set the target subscription channel for Monitor MAS_APP_CHANNEL_OPTIMIZER Set the target subscription channel for Optimizer MAS_APP_CHANNEL_PREDICT Set the target subscription channel for Predict MAS_APP_CHANNEL_VISUALINSPECTION Set the target subscription channel for Visual Inspection","title":"Optional Parameters"},{"location":"playbooks/mas-upgrade/#example","text":"The simplest way to upgrade MAS is to provide only the instance ID that you wish to upgrade, allowing the roles to determine the correct target version each application. export MAS_INSTANCE_ID=instance1 oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_upgrade You can also explicitly specify the target upgrade: export MAS_INSTANCE_ID=instance1 export MAS_CHANNEL=8.8.x export MAS_APP_CHANNEL_IOT=8.5.x oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_upgrade Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm --pull always quay.io/ibmmas/cli","title":"Example"},{"location":"playbooks/mas-visualinspection/","text":"Install Visual Inspection Application \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.9. The [mas_install_core] (mas-core.md) playbook can be used to set this up. Overview \u00a4 This playbook will add Maximo Visual Inspection v8.7 to an existing IBM Maximo Application Suite Core installation. MVI will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install NVIDIA Graphical Processing Unit (GPU) (10 minutes) Install Maximo Visual Inspection application: Install application (15 minutes) Configure workspace (10 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Optional environment variables \u00a4 MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_CLASS Defines a custom file storage class for Visual Inspection application. If none provided, then a default storage class will be auto defined accordingly to your cluster's availability i.e ibmc-file-gold for IBM Cloud or azurefiles-premium for Azure clusters. MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_SIZE Defines persistent storage size for Visual Inspection application. If not provided, default is 100Gi . MAS_APP_SETTINGS_VISUALINSPECTION_OBJECT_STORAGE_ENABLED If set to true , enables Object Storage integration with Visual Inspection . MAS_APP_SETTINGS_VISUALINSPECTION_OBJECT_STORAGE_WORKSPACE Defines the Object Storage bucket name to be used for Visual Inspection integration. CONFIGURE_COS If set to true , an Object Storage instance will be configured as MAS system scope configuration which will be used for Visual Inspection integration. See cos role documentation for detailed information. CONFIGURE_COS_BUCKET If set to true , an Object Storage bucket will be configured to be used for Visual Inspection application. See cos_bucket role documentation for detailed information. Usage \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_visualinspection Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Add Visual Inspection"},{"location":"playbooks/mas-visualinspection/#install-visual-inspection-application","text":"","title":"Install Visual Inspection Application"},{"location":"playbooks/mas-visualinspection/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.9. The [mas_install_core] (mas-core.md) playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/mas-visualinspection/#overview","text":"This playbook will add Maximo Visual Inspection v8.7 to an existing IBM Maximo Application Suite Core installation. MVI will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install NVIDIA Graphical Processing Unit (GPU) (10 minutes) Install Maximo Visual Inspection application: Install application (15 minutes) Configure workspace (10 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/mas-visualinspection/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/mas-visualinspection/#optional-environment-variables","text":"MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_CLASS Defines a custom file storage class for Visual Inspection application. If none provided, then a default storage class will be auto defined accordingly to your cluster's availability i.e ibmc-file-gold for IBM Cloud or azurefiles-premium for Azure clusters. MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_SIZE Defines persistent storage size for Visual Inspection application. If not provided, default is 100Gi . MAS_APP_SETTINGS_VISUALINSPECTION_OBJECT_STORAGE_ENABLED If set to true , enables Object Storage integration with Visual Inspection . MAS_APP_SETTINGS_VISUALINSPECTION_OBJECT_STORAGE_WORKSPACE Defines the Object Storage bucket name to be used for Visual Inspection integration. CONFIGURE_COS If set to true , an Object Storage instance will be configured as MAS system scope configuration which will be used for Visual Inspection integration. See cos role documentation for detailed information. CONFIGURE_COS_BUCKET If set to true , an Object Storage bucket will be configured to be used for Visual Inspection application. See cos_bucket role documentation for detailed information.","title":"Optional environment variables"},{"location":"playbooks/mas-visualinspection/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_visualinspection Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/ocp/","text":"OCP Playbooks \u00a4 Provision \u00a4 Refer to the ocp_provision role documentation for more information. Provision on AWS ROSA \u00a4 This playbook uses your ROSA API Token to provision a brand new OCP cluster, provision an instance of EFS and set up the cluster with a ReadWriteMany storage class named efs utilizing that instance. To obtain your API token login to the OpenShift cluster manager . export AWS_ACCESS_KEY_ID=xxx export AWS_SECRET_ACCESS_KEY=xxx export ROSA_TOKEN=xxx export CLUSTER_NAME=masonrosa export OCP_VERSION=4.19 export ROSA_COMPUTE_NODES=5 export ROSA_CLUSTER_ADMIN_PASSWORD=xxx ansible-playbook ibm.mas_devops.ocp_rosa_provision Provision on IBMCloud ROKS \u00a4 This playbook uses your IBMCloud API key to provision a brand new OCP cluster. The playbook supports installing an IBM entitlement key as a cluster-wide image pull secret and reboot all worker nodes, which is required for IBM Cloud Pak for Data v4; this can be enabled by setting REBOOT_WORKER_NODES to true and providing the entitlement key with CPD_ENTITLEMENT_KEY . This also supports upgrading the storage volume used for the cluster's internal image registry from 100Gb to 400Gb, this must be enabled by setting UPGRADE_IMAGE_REGISTRY_STORAGE to true . This option is stringly recommended if you intend to install the Watson services from Cloud Pak for Data as the default volume size is too small. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.19_openshift export IBMCLOUD_APIKEY=xxx export REBOOT_WORKER_NODES=true export CPD_ENTITLEMENT_KEY=xxx export UPGRADE_IMAGE_REGISTRY_STORAGE=true ansible-playbook ibm.mas_devops.ocp_roks_provision Provision on IBM DevIT Fyre \u00a4 This playbook will provision a QuickBurn OCP cluster in IBM DevIT Fyre service, QuickBurn clusters will be automatically deprovisioned after 36 hours and are only suitable for small scale deployments for local development and demostration systems. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.19 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx export FYRE_PRODUCT_ID=xxx ansible-playbook ibm.mas_devops.ocp_fyre_provision Deprovision \u00a4 Refer to the ocp_deprovision role documentation for more information. Deprovision on IBMCloud ROKS \u00a4 export CLUSTER_NAME=masinst1 export IBMCLOUD_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_roks_deprovision Deprovision on IBM DevIT Fyre \u00a4 export CLUSTER_NAME=masinst1 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_fyre_deprovision","title":"OCP"},{"location":"playbooks/ocp/#ocp-playbooks","text":"","title":"OCP Playbooks"},{"location":"playbooks/ocp/#provision","text":"Refer to the ocp_provision role documentation for more information.","title":"Provision"},{"location":"playbooks/ocp/#provision-on-aws-rosa","text":"This playbook uses your ROSA API Token to provision a brand new OCP cluster, provision an instance of EFS and set up the cluster with a ReadWriteMany storage class named efs utilizing that instance. To obtain your API token login to the OpenShift cluster manager . export AWS_ACCESS_KEY_ID=xxx export AWS_SECRET_ACCESS_KEY=xxx export ROSA_TOKEN=xxx export CLUSTER_NAME=masonrosa export OCP_VERSION=4.19 export ROSA_COMPUTE_NODES=5 export ROSA_CLUSTER_ADMIN_PASSWORD=xxx ansible-playbook ibm.mas_devops.ocp_rosa_provision","title":"Provision on AWS ROSA"},{"location":"playbooks/ocp/#provision-on-ibmcloud-roks","text":"This playbook uses your IBMCloud API key to provision a brand new OCP cluster. The playbook supports installing an IBM entitlement key as a cluster-wide image pull secret and reboot all worker nodes, which is required for IBM Cloud Pak for Data v4; this can be enabled by setting REBOOT_WORKER_NODES to true and providing the entitlement key with CPD_ENTITLEMENT_KEY . This also supports upgrading the storage volume used for the cluster's internal image registry from 100Gb to 400Gb, this must be enabled by setting UPGRADE_IMAGE_REGISTRY_STORAGE to true . This option is stringly recommended if you intend to install the Watson services from Cloud Pak for Data as the default volume size is too small. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.19_openshift export IBMCLOUD_APIKEY=xxx export REBOOT_WORKER_NODES=true export CPD_ENTITLEMENT_KEY=xxx export UPGRADE_IMAGE_REGISTRY_STORAGE=true ansible-playbook ibm.mas_devops.ocp_roks_provision","title":"Provision on IBMCloud ROKS"},{"location":"playbooks/ocp/#provision-on-ibm-devit-fyre","text":"This playbook will provision a QuickBurn OCP cluster in IBM DevIT Fyre service, QuickBurn clusters will be automatically deprovisioned after 36 hours and are only suitable for small scale deployments for local development and demostration systems. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.19 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx export FYRE_PRODUCT_ID=xxx ansible-playbook ibm.mas_devops.ocp_fyre_provision","title":"Provision on IBM DevIT Fyre"},{"location":"playbooks/ocp/#deprovision","text":"Refer to the ocp_deprovision role documentation for more information.","title":"Deprovision"},{"location":"playbooks/ocp/#deprovision-on-ibmcloud-roks","text":"export CLUSTER_NAME=masinst1 export IBMCLOUD_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_roks_deprovision","title":"Deprovision on IBMCloud ROKS"},{"location":"playbooks/ocp/#deprovision-on-ibm-devit-fyre","text":"export CLUSTER_NAME=masinst1 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_fyre_deprovision","title":"Deprovision on IBM DevIT Fyre"},{"location":"roles/aiservice/","text":"aiservice \u00a4 This role provides support to install and configure AI Service for IBM Maximo Application Suite. AI Service enables AI-powered capabilities within MAS applications, particularly for Maximo Manage. The role supports the following operations: - Install AI Service API application - Create and delete AI Service tenants - Manage AI Service API keys - Configure AWS S3 storage integration - Configure WatsonX AI integration Role Variables \u00a4 General Variables \u00a4 tenant_action \u00a4 Action to perform on AI Service tenant. Optional Environment Variable: TENANT_ACTION Default: install Purpose : Specifies whether to install or remove an AI Service tenant. Controls tenant lifecycle management. When to use : - Use install (default) to create a new AI Service tenant - Use remove to delete an existing AI Service tenant - Required for tenant management operations Valid values : install , remove Impact : - install : Creates tenant with specified configuration - remove : Deletes tenant and associated resources Related variables : - tenantName : Name of tenant to install/remove - All other variables apply only when action is install Note : WARNING - remove action permanently deletes the tenant and all associated data. Ensure you have backups before removing a tenant. tenantName \u00a4 AI Service tenant identifier. Optional Environment Variable: AISERVICE_TENANT_NAME Default: user Purpose : Specifies the name/identifier for the AI Service tenant. Tenants provide isolation between different users or environments. When to use : - Use default ( user ) for single-tenant deployments - Set custom name for multi-tenant environments - Use descriptive names (e.g., production , development , team-a ) Valid values : Valid tenant name string (alphanumeric, lowercase recommended) Impact : Tenant name is used in resource names, API keys, and configuration. Must be unique within the AI Service instance. Related variables : - tenant_action : Whether to install or remove this tenant - Tenant name is used in generated API key secrets Note : Choose meaningful tenant names for multi-tenant scenarios. The default user is suitable for single-tenant deployments. app_domain \u00a4 Application domain for AI Service routes. Optional Environment Variable: APP_DOMAIN Default: Auto-detected from cluster Purpose : Specifies the application domain for AI Service routes and endpoints. Used to construct the full URL for AI Service API access. When to use : - Leave unset for automatic detection from cluster configuration - Set explicitly when cluster domain cannot be auto-detected - Required for custom domain configurations Valid values : Domain string in format apps.domain (e.g., apps.mycluster.example.com ) Impact : Determines the URL where AI Service API is accessible. Incorrect domain will prevent API access. Related variables : - aiservice_domain : Custom domain override (takes precedence if set) Note : The role automatically detects the cluster's application domain. Only set this if auto-detection fails or you need a custom domain. Format must be apps.<domain> . aiservice_domain \u00a4 Custom domain override for AI Service. Optional Environment Variable: AISERVICE_DOMAIN Default: None (uses app_domain or cluster default) Purpose : Provides a custom domain specifically for AI Service, overriding the general application domain. Useful for custom DNS configurations. When to use : - Leave unset to use app_domain or cluster default - Set when AI Service needs a different domain than other applications - Required for custom DNS or external domain configurations Valid values : Valid domain string (e.g., aiservice.example.com ) Impact : When set, this domain is used instead of app_domain for AI Service routes. Takes precedence over app_domain . Related variables : - app_domain : General application domain (used if this is not set) Note : This is an advanced configuration option. Most deployments should use app_domain or cluster auto-detection. Only set this if AI Service requires a separate domain. S3 Storage Configuration Variables \u00a4 aiservice_s3_host \u00a4 S3-compatible storage host endpoint. Optional Environment Variable: AISERVICE_S3_HOST Default: None Purpose : Specifies the endpoint URL for S3-compatible object storage used by AI Service for storing models, data, and artifacts. When to use : - Required when configuring S3 storage integration - Set to AWS S3 endpoint (e.g., s3.amazonaws.com ) or compatible service - Must be accessible from the cluster Valid values : Valid S3 endpoint URL (e.g., s3.amazonaws.com , s3.us-east-1.amazonaws.com , MinIO endpoint) Impact : AI Service uses this storage for persistent data. Without proper S3 configuration, AI Service functionality will be limited. Related variables : - aiservice_s3_accesskey : Access credentials for this host - aiservice_s3_secretkey : Secret credentials for this host - aiservice_s3_region : Region for this host Note : All S3 variables ( aiservice_s3_* ) must be configured together for S3 integration. Supports AWS S3 and S3-compatible services like MinIO, IBM Cloud Object Storage. aiservice_s3_accesskey \u00a4 S3 storage access key ID. Optional Environment Variable: AISERVICE_S3_ACCESSKEY Default: None Purpose : Provides the access key ID for authenticating to S3-compatible object storage. Part of the credential pair for S3 access. When to use : - Required when configuring S3 storage integration - Obtain from your S3 provider (AWS IAM, MinIO, etc.) - Must have permissions to create/read/write buckets and objects Valid values : Valid S3 access key ID string Impact : Without valid credentials, AI Service cannot access S3 storage, limiting functionality. Related variables : - aiservice_s3_secretkey : Secret key paired with this access key - aiservice_s3_host : S3 endpoint to authenticate against - aiservice_s3_region : Region for the S3 service Note : SECURITY - Keep access keys secure. Do not commit to source control. Use environment variables or secure secret management. Ensure the access key has appropriate S3 permissions for AI Service operations. aiservice_s3_secretkey \u00a4 S3 storage secret access key. Optional Environment Variable: AISERVICE_S3_SECRETKEY Default: None Purpose : Provides the secret access key for authenticating to S3-compatible object storage. Part of the credential pair for S3 access. When to use : - Required when configuring S3 storage integration - Obtain from your S3 provider (AWS IAM, MinIO, etc.) - Must be paired with corresponding aiservice_s3_accesskey Valid values : Valid S3 secret access key string Impact : Without valid credentials, AI Service cannot access S3 storage, limiting functionality. Related variables : - aiservice_s3_accesskey : Access key ID paired with this secret key - aiservice_s3_host : S3 endpoint to authenticate against - aiservice_s3_region : Region for the S3 service Note : SECURITY - Keep secret keys secure. Never commit to source control or expose in logs. Use environment variables or secure secret management. The secret key must match the access key ID. aiservice_s3_region \u00a4 S3 storage region. Optional Environment Variable: AISERVICE_S3_REGION Default: None Purpose : Specifies the AWS region or region identifier for S3-compatible object storage. Required for proper S3 API operations. When to use : - Required when configuring S3 storage integration - Set to AWS region (e.g., us-east-1 , eu-west-1 ) or compatible service region - Must match the region where your S3 buckets are located Valid values : Valid AWS region code or S3-compatible service region identifier Impact : Incorrect region will cause S3 API calls to fail. Must match the actual bucket location. Related variables : - aiservice_s3_host : S3 endpoint (may include region in URL) - aiservice_s3_accesskey : Access credentials for this region - aiservice_s3_secretkey : Secret credentials for this region Note : For AWS S3, use standard region codes (e.g., us-east-1 ). For S3-compatible services, use the region identifier provided by your service. Some services may not require a region. WatsonX AI Configuration Variables \u00a4 aiservice_watsonx_action \u00a4 Action to perform on WatsonX AI integration. Optional Environment Variable: AISERVICE_WATSONX_ACTION Default: install Purpose : Specifies whether to install or remove WatsonX AI integration with AI Service. Controls WatsonX integration lifecycle. When to use : - Use install (default) to configure WatsonX AI integration - Use remove to delete WatsonX AI integration - Required for WatsonX integration management Valid values : install , remove Impact : - install : Configures AI Service to use WatsonX AI for AI/ML capabilities - remove : Removes WatsonX AI integration configuration Related variables : - aiservice_watsonxai_apikey : API key for WatsonX (required for install) - aiservice_watsonxai_url : WatsonX endpoint (required for install) - aiservice_watsonxai_project_id : WatsonX project (required for install) Note : WatsonX AI integration enables advanced AI capabilities in AI Service. All WatsonX variables must be configured together for successful integration. aiservice_watsonxai_apikey \u00a4 WatsonX AI API key for authentication. Optional Environment Variable: AISERVICE_WATSONXAI_APIKEY Default: None Purpose : Provides the API key for authenticating AI Service with IBM WatsonX AI platform. Required for WatsonX AI integration. When to use : - Required when aiservice_watsonx_action is install - Obtain from IBM Cloud WatsonX AI service - Must have appropriate WatsonX AI permissions Valid values : Valid IBM WatsonX AI API key string Impact : Without valid API key, AI Service cannot access WatsonX AI capabilities. Integration will fail. Related variables : - aiservice_watsonxai_url : WatsonX AI endpoint to authenticate against - aiservice_watsonxai_project_id : WatsonX project to access - aiservice_watsonx_action : Whether to install or remove integration Note : SECURITY - Keep API keys secure. Do not commit to source control. Use environment variables or secure secret management. Obtain from IBM Cloud IAM or WatsonX AI service credentials. aiservice_watsonxai_url \u00a4 WatsonX AI service endpoint URL. Optional Environment Variable: AISERVICE_WATSONXAI_URL Default: None Purpose : Specifies the endpoint URL for IBM WatsonX AI service. Required for AI Service to connect to WatsonX AI platform. When to use : - Required when aiservice_watsonx_action is install - Set to your WatsonX AI region endpoint - Must be accessible from the cluster Valid values : Valid WatsonX AI endpoint URL (e.g., https://us-south.ml.cloud.ibm.com , https://eu-de.ml.cloud.ibm.com ) Impact : AI Service uses this URL to access WatsonX AI APIs. Incorrect URL will prevent WatsonX integration. Related variables : - aiservice_watsonxai_apikey : API key for authenticating to this endpoint - aiservice_watsonxai_project_id : Project to access at this endpoint - aiservice_watsonx_action : Whether to install or remove integration Note : Use the WatsonX AI endpoint for your IBM Cloud region. Common endpoints: https://us-south.ml.cloud.ibm.com (Dallas), https://eu-de.ml.cloud.ibm.com (Frankfurt), https://jp-tok.ml.cloud.ibm.com (Tokyo). aiservice_watsonxai_project_id \u00a4 WatsonX AI project identifier. Optional Environment Variable: AISERVICE_WATSONXAI_PROJECT_ID Default: None Purpose : Specifies the WatsonX AI project ID that AI Service will use for AI/ML operations. Projects organize resources and control access in WatsonX AI. When to use : - Required when aiservice_watsonx_action is install - Obtain from your WatsonX AI project in IBM Cloud - Project must have appropriate models and resources configured Valid values : Valid WatsonX AI project ID (UUID format) Impact : AI Service uses this project for accessing WatsonX AI models and resources. Incorrect project ID will prevent access to AI capabilities. Related variables : - aiservice_watsonxai_apikey : API key must have access to this project - aiservice_watsonxai_url : WatsonX endpoint where this project exists - aiservice_watsonx_action : Whether to install or remove integration Note : The project ID is found in your WatsonX AI project settings in IBM Cloud. Ensure the API key has appropriate permissions for the project. The project should have the required AI models and resources configured. Certificate Management \u00a4 aiservice_certificate_issuer \u00a4 Name of the cert-manager Issuer to use for automatic certificate generation. Optional Environment Variable: AISERVICE_CERTIFICATE_ISSUER Default: None Purpose : Specifies which cert-manager Issuer will generate and manage SSL/TLS certificates for AI Service. The Issuer defines the Certificate Authority and authentication method used for certificate issuance. When to use : - Set to the Issuer created by suite_dns role (e.g., {aiservice_instance_id}-cloudflare-le-prod ) - Set to a custom Issuer if you have specific certificate requirements Valid values : Name of any valid Issuer resource in the cluster (e.g., prod-le-issuer , {mas_instance_id}-cloudflare-le-prod ) Impact : The specified Issuer will be used to generate all certificates for AI Service. If the Issuer is not properly configured or lacks necessary credentials, certificate generation will fail and AI Service will not be accessible. Related variables : - Created by suite_dns role for Let's Encrypt integration - Works with aiservice_certificate_duration and aiservice_certificate_renew_before Note : Ensure the Issuer is created and functional before installing AI Service. Test certificate generation with a test Certificate resource first. aiservice_certificate_duration \u00a4 Specifies the validity period for AI Service certificates. Optional Environment Variable: AISERVICE_CERTIFICATE_DURATION Default: 8760h0m0s (1 year) Purpose : Defines how long certificates will be valid before they expire. This affects how often certificates need to be renewed and the security posture of your installation. When to use : - Use default (8760h = 1 year) for most installations - Reduce for higher security environments requiring frequent rotation - Increase only if certificate renewal is problematic in your environment - Must be longer than aiservice_certificate_renew_before Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 8760h0m0s , 2160h0m0s , 17520h0m0s ) Impact : Shorter durations increase security but require more frequent renewals. Longer durations reduce renewal frequency but increase risk if certificates are compromised. Cert-manager will automatically renew certificates before expiration. Related variables : - Must be greater than aiservice_certificate_renew_before - Only applies when using automatic certificate management - Affects all AI Service certificates aiservice_certificate_renew_before \u00a4 Specifies when to renew certificates before they expire. Optional Environment Variable: AISERVICE_CERTIFICATE_RENEW_BEFORE Default: 720h0m0s (30 days) Purpose : Defines the renewal window - how far in advance cert-manager will renew certificates before they expire. This ensures certificates are renewed with sufficient time to handle any renewal issues. When to use : - Use default (720h = 30 days) for most installations - Increase in environments where certificate renewal may be slow or problematic - Decrease only if you need to minimize the number of certificate changes - Must be shorter than aiservice_certificate_duration Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 720h0m0s , 1440h0m0s , 168h0m0s ) Impact : Longer renewal windows provide more time to resolve renewal issues but result in more frequent certificate changes. Shorter windows reduce certificate churn but increase risk of expiration if renewal fails. Related variables : - Must be less than aiservice_certificate_duration - Only applies when using automatic certificate management - Affects all AI Service certificates Note : Ensure this value provides adequate time to detect and resolve certificate renewal issues before expiration. Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: tenant_action: install tenantName: production app_domain: apps.mycluster.example.com aiservice_s3_host: s3.amazonaws.com aiservice_s3_accesskey: \"{{ lookup('env', 'AWS_ACCESS_KEY') }}\" aiservice_s3_secretkey: \"{{ lookup('env', 'AWS_SECRET_KEY') }}\" aiservice_s3_region: us-east-1 aiservice_watsonxai_apikey: \"{{ lookup('env', 'WATSONX_API_KEY') }}\" aiservice_watsonxai_url: https://us-south.ml.cloud.ibm.com aiservice_watsonxai_project_id: my-project-id roles: - ibm.mas_devops.aiservice Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export TENANT_ACTION=install export AISERVICE_TENANT_NAME=production export APP_DOMAIN=apps.mycluster.example.com export AISERVICE_S3_HOST=s3.amazonaws.com export AISERVICE_S3_ACCESSKEY=your_access_key export AISERVICE_S3_SECRETKEY=your_secret_key export AISERVICE_S3_REGION=us-east-1 export AISERVICE_WATSONXAI_APIKEY=your_watsonx_api_key export AISERVICE_WATSONXAI_URL=https://us-south.ml.cloud.ibm.com export AISERVICE_WATSONXAI_PROJECT_ID=my-project-id ROLE_NAME=aiservice ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aiservice"},{"location":"roles/aiservice/#aiservice","text":"This role provides support to install and configure AI Service for IBM Maximo Application Suite. AI Service enables AI-powered capabilities within MAS applications, particularly for Maximo Manage. The role supports the following operations: - Install AI Service API application - Create and delete AI Service tenants - Manage AI Service API keys - Configure AWS S3 storage integration - Configure WatsonX AI integration","title":"aiservice"},{"location":"roles/aiservice/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aiservice/#general-variables","text":"","title":"General Variables"},{"location":"roles/aiservice/#tenant_action","text":"Action to perform on AI Service tenant. Optional Environment Variable: TENANT_ACTION Default: install Purpose : Specifies whether to install or remove an AI Service tenant. Controls tenant lifecycle management. When to use : - Use install (default) to create a new AI Service tenant - Use remove to delete an existing AI Service tenant - Required for tenant management operations Valid values : install , remove Impact : - install : Creates tenant with specified configuration - remove : Deletes tenant and associated resources Related variables : - tenantName : Name of tenant to install/remove - All other variables apply only when action is install Note : WARNING - remove action permanently deletes the tenant and all associated data. Ensure you have backups before removing a tenant.","title":"tenant_action"},{"location":"roles/aiservice/#tenantname","text":"AI Service tenant identifier. Optional Environment Variable: AISERVICE_TENANT_NAME Default: user Purpose : Specifies the name/identifier for the AI Service tenant. Tenants provide isolation between different users or environments. When to use : - Use default ( user ) for single-tenant deployments - Set custom name for multi-tenant environments - Use descriptive names (e.g., production , development , team-a ) Valid values : Valid tenant name string (alphanumeric, lowercase recommended) Impact : Tenant name is used in resource names, API keys, and configuration. Must be unique within the AI Service instance. Related variables : - tenant_action : Whether to install or remove this tenant - Tenant name is used in generated API key secrets Note : Choose meaningful tenant names for multi-tenant scenarios. The default user is suitable for single-tenant deployments.","title":"tenantName"},{"location":"roles/aiservice/#app_domain","text":"Application domain for AI Service routes. Optional Environment Variable: APP_DOMAIN Default: Auto-detected from cluster Purpose : Specifies the application domain for AI Service routes and endpoints. Used to construct the full URL for AI Service API access. When to use : - Leave unset for automatic detection from cluster configuration - Set explicitly when cluster domain cannot be auto-detected - Required for custom domain configurations Valid values : Domain string in format apps.domain (e.g., apps.mycluster.example.com ) Impact : Determines the URL where AI Service API is accessible. Incorrect domain will prevent API access. Related variables : - aiservice_domain : Custom domain override (takes precedence if set) Note : The role automatically detects the cluster's application domain. Only set this if auto-detection fails or you need a custom domain. Format must be apps.<domain> .","title":"app_domain"},{"location":"roles/aiservice/#aiservice_domain","text":"Custom domain override for AI Service. Optional Environment Variable: AISERVICE_DOMAIN Default: None (uses app_domain or cluster default) Purpose : Provides a custom domain specifically for AI Service, overriding the general application domain. Useful for custom DNS configurations. When to use : - Leave unset to use app_domain or cluster default - Set when AI Service needs a different domain than other applications - Required for custom DNS or external domain configurations Valid values : Valid domain string (e.g., aiservice.example.com ) Impact : When set, this domain is used instead of app_domain for AI Service routes. Takes precedence over app_domain . Related variables : - app_domain : General application domain (used if this is not set) Note : This is an advanced configuration option. Most deployments should use app_domain or cluster auto-detection. Only set this if AI Service requires a separate domain.","title":"aiservice_domain"},{"location":"roles/aiservice/#s3-storage-configuration-variables","text":"","title":"S3 Storage Configuration Variables"},{"location":"roles/aiservice/#aiservice_s3_host","text":"S3-compatible storage host endpoint. Optional Environment Variable: AISERVICE_S3_HOST Default: None Purpose : Specifies the endpoint URL for S3-compatible object storage used by AI Service for storing models, data, and artifacts. When to use : - Required when configuring S3 storage integration - Set to AWS S3 endpoint (e.g., s3.amazonaws.com ) or compatible service - Must be accessible from the cluster Valid values : Valid S3 endpoint URL (e.g., s3.amazonaws.com , s3.us-east-1.amazonaws.com , MinIO endpoint) Impact : AI Service uses this storage for persistent data. Without proper S3 configuration, AI Service functionality will be limited. Related variables : - aiservice_s3_accesskey : Access credentials for this host - aiservice_s3_secretkey : Secret credentials for this host - aiservice_s3_region : Region for this host Note : All S3 variables ( aiservice_s3_* ) must be configured together for S3 integration. Supports AWS S3 and S3-compatible services like MinIO, IBM Cloud Object Storage.","title":"aiservice_s3_host"},{"location":"roles/aiservice/#aiservice_s3_accesskey","text":"S3 storage access key ID. Optional Environment Variable: AISERVICE_S3_ACCESSKEY Default: None Purpose : Provides the access key ID for authenticating to S3-compatible object storage. Part of the credential pair for S3 access. When to use : - Required when configuring S3 storage integration - Obtain from your S3 provider (AWS IAM, MinIO, etc.) - Must have permissions to create/read/write buckets and objects Valid values : Valid S3 access key ID string Impact : Without valid credentials, AI Service cannot access S3 storage, limiting functionality. Related variables : - aiservice_s3_secretkey : Secret key paired with this access key - aiservice_s3_host : S3 endpoint to authenticate against - aiservice_s3_region : Region for the S3 service Note : SECURITY - Keep access keys secure. Do not commit to source control. Use environment variables or secure secret management. Ensure the access key has appropriate S3 permissions for AI Service operations.","title":"aiservice_s3_accesskey"},{"location":"roles/aiservice/#aiservice_s3_secretkey","text":"S3 storage secret access key. Optional Environment Variable: AISERVICE_S3_SECRETKEY Default: None Purpose : Provides the secret access key for authenticating to S3-compatible object storage. Part of the credential pair for S3 access. When to use : - Required when configuring S3 storage integration - Obtain from your S3 provider (AWS IAM, MinIO, etc.) - Must be paired with corresponding aiservice_s3_accesskey Valid values : Valid S3 secret access key string Impact : Without valid credentials, AI Service cannot access S3 storage, limiting functionality. Related variables : - aiservice_s3_accesskey : Access key ID paired with this secret key - aiservice_s3_host : S3 endpoint to authenticate against - aiservice_s3_region : Region for the S3 service Note : SECURITY - Keep secret keys secure. Never commit to source control or expose in logs. Use environment variables or secure secret management. The secret key must match the access key ID.","title":"aiservice_s3_secretkey"},{"location":"roles/aiservice/#aiservice_s3_region","text":"S3 storage region. Optional Environment Variable: AISERVICE_S3_REGION Default: None Purpose : Specifies the AWS region or region identifier for S3-compatible object storage. Required for proper S3 API operations. When to use : - Required when configuring S3 storage integration - Set to AWS region (e.g., us-east-1 , eu-west-1 ) or compatible service region - Must match the region where your S3 buckets are located Valid values : Valid AWS region code or S3-compatible service region identifier Impact : Incorrect region will cause S3 API calls to fail. Must match the actual bucket location. Related variables : - aiservice_s3_host : S3 endpoint (may include region in URL) - aiservice_s3_accesskey : Access credentials for this region - aiservice_s3_secretkey : Secret credentials for this region Note : For AWS S3, use standard region codes (e.g., us-east-1 ). For S3-compatible services, use the region identifier provided by your service. Some services may not require a region.","title":"aiservice_s3_region"},{"location":"roles/aiservice/#watsonx-ai-configuration-variables","text":"","title":"WatsonX AI Configuration Variables"},{"location":"roles/aiservice/#aiservice_watsonx_action","text":"Action to perform on WatsonX AI integration. Optional Environment Variable: AISERVICE_WATSONX_ACTION Default: install Purpose : Specifies whether to install or remove WatsonX AI integration with AI Service. Controls WatsonX integration lifecycle. When to use : - Use install (default) to configure WatsonX AI integration - Use remove to delete WatsonX AI integration - Required for WatsonX integration management Valid values : install , remove Impact : - install : Configures AI Service to use WatsonX AI for AI/ML capabilities - remove : Removes WatsonX AI integration configuration Related variables : - aiservice_watsonxai_apikey : API key for WatsonX (required for install) - aiservice_watsonxai_url : WatsonX endpoint (required for install) - aiservice_watsonxai_project_id : WatsonX project (required for install) Note : WatsonX AI integration enables advanced AI capabilities in AI Service. All WatsonX variables must be configured together for successful integration.","title":"aiservice_watsonx_action"},{"location":"roles/aiservice/#aiservice_watsonxai_apikey","text":"WatsonX AI API key for authentication. Optional Environment Variable: AISERVICE_WATSONXAI_APIKEY Default: None Purpose : Provides the API key for authenticating AI Service with IBM WatsonX AI platform. Required for WatsonX AI integration. When to use : - Required when aiservice_watsonx_action is install - Obtain from IBM Cloud WatsonX AI service - Must have appropriate WatsonX AI permissions Valid values : Valid IBM WatsonX AI API key string Impact : Without valid API key, AI Service cannot access WatsonX AI capabilities. Integration will fail. Related variables : - aiservice_watsonxai_url : WatsonX AI endpoint to authenticate against - aiservice_watsonxai_project_id : WatsonX project to access - aiservice_watsonx_action : Whether to install or remove integration Note : SECURITY - Keep API keys secure. Do not commit to source control. Use environment variables or secure secret management. Obtain from IBM Cloud IAM or WatsonX AI service credentials.","title":"aiservice_watsonxai_apikey"},{"location":"roles/aiservice/#aiservice_watsonxai_url","text":"WatsonX AI service endpoint URL. Optional Environment Variable: AISERVICE_WATSONXAI_URL Default: None Purpose : Specifies the endpoint URL for IBM WatsonX AI service. Required for AI Service to connect to WatsonX AI platform. When to use : - Required when aiservice_watsonx_action is install - Set to your WatsonX AI region endpoint - Must be accessible from the cluster Valid values : Valid WatsonX AI endpoint URL (e.g., https://us-south.ml.cloud.ibm.com , https://eu-de.ml.cloud.ibm.com ) Impact : AI Service uses this URL to access WatsonX AI APIs. Incorrect URL will prevent WatsonX integration. Related variables : - aiservice_watsonxai_apikey : API key for authenticating to this endpoint - aiservice_watsonxai_project_id : Project to access at this endpoint - aiservice_watsonx_action : Whether to install or remove integration Note : Use the WatsonX AI endpoint for your IBM Cloud region. Common endpoints: https://us-south.ml.cloud.ibm.com (Dallas), https://eu-de.ml.cloud.ibm.com (Frankfurt), https://jp-tok.ml.cloud.ibm.com (Tokyo).","title":"aiservice_watsonxai_url"},{"location":"roles/aiservice/#aiservice_watsonxai_project_id","text":"WatsonX AI project identifier. Optional Environment Variable: AISERVICE_WATSONXAI_PROJECT_ID Default: None Purpose : Specifies the WatsonX AI project ID that AI Service will use for AI/ML operations. Projects organize resources and control access in WatsonX AI. When to use : - Required when aiservice_watsonx_action is install - Obtain from your WatsonX AI project in IBM Cloud - Project must have appropriate models and resources configured Valid values : Valid WatsonX AI project ID (UUID format) Impact : AI Service uses this project for accessing WatsonX AI models and resources. Incorrect project ID will prevent access to AI capabilities. Related variables : - aiservice_watsonxai_apikey : API key must have access to this project - aiservice_watsonxai_url : WatsonX endpoint where this project exists - aiservice_watsonx_action : Whether to install or remove integration Note : The project ID is found in your WatsonX AI project settings in IBM Cloud. Ensure the API key has appropriate permissions for the project. The project should have the required AI models and resources configured.","title":"aiservice_watsonxai_project_id"},{"location":"roles/aiservice/#certificate-management","text":"","title":"Certificate Management"},{"location":"roles/aiservice/#aiservice_certificate_issuer","text":"Name of the cert-manager Issuer to use for automatic certificate generation. Optional Environment Variable: AISERVICE_CERTIFICATE_ISSUER Default: None Purpose : Specifies which cert-manager Issuer will generate and manage SSL/TLS certificates for AI Service. The Issuer defines the Certificate Authority and authentication method used for certificate issuance. When to use : - Set to the Issuer created by suite_dns role (e.g., {aiservice_instance_id}-cloudflare-le-prod ) - Set to a custom Issuer if you have specific certificate requirements Valid values : Name of any valid Issuer resource in the cluster (e.g., prod-le-issuer , {mas_instance_id}-cloudflare-le-prod ) Impact : The specified Issuer will be used to generate all certificates for AI Service. If the Issuer is not properly configured or lacks necessary credentials, certificate generation will fail and AI Service will not be accessible. Related variables : - Created by suite_dns role for Let's Encrypt integration - Works with aiservice_certificate_duration and aiservice_certificate_renew_before Note : Ensure the Issuer is created and functional before installing AI Service. Test certificate generation with a test Certificate resource first.","title":"aiservice_certificate_issuer"},{"location":"roles/aiservice/#aiservice_certificate_duration","text":"Specifies the validity period for AI Service certificates. Optional Environment Variable: AISERVICE_CERTIFICATE_DURATION Default: 8760h0m0s (1 year) Purpose : Defines how long certificates will be valid before they expire. This affects how often certificates need to be renewed and the security posture of your installation. When to use : - Use default (8760h = 1 year) for most installations - Reduce for higher security environments requiring frequent rotation - Increase only if certificate renewal is problematic in your environment - Must be longer than aiservice_certificate_renew_before Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 8760h0m0s , 2160h0m0s , 17520h0m0s ) Impact : Shorter durations increase security but require more frequent renewals. Longer durations reduce renewal frequency but increase risk if certificates are compromised. Cert-manager will automatically renew certificates before expiration. Related variables : - Must be greater than aiservice_certificate_renew_before - Only applies when using automatic certificate management - Affects all AI Service certificates","title":"aiservice_certificate_duration"},{"location":"roles/aiservice/#aiservice_certificate_renew_before","text":"Specifies when to renew certificates before they expire. Optional Environment Variable: AISERVICE_CERTIFICATE_RENEW_BEFORE Default: 720h0m0s (30 days) Purpose : Defines the renewal window - how far in advance cert-manager will renew certificates before they expire. This ensures certificates are renewed with sufficient time to handle any renewal issues. When to use : - Use default (720h = 30 days) for most installations - Increase in environments where certificate renewal may be slow or problematic - Decrease only if you need to minimize the number of certificate changes - Must be shorter than aiservice_certificate_duration Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 720h0m0s , 1440h0m0s , 168h0m0s ) Impact : Longer renewal windows provide more time to resolve renewal issues but result in more frequent certificate changes. Shorter windows reduce certificate churn but increase risk of expiration if renewal fails. Related variables : - Must be less than aiservice_certificate_duration - Only applies when using automatic certificate management - Affects all AI Service certificates Note : Ensure this value provides adequate time to detect and resolve certificate renewal issues before expiration.","title":"aiservice_certificate_renew_before"},{"location":"roles/aiservice/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: tenant_action: install tenantName: production app_domain: apps.mycluster.example.com aiservice_s3_host: s3.amazonaws.com aiservice_s3_accesskey: \"{{ lookup('env', 'AWS_ACCESS_KEY') }}\" aiservice_s3_secretkey: \"{{ lookup('env', 'AWS_SECRET_KEY') }}\" aiservice_s3_region: us-east-1 aiservice_watsonxai_apikey: \"{{ lookup('env', 'WATSONX_API_KEY') }}\" aiservice_watsonxai_url: https://us-south.ml.cloud.ibm.com aiservice_watsonxai_project_id: my-project-id roles: - ibm.mas_devops.aiservice","title":"Example Playbook"},{"location":"roles/aiservice/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export TENANT_ACTION=install export AISERVICE_TENANT_NAME=production export APP_DOMAIN=apps.mycluster.example.com export AISERVICE_S3_HOST=s3.amazonaws.com export AISERVICE_S3_ACCESSKEY=your_access_key export AISERVICE_S3_SECRETKEY=your_secret_key export AISERVICE_S3_REGION=us-east-1 export AISERVICE_WATSONXAI_APIKEY=your_watsonx_api_key export AISERVICE_WATSONXAI_URL=https://us-south.ml.cloud.ibm.com export AISERVICE_WATSONXAI_PROJECT_ID=my-project-id ROLE_NAME=aiservice ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aiservice/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aiservice_odh/","text":"aiservice_odh \u00a4 This role provides support to deploy odh components for AI Broker Application: Install Red Hat OpenShift Serverless Operator Install Red Hat OpenShift Service Mesh Operator Install Authorino Operator Install Open Data Hub Operator Create DSCInitialization instance Create Data Science Cluster Create Create Data Science Pipelines Application Role Variables \u00a4 tenantName \u00a4 Tenant identifier for the Open Data Hub deployment. Optional Environment Variable: AISERVICE_TENANT_NAME Default: user Purpose : Identifies the tenant for multi-tenant ODH deployments, used for resource isolation and organization. When to use : Override the default when deploying multiple ODH instances or when organizational naming conventions require specific tenant identifiers. Valid values : Alphanumeric string (e.g., user , team1 , prod-tenant ) Impact : Determines the tenant context for ODH resources and configurations. Related variables : None Notes : The default user is suitable for single-tenant deployments. serverless_catalog_source \u00a4 Catalog source for Red Hat OpenShift Serverless Operator. Optional Environment Variable: SERVERLESS_CATALOG_SOURCE Default: redhat-operators Purpose : Specifies which operator catalog provides the OpenShift Serverless Operator required for ODH. When to use : Use default for standard deployments. Override for custom or mirrored catalogs in air-gapped environments. Valid values : Valid CatalogSource name (e.g., redhat-operators , custom-catalog ) Impact : Determines the source of the Serverless operator. Incorrect catalog will prevent operator installation. Related variables : serverless_channel Notes : - OpenShift Serverless is required for ODH data science pipelines - Verify catalog exists: oc get catalogsource -n openshift-marketplace serverless_channel \u00a4 Subscription channel for OpenShift Serverless Operator. Optional Environment Variable: SERVERLESS_CHANNEL Default: stable Purpose : Controls which version stream of OpenShift Serverless will be installed and receive updates. When to use : Use default stable for production. Override only for specific version requirements. Valid values : Valid Serverless operator channel (e.g., stable , stable-1.28 ) Impact : Determines the Serverless version installed and which automatic updates are received. Related variables : serverless_catalog_source Notes : The stable channel provides the latest stable release with automatic updates. service_mesh_channel \u00a4 Subscription channel for Red Hat OpenShift Service Mesh Operator. Optional Environment Variable: SERVICEMESH_CHANNEL Default: stable Purpose : Controls which version stream of OpenShift Service Mesh will be installed for ODH networking. When to use : Use default stable for production. Override for specific version requirements. Valid values : Valid Service Mesh operator channel (e.g., stable , stable-2.3 ) Impact : Determines the Service Mesh version installed and which automatic updates are received. Related variables : service_mesh_catalog_source Notes : - Service Mesh is required for ODH model serving and networking - The stable channel provides the latest stable release service_mesh_catalog_source \u00a4 Catalog source for Red Hat OpenShift Service Mesh Operator. Optional Environment Variable: SERVICEMESH_CATALOG_SOURCE Default: redhat-operators Purpose : Specifies which operator catalog provides the OpenShift Service Mesh Operator required for ODH. When to use : Use default for standard deployments. Override for custom or mirrored catalogs in air-gapped environments. Valid values : Valid CatalogSource name (e.g., redhat-operators , custom-catalog ) Impact : Determines the source of the Service Mesh operator. Incorrect catalog will prevent operator installation. Related variables : service_mesh_channel Notes : Verify catalog exists: oc get catalogsource -n openshift-marketplace authorino_catalog_source \u00a4 Catalog source for Authorino Operator. Optional Environment Variable: AUTHORINO_CATALOG_SOURCE Default: community-operators Purpose : Specifies which operator catalog provides the Authorino Operator for ODH authorization. When to use : Use default for standard deployments. Override for custom or mirrored catalogs in air-gapped environments. Valid values : Valid CatalogSource name (e.g., community-operators , custom-catalog ) Impact : Determines the source of the Authorino operator. Incorrect catalog will prevent operator installation. Related variables : None Notes : - Authorino provides authorization for ODH model serving - Available in community-operators catalog by default odh_channel \u00a4 Subscription channel for Open Data Hub Operator. Optional Environment Variable: ODH_CHANNEL Default: fast Purpose : Controls which version stream of Open Data Hub will be installed and receive updates. When to use : Use fast for latest features. Use stable for production environments requiring more testing. Valid values : Valid ODH operator channel (e.g., fast , stable ) Impact : Determines the ODH version installed and update frequency. fast channel receives updates more quickly than stable . Related variables : odh_catalog_source Notes : - fast channel provides latest features but may be less stable - stable channel recommended for production deployments odh_catalog_source \u00a4 Catalog source for Open Data Hub Operator. Optional Environment Variable: ODH_CATALOG_SOURCE Default: community-operators Purpose : Specifies which operator catalog provides the Open Data Hub Operator. When to use : Use default for standard deployments. Override for custom or mirrored catalogs in air-gapped environments. Valid values : Valid CatalogSource name (e.g., community-operators , custom-catalog ) Impact : Determines the source of the ODH operator. Incorrect catalog will prevent operator installation. Related variables : odh_channel Notes : - ODH is available in community-operators catalog - For air-gapped environments, mirror the catalog and update this variable Example Playbook \u00a4 - hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.aiservice_odh License \u00a4 EPL-2.0","title":"aiservice_odh"},{"location":"roles/aiservice_odh/#aiservice_odh","text":"This role provides support to deploy odh components for AI Broker Application: Install Red Hat OpenShift Serverless Operator Install Red Hat OpenShift Service Mesh Operator Install Authorino Operator Install Open Data Hub Operator Create DSCInitialization instance Create Data Science Cluster Create Create Data Science Pipelines Application","title":"aiservice_odh"},{"location":"roles/aiservice_odh/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aiservice_odh/#tenantname","text":"Tenant identifier for the Open Data Hub deployment. Optional Environment Variable: AISERVICE_TENANT_NAME Default: user Purpose : Identifies the tenant for multi-tenant ODH deployments, used for resource isolation and organization. When to use : Override the default when deploying multiple ODH instances or when organizational naming conventions require specific tenant identifiers. Valid values : Alphanumeric string (e.g., user , team1 , prod-tenant ) Impact : Determines the tenant context for ODH resources and configurations. Related variables : None Notes : The default user is suitable for single-tenant deployments.","title":"tenantName"},{"location":"roles/aiservice_odh/#serverless_catalog_source","text":"Catalog source for Red Hat OpenShift Serverless Operator. Optional Environment Variable: SERVERLESS_CATALOG_SOURCE Default: redhat-operators Purpose : Specifies which operator catalog provides the OpenShift Serverless Operator required for ODH. When to use : Use default for standard deployments. Override for custom or mirrored catalogs in air-gapped environments. Valid values : Valid CatalogSource name (e.g., redhat-operators , custom-catalog ) Impact : Determines the source of the Serverless operator. Incorrect catalog will prevent operator installation. Related variables : serverless_channel Notes : - OpenShift Serverless is required for ODH data science pipelines - Verify catalog exists: oc get catalogsource -n openshift-marketplace","title":"serverless_catalog_source"},{"location":"roles/aiservice_odh/#serverless_channel","text":"Subscription channel for OpenShift Serverless Operator. Optional Environment Variable: SERVERLESS_CHANNEL Default: stable Purpose : Controls which version stream of OpenShift Serverless will be installed and receive updates. When to use : Use default stable for production. Override only for specific version requirements. Valid values : Valid Serverless operator channel (e.g., stable , stable-1.28 ) Impact : Determines the Serverless version installed and which automatic updates are received. Related variables : serverless_catalog_source Notes : The stable channel provides the latest stable release with automatic updates.","title":"serverless_channel"},{"location":"roles/aiservice_odh/#service_mesh_channel","text":"Subscription channel for Red Hat OpenShift Service Mesh Operator. Optional Environment Variable: SERVICEMESH_CHANNEL Default: stable Purpose : Controls which version stream of OpenShift Service Mesh will be installed for ODH networking. When to use : Use default stable for production. Override for specific version requirements. Valid values : Valid Service Mesh operator channel (e.g., stable , stable-2.3 ) Impact : Determines the Service Mesh version installed and which automatic updates are received. Related variables : service_mesh_catalog_source Notes : - Service Mesh is required for ODH model serving and networking - The stable channel provides the latest stable release","title":"service_mesh_channel"},{"location":"roles/aiservice_odh/#service_mesh_catalog_source","text":"Catalog source for Red Hat OpenShift Service Mesh Operator. Optional Environment Variable: SERVICEMESH_CATALOG_SOURCE Default: redhat-operators Purpose : Specifies which operator catalog provides the OpenShift Service Mesh Operator required for ODH. When to use : Use default for standard deployments. Override for custom or mirrored catalogs in air-gapped environments. Valid values : Valid CatalogSource name (e.g., redhat-operators , custom-catalog ) Impact : Determines the source of the Service Mesh operator. Incorrect catalog will prevent operator installation. Related variables : service_mesh_channel Notes : Verify catalog exists: oc get catalogsource -n openshift-marketplace","title":"service_mesh_catalog_source"},{"location":"roles/aiservice_odh/#authorino_catalog_source","text":"Catalog source for Authorino Operator. Optional Environment Variable: AUTHORINO_CATALOG_SOURCE Default: community-operators Purpose : Specifies which operator catalog provides the Authorino Operator for ODH authorization. When to use : Use default for standard deployments. Override for custom or mirrored catalogs in air-gapped environments. Valid values : Valid CatalogSource name (e.g., community-operators , custom-catalog ) Impact : Determines the source of the Authorino operator. Incorrect catalog will prevent operator installation. Related variables : None Notes : - Authorino provides authorization for ODH model serving - Available in community-operators catalog by default","title":"authorino_catalog_source"},{"location":"roles/aiservice_odh/#odh_channel","text":"Subscription channel for Open Data Hub Operator. Optional Environment Variable: ODH_CHANNEL Default: fast Purpose : Controls which version stream of Open Data Hub will be installed and receive updates. When to use : Use fast for latest features. Use stable for production environments requiring more testing. Valid values : Valid ODH operator channel (e.g., fast , stable ) Impact : Determines the ODH version installed and update frequency. fast channel receives updates more quickly than stable . Related variables : odh_catalog_source Notes : - fast channel provides latest features but may be less stable - stable channel recommended for production deployments","title":"odh_channel"},{"location":"roles/aiservice_odh/#odh_catalog_source","text":"Catalog source for Open Data Hub Operator. Optional Environment Variable: ODH_CATALOG_SOURCE Default: community-operators Purpose : Specifies which operator catalog provides the Open Data Hub Operator. When to use : Use default for standard deployments. Override for custom or mirrored catalogs in air-gapped environments. Valid values : Valid CatalogSource name (e.g., community-operators , custom-catalog ) Impact : Determines the source of the ODH operator. Incorrect catalog will prevent operator installation. Related variables : odh_channel Notes : - ODH is available in community-operators catalog - For air-gapped environments, mirror the catalog and update this variable","title":"odh_catalog_source"},{"location":"roles/aiservice_odh/#example-playbook","text":"- hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.aiservice_odh","title":"Example Playbook"},{"location":"roles/aiservice_odh/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aiservice_tenant/","text":"aiservice_tenant \u00a4 This role provides support to install and configure AI Broker: Install AI Broker api application Create, delete AI Broker tenant Create, delete AI Broker API Key Create, delete AWS S3 API Key Create, delete WatsonX AI API Key Role Variables \u00a4 tenant_action \u00a4 Action to perform on the AI Broker tenant. Optional Environment Variable: TENANT_ACTION Default: install Purpose : Controls whether to create/configure or remove an AI Broker tenant. When to use : Set to install for tenant creation/configuration. Use remove to delete a tenant. Valid values : - install - Create or configure AI Broker tenant (default) - remove - Delete AI Broker tenant Impact : Determines whether tenant resources are created or removed. Removal is permanent and deletes all tenant data. Related variables : tenantID Notes : Warning - remove action permanently deletes the tenant and all associated data. tenantID \u00a4 Unique identifier for the AI Broker tenant. Optional Environment Variable: AISERVICE_TENANT_ID Default: user Purpose : Identifies the AI Broker tenant for resource isolation and management in multi-tenant deployments. When to use : Override the default when creating multiple tenants or when organizational naming conventions require specific identifiers. Valid values : Alphanumeric string (e.g., user , team1 , prod-tenant ) Impact : Determines the tenant context for all AI Broker resources and API keys. Related variables : tenant_action , tenant_entitlement_type Notes : The default user is suitable for single-tenant deployments. Use descriptive names for multi-tenant environments. app_domain \u00a4 Application domain for AI Broker API endpoints. Optional Environment Variable: APP_DOMAIN Default: None (empty string) Purpose : Specifies the base domain for AI Broker application routes and API endpoints. When to use : Required for AI Broker installation. Must match the OpenShift cluster's application domain. Valid values : Valid domain string in format apps.domain (e.g., apps.cluster.example.com ) Impact : Determines the URLs for AI Broker API endpoints. Incorrect domain will prevent API access. Related variables : None Notes : - Get cluster domain: oc get ingress.config cluster -o jsonpath='{.spec.domain}' - Format must be the full apps domain (e.g., apps.mycluster.example.com ) aiservice_watsonx_action \u00a4 Action to perform on WatsonX AI integration. Optional Environment Variable: AISERVICE_WATSONX_ACTION Default: install Purpose : Controls whether to configure or remove WatsonX AI integration for the tenant. When to use : Set to install to configure WatsonX AI credentials. Use remove to delete WatsonX AI integration. Valid values : - install - Configure WatsonX AI integration (default) - remove - Remove WatsonX AI integration Impact : Determines whether WatsonX AI API keys and configuration are created or removed for the tenant. Related variables : aiservice_watsonxai_apikey , aiservice_watsonxai_url , aiservice_watsonxai_project_id Notes : WatsonX AI integration enables AI model access through IBM watsonx.ai platform. aiservice_watsonxai_apikey \u00a4 WatsonX AI API key for authentication. Optional Environment Variable: AISERVICE_WATSONXAI_APIKEY Default: None (empty string) Purpose : Provides authentication credentials for accessing WatsonX AI services. When to use : Required when aiservice_watsonx_action is install . Obtain from IBM Cloud watsonx.ai service. Valid values : Valid WatsonX AI API key string Impact : Enables AI Broker to authenticate with WatsonX AI services. Invalid key will prevent AI model access. Related variables : aiservice_watsonx_action , aiservice_watsonxai_url , aiservice_watsonxai_project_id Notes : - Security : Store securely, never commit to version control - Obtain from IBM Cloud console under watsonx.ai service credentials - API key is stored as Kubernetes secret aiservice_watsonxai_url \u00a4 WatsonX AI service endpoint URL. Optional Environment Variable: AISERVICE_WATSONXAI_URL Default: None (empty string) Purpose : Specifies the WatsonX AI API endpoint for model inference and management. When to use : Required when aiservice_watsonx_action is install . Obtain from IBM Cloud watsonx.ai service details. Valid values : Valid HTTPS URL to WatsonX AI endpoint (e.g., https://us-south.ml.cloud.ibm.com ) Impact : Determines which WatsonX AI region/endpoint the AI Broker connects to. Related variables : aiservice_watsonx_action , aiservice_watsonxai_apikey , aiservice_watsonxai_project_id Notes : - URL varies by IBM Cloud region - Common endpoints: https://us-south.ml.cloud.ibm.com , https://eu-de.ml.cloud.ibm.com - Verify endpoint in IBM Cloud watsonx.ai service details aiservice_watsonxai_project_id \u00a4 WatsonX AI project identifier. Optional Environment Variable: AISERVICE_WATSONXAI_PROJECT_ID Default: None (empty string) Purpose : Identifies the WatsonX AI project containing the AI models and resources to be accessed. When to use : Required when aiservice_watsonx_action is install . Obtain from IBM Cloud watsonx.ai project settings. Valid values : Valid WatsonX AI project ID (UUID format) Impact : Determines which WatsonX AI project's models and resources are accessible to the AI Broker tenant. Related variables : aiservice_watsonx_action , aiservice_watsonxai_apikey , aiservice_watsonxai_url Notes : - Find project ID in IBM Cloud watsonx.ai project settings - Format is typically a UUID (e.g., 12345678-1234-1234-1234-123456789012 ) - Project must exist before configuring AI Broker Entitlement Configuration \u00a4 Configure the tenant's entitlement to the AI Service for licensing and access control. tenant_entitlement_type \u00a4 Type of entitlement for the tenant. Optional Environment Variable: AISERVICE_TENANT_ENTITLEMENT_TYPE Default: None Purpose : Specifies the entitlement type for the tenant, controlling access levels and features. When to use : Set when configuring tenant entitlements for licensing or feature access control. Valid values : Valid entitlement type string (specific values depend on AI Service configuration) Impact : Determines which AI Service features and capacity the tenant can access. Related variables : tenant_entitlement_start_date , tenant_entitlement_end_date Notes : Entitlement types are defined by the AI Service deployment configuration. tenant_entitlement_start_date \u00a4 Start date for tenant entitlement period. Optional Environment Variable: AISERVICE_TENANT_ENTITLEMENT_START_DATE Default: None Purpose : Defines when the tenant's entitlement becomes active. When to use : Set when configuring time-bound entitlements for the tenant. Valid values : Date string in format YYYY-MM-DD (e.g., 2024-01-01 ) Impact : Tenant cannot access AI Service before this date. Related variables : tenant_entitlement_type , tenant_entitlement_end_date Notes : Must be in YYYY-MM-DD format. Ensure date is valid and in the past or present for immediate access. tenant_entitlement_end_date \u00a4 End date for tenant entitlement period. Optional Environment Variable: AISERVICE_TENANT_ENTITLEMENT_END_DATE Default: None Purpose : Defines when the tenant's entitlement expires. When to use : Set when configuring time-bound entitlements for the tenant. Valid values : Date string in format YYYY-MM-DD (e.g., 2024-12-31 ) Impact : Tenant cannot access AI Service after this date. Related variables : tenant_entitlement_type , tenant_entitlement_start_date Notes : - Must be in YYYY-MM-DD format - Should be after tenant_entitlement_start_date - Plan for entitlement renewal before expiration Example Playbook \u00a4 - hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.aiservice_tenant License \u00a4 EPL-2.0","title":"aiservice_tenant"},{"location":"roles/aiservice_tenant/#aiservice_tenant","text":"This role provides support to install and configure AI Broker: Install AI Broker api application Create, delete AI Broker tenant Create, delete AI Broker API Key Create, delete AWS S3 API Key Create, delete WatsonX AI API Key","title":"aiservice_tenant"},{"location":"roles/aiservice_tenant/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aiservice_tenant/#tenant_action","text":"Action to perform on the AI Broker tenant. Optional Environment Variable: TENANT_ACTION Default: install Purpose : Controls whether to create/configure or remove an AI Broker tenant. When to use : Set to install for tenant creation/configuration. Use remove to delete a tenant. Valid values : - install - Create or configure AI Broker tenant (default) - remove - Delete AI Broker tenant Impact : Determines whether tenant resources are created or removed. Removal is permanent and deletes all tenant data. Related variables : tenantID Notes : Warning - remove action permanently deletes the tenant and all associated data.","title":"tenant_action"},{"location":"roles/aiservice_tenant/#tenantid","text":"Unique identifier for the AI Broker tenant. Optional Environment Variable: AISERVICE_TENANT_ID Default: user Purpose : Identifies the AI Broker tenant for resource isolation and management in multi-tenant deployments. When to use : Override the default when creating multiple tenants or when organizational naming conventions require specific identifiers. Valid values : Alphanumeric string (e.g., user , team1 , prod-tenant ) Impact : Determines the tenant context for all AI Broker resources and API keys. Related variables : tenant_action , tenant_entitlement_type Notes : The default user is suitable for single-tenant deployments. Use descriptive names for multi-tenant environments.","title":"tenantID"},{"location":"roles/aiservice_tenant/#app_domain","text":"Application domain for AI Broker API endpoints. Optional Environment Variable: APP_DOMAIN Default: None (empty string) Purpose : Specifies the base domain for AI Broker application routes and API endpoints. When to use : Required for AI Broker installation. Must match the OpenShift cluster's application domain. Valid values : Valid domain string in format apps.domain (e.g., apps.cluster.example.com ) Impact : Determines the URLs for AI Broker API endpoints. Incorrect domain will prevent API access. Related variables : None Notes : - Get cluster domain: oc get ingress.config cluster -o jsonpath='{.spec.domain}' - Format must be the full apps domain (e.g., apps.mycluster.example.com )","title":"app_domain"},{"location":"roles/aiservice_tenant/#aiservice_watsonx_action","text":"Action to perform on WatsonX AI integration. Optional Environment Variable: AISERVICE_WATSONX_ACTION Default: install Purpose : Controls whether to configure or remove WatsonX AI integration for the tenant. When to use : Set to install to configure WatsonX AI credentials. Use remove to delete WatsonX AI integration. Valid values : - install - Configure WatsonX AI integration (default) - remove - Remove WatsonX AI integration Impact : Determines whether WatsonX AI API keys and configuration are created or removed for the tenant. Related variables : aiservice_watsonxai_apikey , aiservice_watsonxai_url , aiservice_watsonxai_project_id Notes : WatsonX AI integration enables AI model access through IBM watsonx.ai platform.","title":"aiservice_watsonx_action"},{"location":"roles/aiservice_tenant/#aiservice_watsonxai_apikey","text":"WatsonX AI API key for authentication. Optional Environment Variable: AISERVICE_WATSONXAI_APIKEY Default: None (empty string) Purpose : Provides authentication credentials for accessing WatsonX AI services. When to use : Required when aiservice_watsonx_action is install . Obtain from IBM Cloud watsonx.ai service. Valid values : Valid WatsonX AI API key string Impact : Enables AI Broker to authenticate with WatsonX AI services. Invalid key will prevent AI model access. Related variables : aiservice_watsonx_action , aiservice_watsonxai_url , aiservice_watsonxai_project_id Notes : - Security : Store securely, never commit to version control - Obtain from IBM Cloud console under watsonx.ai service credentials - API key is stored as Kubernetes secret","title":"aiservice_watsonxai_apikey"},{"location":"roles/aiservice_tenant/#aiservice_watsonxai_url","text":"WatsonX AI service endpoint URL. Optional Environment Variable: AISERVICE_WATSONXAI_URL Default: None (empty string) Purpose : Specifies the WatsonX AI API endpoint for model inference and management. When to use : Required when aiservice_watsonx_action is install . Obtain from IBM Cloud watsonx.ai service details. Valid values : Valid HTTPS URL to WatsonX AI endpoint (e.g., https://us-south.ml.cloud.ibm.com ) Impact : Determines which WatsonX AI region/endpoint the AI Broker connects to. Related variables : aiservice_watsonx_action , aiservice_watsonxai_apikey , aiservice_watsonxai_project_id Notes : - URL varies by IBM Cloud region - Common endpoints: https://us-south.ml.cloud.ibm.com , https://eu-de.ml.cloud.ibm.com - Verify endpoint in IBM Cloud watsonx.ai service details","title":"aiservice_watsonxai_url"},{"location":"roles/aiservice_tenant/#aiservice_watsonxai_project_id","text":"WatsonX AI project identifier. Optional Environment Variable: AISERVICE_WATSONXAI_PROJECT_ID Default: None (empty string) Purpose : Identifies the WatsonX AI project containing the AI models and resources to be accessed. When to use : Required when aiservice_watsonx_action is install . Obtain from IBM Cloud watsonx.ai project settings. Valid values : Valid WatsonX AI project ID (UUID format) Impact : Determines which WatsonX AI project's models and resources are accessible to the AI Broker tenant. Related variables : aiservice_watsonx_action , aiservice_watsonxai_apikey , aiservice_watsonxai_url Notes : - Find project ID in IBM Cloud watsonx.ai project settings - Format is typically a UUID (e.g., 12345678-1234-1234-1234-123456789012 ) - Project must exist before configuring AI Broker","title":"aiservice_watsonxai_project_id"},{"location":"roles/aiservice_tenant/#entitlement-configuration","text":"Configure the tenant's entitlement to the AI Service for licensing and access control.","title":"Entitlement Configuration"},{"location":"roles/aiservice_tenant/#tenant_entitlement_type","text":"Type of entitlement for the tenant. Optional Environment Variable: AISERVICE_TENANT_ENTITLEMENT_TYPE Default: None Purpose : Specifies the entitlement type for the tenant, controlling access levels and features. When to use : Set when configuring tenant entitlements for licensing or feature access control. Valid values : Valid entitlement type string (specific values depend on AI Service configuration) Impact : Determines which AI Service features and capacity the tenant can access. Related variables : tenant_entitlement_start_date , tenant_entitlement_end_date Notes : Entitlement types are defined by the AI Service deployment configuration.","title":"tenant_entitlement_type"},{"location":"roles/aiservice_tenant/#tenant_entitlement_start_date","text":"Start date for tenant entitlement period. Optional Environment Variable: AISERVICE_TENANT_ENTITLEMENT_START_DATE Default: None Purpose : Defines when the tenant's entitlement becomes active. When to use : Set when configuring time-bound entitlements for the tenant. Valid values : Date string in format YYYY-MM-DD (e.g., 2024-01-01 ) Impact : Tenant cannot access AI Service before this date. Related variables : tenant_entitlement_type , tenant_entitlement_end_date Notes : Must be in YYYY-MM-DD format. Ensure date is valid and in the past or present for immediate access.","title":"tenant_entitlement_start_date"},{"location":"roles/aiservice_tenant/#tenant_entitlement_end_date","text":"End date for tenant entitlement period. Optional Environment Variable: AISERVICE_TENANT_ENTITLEMENT_END_DATE Default: None Purpose : Defines when the tenant's entitlement expires. When to use : Set when configuring time-bound entitlements for the tenant. Valid values : Date string in format YYYY-MM-DD (e.g., 2024-12-31 ) Impact : Tenant cannot access AI Service after this date. Related variables : tenant_entitlement_type , tenant_entitlement_start_date Notes : - Must be in YYYY-MM-DD format - Should be after tenant_entitlement_start_date - Plan for entitlement renewal before expiration","title":"tenant_entitlement_end_date"},{"location":"roles/aiservice_tenant/#example-playbook","text":"- hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.aiservice_tenant","title":"Example Playbook"},{"location":"roles/aiservice_tenant/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ansible_version_check/","text":"ansible_version_check \u00a4 Internal-use role that all other roles in the collection declare a dependency upon to ensure that the minimum supported level of Ansible is used. Role Variables \u00a4 This role has no configurable variables. Example Playbook \u00a4 - hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.ansible_version_check License \u00a4 EPL-2.0","title":"ansible_version_check"},{"location":"roles/ansible_version_check/#ansible_version_check","text":"Internal-use role that all other roles in the collection declare a dependency upon to ensure that the minimum supported level of Ansible is used.","title":"ansible_version_check"},{"location":"roles/ansible_version_check/#role-variables","text":"This role has no configurable variables.","title":"Role Variables"},{"location":"roles/ansible_version_check/#example-playbook","text":"- hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.ansible_version_check","title":"Example Playbook"},{"location":"roles/ansible_version_check/#license","text":"EPL-2.0","title":"License"},{"location":"roles/arcgis/","text":"arcgis \u00a4 Installs IBM Maximo Location Services for Esri . This dependency is an alternative solution if you are planning to leverage geospatial and map features with Maximo Spatial. The biggest benefit of using it is that you could have both IBM Maximo Location Services for Esri and Maximo Spatial deployed and running into the same cluster, which improves significantly your overall networking performance. Note: IBM Maximo Location Services for Esri will make use of MAS cluster issuers while managing internal and public certificates thus, you while using suite_dns to setup cluster issuer and public certificates for your MAS instances, these are automatically reused for your instance of IBM Maximo Location Services for Esri . Role Variables - General \u00a4 Deployment details \u00a4 Here are the full deployment details for a default installation, considering number of running pods, cpu/memory and storage utilization: oc get deployments -n mas-$MAS_INSTANCE_ID-arcgis NAME READY UP-TO-DATE AVAILABLE AGE arcgis-enterprise-apps 1/1 1 1 101m arcgis-enterprise-manager 1/1 1 1 114m arcgis-enterprise-portal 1/1 1 1 101m arcgis-enterprise-web-style-app 1/1 1 1 101m arcgis-featureserver-webhook-processor 1/1 1 1 74m arcgis-gpserver-webhook-processor 1/1 1 1 74m arcgis-help 1/1 1 1 114m arcgis-ingress-controller 1/1 1 1 115m arcgis-javascript-api 1/1 1 1 101m arcgis-ki7vnxghb8526ejnxiqcf-mapserver 1/1 1 1 82m arcgis-kphc76hvhto7lzv74xxts-featureserver 1/1 1 1 73m arcgis-kvrl4t01w78hbbyl1fsof-mapserver 1/1 1 1 77m arcgis-private-ingress-controller 2/2 2 2 106m arcgis-rest-administrator-api 1/1 1 1 114m arcgis-rest-services-api 1/1 1 1 97m arcgis-service-lifecycle-manager 1/1 1 1 97m arcgis-system-cachingcontrollers-gpserver 1/1 1 1 89m arcgis-system-cachingcontrollers-gpsyncserver 1/1 1 1 89m arcgis-system-cachingtools-gpserver 1/1 1 1 89m arcgis-system-cachingtools-gpsyncserver 1/1 1 1 89m arcgis-system-featureservicetools-gpserver 1/1 1 1 84m arcgis-system-featureservicetools-gpsyncserver 1/1 1 1 84m arcgis-system-publishingtools-gpserver 1/1 1 1 89m arcgis-system-publishingtools-gpsyncserver 3/3 3 3 89m arcgis-system-reportingtools-gpserver 1/1 1 1 89m arcgis-system-spatialanalysistools-gpserver 1/1 1 1 82m arcgis-system-spatialanalysistools-gpsyncserver 1/1 1 1 82m arcgis-system-synctools-gpserver 1/1 1 1 84m arcgis-system-synctools-gpsyncserver 1/1 1 1 84m arcgis-utilities-geocodingtools-gpserver 1/1 1 1 80m arcgis-utilities-geocodingtools-gpsyncserver 1/1 1 1 80m arcgis-utilities-geometry-geometryserver 1/1 1 1 81m arcgis-utilities-offlinepackaging-gpserver 1/1 1 1 79m arcgis-utilities-offlinepackaging-gpsyncserver 1/1 1 1 79m arcgis-utilities-printingtools-gpserver 1/1 1 1 80m arcgis-utilities-symbols-symbolserver- 1/1 1 1 79m ibm-mas-arcgis-entitymgr-ws 1/1 1 1 118m ibm-mas-arcgis-operator 1/1 1 1 121m Total of 49 running pods. oc adm top pods -n mas-$MAS_INSTANCE_ID-arcgis NAME CPU(cores) MEMORY(bytes) arcgis-enterprise-apps 1m 105Mi arcgis-enterprise-manager 0m 132Mi arcgis-enterprise-portal 1m 144Mi arcgis-enterprise-web-style-app 1m 80Mi arcgis-featureserver-webhook-processor 2m 426Mi arcgis-gpserver-webhook-processor 5m 438Mi arcgis-help 1m 96Mi arcgis-in-memory-store 5m 378Mi arcgis-ingress-controller 2m 122Mi arcgis-javascript-api 0m 12Mi arcgis-ki7vnxghb8526ejnxiqcf-mapserver 8m 1102Mi arcgis-kphc76hvhto7lzv74xxts-featureserver 4m 2250Mi arcgis-kvrl4t01w78hbbyl1fsof-mapserver 8m 934Mi arcgis-object-store 29m 3355Mi arcgis-private-ingress-controller 4m 114Mi arcgis-private-ingress-controller 2m 113Mi arcgis-queue-store-cgatl-0 16m 186Mi arcgis-relational-store-pfxpx-mcap-0 14m 894Mi arcgis-relational-store-pfxpx-yjnr-0 4m 687Mi arcgis-rest-administrator-api 26m 706Mi arcgis-rest-metrics-api-nmbtw-0 4m 62Mi arcgis-rest-portal-api-rpcnv-0 4m 678Mi arcgis-rest-services-api 24m 876Mi arcgis-service-lifecycle-manager 6m 744Mi arcgis-spatiotemporal-index-store-dejcm-coordinator-0 4m 3612Mi arcgis-system-cachingcontrollers-gpserver 12m 919Mi arcgis-system-cachingcontrollers-gpsyncserver 2m 1179Mi arcgis-system-cachingtools-gpserver 7m 906Mi arcgis-system-cachingtools-gpsyncserver 1m 1273Mi arcgis-system-featureservicetools-gpserver 7m 985Mi arcgis-system-featureservicetools-gpsyncserver 6m 977Mi arcgis-system-publishingtools-gpserver 13m 1043Mi arcgis-system-publishingtools-gpsyncserver 8m 939Mi arcgis-system-publishingtools-gpsyncserver 10m 1189Mi arcgis-system-publishingtools-gpsyncserver 16m 891Mi arcgis-system-reportingtools-gpserver 20m 636Mi arcgis-system-spatialanalysistools-gpserver 8m 982Mi arcgis-system-spatialanalysistools-gpsyncserver 14m 927Mi arcgis-system-synctools-gpserver 11m 1093Mi arcgis-system-synctools-gpsyncserver 23m 960Mi arcgis-utilities-geocodingtools-gpserver 24m 959Mi arcgis-utilities-geocodingtools-gpsyncserver 10m 1189Mi arcgis-utilities-geometry-geometryserver 6m 1053Mi arcgis-utilities-offlinepackaging-gpserver 19m 983Mi arcgis-utilities-offlinepackaging-gpsyncserver 6m 914Mi arcgis-utilities-printingtools-gpserver 21m 1323Mi arcgis-utilities-symbols-symbolserver 9m 580Mi ibm-mas-arcgis-entitymgr-ws 1222m 201Mi ibm-mas-arcgis-operator 0m 48Mi Average of 1650 milicores (1.65 vCPUs) and 40 gigabytes of memory RAM. oc get pvc -n mas-$MAS_INSTANCE_ID-arcgis NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE arcgis-in-memory-store-feiz3-0-data-volume Bound pvc-432e190c-dbe4-44d5-828b-bf8126a326fe 20Gi RWO ibmc-block-gold 108m arcgis-rest-portal-api-rpcnv-0-portal-sharing-volume Bound pvc-a1201747-6b7e-42b8-beed-c842c21cfa01 20Gi RWO ibmc-block-gold 62m data-volume-arcgis-object-store-o0vq5-awsrx-0 Bound pvc-6c97716f-20d4-46b2-a110-706546dda95d 32Gi RWO ibmc-block-gold 108m data-volume-arcgis-relational-store-pfxpx-mcap-0 Bound pvc-28635771-0e4a-475d-b6aa-2012c48dd470 20Gi RWO ibmc-block-gold 111m data-volume-arcgis-relational-store-pfxpx-yjnr-0 Bound pvc-0e2dbc22-11af-4c5c-b5e7-b00d21a060fe 20Gi RWO ibmc-block-gold 100m data-volume-arcgis-spatiotemporal-index-store-dejcm-coordinator-0 Bound pvc-2e0ca0f0-c830-4353-abf0-196ce0e75b87 20Gi RWO ibmc-block-gold 108m prometheus-volume-arcgis-rest-metrics-api-nmbtw-0 Bound pvc-fbc5c0ce-26eb-441f-8161-2191fd113a80 30Gi RWO ibmc-block-gold 108m queue-data-volume-arcgis-queue-store-cgatl-0 Bound pvc-93451297-0e3d-4a56-bf4e-cff9bda43fb7 20Gi RWO ibmc-block-gold 108m Average of 182 gigabyes of required capacity. Role Variables - Installation \u00a4 ibm_entitlement_key \u00a4 IBM entitlement key for accessing container images. Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Authenticates access to IBM container registry for pulling IBM Maximo Location Services for Esri images. When to use : Required for all ArcGIS installations. Obtain from IBM Container Library . Valid values : Valid IBM entitlement key string from your IBM account. Impact : Without a valid key, the ArcGIS operator and component images cannot be pulled and installation will fail. Related variables : None Notes : - Keep the entitlement key secure and do not commit it to version control - The key is associated with your IBM ID and product entitlements - Verify key validity before deployment to avoid installation failures mas_catalog_source \u00a4 Catalog source for MAS operator installation. Optional Environment Variable: MAS_CATALOG_SOURCE Default: ibm-operator-catalog Purpose : Specifies which operator catalog to use for installing IBM Maximo Location Services for Esri. When to use : The default is appropriate for both release and development installations. Override only if using a custom or mirrored catalog. Valid values : - ibm-operator-catalog (default) - Standard IBM operator catalog - Custom catalog name for airgap or development environments Impact : Determines the source of operator images and available versions. Related variables : mas_arcgis_channel Notes : For airgap installations, ensure the catalog has been properly mirrored with all required ArcGIS images. mas_arcgis_channel \u00a4 Subscription channel for IBM Maximo Location Services for Esri operator. Optional Environment Variable: MAS_ARCGIS_CHANNEL Default: 9.1.x Purpose : Controls which version stream of IBM Maximo Location Services for Esri will be installed and receive updates. When to use : Override the default when you need a specific version or want to control upgrade timing. The channel determines which updates are automatically applied. Valid values : Version-specific channels (e.g., 9.1.x , 9.0.x ) Impact : Determines the ArcGIS version installed and which automatic updates are received. Changing channels may trigger upgrades. Related variables : mas_catalog_source Notes : - The 9.1.x channel receives updates within the 9.1 version stream - Review release notes before changing channels - Channel changes may require operator restarts Role Variables - MAS Configuration \u00a4 mas_instance_id \u00a4 MAS instance identifier for ArcGIS deployment. Optional Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Associates the IBM Maximo Location Services for Esri deployment with a specific MAS instance. When to use : Required when deploying ArcGIS for a specific MAS instance. The ArcGIS namespace will be created as mas-<instance-id>-arcgis . Valid values : Valid MAS instance ID (lowercase alphanumeric, max 12 characters) Impact : Determines the namespace where ArcGIS will be deployed and which MAS instance it will integrate with. Related variables : None Notes : - The deployment namespace will be mas-<mas_instance_id>-arcgis - ArcGIS will automatically use the MAS cluster issuers for certificate management - Ensure the MAS instance exists before deploying ArcGIS custom_labels \u00a4 Custom labels to apply to ArcGIS instance resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Enables tagging of ArcGIS resources with custom metadata for organization, tracking, or automation purposes. When to use : When you need to apply organizational labels for cost tracking, environment identification, or resource management. Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=spatial,cost-center=12345 ) Impact : Labels are applied to instance-specific ArcGIS resources for identification and filtering. Related variables : None Notes : - Labels must follow Kubernetes label syntax (alphanumeric, hyphens, underscores, dots) - Useful for cost allocation, resource queries, and automation scripts - Applied to all 49 ArcGIS deployment resources Example Playbooks Install IBM Maximo Location Services for Esri \u00a4 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxx roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.arcgis Example Playbook \u00a4 - hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.arcgis License \u00a4 EPL-2.0","title":"arcgis"},{"location":"roles/arcgis/#arcgis","text":"Installs IBM Maximo Location Services for Esri . This dependency is an alternative solution if you are planning to leverage geospatial and map features with Maximo Spatial. The biggest benefit of using it is that you could have both IBM Maximo Location Services for Esri and Maximo Spatial deployed and running into the same cluster, which improves significantly your overall networking performance. Note: IBM Maximo Location Services for Esri will make use of MAS cluster issuers while managing internal and public certificates thus, you while using suite_dns to setup cluster issuer and public certificates for your MAS instances, these are automatically reused for your instance of IBM Maximo Location Services for Esri .","title":"arcgis"},{"location":"roles/arcgis/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/arcgis/#deployment-details","text":"Here are the full deployment details for a default installation, considering number of running pods, cpu/memory and storage utilization: oc get deployments -n mas-$MAS_INSTANCE_ID-arcgis NAME READY UP-TO-DATE AVAILABLE AGE arcgis-enterprise-apps 1/1 1 1 101m arcgis-enterprise-manager 1/1 1 1 114m arcgis-enterprise-portal 1/1 1 1 101m arcgis-enterprise-web-style-app 1/1 1 1 101m arcgis-featureserver-webhook-processor 1/1 1 1 74m arcgis-gpserver-webhook-processor 1/1 1 1 74m arcgis-help 1/1 1 1 114m arcgis-ingress-controller 1/1 1 1 115m arcgis-javascript-api 1/1 1 1 101m arcgis-ki7vnxghb8526ejnxiqcf-mapserver 1/1 1 1 82m arcgis-kphc76hvhto7lzv74xxts-featureserver 1/1 1 1 73m arcgis-kvrl4t01w78hbbyl1fsof-mapserver 1/1 1 1 77m arcgis-private-ingress-controller 2/2 2 2 106m arcgis-rest-administrator-api 1/1 1 1 114m arcgis-rest-services-api 1/1 1 1 97m arcgis-service-lifecycle-manager 1/1 1 1 97m arcgis-system-cachingcontrollers-gpserver 1/1 1 1 89m arcgis-system-cachingcontrollers-gpsyncserver 1/1 1 1 89m arcgis-system-cachingtools-gpserver 1/1 1 1 89m arcgis-system-cachingtools-gpsyncserver 1/1 1 1 89m arcgis-system-featureservicetools-gpserver 1/1 1 1 84m arcgis-system-featureservicetools-gpsyncserver 1/1 1 1 84m arcgis-system-publishingtools-gpserver 1/1 1 1 89m arcgis-system-publishingtools-gpsyncserver 3/3 3 3 89m arcgis-system-reportingtools-gpserver 1/1 1 1 89m arcgis-system-spatialanalysistools-gpserver 1/1 1 1 82m arcgis-system-spatialanalysistools-gpsyncserver 1/1 1 1 82m arcgis-system-synctools-gpserver 1/1 1 1 84m arcgis-system-synctools-gpsyncserver 1/1 1 1 84m arcgis-utilities-geocodingtools-gpserver 1/1 1 1 80m arcgis-utilities-geocodingtools-gpsyncserver 1/1 1 1 80m arcgis-utilities-geometry-geometryserver 1/1 1 1 81m arcgis-utilities-offlinepackaging-gpserver 1/1 1 1 79m arcgis-utilities-offlinepackaging-gpsyncserver 1/1 1 1 79m arcgis-utilities-printingtools-gpserver 1/1 1 1 80m arcgis-utilities-symbols-symbolserver- 1/1 1 1 79m ibm-mas-arcgis-entitymgr-ws 1/1 1 1 118m ibm-mas-arcgis-operator 1/1 1 1 121m Total of 49 running pods. oc adm top pods -n mas-$MAS_INSTANCE_ID-arcgis NAME CPU(cores) MEMORY(bytes) arcgis-enterprise-apps 1m 105Mi arcgis-enterprise-manager 0m 132Mi arcgis-enterprise-portal 1m 144Mi arcgis-enterprise-web-style-app 1m 80Mi arcgis-featureserver-webhook-processor 2m 426Mi arcgis-gpserver-webhook-processor 5m 438Mi arcgis-help 1m 96Mi arcgis-in-memory-store 5m 378Mi arcgis-ingress-controller 2m 122Mi arcgis-javascript-api 0m 12Mi arcgis-ki7vnxghb8526ejnxiqcf-mapserver 8m 1102Mi arcgis-kphc76hvhto7lzv74xxts-featureserver 4m 2250Mi arcgis-kvrl4t01w78hbbyl1fsof-mapserver 8m 934Mi arcgis-object-store 29m 3355Mi arcgis-private-ingress-controller 4m 114Mi arcgis-private-ingress-controller 2m 113Mi arcgis-queue-store-cgatl-0 16m 186Mi arcgis-relational-store-pfxpx-mcap-0 14m 894Mi arcgis-relational-store-pfxpx-yjnr-0 4m 687Mi arcgis-rest-administrator-api 26m 706Mi arcgis-rest-metrics-api-nmbtw-0 4m 62Mi arcgis-rest-portal-api-rpcnv-0 4m 678Mi arcgis-rest-services-api 24m 876Mi arcgis-service-lifecycle-manager 6m 744Mi arcgis-spatiotemporal-index-store-dejcm-coordinator-0 4m 3612Mi arcgis-system-cachingcontrollers-gpserver 12m 919Mi arcgis-system-cachingcontrollers-gpsyncserver 2m 1179Mi arcgis-system-cachingtools-gpserver 7m 906Mi arcgis-system-cachingtools-gpsyncserver 1m 1273Mi arcgis-system-featureservicetools-gpserver 7m 985Mi arcgis-system-featureservicetools-gpsyncserver 6m 977Mi arcgis-system-publishingtools-gpserver 13m 1043Mi arcgis-system-publishingtools-gpsyncserver 8m 939Mi arcgis-system-publishingtools-gpsyncserver 10m 1189Mi arcgis-system-publishingtools-gpsyncserver 16m 891Mi arcgis-system-reportingtools-gpserver 20m 636Mi arcgis-system-spatialanalysistools-gpserver 8m 982Mi arcgis-system-spatialanalysistools-gpsyncserver 14m 927Mi arcgis-system-synctools-gpserver 11m 1093Mi arcgis-system-synctools-gpsyncserver 23m 960Mi arcgis-utilities-geocodingtools-gpserver 24m 959Mi arcgis-utilities-geocodingtools-gpsyncserver 10m 1189Mi arcgis-utilities-geometry-geometryserver 6m 1053Mi arcgis-utilities-offlinepackaging-gpserver 19m 983Mi arcgis-utilities-offlinepackaging-gpsyncserver 6m 914Mi arcgis-utilities-printingtools-gpserver 21m 1323Mi arcgis-utilities-symbols-symbolserver 9m 580Mi ibm-mas-arcgis-entitymgr-ws 1222m 201Mi ibm-mas-arcgis-operator 0m 48Mi Average of 1650 milicores (1.65 vCPUs) and 40 gigabytes of memory RAM. oc get pvc -n mas-$MAS_INSTANCE_ID-arcgis NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE arcgis-in-memory-store-feiz3-0-data-volume Bound pvc-432e190c-dbe4-44d5-828b-bf8126a326fe 20Gi RWO ibmc-block-gold 108m arcgis-rest-portal-api-rpcnv-0-portal-sharing-volume Bound pvc-a1201747-6b7e-42b8-beed-c842c21cfa01 20Gi RWO ibmc-block-gold 62m data-volume-arcgis-object-store-o0vq5-awsrx-0 Bound pvc-6c97716f-20d4-46b2-a110-706546dda95d 32Gi RWO ibmc-block-gold 108m data-volume-arcgis-relational-store-pfxpx-mcap-0 Bound pvc-28635771-0e4a-475d-b6aa-2012c48dd470 20Gi RWO ibmc-block-gold 111m data-volume-arcgis-relational-store-pfxpx-yjnr-0 Bound pvc-0e2dbc22-11af-4c5c-b5e7-b00d21a060fe 20Gi RWO ibmc-block-gold 100m data-volume-arcgis-spatiotemporal-index-store-dejcm-coordinator-0 Bound pvc-2e0ca0f0-c830-4353-abf0-196ce0e75b87 20Gi RWO ibmc-block-gold 108m prometheus-volume-arcgis-rest-metrics-api-nmbtw-0 Bound pvc-fbc5c0ce-26eb-441f-8161-2191fd113a80 30Gi RWO ibmc-block-gold 108m queue-data-volume-arcgis-queue-store-cgatl-0 Bound pvc-93451297-0e3d-4a56-bf4e-cff9bda43fb7 20Gi RWO ibmc-block-gold 108m Average of 182 gigabyes of required capacity.","title":"Deployment details"},{"location":"roles/arcgis/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/arcgis/#ibm_entitlement_key","text":"IBM entitlement key for accessing container images. Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Authenticates access to IBM container registry for pulling IBM Maximo Location Services for Esri images. When to use : Required for all ArcGIS installations. Obtain from IBM Container Library . Valid values : Valid IBM entitlement key string from your IBM account. Impact : Without a valid key, the ArcGIS operator and component images cannot be pulled and installation will fail. Related variables : None Notes : - Keep the entitlement key secure and do not commit it to version control - The key is associated with your IBM ID and product entitlements - Verify key validity before deployment to avoid installation failures","title":"ibm_entitlement_key"},{"location":"roles/arcgis/#mas_catalog_source","text":"Catalog source for MAS operator installation. Optional Environment Variable: MAS_CATALOG_SOURCE Default: ibm-operator-catalog Purpose : Specifies which operator catalog to use for installing IBM Maximo Location Services for Esri. When to use : The default is appropriate for both release and development installations. Override only if using a custom or mirrored catalog. Valid values : - ibm-operator-catalog (default) - Standard IBM operator catalog - Custom catalog name for airgap or development environments Impact : Determines the source of operator images and available versions. Related variables : mas_arcgis_channel Notes : For airgap installations, ensure the catalog has been properly mirrored with all required ArcGIS images.","title":"mas_catalog_source"},{"location":"roles/arcgis/#mas_arcgis_channel","text":"Subscription channel for IBM Maximo Location Services for Esri operator. Optional Environment Variable: MAS_ARCGIS_CHANNEL Default: 9.1.x Purpose : Controls which version stream of IBM Maximo Location Services for Esri will be installed and receive updates. When to use : Override the default when you need a specific version or want to control upgrade timing. The channel determines which updates are automatically applied. Valid values : Version-specific channels (e.g., 9.1.x , 9.0.x ) Impact : Determines the ArcGIS version installed and which automatic updates are received. Changing channels may trigger upgrades. Related variables : mas_catalog_source Notes : - The 9.1.x channel receives updates within the 9.1 version stream - Review release notes before changing channels - Channel changes may require operator restarts","title":"mas_arcgis_channel"},{"location":"roles/arcgis/#role-variables-mas-configuration","text":"","title":"Role Variables - MAS Configuration"},{"location":"roles/arcgis/#mas_instance_id","text":"MAS instance identifier for ArcGIS deployment. Optional Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Associates the IBM Maximo Location Services for Esri deployment with a specific MAS instance. When to use : Required when deploying ArcGIS for a specific MAS instance. The ArcGIS namespace will be created as mas-<instance-id>-arcgis . Valid values : Valid MAS instance ID (lowercase alphanumeric, max 12 characters) Impact : Determines the namespace where ArcGIS will be deployed and which MAS instance it will integrate with. Related variables : None Notes : - The deployment namespace will be mas-<mas_instance_id>-arcgis - ArcGIS will automatically use the MAS cluster issuers for certificate management - Ensure the MAS instance exists before deploying ArcGIS","title":"mas_instance_id"},{"location":"roles/arcgis/#custom_labels","text":"Custom labels to apply to ArcGIS instance resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Enables tagging of ArcGIS resources with custom metadata for organization, tracking, or automation purposes. When to use : When you need to apply organizational labels for cost tracking, environment identification, or resource management. Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=spatial,cost-center=12345 ) Impact : Labels are applied to instance-specific ArcGIS resources for identification and filtering. Related variables : None Notes : - Labels must follow Kubernetes label syntax (alphanumeric, hyphens, underscores, dots) - Useful for cost allocation, resource queries, and automation scripts - Applied to all 49 ArcGIS deployment resources Example Playbooks","title":"custom_labels"},{"location":"roles/arcgis/#install-ibm-maximo-location-services-for-esri","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxx roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.arcgis","title":"Install IBM Maximo Location Services for Esri"},{"location":"roles/arcgis/#example-playbook","text":"- hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.arcgis","title":"Example Playbook"},{"location":"roles/arcgis/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_bucket_access_point/","text":"aws_bucket_access_point \u00a4 Create AWS S3 bucket access points with controlled permissions for secure, isolated access to S3 buckets. Access points simplify managing data access at scale by providing dedicated endpoints with specific permissions, enabling fine-grained access control without modifying bucket policies. This role creates an access point associated with an existing S3 bucket and configures a read-only access policy for a specified AWS user or account. Prerequisites : - AWS CLI must be installed - AWS credentials configured via aws configure or environment variables ( AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY ) - Target S3 bucket must already exist Role Variables \u00a4 aws_access_point_name \u00a4 Name for the S3 bucket access point. Optional Environment Variable: AWS_ACCESS_POINT_NAME Default: access-point-c1 Purpose : Provides a unique identifier for the access point within your AWS account. When to use : Customize to reflect the purpose or user of the access point (e.g., mas-readonly-access , backup-access-point ). Valid values : Valid S3 access point name (3-50 characters, lowercase letters, numbers, hyphens). Impact : Access point name is used in the access point ARN and alias for connecting to the bucket. Related variables : aws_access_point_bucket_name , aws_access_point_username Notes : - Must be unique within the AWS account and region - Cannot be changed after creation - Use descriptive names for easier management - Access point alias format: <access-point-name>-<account-id>.s3-accesspoint.<region>.amazonaws.com aws_access_point_bucket_name \u00a4 Name of the existing S3 bucket to associate with the access point. Required Environment Variable: COS_BUCKET_NAME Default: None Purpose : Specifies which S3 bucket the access point will provide access to. When to use : Always required. Must be an existing bucket in the same AWS account and region. Valid values : Valid S3 bucket name that exists in your AWS account. Impact : The access point will provide controlled access to this bucket's objects based on the access point policy. Related variables : aws_access_point_region , aws_access_point_name Notes : - Bucket must exist before creating the access point - Bucket and access point must be in the same region - Multiple access points can be created for the same bucket - Verify bucket name: aws s3 ls s3://<bucket-name> aws_access_point_region \u00a4 AWS region where the bucket and access point are located. Optional Environment Variable: AWS_REGION Default: us-east-2 Purpose : Specifies the AWS region for the access point. Must match the bucket's region. When to use : Set to match your S3 bucket's region. Default is suitable for US East deployments. Valid values : Valid AWS region code (e.g., us-east-1 , us-east-2 , us-west-2 , eu-west-1 ). Impact : Access point must be in the same region as the bucket. Cross-region access points are not supported. Related variables : aws_access_point_bucket_name Notes : - Critical : Must match the bucket's region exactly - Verify bucket region: aws s3api get-bucket-location --bucket <bucket-name> - Access point ARN includes the region - Network latency depends on region proximity to users aws_access_point_username \u00a4 AWS IAM user or account ID granted access through the access point. Required Environment Variable: AWS_ACCESS_POINT_USERNAME Default: None Purpose : Specifies which AWS IAM user or account is allowed to access the bucket through this access point. When to use : Always required. Provide the IAM username or AWS account ID that needs read-only access to the bucket. Valid values : - IAM username (e.g., backup-user , readonly-service ) - AWS account ID (12-digit number) Impact : The specified user/account will have read-only permissions ( s3:GetObject , s3:ListBucket ) to bucket objects through the access point. Related variables : aws_access_point_policy_actions Notes : - Default policy grants read-only access ( s3:GetObject , s3:ListBucket ) - User must exist in the AWS account - Access point policy is separate from bucket policy - User still needs IAM permissions to use the access point - For cross-account access, provide the account ID aws_access_point_policy_actions \u00a4 List of S3 actions allowed through the access point. Optional (defined in defaults) Environment Variable: None (hardcoded in defaults) Default: [\"s3:GetObject\", \"s3:ListBucket\"] Purpose : Defines which S3 operations are permitted through the access point policy. When to use : Default read-only actions are suitable for most use cases. Modify in defaults file if different permissions are needed. Valid values : List of valid S3 action strings. Default actions: - s3:GetObject - Read object data - s3:ListBucket - List bucket contents Impact : Controls what operations the specified user can perform on bucket objects through the access point. Related variables : aws_access_point_username Notes : - Default provides read-only access - To grant write access, add actions like s3:PutObject , s3:DeleteObject - Access point policy works in conjunction with IAM and bucket policies - Most restrictive policy applies - Modify in defaults/main.yml if custom permissions needed Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_access_point_name: \"{{ lookup('env', 'AWS_ACCESS_POINT_NAME') | default('access-point-c1', True) }}\" aws_access_point_bucket_name: \"{{ lookup('env', 'COS_BUCKET_NAME') }}\" aws_access_point_region: \"{{ lookup('env', 'AWS_REGION') | default('us-east-2', True) }}\" aws_access_point_username: \"{{ lookup('env', 'AWS_ACCESS_POINT_USERNAME') }}\" roles: - ibm.mas_devops.aws_bucket_access_point Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_ACCESS_POINT_NAME=my-aws-access-point export COS_BUCKET_NAME=my-aws-bucket export AWS_ACCESS_POINT_USERNAME=my-aws-username ROLE_NAME=aws_bucket_access_point ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aws_bucket_access_point"},{"location":"roles/aws_bucket_access_point/#aws_bucket_access_point","text":"Create AWS S3 bucket access points with controlled permissions for secure, isolated access to S3 buckets. Access points simplify managing data access at scale by providing dedicated endpoints with specific permissions, enabling fine-grained access control without modifying bucket policies. This role creates an access point associated with an existing S3 bucket and configures a read-only access policy for a specified AWS user or account. Prerequisites : - AWS CLI must be installed - AWS credentials configured via aws configure or environment variables ( AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY ) - Target S3 bucket must already exist","title":"aws_bucket_access_point"},{"location":"roles/aws_bucket_access_point/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_bucket_access_point/#aws_access_point_name","text":"Name for the S3 bucket access point. Optional Environment Variable: AWS_ACCESS_POINT_NAME Default: access-point-c1 Purpose : Provides a unique identifier for the access point within your AWS account. When to use : Customize to reflect the purpose or user of the access point (e.g., mas-readonly-access , backup-access-point ). Valid values : Valid S3 access point name (3-50 characters, lowercase letters, numbers, hyphens). Impact : Access point name is used in the access point ARN and alias for connecting to the bucket. Related variables : aws_access_point_bucket_name , aws_access_point_username Notes : - Must be unique within the AWS account and region - Cannot be changed after creation - Use descriptive names for easier management - Access point alias format: <access-point-name>-<account-id>.s3-accesspoint.<region>.amazonaws.com","title":"aws_access_point_name"},{"location":"roles/aws_bucket_access_point/#aws_access_point_bucket_name","text":"Name of the existing S3 bucket to associate with the access point. Required Environment Variable: COS_BUCKET_NAME Default: None Purpose : Specifies which S3 bucket the access point will provide access to. When to use : Always required. Must be an existing bucket in the same AWS account and region. Valid values : Valid S3 bucket name that exists in your AWS account. Impact : The access point will provide controlled access to this bucket's objects based on the access point policy. Related variables : aws_access_point_region , aws_access_point_name Notes : - Bucket must exist before creating the access point - Bucket and access point must be in the same region - Multiple access points can be created for the same bucket - Verify bucket name: aws s3 ls s3://<bucket-name>","title":"aws_access_point_bucket_name"},{"location":"roles/aws_bucket_access_point/#aws_access_point_region","text":"AWS region where the bucket and access point are located. Optional Environment Variable: AWS_REGION Default: us-east-2 Purpose : Specifies the AWS region for the access point. Must match the bucket's region. When to use : Set to match your S3 bucket's region. Default is suitable for US East deployments. Valid values : Valid AWS region code (e.g., us-east-1 , us-east-2 , us-west-2 , eu-west-1 ). Impact : Access point must be in the same region as the bucket. Cross-region access points are not supported. Related variables : aws_access_point_bucket_name Notes : - Critical : Must match the bucket's region exactly - Verify bucket region: aws s3api get-bucket-location --bucket <bucket-name> - Access point ARN includes the region - Network latency depends on region proximity to users","title":"aws_access_point_region"},{"location":"roles/aws_bucket_access_point/#aws_access_point_username","text":"AWS IAM user or account ID granted access through the access point. Required Environment Variable: AWS_ACCESS_POINT_USERNAME Default: None Purpose : Specifies which AWS IAM user or account is allowed to access the bucket through this access point. When to use : Always required. Provide the IAM username or AWS account ID that needs read-only access to the bucket. Valid values : - IAM username (e.g., backup-user , readonly-service ) - AWS account ID (12-digit number) Impact : The specified user/account will have read-only permissions ( s3:GetObject , s3:ListBucket ) to bucket objects through the access point. Related variables : aws_access_point_policy_actions Notes : - Default policy grants read-only access ( s3:GetObject , s3:ListBucket ) - User must exist in the AWS account - Access point policy is separate from bucket policy - User still needs IAM permissions to use the access point - For cross-account access, provide the account ID","title":"aws_access_point_username"},{"location":"roles/aws_bucket_access_point/#aws_access_point_policy_actions","text":"List of S3 actions allowed through the access point. Optional (defined in defaults) Environment Variable: None (hardcoded in defaults) Default: [\"s3:GetObject\", \"s3:ListBucket\"] Purpose : Defines which S3 operations are permitted through the access point policy. When to use : Default read-only actions are suitable for most use cases. Modify in defaults file if different permissions are needed. Valid values : List of valid S3 action strings. Default actions: - s3:GetObject - Read object data - s3:ListBucket - List bucket contents Impact : Controls what operations the specified user can perform on bucket objects through the access point. Related variables : aws_access_point_username Notes : - Default provides read-only access - To grant write access, add actions like s3:PutObject , s3:DeleteObject - Access point policy works in conjunction with IAM and bucket policies - Most restrictive policy applies - Modify in defaults/main.yml if custom permissions needed","title":"aws_access_point_policy_actions"},{"location":"roles/aws_bucket_access_point/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_access_point_name: \"{{ lookup('env', 'AWS_ACCESS_POINT_NAME') | default('access-point-c1', True) }}\" aws_access_point_bucket_name: \"{{ lookup('env', 'COS_BUCKET_NAME') }}\" aws_access_point_region: \"{{ lookup('env', 'AWS_REGION') | default('us-east-2', True) }}\" aws_access_point_username: \"{{ lookup('env', 'AWS_ACCESS_POINT_USERNAME') }}\" roles: - ibm.mas_devops.aws_bucket_access_point","title":"Example Playbook"},{"location":"roles/aws_bucket_access_point/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_ACCESS_POINT_NAME=my-aws-access-point export COS_BUCKET_NAME=my-aws-bucket export AWS_ACCESS_POINT_USERNAME=my-aws-username ROLE_NAME=aws_bucket_access_point ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aws_bucket_access_point/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_documentdb_user/","text":"aws_documentdb_user \u00a4 Create dedicated database users in AWS DocumentDB for Maximo Application Suite instances. This role automates user creation with appropriate permissions and generates Kubernetes secrets containing the credentials for MAS MongoDB configuration. AWS DocumentDB is a MongoDB-compatible database service that can serve as the system database for MAS. This role creates instance-specific users with proper authentication credentials. Prerequisites : - MongoDB Shell (mongosh) must be installed - AWS DocumentDB cluster must be running and accessible - Master user credentials for DocumentDB must be available Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier for which the DocumentDB user will be created. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies the MAS instance and is used to generate the DocumentDB username and Kubernetes secret name. When to use : Always required. Must match the MAS instance that will use this DocumentDB. Valid values : Valid MAS instance ID (typically 3-12 lowercase alphanumeric characters). Impact : Used to create username format <instance-id>-docdb-user and secret name docdb-<instance-id>-instance-credentials . Related variables : mas_config_dir Notes : - Username will be <mas_instance_id>-docdb-user - Secret name will be docdb-<mas_instance_id>-instance-credentials - Must match the instance ID used during MAS installation docdb_host \u00a4 AWS DocumentDB cluster endpoint hostname. Required if docdb_hosts is not set Environment Variable: DOCDB_HOST Default: None Purpose : Specifies the DocumentDB cluster endpoint for single-host connection configuration. When to use : Use for simple single-endpoint connections. For replica sets with multiple endpoints, use docdb_hosts instead. Valid values : Valid DocumentDB cluster endpoint hostname (e.g., docdb-cluster.abc123.us-east-1.docdb.amazonaws.com ). Impact : Combined with docdb_port to form the connection string. If docdb_hosts is also set, docdb_hosts takes precedence. Related variables : docdb_port , docdb_hosts Notes : - Use cluster endpoint for automatic failover - Instance endpoints can also be used but don't provide automatic failover - docdb_hosts takes precedence if both are set - Obtain from AWS DocumentDB console or CLI docdb_port \u00a4 AWS DocumentDB connection port. Required if docdb_hosts is not set Environment Variable: DOCDB_PORT Default: None Purpose : Specifies the port number for DocumentDB connections. When to use : Required when using docdb_host . Not needed if using docdb_hosts (port included in hosts string). Valid values : Valid port number, typically 27017 (default MongoDB port). Impact : Combined with docdb_host to form the connection string. Related variables : docdb_host , docdb_hosts Notes : - Default DocumentDB port is 27017 - Must match the port configured in DocumentDB cluster - Not used if docdb_hosts is set docdb_hosts \u00a4 AWS DocumentDB connection string with multiple hosts and ports. Required if both docdb_host and docdb_port are not set Environment Variable: DOCDB_HOSTS Default: None Purpose : Provides a complete connection string with multiple DocumentDB endpoints for replica set configurations. When to use : Use for replica set deployments with multiple endpoints. Takes precedence over docdb_host and docdb_port if all are set. Valid values : Comma-separated list of host:port pairs (e.g., docdb-1.abc.us-east-1.docdb.amazonaws.com:27017,docdb-2.abc.us-east-1.docdb.amazonaws.com:27017,docdb-3.abc.us-east-1.docdb.amazonaws.com:27017 ). Impact : Enables connection to multiple DocumentDB instances for high availability and automatic failover. Related variables : docdb_host , docdb_port Notes : - Recommended for production deployments - Takes precedence over docdb_host and docdb_port - Include all replica set members for best availability - Format: host1:port1,host2:port2,host3:port3 - Obtain from AWS DocumentDB cluster details docdb_master_username \u00a4 AWS DocumentDB master username for administrative access. Required Environment Variable: DOCDB_MASTER_USERNAME Default: None Purpose : Provides the master user credentials to create the MAS-specific database user. When to use : Always required. Must be the master username configured during DocumentDB cluster creation. Valid values : Valid DocumentDB master username. Impact : Used to authenticate to DocumentDB and create the new MAS user. Must have permissions to create users and grant roles. Related variables : docdb_master_password Notes : - This is the master user created with the DocumentDB cluster - Credentials are only used during user creation, not stored permanently - Ensure master user has userAdmin or equivalent permissions - Obtain from AWS Secrets Manager or secure credential store docdb_master_password \u00a4 AWS DocumentDB master password for administrative access. Required Environment Variable: DOCDB_MASTER_PASSWORD Default: None Purpose : Provides the master user password to authenticate and create the MAS-specific database user. When to use : Always required. Must be the master password configured during DocumentDB cluster creation. Valid values : Valid DocumentDB master password string. Impact : Used to authenticate to DocumentDB and create the new MAS user. Related variables : docdb_master_username Notes : - Store securely and never commit to version control - Credentials are only used during user creation - Consider using AWS Secrets Manager for credential management - Rotate master password regularly per security best practices docdb_instance_password \u00a4 Password for the MAS-specific DocumentDB user being created. Optional Environment Variable: DOCDB_INSTANCE_PASSWORD Default: Auto-generated if not provided Purpose : Specifies the password for the new MAS DocumentDB user. If not provided, a secure password is automatically generated. When to use : Provide if you need a specific password. Otherwise, let the role generate a secure random password. Valid values : Strong password string meeting DocumentDB password requirements. Impact : This password will be stored in the Kubernetes secret and used by MAS to connect to DocumentDB. Related variables : mas_instance_id , mas_config_dir Notes : - Auto-generation is recommended for security - If provided, ensure it meets complexity requirements - Password is stored in Kubernetes secret docdb-<instance-id>-instance-credentials - Keep password secure and rotate regularly user_action \u00a4 Action to perform on the DocumentDB user. Optional Environment Variable: USER_ACTION Default: add Purpose : Controls whether to create or remove the DocumentDB user. When to use : Set to add to create user (default), or remove to delete the user. Valid values : - add - Create the DocumentDB user (default) - remove - Delete the DocumentDB user Impact : Determines whether the role creates or removes the user from DocumentDB. Related variables : mas_instance_id Notes : - Default add is for normal user creation - Use remove when decommissioning a MAS instance - Removing user does not delete the Kubernetes secret - Verify user removal: mongosh to DocumentDB and check users mas_config_dir \u00a4 Local directory where the generated Kubernetes secret YAML will be saved. Optional Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the output directory for the generated Kubernetes secret containing DocumentDB credentials. When to use : Set when you want to save the generated secret YAML for later application or documentation. Valid values : Valid directory path where the Ansible controller has write permissions. Impact : If set, a YAML file with the DocumentDB credentials secret will be created in this directory. Related variables : mas_instance_id Notes : - Secret filename: docdb-<instance-id>-instance-credentials.yaml - Can be applied with: oc apply -f <file> - Keep generated files secure as they contain credentials - Useful for GitOps workflows or manual secret management - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 docdb_master_username: test-user docdb_master_password: test-pass-*** docdb_host: test1.aws-01.... docdb_port: 27017 roles: - ibm.mas_devops.aws_documentdb_user - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 docdb_master_username: test-user docdb_master_password: test-pass-*** docdb_hosts: test1.aws-01:27017,test1.aws-02:27017,test1.aws-03:27017 roles: - ibm.mas_devops.aws_documentdb_user Example Playbook \u00a4 - hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.aws_documentdb_user License \u00a4 EPL-2.0","title":"aws_documentdb_user"},{"location":"roles/aws_documentdb_user/#aws_documentdb_user","text":"Create dedicated database users in AWS DocumentDB for Maximo Application Suite instances. This role automates user creation with appropriate permissions and generates Kubernetes secrets containing the credentials for MAS MongoDB configuration. AWS DocumentDB is a MongoDB-compatible database service that can serve as the system database for MAS. This role creates instance-specific users with proper authentication credentials. Prerequisites : - MongoDB Shell (mongosh) must be installed - AWS DocumentDB cluster must be running and accessible - Master user credentials for DocumentDB must be available","title":"aws_documentdb_user"},{"location":"roles/aws_documentdb_user/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_documentdb_user/#mas_instance_id","text":"MAS instance identifier for which the DocumentDB user will be created. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies the MAS instance and is used to generate the DocumentDB username and Kubernetes secret name. When to use : Always required. Must match the MAS instance that will use this DocumentDB. Valid values : Valid MAS instance ID (typically 3-12 lowercase alphanumeric characters). Impact : Used to create username format <instance-id>-docdb-user and secret name docdb-<instance-id>-instance-credentials . Related variables : mas_config_dir Notes : - Username will be <mas_instance_id>-docdb-user - Secret name will be docdb-<mas_instance_id>-instance-credentials - Must match the instance ID used during MAS installation","title":"mas_instance_id"},{"location":"roles/aws_documentdb_user/#docdb_host","text":"AWS DocumentDB cluster endpoint hostname. Required if docdb_hosts is not set Environment Variable: DOCDB_HOST Default: None Purpose : Specifies the DocumentDB cluster endpoint for single-host connection configuration. When to use : Use for simple single-endpoint connections. For replica sets with multiple endpoints, use docdb_hosts instead. Valid values : Valid DocumentDB cluster endpoint hostname (e.g., docdb-cluster.abc123.us-east-1.docdb.amazonaws.com ). Impact : Combined with docdb_port to form the connection string. If docdb_hosts is also set, docdb_hosts takes precedence. Related variables : docdb_port , docdb_hosts Notes : - Use cluster endpoint for automatic failover - Instance endpoints can also be used but don't provide automatic failover - docdb_hosts takes precedence if both are set - Obtain from AWS DocumentDB console or CLI","title":"docdb_host"},{"location":"roles/aws_documentdb_user/#docdb_port","text":"AWS DocumentDB connection port. Required if docdb_hosts is not set Environment Variable: DOCDB_PORT Default: None Purpose : Specifies the port number for DocumentDB connections. When to use : Required when using docdb_host . Not needed if using docdb_hosts (port included in hosts string). Valid values : Valid port number, typically 27017 (default MongoDB port). Impact : Combined with docdb_host to form the connection string. Related variables : docdb_host , docdb_hosts Notes : - Default DocumentDB port is 27017 - Must match the port configured in DocumentDB cluster - Not used if docdb_hosts is set","title":"docdb_port"},{"location":"roles/aws_documentdb_user/#docdb_hosts","text":"AWS DocumentDB connection string with multiple hosts and ports. Required if both docdb_host and docdb_port are not set Environment Variable: DOCDB_HOSTS Default: None Purpose : Provides a complete connection string with multiple DocumentDB endpoints for replica set configurations. When to use : Use for replica set deployments with multiple endpoints. Takes precedence over docdb_host and docdb_port if all are set. Valid values : Comma-separated list of host:port pairs (e.g., docdb-1.abc.us-east-1.docdb.amazonaws.com:27017,docdb-2.abc.us-east-1.docdb.amazonaws.com:27017,docdb-3.abc.us-east-1.docdb.amazonaws.com:27017 ). Impact : Enables connection to multiple DocumentDB instances for high availability and automatic failover. Related variables : docdb_host , docdb_port Notes : - Recommended for production deployments - Takes precedence over docdb_host and docdb_port - Include all replica set members for best availability - Format: host1:port1,host2:port2,host3:port3 - Obtain from AWS DocumentDB cluster details","title":"docdb_hosts"},{"location":"roles/aws_documentdb_user/#docdb_master_username","text":"AWS DocumentDB master username for administrative access. Required Environment Variable: DOCDB_MASTER_USERNAME Default: None Purpose : Provides the master user credentials to create the MAS-specific database user. When to use : Always required. Must be the master username configured during DocumentDB cluster creation. Valid values : Valid DocumentDB master username. Impact : Used to authenticate to DocumentDB and create the new MAS user. Must have permissions to create users and grant roles. Related variables : docdb_master_password Notes : - This is the master user created with the DocumentDB cluster - Credentials are only used during user creation, not stored permanently - Ensure master user has userAdmin or equivalent permissions - Obtain from AWS Secrets Manager or secure credential store","title":"docdb_master_username"},{"location":"roles/aws_documentdb_user/#docdb_master_password","text":"AWS DocumentDB master password for administrative access. Required Environment Variable: DOCDB_MASTER_PASSWORD Default: None Purpose : Provides the master user password to authenticate and create the MAS-specific database user. When to use : Always required. Must be the master password configured during DocumentDB cluster creation. Valid values : Valid DocumentDB master password string. Impact : Used to authenticate to DocumentDB and create the new MAS user. Related variables : docdb_master_username Notes : - Store securely and never commit to version control - Credentials are only used during user creation - Consider using AWS Secrets Manager for credential management - Rotate master password regularly per security best practices","title":"docdb_master_password"},{"location":"roles/aws_documentdb_user/#docdb_instance_password","text":"Password for the MAS-specific DocumentDB user being created. Optional Environment Variable: DOCDB_INSTANCE_PASSWORD Default: Auto-generated if not provided Purpose : Specifies the password for the new MAS DocumentDB user. If not provided, a secure password is automatically generated. When to use : Provide if you need a specific password. Otherwise, let the role generate a secure random password. Valid values : Strong password string meeting DocumentDB password requirements. Impact : This password will be stored in the Kubernetes secret and used by MAS to connect to DocumentDB. Related variables : mas_instance_id , mas_config_dir Notes : - Auto-generation is recommended for security - If provided, ensure it meets complexity requirements - Password is stored in Kubernetes secret docdb-<instance-id>-instance-credentials - Keep password secure and rotate regularly","title":"docdb_instance_password"},{"location":"roles/aws_documentdb_user/#user_action","text":"Action to perform on the DocumentDB user. Optional Environment Variable: USER_ACTION Default: add Purpose : Controls whether to create or remove the DocumentDB user. When to use : Set to add to create user (default), or remove to delete the user. Valid values : - add - Create the DocumentDB user (default) - remove - Delete the DocumentDB user Impact : Determines whether the role creates or removes the user from DocumentDB. Related variables : mas_instance_id Notes : - Default add is for normal user creation - Use remove when decommissioning a MAS instance - Removing user does not delete the Kubernetes secret - Verify user removal: mongosh to DocumentDB and check users","title":"user_action"},{"location":"roles/aws_documentdb_user/#mas_config_dir","text":"Local directory where the generated Kubernetes secret YAML will be saved. Optional Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the output directory for the generated Kubernetes secret containing DocumentDB credentials. When to use : Set when you want to save the generated secret YAML for later application or documentation. Valid values : Valid directory path where the Ansible controller has write permissions. Impact : If set, a YAML file with the DocumentDB credentials secret will be created in this directory. Related variables : mas_instance_id Notes : - Secret filename: docdb-<instance-id>-instance-credentials.yaml - Can be applied with: oc apply -f <file> - Keep generated files secure as they contain credentials - Useful for GitOps workflows or manual secret management - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 docdb_master_username: test-user docdb_master_password: test-pass-*** docdb_host: test1.aws-01.... docdb_port: 27017 roles: - ibm.mas_devops.aws_documentdb_user - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 docdb_master_username: test-user docdb_master_password: test-pass-*** docdb_hosts: test1.aws-01:27017,test1.aws-02:27017,test1.aws-03:27017 roles: - ibm.mas_devops.aws_documentdb_user","title":"mas_config_dir"},{"location":"roles/aws_documentdb_user/#example-playbook","text":"- hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.aws_documentdb_user","title":"Example Playbook"},{"location":"roles/aws_documentdb_user/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_policy/","text":"aws_policy \u00a4 Create AWS IAM policies from JSON policy documents in your AWS account. This role automates IAM policy creation, enabling you to define fine-grained permissions for AWS resources and services used by MAS deployments. IAM policies define permissions for AWS identities (users, groups, roles) to access AWS resources. This role creates customer-managed policies that can be attached to IAM users or roles. Prerequisites : - AWS CLI must be installed - AWS credentials configured via aws configure or environment variables ( AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY ) - IAM permissions to create policies Role Variables \u00a4 aws_policy_name \u00a4 Name for the IAM policy to be created. Required Environment Variable: AWS_POLICY_NAME Default: None Purpose : Provides a unique identifier for the IAM policy within your AWS account. When to use : Always required. Use descriptive names that reflect the policy's purpose (e.g., MAS-S3-ReadOnly , DocumentDB-Access-Policy ). Valid values : Valid IAM policy name (alphanumeric characters, plus +=,.@-_ symbols, 1-128 characters). Impact : Policy name is used in the policy ARN and for attaching to IAM users/roles. Related variables : aws_policy_json_file_path_local Notes : - Must be unique within the AWS account - Cannot be changed after creation - Use naming conventions for easier management (e.g., <service>-<purpose>-Policy ) - Policy ARN format: arn:aws:iam::<account-id>:policy/<policy-name> aws_policy_json_file_path_local \u00a4 Local file path to the IAM policy JSON document. Required Environment Variable: AWS_POLICY_JSON_FILE_PATH_LOCAL Default: None Purpose : Specifies the location of the JSON file containing the IAM policy document that defines permissions. When to use : Always required. Must point to a valid JSON file with proper IAM policy syntax. Valid values : Absolute or relative file path to a valid JSON policy document (e.g., /tmp/my-policy.json , ./policies/s3-access.json ). Impact : The policy document defines what actions are allowed or denied on which AWS resources. Related variables : aws_policy_name Notes : - File must be accessible from the Ansible controller - Must follow AWS IAM policy JSON syntax - Sample template available in role's /files/policy-template-sample.json - Validate policy syntax before applying: aws iam validate-policy-document --policy-document file://policy.json - Policy document structure: json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"], \"Resource\": [\"arn:aws:s3:::my-bucket/*\"] } ] } - Common policy elements: - Version : Policy language version (always 2012-10-17 ) - Statement : Array of permission statements - Effect : Allow or Deny - Action : AWS service actions (e.g., s3:GetObject ) - Resource : ARNs of resources the policy applies to - Condition : Optional conditions for when policy applies Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_policy: \"{{ lookup('env', 'AWS_POLICY_NAME') }}\" aws_policy_json_file_path_local: \"{{ lookup('env', 'AWS_POLICY_JSON_FILE_PATH_LOCAL') }}\" roles: - ibm.mas_devops.aws_policy Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_POLICY_NAME=my-aws-policy export AWS_POLICY_JSON_FILE_PATH_LOCAL=/tmp/local/my-aws-policy.json ROLE_NAME=aws_policy ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aws_policy"},{"location":"roles/aws_policy/#aws_policy","text":"Create AWS IAM policies from JSON policy documents in your AWS account. This role automates IAM policy creation, enabling you to define fine-grained permissions for AWS resources and services used by MAS deployments. IAM policies define permissions for AWS identities (users, groups, roles) to access AWS resources. This role creates customer-managed policies that can be attached to IAM users or roles. Prerequisites : - AWS CLI must be installed - AWS credentials configured via aws configure or environment variables ( AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY ) - IAM permissions to create policies","title":"aws_policy"},{"location":"roles/aws_policy/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_policy/#aws_policy_name","text":"Name for the IAM policy to be created. Required Environment Variable: AWS_POLICY_NAME Default: None Purpose : Provides a unique identifier for the IAM policy within your AWS account. When to use : Always required. Use descriptive names that reflect the policy's purpose (e.g., MAS-S3-ReadOnly , DocumentDB-Access-Policy ). Valid values : Valid IAM policy name (alphanumeric characters, plus +=,.@-_ symbols, 1-128 characters). Impact : Policy name is used in the policy ARN and for attaching to IAM users/roles. Related variables : aws_policy_json_file_path_local Notes : - Must be unique within the AWS account - Cannot be changed after creation - Use naming conventions for easier management (e.g., <service>-<purpose>-Policy ) - Policy ARN format: arn:aws:iam::<account-id>:policy/<policy-name>","title":"aws_policy_name"},{"location":"roles/aws_policy/#aws_policy_json_file_path_local","text":"Local file path to the IAM policy JSON document. Required Environment Variable: AWS_POLICY_JSON_FILE_PATH_LOCAL Default: None Purpose : Specifies the location of the JSON file containing the IAM policy document that defines permissions. When to use : Always required. Must point to a valid JSON file with proper IAM policy syntax. Valid values : Absolute or relative file path to a valid JSON policy document (e.g., /tmp/my-policy.json , ./policies/s3-access.json ). Impact : The policy document defines what actions are allowed or denied on which AWS resources. Related variables : aws_policy_name Notes : - File must be accessible from the Ansible controller - Must follow AWS IAM policy JSON syntax - Sample template available in role's /files/policy-template-sample.json - Validate policy syntax before applying: aws iam validate-policy-document --policy-document file://policy.json - Policy document structure: json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"], \"Resource\": [\"arn:aws:s3:::my-bucket/*\"] } ] } - Common policy elements: - Version : Policy language version (always 2012-10-17 ) - Statement : Array of permission statements - Effect : Allow or Deny - Action : AWS service actions (e.g., s3:GetObject ) - Resource : ARNs of resources the policy applies to - Condition : Optional conditions for when policy applies","title":"aws_policy_json_file_path_local"},{"location":"roles/aws_policy/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_policy: \"{{ lookup('env', 'AWS_POLICY_NAME') }}\" aws_policy_json_file_path_local: \"{{ lookup('env', 'AWS_POLICY_JSON_FILE_PATH_LOCAL') }}\" roles: - ibm.mas_devops.aws_policy","title":"Example Playbook"},{"location":"roles/aws_policy/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_POLICY_NAME=my-aws-policy export AWS_POLICY_JSON_FILE_PATH_LOCAL=/tmp/local/my-aws-policy.json ROLE_NAME=aws_policy ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aws_policy/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_route53/","text":"aws_route53 \u00a4 This role will create an AWS Route53 public hosted zone in the targeted AWS Account. For further details on how to create and configure an AWS Route53 instance, refer to AWS Route53 documentation . Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Role Variables \u00a4 route53_hosted_zone_name \u00a4 The domain name for the Route53 public hosted zone to be created. Required Environment Variable: ROUTE53_HOSTED_ZONE_NAME Default Value: None Purpose : Defines the DNS domain name that will be managed by the Route53 hosted zone. This zone will contain DNS records for routing traffic to your MAS deployment. When to use : Always required when creating a Route53 hosted zone. The domain name should match your organization's domain or subdomain that you want to use for MAS endpoints. Valid values : - Any valid domain name format (e.g., mycompany.com , mas.example.org ) - Can be a root domain or subdomain - Must be unique within your AWS account - Should not include protocol (http/https) or trailing slashes Impact : - Creates a public hosted zone with this domain name in Route53 - Generates NS (nameserver) records that must be configured with your domain registrar - All DNS records for MAS endpoints will be created under this domain - Cannot be changed after creation without recreating the hosted zone Related variables : - route53_hosted_zone_region - Specifies the AWS region for the hosted zone Notes : - After creation, you must update your domain registrar's nameservers to point to the Route53 NS records - The hosted zone will incur AWS charges based on the number of hosted zones and queries - Ensure you have ownership/control of the domain before creating the hosted zone - For production deployments, consider using a subdomain (e.g., mas.mycompany.com ) to isolate MAS DNS records route53_hosted_zone_region \u00a4 The AWS region where the Route53 hosted zone will be created. Required Environment Variable: ROUTE53_HOSTED_ZONE_REGION Default Value: Value from AWS_REGION environment variable, or us-east-2 if not set Purpose : Specifies the AWS region for creating the Route53 hosted zone. While Route53 is a global service, the API calls and zone metadata are associated with a specific region. When to use : Should match the region where your primary MAS infrastructure is deployed, or use the default region configured in your AWS CLI. Valid values : - Any valid AWS region identifier (e.g., us-east-1 , us-west-2 , eu-west-1 , ap-southeast-1 ) - Must be a region where Route53 service is available (all standard AWS regions) - Defaults to us-east-2 if neither this variable nor AWS_REGION is set Impact : - Determines which regional Route53 API endpoint is used for zone creation - Does not affect DNS query routing (Route53 is globally distributed) - Should align with your AWS CLI configuration for consistency - May affect API latency for zone management operations Related variables : - route53_hosted_zone_name - The domain name for the hosted zone - Inherits from AWS_REGION environment variable if not explicitly set Notes : - Route53 hosted zones are globally accessible regardless of the creation region - The region setting primarily affects API operations, not DNS resolution - For multi-region deployments, you only need one hosted zone (not per region) - Ensure your AWS credentials have Route53 permissions in the specified region Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: route53_hosted_zone_name: \"{{ lookup('env', 'ROUTE53_HOSTED_ZONE_NAME') }}\" # mycompany.com route53_hosted_zone_region: \"{{ lookup('env', 'ROUTE53_HOSTED_ZONE_REGION') }}\" # us-east-2 roles: - ibm.mas_devops.aws_route53 Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export ROUTE53_HOSTED_ZONE_NAME=mycompany.com export ROUTE53_HOSTED_ZONE_REGION=us-east-2 ROLE_NAME=aws_route53 ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aws_route53"},{"location":"roles/aws_route53/#aws_route53","text":"This role will create an AWS Route53 public hosted zone in the targeted AWS Account. For further details on how to create and configure an AWS Route53 instance, refer to AWS Route53 documentation .","title":"aws_route53"},{"location":"roles/aws_route53/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/aws_route53/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_route53/#route53_hosted_zone_name","text":"The domain name for the Route53 public hosted zone to be created. Required Environment Variable: ROUTE53_HOSTED_ZONE_NAME Default Value: None Purpose : Defines the DNS domain name that will be managed by the Route53 hosted zone. This zone will contain DNS records for routing traffic to your MAS deployment. When to use : Always required when creating a Route53 hosted zone. The domain name should match your organization's domain or subdomain that you want to use for MAS endpoints. Valid values : - Any valid domain name format (e.g., mycompany.com , mas.example.org ) - Can be a root domain or subdomain - Must be unique within your AWS account - Should not include protocol (http/https) or trailing slashes Impact : - Creates a public hosted zone with this domain name in Route53 - Generates NS (nameserver) records that must be configured with your domain registrar - All DNS records for MAS endpoints will be created under this domain - Cannot be changed after creation without recreating the hosted zone Related variables : - route53_hosted_zone_region - Specifies the AWS region for the hosted zone Notes : - After creation, you must update your domain registrar's nameservers to point to the Route53 NS records - The hosted zone will incur AWS charges based on the number of hosted zones and queries - Ensure you have ownership/control of the domain before creating the hosted zone - For production deployments, consider using a subdomain (e.g., mas.mycompany.com ) to isolate MAS DNS records","title":"route53_hosted_zone_name"},{"location":"roles/aws_route53/#route53_hosted_zone_region","text":"The AWS region where the Route53 hosted zone will be created. Required Environment Variable: ROUTE53_HOSTED_ZONE_REGION Default Value: Value from AWS_REGION environment variable, or us-east-2 if not set Purpose : Specifies the AWS region for creating the Route53 hosted zone. While Route53 is a global service, the API calls and zone metadata are associated with a specific region. When to use : Should match the region where your primary MAS infrastructure is deployed, or use the default region configured in your AWS CLI. Valid values : - Any valid AWS region identifier (e.g., us-east-1 , us-west-2 , eu-west-1 , ap-southeast-1 ) - Must be a region where Route53 service is available (all standard AWS regions) - Defaults to us-east-2 if neither this variable nor AWS_REGION is set Impact : - Determines which regional Route53 API endpoint is used for zone creation - Does not affect DNS query routing (Route53 is globally distributed) - Should align with your AWS CLI configuration for consistency - May affect API latency for zone management operations Related variables : - route53_hosted_zone_name - The domain name for the hosted zone - Inherits from AWS_REGION environment variable if not explicitly set Notes : - Route53 hosted zones are globally accessible regardless of the creation region - The region setting primarily affects API operations, not DNS resolution - For multi-region deployments, you only need one hosted zone (not per region) - Ensure your AWS credentials have Route53 permissions in the specified region","title":"route53_hosted_zone_region"},{"location":"roles/aws_route53/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: route53_hosted_zone_name: \"{{ lookup('env', 'ROUTE53_HOSTED_ZONE_NAME') }}\" # mycompany.com route53_hosted_zone_region: \"{{ lookup('env', 'ROUTE53_HOSTED_ZONE_REGION') }}\" # us-east-2 roles: - ibm.mas_devops.aws_route53","title":"Example Playbook"},{"location":"roles/aws_route53/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export ROUTE53_HOSTED_ZONE_NAME=mycompany.com export ROUTE53_HOSTED_ZONE_REGION=us-east-2 ROLE_NAME=aws_route53 ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aws_route53/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_user_creation/","text":"aws_user_creation \u00a4 This role will create an AWS IAM Username and corresponding IAM Access Key ID and Secret Access Key in the targeted AWS account. Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Role Variables \u00a4 aws_username \u00a4 The IAM username to create in AWS. Required Environment Variable: AWS_USERNAME Default Value: None Purpose : Specifies the name of the IAM user to be created in your AWS account. This user will be used for programmatic access to AWS services required by MAS. When to use : Always required when creating a new IAM user. The username should follow your organization's naming conventions and clearly identify its purpose (e.g., mas-service-user , maximo-automation ). Valid values : - 1-64 characters - Alphanumeric characters, plus signs (+), equals signs (=), commas (,), periods (.), at signs (@), underscores (_), and hyphens (-) - Must be unique within your AWS account - Case-sensitive Impact : - Creates a new IAM user with this name in your AWS account - The user can be assigned policies and access keys for authentication - Username cannot be changed after creation (requires deletion and recreation) - Will appear in AWS CloudTrail logs for audit purposes Related variables : - aws_username_create_access_key_flag - Controls access key creation - aws_policy_arn - Policy to attach to the user - aws_username_access_key_id - Existing access key (optional) - aws_username_secret_access_key - Existing secret key (optional) Notes : - Follow the principle of least privilege when assigning permissions - Consider using a descriptive name that indicates the user's purpose - IAM users incur no direct costs but their actions may incur AWS service charges - For production, consider using IAM roles with temporary credentials instead of long-lived access keys aws_username_create_access_key_flag \u00a4 Controls whether to create new IAM access keys for the user. Optional Environment Variable: AWS_USERNAME_CREATE_ACCESS_KEY_FLAG Default Value: True Purpose : Determines if the role should generate a new IAM Access Key ID and Secret Access Key pair for the created user. Access keys enable programmatic access to AWS services. When to use : - Set to True (default) when creating a new user that needs programmatic access - Set to False when you only need to create the user identity without immediate access keys - Automatically set to False if both aws_username_access_key_id and aws_username_secret_access_key are provided Valid values : - True - Create new access key pair - False - Do not create access keys Impact : - When True : Generates new access key credentials that must be securely stored - When False : User is created but cannot authenticate programmatically until keys are added - Access keys are displayed only once during creation and cannot be retrieved later - Each IAM user can have a maximum of 2 active access keys Related variables : - aws_username - The user for which keys are created - aws_username_access_key_id - Overrides key creation if provided - aws_username_secret_access_key - Overrides key creation if provided Notes : - Store generated access keys securely (e.g., in a secrets manager) - Rotate access keys regularly for security best practices - If you provide existing credentials via aws_username_access_key_id and aws_username_secret_access_key , this flag is automatically set to False - Consider using temporary credentials (STS) for enhanced security in production aws_username_access_key_id \u00a4 An existing IAM Access Key ID to use instead of creating a new one. Optional Environment Variable: AWS_USERNAME_ACCESS_KEY_ID Default Value: None Purpose : Allows you to specify an existing IAM Access Key ID for the user instead of generating a new one. This is useful when you already have credentials and want to reuse them. When to use : - When you have pre-existing access keys that you want to associate with the user - When rotating credentials and you want to use a specific key pair - Must be used together with aws_username_secret_access_key Valid values : - A valid AWS IAM Access Key ID (20 characters, alphanumeric, starting with \"AKIA\" for long-term credentials) - Must be a key that exists and is associated with the specified username - Example format: AKIAIOSFODNN7EXAMPLE Impact : - When provided (along with secret key), automatically sets aws_username_create_access_key_flag to False - No new access keys will be generated - The role will use these existing credentials for validation - Does not modify or rotate the existing access key Related variables : - aws_username_secret_access_key - Must be provided together with this variable - aws_username_create_access_key_flag - Automatically set to False when this is provided - aws_username - The user these credentials belong to Notes : - Both access key ID and secret access key must be provided together - Ensure the credentials are valid and not expired - The access key must belong to the user specified in aws_username - Do not set this variable if you want to generate new credentials aws_username_secret_access_key \u00a4 An existing IAM Secret Access Key to use instead of creating a new one. Optional Environment Variable: AWS_USERNAME_SECRET_ACCESS_KEY Default Value: None Purpose : Allows you to specify an existing IAM Secret Access Key for the user instead of generating a new one. This must be used in conjunction with aws_username_access_key_id . When to use : - When you have pre-existing access keys that you want to associate with the user - When you need to use specific credentials for compliance or security reasons - Must be used together with aws_username_access_key_id Valid values : - A valid AWS IAM Secret Access Key (40 characters, alphanumeric with special characters) - Must correspond to the Access Key ID provided in aws_username_access_key_id - Example format: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY Impact : - When provided (along with access key ID), automatically sets aws_username_create_access_key_flag to False - No new access keys will be generated - The role will use these existing credentials for validation - Does not modify or rotate the existing secret key Related variables : - aws_username_access_key_id - Must be provided together with this variable - aws_username_create_access_key_flag - Automatically set to False when this is provided - aws_username - The user these credentials belong to Notes : - Both access key ID and secret access key must be provided together - Secret keys are sensitive credentials and should never be committed to version control - Store secret keys securely using environment variables or secrets management systems - The secret key must correspond to the access key ID provided - Do not set this variable if you want to generate new credentials aws_policy_arn \u00a4 The ARN of an IAM policy to attach to the created user. Optional Environment Variable: AWS_POLICY_ARN Default Value: None Purpose : Specifies an IAM policy to attach to the user, granting specific permissions required for MAS operations. This allows you to control what AWS resources and actions the user can access. When to use : - When the user needs specific permissions to AWS services (S3, DocumentDB, EFS, etc.) - After creating a custom policy using the aws_policy role - When attaching AWS managed policies (e.g., arn:aws:iam::aws:policy/AmazonS3FullAccess ) Valid values : - A valid IAM policy ARN in the format: arn:aws:iam::<account-id>:policy/<policy-name> - Can be a customer-managed policy or AWS managed policy - The policy must exist in your AWS account before attaching - Examples: - Custom policy: arn:aws:iam::123456789012:policy/MASServicePolicy - AWS managed: arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess Impact : - Grants the user permissions defined in the specified policy - Multiple policies can be attached by running the role multiple times with different ARNs - Policy changes take effect immediately - User actions will be constrained by the policy's permissions Related variables : - aws_username - The user to which the policy is attached - Can be used with policies created by the aws_policy role Notes : - Follow the principle of least privilege - only grant necessary permissions - Consider using customer-managed policies for better control and auditability - You can attach up to 10 managed policies per IAM user - Policy attachment is idempotent - attaching the same policy multiple times has no effect - For MAS deployments, typical policies include S3 access, DocumentDB access, and EFS permissions Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_username: \"{{ lookup('env', 'AWS_USERNAME') }}\" aws_username_create_access_key_flag: \"{{ lookup('env', 'AWS_USERNAME_CREATE_ACCESS_KEY_FLAG') }}\" aws_policy_arn: \"{{ lookup('env', 'AWS_POLICY_ARN') }}\" roles: - ibm.mas_devops.aws_policy Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_USERNAME=my-aws-username export AWS_USERNAME_CREATE_ACCESS_KEY_FLAG=True export AWS_POLICY_ARN=arn:aws:iam::my-id:policy/my-policy-name ROLE_NAME=aws_user_creation ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aws_user_creation"},{"location":"roles/aws_user_creation/#aws_user_creation","text":"This role will create an AWS IAM Username and corresponding IAM Access Key ID and Secret Access Key in the targeted AWS account.","title":"aws_user_creation"},{"location":"roles/aws_user_creation/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/aws_user_creation/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_user_creation/#aws_username","text":"The IAM username to create in AWS. Required Environment Variable: AWS_USERNAME Default Value: None Purpose : Specifies the name of the IAM user to be created in your AWS account. This user will be used for programmatic access to AWS services required by MAS. When to use : Always required when creating a new IAM user. The username should follow your organization's naming conventions and clearly identify its purpose (e.g., mas-service-user , maximo-automation ). Valid values : - 1-64 characters - Alphanumeric characters, plus signs (+), equals signs (=), commas (,), periods (.), at signs (@), underscores (_), and hyphens (-) - Must be unique within your AWS account - Case-sensitive Impact : - Creates a new IAM user with this name in your AWS account - The user can be assigned policies and access keys for authentication - Username cannot be changed after creation (requires deletion and recreation) - Will appear in AWS CloudTrail logs for audit purposes Related variables : - aws_username_create_access_key_flag - Controls access key creation - aws_policy_arn - Policy to attach to the user - aws_username_access_key_id - Existing access key (optional) - aws_username_secret_access_key - Existing secret key (optional) Notes : - Follow the principle of least privilege when assigning permissions - Consider using a descriptive name that indicates the user's purpose - IAM users incur no direct costs but their actions may incur AWS service charges - For production, consider using IAM roles with temporary credentials instead of long-lived access keys","title":"aws_username"},{"location":"roles/aws_user_creation/#aws_username_create_access_key_flag","text":"Controls whether to create new IAM access keys for the user. Optional Environment Variable: AWS_USERNAME_CREATE_ACCESS_KEY_FLAG Default Value: True Purpose : Determines if the role should generate a new IAM Access Key ID and Secret Access Key pair for the created user. Access keys enable programmatic access to AWS services. When to use : - Set to True (default) when creating a new user that needs programmatic access - Set to False when you only need to create the user identity without immediate access keys - Automatically set to False if both aws_username_access_key_id and aws_username_secret_access_key are provided Valid values : - True - Create new access key pair - False - Do not create access keys Impact : - When True : Generates new access key credentials that must be securely stored - When False : User is created but cannot authenticate programmatically until keys are added - Access keys are displayed only once during creation and cannot be retrieved later - Each IAM user can have a maximum of 2 active access keys Related variables : - aws_username - The user for which keys are created - aws_username_access_key_id - Overrides key creation if provided - aws_username_secret_access_key - Overrides key creation if provided Notes : - Store generated access keys securely (e.g., in a secrets manager) - Rotate access keys regularly for security best practices - If you provide existing credentials via aws_username_access_key_id and aws_username_secret_access_key , this flag is automatically set to False - Consider using temporary credentials (STS) for enhanced security in production","title":"aws_username_create_access_key_flag"},{"location":"roles/aws_user_creation/#aws_username_access_key_id","text":"An existing IAM Access Key ID to use instead of creating a new one. Optional Environment Variable: AWS_USERNAME_ACCESS_KEY_ID Default Value: None Purpose : Allows you to specify an existing IAM Access Key ID for the user instead of generating a new one. This is useful when you already have credentials and want to reuse them. When to use : - When you have pre-existing access keys that you want to associate with the user - When rotating credentials and you want to use a specific key pair - Must be used together with aws_username_secret_access_key Valid values : - A valid AWS IAM Access Key ID (20 characters, alphanumeric, starting with \"AKIA\" for long-term credentials) - Must be a key that exists and is associated with the specified username - Example format: AKIAIOSFODNN7EXAMPLE Impact : - When provided (along with secret key), automatically sets aws_username_create_access_key_flag to False - No new access keys will be generated - The role will use these existing credentials for validation - Does not modify or rotate the existing access key Related variables : - aws_username_secret_access_key - Must be provided together with this variable - aws_username_create_access_key_flag - Automatically set to False when this is provided - aws_username - The user these credentials belong to Notes : - Both access key ID and secret access key must be provided together - Ensure the credentials are valid and not expired - The access key must belong to the user specified in aws_username - Do not set this variable if you want to generate new credentials","title":"aws_username_access_key_id"},{"location":"roles/aws_user_creation/#aws_username_secret_access_key","text":"An existing IAM Secret Access Key to use instead of creating a new one. Optional Environment Variable: AWS_USERNAME_SECRET_ACCESS_KEY Default Value: None Purpose : Allows you to specify an existing IAM Secret Access Key for the user instead of generating a new one. This must be used in conjunction with aws_username_access_key_id . When to use : - When you have pre-existing access keys that you want to associate with the user - When you need to use specific credentials for compliance or security reasons - Must be used together with aws_username_access_key_id Valid values : - A valid AWS IAM Secret Access Key (40 characters, alphanumeric with special characters) - Must correspond to the Access Key ID provided in aws_username_access_key_id - Example format: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY Impact : - When provided (along with access key ID), automatically sets aws_username_create_access_key_flag to False - No new access keys will be generated - The role will use these existing credentials for validation - Does not modify or rotate the existing secret key Related variables : - aws_username_access_key_id - Must be provided together with this variable - aws_username_create_access_key_flag - Automatically set to False when this is provided - aws_username - The user these credentials belong to Notes : - Both access key ID and secret access key must be provided together - Secret keys are sensitive credentials and should never be committed to version control - Store secret keys securely using environment variables or secrets management systems - The secret key must correspond to the access key ID provided - Do not set this variable if you want to generate new credentials","title":"aws_username_secret_access_key"},{"location":"roles/aws_user_creation/#aws_policy_arn","text":"The ARN of an IAM policy to attach to the created user. Optional Environment Variable: AWS_POLICY_ARN Default Value: None Purpose : Specifies an IAM policy to attach to the user, granting specific permissions required for MAS operations. This allows you to control what AWS resources and actions the user can access. When to use : - When the user needs specific permissions to AWS services (S3, DocumentDB, EFS, etc.) - After creating a custom policy using the aws_policy role - When attaching AWS managed policies (e.g., arn:aws:iam::aws:policy/AmazonS3FullAccess ) Valid values : - A valid IAM policy ARN in the format: arn:aws:iam::<account-id>:policy/<policy-name> - Can be a customer-managed policy or AWS managed policy - The policy must exist in your AWS account before attaching - Examples: - Custom policy: arn:aws:iam::123456789012:policy/MASServicePolicy - AWS managed: arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess Impact : - Grants the user permissions defined in the specified policy - Multiple policies can be attached by running the role multiple times with different ARNs - Policy changes take effect immediately - User actions will be constrained by the policy's permissions Related variables : - aws_username - The user to which the policy is attached - Can be used with policies created by the aws_policy role Notes : - Follow the principle of least privilege - only grant necessary permissions - Consider using customer-managed policies for better control and auditability - You can attach up to 10 managed policies per IAM user - Policy attachment is idempotent - attaching the same policy multiple times has no effect - For MAS deployments, typical policies include S3 access, DocumentDB access, and EFS permissions","title":"aws_policy_arn"},{"location":"roles/aws_user_creation/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_username: \"{{ lookup('env', 'AWS_USERNAME') }}\" aws_username_create_access_key_flag: \"{{ lookup('env', 'AWS_USERNAME_CREATE_ACCESS_KEY_FLAG') }}\" aws_policy_arn: \"{{ lookup('env', 'AWS_POLICY_ARN') }}\" roles: - ibm.mas_devops.aws_policy","title":"Example Playbook"},{"location":"roles/aws_user_creation/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_USERNAME=my-aws-username export AWS_USERNAME_CREATE_ACCESS_KEY_FLAG=True export AWS_POLICY_ARN=arn:aws:iam::my-id:policy/my-policy-name ROLE_NAME=aws_user_creation ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aws_user_creation/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_vpc/","text":"aws_vpc \u00a4 This role will create VPC with specified CIDR IP Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Role Variables \u00a4 mas_config_dir \u00a4 Directory where generated Kubernetes resources and VPC configuration will be saved. Required Environment Variable: MAS_CONFIG_DIR Default Value: None Purpose : Specifies the local directory path where the role will save generated Kubernetes resources, VPC configuration details, and any credentials or metadata related to the created VPC. When to use : Always required when provisioning a VPC. This directory serves as the central location for all MAS configuration artifacts and should be consistent across all roles. Valid values : - Any valid local filesystem path (e.g., ~/masconfig , /opt/mas/config ) - Directory will be created if it doesn't exist - Should have write permissions for the user running the playbook - Recommended to use an absolute path for consistency Impact : - All VPC-related configuration files will be stored in this directory - Kubernetes secrets and resources will be generated here for later use - Required for subsequent MAS installation and configuration steps - Should be backed up as it contains important configuration data Related variables : - Used by all MAS DevOps roles for consistent configuration storage - vpc_name - Used in naming generated configuration files Notes : - Use the same mas_config_dir across all MAS DevOps roles for consistency - Ensure the directory is accessible and has sufficient storage space - Consider using version control for the configuration directory (excluding sensitive data) - The directory structure follows MAS configuration conventions aws_region \u00a4 The AWS region where the VPC will be created. Optional Environment Variable: AWS_REGION Default Value: us-east-1 Purpose : Specifies the AWS region for VPC creation. The VPC and all its associated resources (subnets, route tables, internet gateways) will be created in this region. When to use : Should match the region where you plan to deploy your OpenShift cluster and MAS workloads. Consider data residency requirements and proximity to users. Valid values : - Any valid AWS region identifier (e.g., us-east-1 , us-west-2 , eu-west-1 , ap-southeast-1 ) - Must be a region where your AWS account has access - Consider regions with availability zone requirements for high availability Impact : - Determines the geographic location of your VPC and all network resources - Affects network latency for resources accessing the VPC - Some AWS services may have different availability or pricing in different regions - Cannot be changed after VPC creation (requires recreation) Related variables : - Should align with the region used in other AWS roles (DocumentDB, EFS, S3) - vpc_name - VPC identifier within the region Notes : - Choose a region close to your users for better performance - Ensure the region supports all AWS services required by MAS (EFS, DocumentDB, etc.) - Consider compliance and data residency requirements - Some regions may have capacity constraints for certain instance types vpc_action \u00a4 The action to perform on the VPC (provision or deprovision). Optional Environment Variable: VPC_ACTION Default Value: provision Purpose : Controls whether the role should create a new VPC or delete an existing one. This allows the same role to handle both lifecycle operations. When to use : - Use provision (default) when creating a new VPC for MAS deployment - Use deprovision when cleaning up and removing the VPC after MAS uninstallation Valid values : - provision - Create a new VPC with the specified configuration - deprovision - Delete the VPC identified by vpc_name Impact : - provision : Creates VPC, subnets, route tables, internet gateway, and associated resources - deprovision : Permanently deletes the VPC and all associated resources - Deprovisioning is irreversible and will fail if resources are still attached to the VPC - All data and configurations within the VPC will be lost during deprovisioning Related variables : - vpc_name - Identifies which VPC to provision or deprovision - vpc_cidr - Required for provisioning, ignored for deprovisioning Notes : - Before deprovisioning, ensure all OpenShift clusters and MAS workloads are removed - Deprovisioning will fail if EC2 instances, load balancers, or other resources are still using the VPC - Consider taking backups before deprovisioning - The role will clean up associated resources (subnets, route tables, internet gateways) vpc_cidr \u00a4 The CIDR block (IP address range) for the VPC. Required (for provisioning) Environment Variable: VPC_CIDR Default Value: None Purpose : Defines the IP address range for the VPC using CIDR notation. This determines the total number of IP addresses available within the VPC for subnets, instances, and other resources. When to use : Required when provisioning a new VPC. The CIDR block should be large enough to accommodate all planned subnets and resources while avoiding conflicts with existing networks. Valid values : - Valid IPv4 CIDR block between /16 and /28 netmask - Must be from private IP ranges: - 10.0.0.0/8 (10.0.0.0 - 10.255.255.255) - 172.16.0.0/12 (172.16.0.0 - 172.31.255.255) - 192.168.0.0/16 (192.168.0.0 - 192.168.255.255) - Common examples: - 10.0.0.0/16 - Provides 65,536 IP addresses - 172.31.0.0/16 - Provides 65,536 IP addresses - 192.168.0.0/20 - Provides 4,096 IP addresses Impact : - Determines the maximum number of IP addresses available in the VPC - Cannot be changed after VPC creation (requires recreation) - Affects subnet design and IP address allocation strategy - Must not overlap with other VPCs if VPC peering is planned - Smaller CIDR blocks (/24, /28) limit scalability Related variables : - vpc_name - Identifies the VPC using this CIDR block - aws_region - Region where the VPC with this CIDR is created Notes : - Plan for future growth when selecting CIDR block size - Reserve IP addresses for AWS services (first 4 and last 1 in each subnet) - For OpenShift on AWS (ROSA), a /16 CIDR is recommended for production - Avoid overlapping with on-premises networks if VPN connectivity is planned - Document your CIDR allocation strategy for network management vpc_name \u00a4 The name tag for the VPC. Required Environment Variable: VPC_NAME Default Value: None Purpose : Assigns a human-readable name to the VPC for identification and management purposes. This name appears in the AWS console and is used to identify the VPC in subsequent operations. When to use : Always required for both provisioning and deprovisioning operations. The name should be descriptive and follow your organization's naming conventions. Valid values : - Any string that follows AWS tag naming conventions - Recommended format: <environment>-<purpose>-vpc (e.g., prod-mas-vpc , dev-maximo-vpc ) - Should be unique within your AWS account for clarity - Alphanumeric characters, hyphens, and underscores are recommended Impact : - Used as the \"Name\" tag on the VPC in AWS console - Helps identify the VPC in AWS CLI and API operations - Used by the role to locate the VPC during deprovisioning - Appears in AWS cost allocation reports if cost tags are enabled Related variables : - vpc_cidr - The IP range for this named VPC - vpc_action - Whether to provision or deprovision this VPC - mas_config_dir - Where configuration for this VPC is stored Notes : - Use consistent naming conventions across all AWS resources - Include environment indicators (dev, test, prod) in the name - The name is stored as a tag and can be changed after creation - Consider including the region or purpose in the name for multi-region deployments - Document your VPC naming strategy for team consistency Example Playbook \u00a4 - hosts: localhost vars: mas_config_dir: ~/masconfig vpc_name: test-vpc vpc_cidr: 10.0.0.0/16 vpc_action: provision roles: - ibm.mas_devops.aws_vpc Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export VPC_NAME=test-vpc export VPC_CIDR='10.0.0.0/16' export VPC_ACTION=provision export MAS_CONFIG_DIR=/pathtoconfig ROLE_NAME=aws_vpc ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aws_vpc"},{"location":"roles/aws_vpc/#aws_vpc","text":"This role will create VPC with specified CIDR IP","title":"aws_vpc"},{"location":"roles/aws_vpc/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/aws_vpc/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_vpc/#mas_config_dir","text":"Directory where generated Kubernetes resources and VPC configuration will be saved. Required Environment Variable: MAS_CONFIG_DIR Default Value: None Purpose : Specifies the local directory path where the role will save generated Kubernetes resources, VPC configuration details, and any credentials or metadata related to the created VPC. When to use : Always required when provisioning a VPC. This directory serves as the central location for all MAS configuration artifacts and should be consistent across all roles. Valid values : - Any valid local filesystem path (e.g., ~/masconfig , /opt/mas/config ) - Directory will be created if it doesn't exist - Should have write permissions for the user running the playbook - Recommended to use an absolute path for consistency Impact : - All VPC-related configuration files will be stored in this directory - Kubernetes secrets and resources will be generated here for later use - Required for subsequent MAS installation and configuration steps - Should be backed up as it contains important configuration data Related variables : - Used by all MAS DevOps roles for consistent configuration storage - vpc_name - Used in naming generated configuration files Notes : - Use the same mas_config_dir across all MAS DevOps roles for consistency - Ensure the directory is accessible and has sufficient storage space - Consider using version control for the configuration directory (excluding sensitive data) - The directory structure follows MAS configuration conventions","title":"mas_config_dir"},{"location":"roles/aws_vpc/#aws_region","text":"The AWS region where the VPC will be created. Optional Environment Variable: AWS_REGION Default Value: us-east-1 Purpose : Specifies the AWS region for VPC creation. The VPC and all its associated resources (subnets, route tables, internet gateways) will be created in this region. When to use : Should match the region where you plan to deploy your OpenShift cluster and MAS workloads. Consider data residency requirements and proximity to users. Valid values : - Any valid AWS region identifier (e.g., us-east-1 , us-west-2 , eu-west-1 , ap-southeast-1 ) - Must be a region where your AWS account has access - Consider regions with availability zone requirements for high availability Impact : - Determines the geographic location of your VPC and all network resources - Affects network latency for resources accessing the VPC - Some AWS services may have different availability or pricing in different regions - Cannot be changed after VPC creation (requires recreation) Related variables : - Should align with the region used in other AWS roles (DocumentDB, EFS, S3) - vpc_name - VPC identifier within the region Notes : - Choose a region close to your users for better performance - Ensure the region supports all AWS services required by MAS (EFS, DocumentDB, etc.) - Consider compliance and data residency requirements - Some regions may have capacity constraints for certain instance types","title":"aws_region"},{"location":"roles/aws_vpc/#vpc_action","text":"The action to perform on the VPC (provision or deprovision). Optional Environment Variable: VPC_ACTION Default Value: provision Purpose : Controls whether the role should create a new VPC or delete an existing one. This allows the same role to handle both lifecycle operations. When to use : - Use provision (default) when creating a new VPC for MAS deployment - Use deprovision when cleaning up and removing the VPC after MAS uninstallation Valid values : - provision - Create a new VPC with the specified configuration - deprovision - Delete the VPC identified by vpc_name Impact : - provision : Creates VPC, subnets, route tables, internet gateway, and associated resources - deprovision : Permanently deletes the VPC and all associated resources - Deprovisioning is irreversible and will fail if resources are still attached to the VPC - All data and configurations within the VPC will be lost during deprovisioning Related variables : - vpc_name - Identifies which VPC to provision or deprovision - vpc_cidr - Required for provisioning, ignored for deprovisioning Notes : - Before deprovisioning, ensure all OpenShift clusters and MAS workloads are removed - Deprovisioning will fail if EC2 instances, load balancers, or other resources are still using the VPC - Consider taking backups before deprovisioning - The role will clean up associated resources (subnets, route tables, internet gateways)","title":"vpc_action"},{"location":"roles/aws_vpc/#vpc_cidr","text":"The CIDR block (IP address range) for the VPC. Required (for provisioning) Environment Variable: VPC_CIDR Default Value: None Purpose : Defines the IP address range for the VPC using CIDR notation. This determines the total number of IP addresses available within the VPC for subnets, instances, and other resources. When to use : Required when provisioning a new VPC. The CIDR block should be large enough to accommodate all planned subnets and resources while avoiding conflicts with existing networks. Valid values : - Valid IPv4 CIDR block between /16 and /28 netmask - Must be from private IP ranges: - 10.0.0.0/8 (10.0.0.0 - 10.255.255.255) - 172.16.0.0/12 (172.16.0.0 - 172.31.255.255) - 192.168.0.0/16 (192.168.0.0 - 192.168.255.255) - Common examples: - 10.0.0.0/16 - Provides 65,536 IP addresses - 172.31.0.0/16 - Provides 65,536 IP addresses - 192.168.0.0/20 - Provides 4,096 IP addresses Impact : - Determines the maximum number of IP addresses available in the VPC - Cannot be changed after VPC creation (requires recreation) - Affects subnet design and IP address allocation strategy - Must not overlap with other VPCs if VPC peering is planned - Smaller CIDR blocks (/24, /28) limit scalability Related variables : - vpc_name - Identifies the VPC using this CIDR block - aws_region - Region where the VPC with this CIDR is created Notes : - Plan for future growth when selecting CIDR block size - Reserve IP addresses for AWS services (first 4 and last 1 in each subnet) - For OpenShift on AWS (ROSA), a /16 CIDR is recommended for production - Avoid overlapping with on-premises networks if VPN connectivity is planned - Document your CIDR allocation strategy for network management","title":"vpc_cidr"},{"location":"roles/aws_vpc/#vpc_name","text":"The name tag for the VPC. Required Environment Variable: VPC_NAME Default Value: None Purpose : Assigns a human-readable name to the VPC for identification and management purposes. This name appears in the AWS console and is used to identify the VPC in subsequent operations. When to use : Always required for both provisioning and deprovisioning operations. The name should be descriptive and follow your organization's naming conventions. Valid values : - Any string that follows AWS tag naming conventions - Recommended format: <environment>-<purpose>-vpc (e.g., prod-mas-vpc , dev-maximo-vpc ) - Should be unique within your AWS account for clarity - Alphanumeric characters, hyphens, and underscores are recommended Impact : - Used as the \"Name\" tag on the VPC in AWS console - Helps identify the VPC in AWS CLI and API operations - Used by the role to locate the VPC during deprovisioning - Appears in AWS cost allocation reports if cost tags are enabled Related variables : - vpc_cidr - The IP range for this named VPC - vpc_action - Whether to provision or deprovision this VPC - mas_config_dir - Where configuration for this VPC is stored Notes : - Use consistent naming conventions across all AWS resources - Include environment indicators (dev, test, prod) in the name - The name is stored as a tag and can be changed after creation - Consider including the region or purpose in the name for multi-region deployments - Document your VPC naming strategy for team consistency","title":"vpc_name"},{"location":"roles/aws_vpc/#example-playbook","text":"- hosts: localhost vars: mas_config_dir: ~/masconfig vpc_name: test-vpc vpc_cidr: 10.0.0.0/16 vpc_action: provision roles: - ibm.mas_devops.aws_vpc","title":"Example Playbook"},{"location":"roles/aws_vpc/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export VPC_NAME=test-vpc export VPC_CIDR='10.0.0.0/16' export VPC_ACTION=provision export MAS_CONFIG_DIR=/pathtoconfig ROLE_NAME=aws_vpc ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aws_vpc/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cert_manager/","text":"cert_manager \u00a4 This role deploys the Red Hat Certificate Manager Operator into the target OpenShift cluster. The operator will be installed into the cert-manager-operator namespace, and the operand will be created in the cert-manager namespace. Certificate Manager provides certificate management capabilities for Kubernetes and OpenShift clusters, enabling automated certificate provisioning and renewal. Prerequisites \u00a4 Red Hat Operators CatalogSource must be installed in the cluster Cluster administrator access Role Variables \u00a4 General Variables \u00a4 cert_manager_action \u00a4 Specifies which operation to perform on the Certificate Manager operator. Optional Environment Variable: CERT_MANAGER_ACTION Default Value: install Purpose : Controls what action the role executes against the Certificate Manager operator. This allows the same role to handle installation, removal, or no action on the cert-manager deployment. When to use : - Use install (default) for initial deployment or to ensure cert-manager is present - Use uninstall to remove cert-manager (use with extreme caution) - Use none to skip cert-manager operations while running broader playbooks Valid values : install , uninstall , none Impact : - install : Deploys Red Hat Certificate Manager Operator to cert-manager-operator namespace and creates operand in cert-manager namespace - uninstall : Removes cert-manager operator and operand (destructive operation) - none : Role takes no action Related variables : None Note : WARNING - Certificate Manager is a cluster-wide dependency used by MAS, SLS, and other components. Uninstalling it will break certificate management for all dependent applications. Only use uninstall if you are certain no applications depend on it. Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: cert_manager_action: install roles: - ibm.mas_devops.cert_manager Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export CERT_MANAGER_ACTION=install ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"cert_manager"},{"location":"roles/cert_manager/#cert_manager","text":"This role deploys the Red Hat Certificate Manager Operator into the target OpenShift cluster. The operator will be installed into the cert-manager-operator namespace, and the operand will be created in the cert-manager namespace. Certificate Manager provides certificate management capabilities for Kubernetes and OpenShift clusters, enabling automated certificate provisioning and renewal.","title":"cert_manager"},{"location":"roles/cert_manager/#prerequisites","text":"Red Hat Operators CatalogSource must be installed in the cluster Cluster administrator access","title":"Prerequisites"},{"location":"roles/cert_manager/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cert_manager/#general-variables","text":"","title":"General Variables"},{"location":"roles/cert_manager/#cert_manager_action","text":"Specifies which operation to perform on the Certificate Manager operator. Optional Environment Variable: CERT_MANAGER_ACTION Default Value: install Purpose : Controls what action the role executes against the Certificate Manager operator. This allows the same role to handle installation, removal, or no action on the cert-manager deployment. When to use : - Use install (default) for initial deployment or to ensure cert-manager is present - Use uninstall to remove cert-manager (use with extreme caution) - Use none to skip cert-manager operations while running broader playbooks Valid values : install , uninstall , none Impact : - install : Deploys Red Hat Certificate Manager Operator to cert-manager-operator namespace and creates operand in cert-manager namespace - uninstall : Removes cert-manager operator and operand (destructive operation) - none : Role takes no action Related variables : None Note : WARNING - Certificate Manager is a cluster-wide dependency used by MAS, SLS, and other components. Uninstalling it will break certificate management for all dependent applications. Only use uninstall if you are certain no applications depend on it.","title":"cert_manager_action"},{"location":"roles/cert_manager/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: cert_manager_action: install roles: - ibm.mas_devops.cert_manager","title":"Example Playbook"},{"location":"roles/cert_manager/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export CERT_MANAGER_ACTION=install ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/cert_manager/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cis/","text":"cis \u00a4 This role provides support for Configuring IBM Cloud Internet Services. During CIS provisioning it performs four tasks during provisioning in given order: Provision CIS Instance in customer account Add customer domain to customer's CIS Instance Configure Domain settings in customer CIS Instance Add DNS Records of type NS for customer's Domain nameservers to Master CIS Account During CIS Instance deprovisioning role will perform following tasks: Delete DNS Record from Master Account Delete Domain from Customer Account Delete Customer CIS Instance Role Variables \u00a4 cis_action \u00a4 The action to perform on the Cloud Internet Services instance. Required Environment Variable: CIS_ACTION Default Value: provision Purpose : Controls whether to provision a new CIS instance with domain configuration or deprovision an existing one. This enables the same role to handle both lifecycle operations. When to use : - Use provision (default) when setting up DNS and CDN services for a new MAS deployment - Use deprovision when cleaning up CIS resources after MAS uninstallation Valid values : - provision - Create CIS instance, add domain, configure settings, and add NS records to master account - deprovision - Delete DNS records from master account, remove domain, and delete CIS instance Impact : - Provision : Creates a complete CIS setup with domain delegation from master account - Deprovision : Permanently removes all CIS resources and DNS records (irreversible) - Provisioning requires coordination between customer and master IBM Cloud accounts - Deprovisioning must be done before removing the OpenShift cluster Related variables : - cis_plan - Service plan for provisioning - master_cis_base_domain - Master domain for NS record delegation - ibmcloud_apikey - Customer account credentials - master_ibmcloud_api_key - Master account credentials Notes : - Provisioning creates a multi-account DNS delegation architecture - Deprovisioning removes DNS records that may still be in use - verify before proceeding - The role performs operations in a specific order to maintain DNS consistency - Failed provisioning may require manual cleanup in IBM Cloud console cis_plan \u00a4 The IBM Cloud Internet Services plan type to provision. Required Environment Variable: CIS_PLAN Default Value: standard Purpose : Specifies the CIS service plan tier, which determines available features, performance limits, and pricing for the CIS instance. When to use : The default standard plan is suitable for most MAS deployments. Consider enterprise plan for production deployments requiring advanced features or higher limits. Valid values : - standard - Standard plan with basic CDN, DDoS protection, and DNS features - enterprise - Enterprise plan with advanced security, performance, and support features - Plan availability and features may vary by region Impact : - Determines available CIS features (WAF rules, rate limiting, page rules, etc.) - Affects pricing and billing for the CIS instance - Enterprise plan provides higher performance limits and SLA guarantees - Cannot be changed after provisioning (requires recreation) Related variables : - cis_action - Must be provision for this to take effect - ibmcloud_resourcegroup - Resource group for billing Notes : - Standard plan is sufficient for most development and test environments - Enterprise plan recommended for production deployments with high traffic - Review IBM Cloud CIS pricing before selecting a plan - Plan features include DNS, CDN, DDoS protection, WAF, and load balancing ibmcloud_apikey \u00a4 The IBM Cloud API key for the customer account where CIS will be provisioned. Required Environment Variable: IBMCLOUD_APIKEY Default Value: None Purpose : Provides authentication credentials for the customer's IBM Cloud account where the CIS instance will be created and managed. When to use : Always required for both provisioning and deprovisioning operations. This API key must have permissions to create and manage CIS instances in the specified resource group. Valid values : - A valid IBM Cloud API key with CIS service permissions - Must have Editor or Administrator role on the CIS service - Must have access to the specified resource group - Format: 44-character alphanumeric string Impact : - Authenticates all operations in the customer's IBM Cloud account - Determines which account owns the CIS instance and incurs charges - Required permissions: CIS service management, resource group access - Invalid or expired keys will cause provisioning to fail Related variables : - ibmcloud_resourcegroup - Resource group where CIS is created - master_ibmcloud_api_key - Separate key for master account operations Notes : - Store API keys securely using environment variables or secrets management - Never commit API keys to version control - Consider using service IDs with restricted permissions instead of user API keys - Rotate API keys regularly for security best practices - The customer account will be billed for CIS usage ibmcloud_resourcegroup \u00a4 The IBM Cloud resource group that will own the CIS instance. Optional Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default Purpose : Specifies the resource group in the customer's IBM Cloud account where the CIS instance will be created. Resource groups are used for access control and billing organization. When to use : Set this to organize CIS resources within a specific resource group, especially in accounts with multiple projects or teams. Use the default Default resource group for simple deployments. Valid values : - Any existing resource group name in your IBM Cloud account - Common examples: Default , Production , Development , MAS-Resources - Resource group must exist before provisioning - Case-sensitive Impact : - Determines billing and cost allocation for the CIS instance - Controls IAM access policies and permissions - Affects resource organization in IBM Cloud console - Cannot be changed after CIS instance creation Related variables : - ibmcloud_apikey - Must have access to this resource group - master_cis_resource_group - Resource group in master account Notes : - Ensure the API key has access to the specified resource group - Use consistent resource groups across all MAS-related services - Resource groups help with cost tracking and access management - The Default resource group is created automatically in all IBM Cloud accounts master_ibmcloud_api_key \u00a4 The IBM Cloud API key for the master account that hosts the base domain. Required Environment Variable: MASTER_IBMCLOUD_APIKEY Default Value: None Purpose : Provides authentication credentials for the master IBM Cloud account where the base domain's CIS instance is hosted. This is required to create NS (nameserver) records that delegate the subdomain to the customer's CIS instance. When to use : Always required for provisioning. The master account typically belongs to the organization managing the base domain (e.g., mas.ibm.com ) and delegates subdomains to customer accounts. Valid values : - A valid IBM Cloud API key for the master account - Must have permissions to manage DNS records in the master CIS instance - Must have access to the master CIS resource group - Format: 44-character alphanumeric string Impact : - Enables DNS delegation from master domain to customer subdomain - Required to create NS records pointing to customer's CIS nameservers - Without this, the customer's domain will not be resolvable - Used only during provisioning and deprovisioning operations Related variables : - master_cis_resource_group - Resource group in master account - master_cis_resource_name - Master CIS instance name - master_cis_base_domain - Base domain in master account - ibmcloud_apikey - Customer account API key (separate) Notes : - This is a separate API key from the customer account key - Typically provided by the organization managing the master domain - Store securely as it has access to the master DNS infrastructure - Only needs DNS record management permissions, not full CIS access - Required for multi-tenant MAS deployments with domain delegation master_cis_resource_group \u00a4 The resource group in the master account that owns the master CIS instance. Required Environment Variable: MASTER_CIS_RESOURCE_GROUP Default Value: manager Purpose : Identifies the resource group in the master IBM Cloud account where the master CIS instance (hosting the base domain) is located. When to use : Required when the master CIS instance is not in the default resource group. This is typically set by the organization managing the master domain infrastructure. Valid values : - Any valid resource group name in the master IBM Cloud account - Common values: manager , Default , DNS-Infrastructure - Must match the actual resource group of the master CIS instance - Case-sensitive Impact : - Used to locate the master CIS instance for NS record creation - Incorrect value will cause DNS delegation to fail - Must match the resource group where master_cis_resource_name exists Related variables : - master_ibmcloud_api_key - Must have access to this resource group - master_cis_resource_name - CIS instance in this resource group - master_cis_base_domain - Domain hosted in this CIS instance Notes : - Default value manager is commonly used in IBM MAS deployments - Verify the correct resource group with the master account administrator - The master API key must have access to this resource group - This is in the master account, not the customer account master_cis_resource_name \u00a4 The name of the master CIS instance in the master account. Required Environment Variable: MASTER_CIS_RESOURCE_NAME Default Value: <mas_instance_id>-cis Purpose : Identifies the specific CIS instance in the master account that hosts the base domain. This instance will receive the NS records for subdomain delegation. When to use : The default value is usually sufficient, but can be overridden if the master CIS instance has a different naming convention. Valid values : - Any valid CIS instance name in the master account - Default format: <mas_instance_id>-cis (e.g., prod-cis , dev-cis ) - Must match an existing CIS instance in the master account - Case-sensitive Impact : - Used to locate the correct CIS instance for NS record creation - Incorrect value will cause DNS delegation to fail - Must exist in the master_cis_resource_group Related variables : - mas_instance_id - Used to generate default name - master_cis_resource_group - Resource group containing this instance - master_cis_base_domain - Domain hosted in this instance Notes : - Verify the correct instance name with the master account administrator - The default naming convention uses the MAS instance ID - This instance must already exist in the master account - Used only for NS record management, not for creating the instance master_cis_base_domain \u00a4 The base domain hosted in the master CIS instance. Required Environment Variable: MASTER_CIS_BASE_DOMAIN Default Value: None Purpose : Specifies the parent domain in the master CIS instance under which the customer's subdomain will be delegated. This is the domain that will contain the NS records pointing to the customer's CIS nameservers. When to use : Always required for provisioning. This is typically a domain like mas.ibm.com or maximo.company.com that is managed centrally and delegates subdomains to individual deployments. Valid values : - A valid domain name hosted in the master CIS instance - Must be a domain you control and have configured in the master CIS - Examples: mas.ibm.com , maximo.example.com , apps.company.com - Should not include protocol (http/https) or trailing slashes Impact : - Customer's domain will be created as a subdomain of this base domain - NS records will be created in this domain to delegate to customer's CIS - The full customer domain will be <cluster_name>.<mas_instance_id>.<master_cis_base_domain> - Must be properly configured in the master CIS instance Related variables : - cluster_name - First part of the subdomain - mas_instance_id - Second part of the subdomain - master_cis_resource_name - CIS instance hosting this domain Notes : - This domain must already exist and be active in the master CIS instance - Verify DNS propagation of the base domain before provisioning - The base domain typically belongs to the organization, not individual customers - Example: If base domain is mas.ibm.com , customer domain might be cluster1.prod.mas.ibm.com mas_instance_id \u00a4 The MAS instance identifier used in naming the CIS service and domain. Required Environment Variable: MAS_INSTANCE_ID Default Value: None Purpose : Provides a unique identifier for the MAS instance, used to generate the CIS service name and as part of the domain name structure. This ensures uniqueness across multiple MAS deployments. When to use : Always required. Should match the MAS instance ID used throughout your MAS deployment for consistency. Valid values : - Alphanumeric string, typically 3-12 characters - Common examples: prod , dev , test , masinst1 , mas-prod-01 - Should be unique within your organization - Lowercase recommended for DNS compatibility Impact : - Used to generate default CIS instance name: <mas_instance_id>-cis - Forms part of the domain name: <cluster_name>.<mas_instance_id>.<base_domain> - Appears in resource names and tags for identification - Should remain consistent across all MAS components Related variables : - cluster_name - Combined with this to form the full domain - master_cis_resource_name - Defaults to <mas_instance_id>-cis - master_cis_base_domain - Base domain for the subdomain Notes : - Use the same mas_instance_id across all MAS DevOps roles - Choose a meaningful identifier that indicates the environment or purpose - Cannot be changed after provisioning without recreating resources - Appears in DNS names, so keep it short and DNS-compatible cluster_name \u00a4 The OpenShift cluster name used as a prefix in the domain name. Required Environment Variable: CLUSTER_NAME Default Value: None Purpose : Identifies the specific OpenShift cluster for this CIS configuration. Used as the first component of the subdomain name, allowing multiple clusters to share the same MAS instance ID. When to use : Always required. Should match the OpenShift cluster name used in your deployment. Valid values : - Alphanumeric string with hyphens - Common examples: ocp-prod , rosa-cluster , roks-dev , cluster1 - Should be unique within the MAS instance - Lowercase recommended for DNS compatibility Impact : - Forms the first part of the domain: <cluster_name>.<mas_instance_id>.<base_domain> - Used to generate the CIS service name - Helps identify which cluster the CIS instance serves - Appears in DNS records and resource names Related variables : - mas_instance_id - Combined with this to form the full domain - master_cis_base_domain - Base domain for the subdomain Notes : - Should match your OpenShift cluster name for consistency - Multiple clusters can use the same mas_instance_id with different cluster_name values - Keep it short and DNS-compatible (no special characters except hyphens) - Example: cluster_name= rosa-prod , mas_instance_id= mas01 , base= ibm.com \u2192 rosa-prod.mas01.ibm.com mas_config_dir \u00a4 Local directory where generated CIS configuration will be saved. Required Environment Variable: MAS_CONFIG_DIR Default Value: None Purpose : Specifies the local directory path where the role will save generated Kubernetes ConfigMaps and CIS instance details for use by other MAS components. When to use : Always required. This directory serves as the central location for all MAS configuration artifacts and should be consistent across all MAS DevOps roles. Valid values : - Any valid local filesystem path (e.g., ~/masconfig , /opt/mas/config ) - Directory will be created if it doesn't exist - Should have write permissions for the user running the playbook - Recommended to use an absolute path for consistency Impact : - CIS instance details are saved as ConfigMap YAML files in this directory - Required for subsequent MAS installation and configuration steps - Other roles will read CIS configuration from this location - Should be backed up as it contains important configuration data Related variables : - Used by all MAS DevOps roles for consistent configuration storage - CIS-specific files will be created with identifiable names Notes : - Use the same mas_config_dir across all MAS DevOps roles for consistency - Ensure the directory is accessible and has sufficient storage space - Consider using version control for the configuration directory (excluding sensitive data) - The directory structure follows MAS configuration conventions - ConfigMaps generated here are applied to the OpenShift cluster during MAS installation Example Playbook \u00a4 Create CIS Instance alongwith save Instance details in MAS_CONFIG_DIR path as ConfigMap - hosts: localhost any_errors_fatal: true vars: cis_action: provision mas_instance_id: masinst1 mas_config_dir: ~/masconfig ibmcloud_apikey: \"****\" master_ibmcloud_api_key: \"******\" cluster_name: \"test\" roles: - ibm.mas_devops.cis License \u00a4 EPL-2.0","title":"cis"},{"location":"roles/cis/#cis","text":"This role provides support for Configuring IBM Cloud Internet Services. During CIS provisioning it performs four tasks during provisioning in given order: Provision CIS Instance in customer account Add customer domain to customer's CIS Instance Configure Domain settings in customer CIS Instance Add DNS Records of type NS for customer's Domain nameservers to Master CIS Account During CIS Instance deprovisioning role will perform following tasks: Delete DNS Record from Master Account Delete Domain from Customer Account Delete Customer CIS Instance","title":"cis"},{"location":"roles/cis/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cis/#cis_action","text":"The action to perform on the Cloud Internet Services instance. Required Environment Variable: CIS_ACTION Default Value: provision Purpose : Controls whether to provision a new CIS instance with domain configuration or deprovision an existing one. This enables the same role to handle both lifecycle operations. When to use : - Use provision (default) when setting up DNS and CDN services for a new MAS deployment - Use deprovision when cleaning up CIS resources after MAS uninstallation Valid values : - provision - Create CIS instance, add domain, configure settings, and add NS records to master account - deprovision - Delete DNS records from master account, remove domain, and delete CIS instance Impact : - Provision : Creates a complete CIS setup with domain delegation from master account - Deprovision : Permanently removes all CIS resources and DNS records (irreversible) - Provisioning requires coordination between customer and master IBM Cloud accounts - Deprovisioning must be done before removing the OpenShift cluster Related variables : - cis_plan - Service plan for provisioning - master_cis_base_domain - Master domain for NS record delegation - ibmcloud_apikey - Customer account credentials - master_ibmcloud_api_key - Master account credentials Notes : - Provisioning creates a multi-account DNS delegation architecture - Deprovisioning removes DNS records that may still be in use - verify before proceeding - The role performs operations in a specific order to maintain DNS consistency - Failed provisioning may require manual cleanup in IBM Cloud console","title":"cis_action"},{"location":"roles/cis/#cis_plan","text":"The IBM Cloud Internet Services plan type to provision. Required Environment Variable: CIS_PLAN Default Value: standard Purpose : Specifies the CIS service plan tier, which determines available features, performance limits, and pricing for the CIS instance. When to use : The default standard plan is suitable for most MAS deployments. Consider enterprise plan for production deployments requiring advanced features or higher limits. Valid values : - standard - Standard plan with basic CDN, DDoS protection, and DNS features - enterprise - Enterprise plan with advanced security, performance, and support features - Plan availability and features may vary by region Impact : - Determines available CIS features (WAF rules, rate limiting, page rules, etc.) - Affects pricing and billing for the CIS instance - Enterprise plan provides higher performance limits and SLA guarantees - Cannot be changed after provisioning (requires recreation) Related variables : - cis_action - Must be provision for this to take effect - ibmcloud_resourcegroup - Resource group for billing Notes : - Standard plan is sufficient for most development and test environments - Enterprise plan recommended for production deployments with high traffic - Review IBM Cloud CIS pricing before selecting a plan - Plan features include DNS, CDN, DDoS protection, WAF, and load balancing","title":"cis_plan"},{"location":"roles/cis/#ibmcloud_apikey","text":"The IBM Cloud API key for the customer account where CIS will be provisioned. Required Environment Variable: IBMCLOUD_APIKEY Default Value: None Purpose : Provides authentication credentials for the customer's IBM Cloud account where the CIS instance will be created and managed. When to use : Always required for both provisioning and deprovisioning operations. This API key must have permissions to create and manage CIS instances in the specified resource group. Valid values : - A valid IBM Cloud API key with CIS service permissions - Must have Editor or Administrator role on the CIS service - Must have access to the specified resource group - Format: 44-character alphanumeric string Impact : - Authenticates all operations in the customer's IBM Cloud account - Determines which account owns the CIS instance and incurs charges - Required permissions: CIS service management, resource group access - Invalid or expired keys will cause provisioning to fail Related variables : - ibmcloud_resourcegroup - Resource group where CIS is created - master_ibmcloud_api_key - Separate key for master account operations Notes : - Store API keys securely using environment variables or secrets management - Never commit API keys to version control - Consider using service IDs with restricted permissions instead of user API keys - Rotate API keys regularly for security best practices - The customer account will be billed for CIS usage","title":"ibmcloud_apikey"},{"location":"roles/cis/#ibmcloud_resourcegroup","text":"The IBM Cloud resource group that will own the CIS instance. Optional Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default Purpose : Specifies the resource group in the customer's IBM Cloud account where the CIS instance will be created. Resource groups are used for access control and billing organization. When to use : Set this to organize CIS resources within a specific resource group, especially in accounts with multiple projects or teams. Use the default Default resource group for simple deployments. Valid values : - Any existing resource group name in your IBM Cloud account - Common examples: Default , Production , Development , MAS-Resources - Resource group must exist before provisioning - Case-sensitive Impact : - Determines billing and cost allocation for the CIS instance - Controls IAM access policies and permissions - Affects resource organization in IBM Cloud console - Cannot be changed after CIS instance creation Related variables : - ibmcloud_apikey - Must have access to this resource group - master_cis_resource_group - Resource group in master account Notes : - Ensure the API key has access to the specified resource group - Use consistent resource groups across all MAS-related services - Resource groups help with cost tracking and access management - The Default resource group is created automatically in all IBM Cloud accounts","title":"ibmcloud_resourcegroup"},{"location":"roles/cis/#master_ibmcloud_api_key","text":"The IBM Cloud API key for the master account that hosts the base domain. Required Environment Variable: MASTER_IBMCLOUD_APIKEY Default Value: None Purpose : Provides authentication credentials for the master IBM Cloud account where the base domain's CIS instance is hosted. This is required to create NS (nameserver) records that delegate the subdomain to the customer's CIS instance. When to use : Always required for provisioning. The master account typically belongs to the organization managing the base domain (e.g., mas.ibm.com ) and delegates subdomains to customer accounts. Valid values : - A valid IBM Cloud API key for the master account - Must have permissions to manage DNS records in the master CIS instance - Must have access to the master CIS resource group - Format: 44-character alphanumeric string Impact : - Enables DNS delegation from master domain to customer subdomain - Required to create NS records pointing to customer's CIS nameservers - Without this, the customer's domain will not be resolvable - Used only during provisioning and deprovisioning operations Related variables : - master_cis_resource_group - Resource group in master account - master_cis_resource_name - Master CIS instance name - master_cis_base_domain - Base domain in master account - ibmcloud_apikey - Customer account API key (separate) Notes : - This is a separate API key from the customer account key - Typically provided by the organization managing the master domain - Store securely as it has access to the master DNS infrastructure - Only needs DNS record management permissions, not full CIS access - Required for multi-tenant MAS deployments with domain delegation","title":"master_ibmcloud_api_key"},{"location":"roles/cis/#master_cis_resource_group","text":"The resource group in the master account that owns the master CIS instance. Required Environment Variable: MASTER_CIS_RESOURCE_GROUP Default Value: manager Purpose : Identifies the resource group in the master IBM Cloud account where the master CIS instance (hosting the base domain) is located. When to use : Required when the master CIS instance is not in the default resource group. This is typically set by the organization managing the master domain infrastructure. Valid values : - Any valid resource group name in the master IBM Cloud account - Common values: manager , Default , DNS-Infrastructure - Must match the actual resource group of the master CIS instance - Case-sensitive Impact : - Used to locate the master CIS instance for NS record creation - Incorrect value will cause DNS delegation to fail - Must match the resource group where master_cis_resource_name exists Related variables : - master_ibmcloud_api_key - Must have access to this resource group - master_cis_resource_name - CIS instance in this resource group - master_cis_base_domain - Domain hosted in this CIS instance Notes : - Default value manager is commonly used in IBM MAS deployments - Verify the correct resource group with the master account administrator - The master API key must have access to this resource group - This is in the master account, not the customer account","title":"master_cis_resource_group"},{"location":"roles/cis/#master_cis_resource_name","text":"The name of the master CIS instance in the master account. Required Environment Variable: MASTER_CIS_RESOURCE_NAME Default Value: <mas_instance_id>-cis Purpose : Identifies the specific CIS instance in the master account that hosts the base domain. This instance will receive the NS records for subdomain delegation. When to use : The default value is usually sufficient, but can be overridden if the master CIS instance has a different naming convention. Valid values : - Any valid CIS instance name in the master account - Default format: <mas_instance_id>-cis (e.g., prod-cis , dev-cis ) - Must match an existing CIS instance in the master account - Case-sensitive Impact : - Used to locate the correct CIS instance for NS record creation - Incorrect value will cause DNS delegation to fail - Must exist in the master_cis_resource_group Related variables : - mas_instance_id - Used to generate default name - master_cis_resource_group - Resource group containing this instance - master_cis_base_domain - Domain hosted in this instance Notes : - Verify the correct instance name with the master account administrator - The default naming convention uses the MAS instance ID - This instance must already exist in the master account - Used only for NS record management, not for creating the instance","title":"master_cis_resource_name"},{"location":"roles/cis/#master_cis_base_domain","text":"The base domain hosted in the master CIS instance. Required Environment Variable: MASTER_CIS_BASE_DOMAIN Default Value: None Purpose : Specifies the parent domain in the master CIS instance under which the customer's subdomain will be delegated. This is the domain that will contain the NS records pointing to the customer's CIS nameservers. When to use : Always required for provisioning. This is typically a domain like mas.ibm.com or maximo.company.com that is managed centrally and delegates subdomains to individual deployments. Valid values : - A valid domain name hosted in the master CIS instance - Must be a domain you control and have configured in the master CIS - Examples: mas.ibm.com , maximo.example.com , apps.company.com - Should not include protocol (http/https) or trailing slashes Impact : - Customer's domain will be created as a subdomain of this base domain - NS records will be created in this domain to delegate to customer's CIS - The full customer domain will be <cluster_name>.<mas_instance_id>.<master_cis_base_domain> - Must be properly configured in the master CIS instance Related variables : - cluster_name - First part of the subdomain - mas_instance_id - Second part of the subdomain - master_cis_resource_name - CIS instance hosting this domain Notes : - This domain must already exist and be active in the master CIS instance - Verify DNS propagation of the base domain before provisioning - The base domain typically belongs to the organization, not individual customers - Example: If base domain is mas.ibm.com , customer domain might be cluster1.prod.mas.ibm.com","title":"master_cis_base_domain"},{"location":"roles/cis/#mas_instance_id","text":"The MAS instance identifier used in naming the CIS service and domain. Required Environment Variable: MAS_INSTANCE_ID Default Value: None Purpose : Provides a unique identifier for the MAS instance, used to generate the CIS service name and as part of the domain name structure. This ensures uniqueness across multiple MAS deployments. When to use : Always required. Should match the MAS instance ID used throughout your MAS deployment for consistency. Valid values : - Alphanumeric string, typically 3-12 characters - Common examples: prod , dev , test , masinst1 , mas-prod-01 - Should be unique within your organization - Lowercase recommended for DNS compatibility Impact : - Used to generate default CIS instance name: <mas_instance_id>-cis - Forms part of the domain name: <cluster_name>.<mas_instance_id>.<base_domain> - Appears in resource names and tags for identification - Should remain consistent across all MAS components Related variables : - cluster_name - Combined with this to form the full domain - master_cis_resource_name - Defaults to <mas_instance_id>-cis - master_cis_base_domain - Base domain for the subdomain Notes : - Use the same mas_instance_id across all MAS DevOps roles - Choose a meaningful identifier that indicates the environment or purpose - Cannot be changed after provisioning without recreating resources - Appears in DNS names, so keep it short and DNS-compatible","title":"mas_instance_id"},{"location":"roles/cis/#cluster_name","text":"The OpenShift cluster name used as a prefix in the domain name. Required Environment Variable: CLUSTER_NAME Default Value: None Purpose : Identifies the specific OpenShift cluster for this CIS configuration. Used as the first component of the subdomain name, allowing multiple clusters to share the same MAS instance ID. When to use : Always required. Should match the OpenShift cluster name used in your deployment. Valid values : - Alphanumeric string with hyphens - Common examples: ocp-prod , rosa-cluster , roks-dev , cluster1 - Should be unique within the MAS instance - Lowercase recommended for DNS compatibility Impact : - Forms the first part of the domain: <cluster_name>.<mas_instance_id>.<base_domain> - Used to generate the CIS service name - Helps identify which cluster the CIS instance serves - Appears in DNS records and resource names Related variables : - mas_instance_id - Combined with this to form the full domain - master_cis_base_domain - Base domain for the subdomain Notes : - Should match your OpenShift cluster name for consistency - Multiple clusters can use the same mas_instance_id with different cluster_name values - Keep it short and DNS-compatible (no special characters except hyphens) - Example: cluster_name= rosa-prod , mas_instance_id= mas01 , base= ibm.com \u2192 rosa-prod.mas01.ibm.com","title":"cluster_name"},{"location":"roles/cis/#mas_config_dir","text":"Local directory where generated CIS configuration will be saved. Required Environment Variable: MAS_CONFIG_DIR Default Value: None Purpose : Specifies the local directory path where the role will save generated Kubernetes ConfigMaps and CIS instance details for use by other MAS components. When to use : Always required. This directory serves as the central location for all MAS configuration artifacts and should be consistent across all MAS DevOps roles. Valid values : - Any valid local filesystem path (e.g., ~/masconfig , /opt/mas/config ) - Directory will be created if it doesn't exist - Should have write permissions for the user running the playbook - Recommended to use an absolute path for consistency Impact : - CIS instance details are saved as ConfigMap YAML files in this directory - Required for subsequent MAS installation and configuration steps - Other roles will read CIS configuration from this location - Should be backed up as it contains important configuration data Related variables : - Used by all MAS DevOps roles for consistent configuration storage - CIS-specific files will be created with identifiable names Notes : - Use the same mas_config_dir across all MAS DevOps roles for consistency - Ensure the directory is accessible and has sufficient storage space - Consider using version control for the configuration directory (excluding sensitive data) - The directory structure follows MAS configuration conventions - ConfigMaps generated here are applied to the OpenShift cluster during MAS installation","title":"mas_config_dir"},{"location":"roles/cis/#example-playbook","text":"Create CIS Instance alongwith save Instance details in MAS_CONFIG_DIR path as ConfigMap - hosts: localhost any_errors_fatal: true vars: cis_action: provision mas_instance_id: masinst1 mas_config_dir: ~/masconfig ibmcloud_apikey: \"****\" master_ibmcloud_api_key: \"******\" cluster_name: \"test\" roles: - ibm.mas_devops.cis","title":"Example Playbook"},{"location":"roles/cis/#license","text":"EPL-2.0","title":"License"},{"location":"roles/configure_manage_eventstreams/","text":"configure_manage_eventstreams \u00a4 This role configures manage to use IBM Cloud Eventstreams. NOTE This role inserts dummy kafka password during configuration (not the actual one),so user has to follow below guide to configure kafka password manually via Manage Dashboard. Role Variables \u00a4 mas_instance_id \u00a4 Required Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \u00a4 Required Environment Variable: MAS_WORKSPACE_ID Default Value: None ibmcloud_apikey \u00a4 Required Environment Variable: IBMCLOUD_APIKEY Default Value: None ibmcloud_region \u00a4 Optional Environment Variable: IBMCLOUD_REGION Default Value: us-east ibmcloud_resourcegroup \u00a4 IBM Cloud Resource Group Name where the IBM Cloud Eventstreams is provisioned. - Optional - Environment Variable: IBMCLOUD_RESOURCEGROUP - Default Value: Default eventstreams_name \u00a4 IBM Cloud Eventstreams Service Name - Required - Environment Variable: EVENTSTREAMS_NAME - Default Value: None eventstreams_location \u00a4 IBM Cloud Eventstreams Service Location - Optional - Environment Variable: EVENTSTREAMS_LOCATION - Default Value: us-east db2wh_dbname \u00a4 DB2 Database name where configurations will be done for Manage to use IBM Cloud Eventstreams - Optional - Environment Variable: DB2WH_DBNAME - Default Value: BLUDB cpd_meta_namespace \u00a4 Required Environment Variable: CPD_NAMESPACE Default Value: None db2_instance_name \u00a4 Required to build up pod name running db2 Required Environment Variable: DB2_INSTANCE_NAME Default Value: None mas_app_id \u00a4 Optional Environment Variable: MAS_APP_ID Default Value: manage mas_app_ws_fqn \u00a4 Fully Qualified Name for MAS Application - Optional - Environment Variable: MAS_APP_WS_FQN - Default Value: manageworkspaces.apps.mas.ibm.com Example Playbook \u00a4 Configures IBM Cloud Evenstreams service with Manage - hosts: localhost any_errors_fatal: true vars: mas_instance_id: 'test-instance' mas_workspace_id: 'main' ibmcloud_apikey: 'test-api-key' eventstreams_name: 'test-es' cpd_meta_namespace: 'db2u' db2_instance_name: 'test-db2' roles: - ibm.mas_devops.configure_manage_cfg License \u00a4 EPL-2.0","title":"configure_manage_eventstreams"},{"location":"roles/configure_manage_eventstreams/#configure_manage_eventstreams","text":"This role configures manage to use IBM Cloud Eventstreams. NOTE This role inserts dummy kafka password during configuration (not the actual one),so user has to follow below guide to configure kafka password manually via Manage Dashboard.","title":"configure_manage_eventstreams"},{"location":"roles/configure_manage_eventstreams/#role-variables","text":"","title":"Role Variables"},{"location":"roles/configure_manage_eventstreams/#mas_instance_id","text":"Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/configure_manage_eventstreams/#mas_workspace_id","text":"Required Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/configure_manage_eventstreams/#ibmcloud_apikey","text":"Required Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/configure_manage_eventstreams/#ibmcloud_region","text":"Optional Environment Variable: IBMCLOUD_REGION Default Value: us-east","title":"ibmcloud_region"},{"location":"roles/configure_manage_eventstreams/#ibmcloud_resourcegroup","text":"IBM Cloud Resource Group Name where the IBM Cloud Eventstreams is provisioned. - Optional - Environment Variable: IBMCLOUD_RESOURCEGROUP - Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/configure_manage_eventstreams/#eventstreams_name","text":"IBM Cloud Eventstreams Service Name - Required - Environment Variable: EVENTSTREAMS_NAME - Default Value: None","title":"eventstreams_name"},{"location":"roles/configure_manage_eventstreams/#eventstreams_location","text":"IBM Cloud Eventstreams Service Location - Optional - Environment Variable: EVENTSTREAMS_LOCATION - Default Value: us-east","title":"eventstreams_location"},{"location":"roles/configure_manage_eventstreams/#db2wh_dbname","text":"DB2 Database name where configurations will be done for Manage to use IBM Cloud Eventstreams - Optional - Environment Variable: DB2WH_DBNAME - Default Value: BLUDB","title":"db2wh_dbname"},{"location":"roles/configure_manage_eventstreams/#cpd_meta_namespace","text":"Required Environment Variable: CPD_NAMESPACE Default Value: None","title":"cpd_meta_namespace"},{"location":"roles/configure_manage_eventstreams/#db2_instance_name","text":"Required to build up pod name running db2 Required Environment Variable: DB2_INSTANCE_NAME Default Value: None","title":"db2_instance_name"},{"location":"roles/configure_manage_eventstreams/#mas_app_id","text":"Optional Environment Variable: MAS_APP_ID Default Value: manage","title":"mas_app_id"},{"location":"roles/configure_manage_eventstreams/#mas_app_ws_fqn","text":"Fully Qualified Name for MAS Application - Optional - Environment Variable: MAS_APP_WS_FQN - Default Value: manageworkspaces.apps.mas.ibm.com","title":"mas_app_ws_fqn"},{"location":"roles/configure_manage_eventstreams/#example-playbook","text":"Configures IBM Cloud Evenstreams service with Manage - hosts: localhost any_errors_fatal: true vars: mas_instance_id: 'test-instance' mas_workspace_id: 'main' ibmcloud_apikey: 'test-api-key' eventstreams_name: 'test-es' cpd_meta_namespace: 'db2u' db2_instance_name: 'test-db2' roles: - ibm.mas_devops.configure_manage_cfg","title":"Example Playbook"},{"location":"roles/configure_manage_eventstreams/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cos/","text":"cos \u00a4 This role provides support for: Provisioning and Configuring Cloud Object Storage in MAS. It currently supports two providers: In-cluster Ceph Object Storage leveraging OpenShift Container Storage IBM Cloud Object Storage Deprovision Cloud Object Store. It currently supports one provider: IBM Cloud Object Storage Currently this role only supports generating a system-scoped ObjectStorageCfg resource, but the generated file can be modified if you wish to use other scopes. Role Variables - General \u00a4 Role Variables - General \u00a4 cos_type \u00a4 The Cloud Object Storage provider to use for MAS. Required Environment Variable: COS_TYPE Default Value: None Purpose : Determines which object storage backend will be provisioned and configured for MAS. Different providers have different capabilities, costs, and operational characteristics. When to use : Always required when provisioning object storage. Choose based on your deployment platform, cost requirements, and operational preferences. Valid values : - ibm - IBM Cloud Object Storage (managed cloud service) - ocs - OpenShift Container Storage / OpenShift Data Foundation (in-cluster Ceph-based storage) Impact : - ibm : Provisions IBM Cloud COS instance, requires IBM Cloud account and API key, incurs cloud costs, provides unlimited scalability - ocs : Creates Ceph object store within OpenShift cluster, requires OCS/ODF operator installed, uses cluster storage capacity, no external costs - Determines which set of configuration variables are required - Affects the generated ObjectStorageCfg resource format Related variables : - When cos_type=ibm : Requires ibmcloud_apikey , cos_instance_name , cos_url - When cos_type=ocs : Requires OCS/ODF operator to be installed in the cluster - cos_action - Whether to provision or deprovision Notes : - IBM COS is recommended for production deployments requiring high availability and unlimited capacity - OCS is suitable for on-premises or air-gapped deployments where external cloud services are not available - OCS requires sufficient cluster storage capacity and OCS/ODF operator installation - IBM COS provides better separation of concerns (storage managed separately from compute) - Consider data residency and compliance requirements when choosing provider cos_action \u00a4 The action to perform on the Cloud Object Storage instance. Required Environment Variable: COS_ACTION Default Value: provision Purpose : Controls whether to create a new COS instance or delete an existing one. This allows the same role to handle both lifecycle operations. When to use : - Use provision (default) when setting up object storage for a new MAS deployment - Use deprovision when cleaning up COS resources after MAS uninstallation Valid values : - provision - Create and configure COS instance, generate ObjectStorageCfg - deprovision - Delete COS instance (IBM COS only, not supported for OCS) Impact : - Provision : Creates COS instance, generates credentials, creates ObjectStorageCfg resource - Deprovision : Permanently deletes IBM Cloud COS instance and all data (irreversible) - Deprovisioning is only supported for cos_type=ibm , not for OCS - All data stored in the COS instance will be lost during deprovisioning Related variables : - cos_type - Determines which provider to provision/deprovision - cos_instance_name - Identifies which instance to deprovision Notes : - Deprovisioning is irreversible - ensure all data is backed up first - For OCS, deprovisioning must be done manually through OpenShift console - Verify no applications are using the COS instance before deprovisioning - IBM Cloud COS deprovisioning may fail if buckets contain data ocp_ingress_tls_secret_name \u00a4 The name of the OpenShift cluster's default router certificate secret. Optional Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default Purpose : Specifies the secret containing the cluster's ingress TLS certificate. This is used when configuring OCS to ensure proper certificate validation for S3 API access. When to use : Only needed in rare cases where the cluster uses a non-standard ingress certificate secret name, or when the role cannot automatically determine the correct secret name. Valid values : - Any valid secret name in the openshift-ingress namespace - Default router-certs-default works for most OpenShift clusters - Must contain TLS certificate and key Impact : - Used to extract the cluster's ingress certificate for OCS configuration - Incorrect value will cause certificate validation failures when accessing OCS - Only relevant when cos_type=ocs Related variables : - cos_type - Only used when set to ocs - include_cluster_ingress_cert_chain - Controls certificate chain inclusion Notes : - The default value works for standard OpenShift installations - Only override if you have customized your cluster's ingress certificates - The secret must exist in the openshift-ingress namespace - Not used for IBM Cloud COS ( cos_type=ibm ) custom_labels \u00a4 Custom key-value labels to apply to provisioned resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Purpose : Allows adding custom labels to COS-related resources for organization, cost tracking, or automation purposes. Labels are applied to instance-specific resources. When to use : When you need to tag resources for cost allocation, environment identification, team ownership, or integration with other automation tools. Valid values : - Comma-separated key=value pairs - Example: environment=production,team=platform,cost-center=engineering - Keys and values must follow Kubernetes label naming conventions - Maximum 63 characters per key/value Impact : - Labels are applied to COS instance and related resources - Useful for cost tracking and resource organization in IBM Cloud - Can be used by automation tools for resource discovery - Does not affect functionality, only metadata Related variables : - Applied to resources created by this role regardless of cos_type Notes : - Labels are metadata only and do not affect resource behavior - Use consistent labeling across all MAS resources for better management - IBM Cloud supports label-based cost allocation and reporting - Labels can be modified after resource creation Role Variables - IBM COS \u00a4 cos_instance_name \u00a4 The name for the IBM Cloud Object Storage instance. Optional (IBM COS only) Environment Variable: COS_INSTANCE_NAME Default Value: Object Storage for MAS (appended with MAS instance ID if set) Purpose : Specifies a custom name for the IBM Cloud COS instance. This name appears in the IBM Cloud console and is used for identification and management. When to use : Only relevant when cos_type=ibm . Provide a descriptive name that follows your organization's naming conventions. Valid values : - Any string following IBM Cloud resource naming conventions - If not specified, defaults to Object Storage for MAS or Object Storage for MAS <mas_instance_id> - Recommended format: <environment>-<purpose>-cos (e.g., prod-mas-cos , dev-maximo-cos ) Impact : - Used as the display name in IBM Cloud console - Helps identify the COS instance among other cloud resources - Used during deprovisioning to locate the instance - Does not affect functionality, only identification Related variables : - cos_type - Must be ibm for this to be used - mas_instance_id - Appended to default name if set - cos_resourcegroup - Resource group containing this instance Notes : - Not used for OCS ( cos_type=ocs ) - Use descriptive names for easier management in multi-instance environments - The name can include the MAS instance ID for clarity - IBM Cloud allows renaming instances after creation cos_location_info \u00a4 The geographic location where the IBM Cloud COS instance will be available. Required (IBM COS only) Environment Variable: COS_LOCATION Default Value: global Purpose : Specifies the geographic scope of the IBM Cloud COS instance. This determines data residency and availability characteristics. When to use : Only relevant when cos_type=ibm . Use global for maximum flexibility, or specify a region for data residency compliance. Valid values : - global - Instance available globally with automatic region selection - Specific region codes: us-south , us-east , eu-gb , eu-de , jp-tok , au-syd , etc. - Cross-region: us , eu , ap for multi-region redundancy Impact : - global provides maximum flexibility and automatic failover - Regional instances ensure data stays within specific geographic boundaries - Affects data residency compliance and latency - Cannot be changed after instance creation Related variables : - cos_type - Must be ibm for this to be used - cos_url - Should match the location for optimal performance Notes : - global is recommended for most deployments unless data residency is required - Regional instances may have lower latency for local access - Consider compliance requirements (GDPR, data sovereignty) when choosing location - Not applicable for OCS ( cos_type=ocs ) cos_plan_type \u00a4 The IBM Cloud COS service plan tier. Required (IBM COS only, for provisioning) Environment Variable: COS_PLAN Default Value: standard Purpose : Specifies the IBM Cloud COS service plan, which determines pricing and available features. When to use : Only relevant when cos_type=ibm and cos_action=provision . The standard plan is suitable for most MAS deployments. Valid values : - standard - Standard plan with pay-as-you-go pricing - lite - Free tier with limited capacity (not recommended for production) Impact : - Determines pricing model and billing - Standard plan provides unlimited capacity with usage-based pricing - Lite plan has capacity limits and is not suitable for production MAS - Cannot be changed after provisioning (requires recreation) Related variables : - cos_type - Must be ibm for this to be used - cos_action - Only used during provisioning Notes : - Always use standard plan for production MAS deployments - Lite plan is only suitable for development/testing with minimal data - Pricing is based on storage capacity, requests, and data transfer - Not applicable for OCS ( cos_type=ocs ) cos_url \u00a4 The IBM Cloud COS regional endpoint URL for S3 API access. Required (IBM COS only, for provisioning) Environment Variable: COS_REGION_LOCATION_URL Default Value: https://s3.us.cloud-object-storage.appdomain.cloud Purpose : Specifies the S3-compatible API endpoint URL for accessing the IBM Cloud COS instance. This URL is used in the generated ObjectStorageCfg resource for MAS to connect to COS. When to use : Only relevant when cos_type=ibm . Should match the region where your COS instance is located for optimal performance. Valid values : - Regional endpoints: https://s3.<region>.cloud-object-storage.appdomain.cloud - Examples: - US: https://s3.us.cloud-object-storage.appdomain.cloud - EU: https://s3.eu.cloud-object-storage.appdomain.cloud - AP: https://s3.ap.cloud-object-storage.appdomain.cloud - Private endpoints: https://s3.private.<region>.cloud-object-storage.appdomain.cloud - Direct endpoints: https://s3.direct.<region>.cloud-object-storage.appdomain.cloud Impact : - Determines which IBM Cloud COS endpoint MAS will use for object storage operations - Affects network latency and data transfer costs - Private endpoints provide better security and lower costs for in-cloud access - Must be accessible from the OpenShift cluster Related variables : - cos_type - Must be ibm for this to be used - cos_location_info - Should align with the endpoint region Notes : - Use regional endpoints matching your COS instance location for best performance - Private endpoints recommended for OpenShift clusters running in IBM Cloud - Direct endpoints provide better performance for high-throughput workloads - Ensure network connectivity from OpenShift cluster to the endpoint - Not applicable for OCS ( cos_type=ocs ) cos_resource_key_iam_role \u00a4 The IAM role to assign to the COS service credentials. Optional (IBM COS only) Environment Variable: COS_RESOURCE_KEY_IAM_ROLE Default Value: Manager Purpose : Specifies the IAM role level for the service credentials (resource key) created during COS provisioning. This determines the permissions granted to MAS for accessing the COS instance. When to use : Only relevant when cos_type=ibm . The default Manager role provides full access required by MAS. Valid values : - Manager - Full access to COS instance (create/read/update/delete buckets and objects) - Writer - Read and write access to objects, limited bucket operations - Reader - Read-only access to objects - Content Reader - Read-only access without listing capabilities Impact : - Determines what operations MAS can perform on the COS instance - Manager role is required for MAS to create and manage buckets - Lower privilege roles will cause MAS functionality to fail - Credentials are generated with this role during provisioning Related variables : - cos_type - Must be ibm for this to be used - cos_use_hmac - Controls credential format Notes : - Always use Manager role for MAS deployments (default) - Lower privilege roles are not sufficient for MAS operations - The role is assigned to the service credentials, not the instance itself - Not applicable for OCS ( cos_type=ocs ) cos_use_hmac \u00a4 Controls whether to use HMAC-style credentials for COS access. Optional (IBM COS only) Environment Variable: COS_USE_HMAC Default Value: true Purpose : Determines whether to generate HMAC (AWS S3-compatible) credentials or IAM-based credentials for accessing IBM Cloud COS. MAS requires HMAC credentials for S3 API compatibility. When to use : Should always be true for MAS deployments. Setting to false will prevent MAS from accessing the COS instance. Valid values : - true - Generate HMAC credentials (AWS S3-compatible access key and secret key) - false - Use IAM API key authentication (not compatible with MAS) Impact : - When true : Generates S3-compatible access key ID and secret access key - When false : MAS will not be able to access the COS instance - HMAC credentials are required for S3 API compatibility - Cannot be changed after credential creation (requires new credentials) Related variables : - cos_type - Must be ibm for this to be used - cos_resource_key_iam_role - Role assigned to HMAC credentials Notes : - Always use true for MAS deployments - this is critical - HMAC credentials provide S3 API compatibility required by MAS - Setting to false will cause MAS object storage configuration to fail - Not applicable for OCS ( cos_type=ocs ) cos_apikey \u00a4 The IBM Cloud API key specifically for COS operations. Required (IBM COS only) Environment Variable: COS_APIKEY Default Value: Falls back to ibmcloud_apikey if not set Purpose : Provides an optional dedicated API key for COS operations. This allows using a less privileged API key specifically for COS management, following the principle of least privilege. When to use : Only relevant when cos_type=ibm . Set this if you want to use a different API key for COS than for other IBM Cloud operations. Valid values : - A valid IBM Cloud API key with COS service permissions - Must have permissions to create and manage COS instances - Format: 44-character alphanumeric string Impact : - If set, this API key is used instead of ibmcloud_apikey for COS operations - Allows separation of permissions between COS and other IBM Cloud services - Falls back to ibmcloud_apikey if not specified Related variables : - ibmcloud_apikey - Fallback if this is not set - cos_type - Must be ibm for this to be used Notes : - Optional - defaults to ibmcloud_apikey if not provided - Use a dedicated key for better security and permission isolation - The key must have COS service management permissions - Not applicable for OCS ( cos_type=ocs ) ibmcloud_apikey \u00a4 The default IBM Cloud API key for all IBM Cloud operations. Required (IBM COS only) Environment Variable: IBMCLOUD_APIKEY Default Value: None Purpose : Provides the primary IBM Cloud API key used across multiple roles in the collection. This is the default authentication method for IBM Cloud services. When to use : Always required when cos_type=ibm . This API key must have permissions to create and manage COS instances. Valid values : - A valid IBM Cloud API key with appropriate permissions - Must have Editor or Administrator role on COS service - Must have access to the specified resource group - Format: 44-character alphanumeric string Impact : - Used for all IBM Cloud operations unless cos_apikey is specified - Determines which IBM Cloud account owns the COS instance - Account will be billed for COS usage - Required permissions: COS service management, resource group access Related variables : - cos_apikey - Can override this for COS-specific operations - ibmcloud_resourcegroup - Must have access to this resource group - cos_type - Must be ibm for this to be used Notes : - Store API keys securely using environment variables or secrets management - Never commit API keys to version control - Consider using service IDs with restricted permissions - Used by multiple roles in the collection for consistency - Not applicable for OCS ( cos_type=ocs ) cos_resourcegroup \u00a4 The IBM Cloud resource group for the COS instance. Optional (IBM COS only) Environment Variable: COS_RESOURCEGROUP Default Value: Falls back to ibmcloud_resourcegroup Purpose : Specifies the resource group in IBM Cloud where the COS instance will be created. This allows using a different resource group for COS than for other IBM Cloud resources. When to use : Only relevant when cos_type=ibm . Set this if you want to organize COS in a different resource group than other IBM Cloud resources. Valid values : - Any existing resource group name in your IBM Cloud account - Common examples: Default , Storage , MAS-Resources - Must exist before provisioning - Case-sensitive Impact : - Determines billing and cost allocation for the COS instance - Controls IAM access policies - Falls back to ibmcloud_resourcegroup if not specified - Cannot be changed after instance creation Related variables : - ibmcloud_resourcegroup - Fallback if this is not set - ibmcloud_apikey - Must have access to this resource group - cos_type - Must be ibm for this to be used Notes : - Optional - defaults to ibmcloud_resourcegroup if not provided - Use for organizing COS separately from other resources - Ensure the API key has access to the specified resource group - Not applicable for OCS ( cos_type=ocs ) ibmcloud_resourcegroup \u00a4 The default IBM Cloud resource group for all IBM Cloud resources. Optional (IBM COS only) Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default Purpose : Specifies the default resource group used across multiple roles in the collection. This provides consistency in resource organization. When to use : Only relevant when cos_type=ibm . Set this to organize all IBM Cloud resources in a specific resource group. Valid values : - Any existing resource group name in your IBM Cloud account - Common examples: Default , Production , Development - Must exist before provisioning - Case-sensitive Impact : - Used as the default resource group for COS and other IBM Cloud resources - Determines billing and cost allocation - Controls IAM access policies - Can be overridden by cos_resourcegroup for COS-specific placement Related variables : - cos_resourcegroup - Can override this for COS - ibmcloud_apikey - Must have access to this resource group - cos_type - Must be ibm for this to be used Notes : - Defaults to Default resource group if not specified - Use consistent resource groups across all MAS-related services - Helps with cost tracking and access management - Not applicable for OCS ( cos_type=ocs ) Role Variables - MAS Configuration \u00a4 mas_instance_id \u00a4 The MAS instance ID that the ObjectStorageCfg will target. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None Purpose : Identifies the specific MAS instance for which the ObjectStorageCfg resource will be generated. This ensures the object storage configuration is associated with the correct MAS deployment. When to use : Required if you want the role to generate an ObjectStorageCfg resource. If not set (along with mas_config_dir ), the role will only provision the storage but not generate MAS configuration. Valid values : - Alphanumeric string, typically 3-12 characters - Common examples: prod , dev , test , masinst1 - Should match the MAS instance ID used throughout your deployment - Lowercase recommended Impact : - Used to generate the ObjectStorageCfg resource name and namespace - If not set, no ObjectStorageCfg will be generated (storage only) - Must match the actual MAS instance ID for configuration to work - Both this and mas_config_dir must be set for config generation Related variables : - mas_config_dir - Both must be set for config generation - Used consistently across all MAS DevOps roles Notes : - Use the same mas_instance_id across all MAS components - If you only want to provision storage without MAS config, leave this unset - The ObjectStorageCfg will be created in the mas-<instance-id>-core namespace - Required for automated MAS configuration via suite_config role mas_config_dir \u00a4 Local directory where the ObjectStorageCfg resource will be saved. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None Purpose : Specifies the local directory path where the generated ObjectStorageCfg YAML file will be saved. This file can be used to manually configure MAS or as input to the suite_config role. When to use : Required if you want the role to generate an ObjectStorageCfg resource. If not set (along with mas_instance_id ), the role will only provision the storage but not generate MAS configuration. Valid values : - Any valid local filesystem path (e.g., ~/masconfig , /opt/mas/config ) - Directory will be created if it doesn't exist - Should have write permissions - Recommended to use an absolute path Impact : - ObjectStorageCfg YAML file is saved in this directory - If not set, no ObjectStorageCfg will be generated (storage only) - Required for subsequent MAS configuration steps - Both this and mas_instance_id must be set for config generation Related variables : - mas_instance_id - Both must be set for config generation - Used consistently across all MAS DevOps roles for configuration storage Notes : - Use the same mas_config_dir across all MAS DevOps roles - The generated file can be applied manually with oc apply -f - Can be used as input to the suite_config role for automated configuration - If you only want to provision storage without MAS config, leave this unset include_cluster_ingress_cert_chain \u00a4 Controls whether to include the complete certificate chain in the ObjectStorageCfg. Optional Environment Variable: INCLUDE_CLUSTER_INGRESS_CERT_CHAIN Default Value: False Purpose : When enabled, includes the complete TLS certificate chain from the cluster's ingress in the generated ObjectStorageCfg resource. This is useful when the cluster uses certificates from a trusted certificate authority. When to use : Set to True when your OpenShift cluster's ingress uses certificates signed by a trusted CA and you want MAS to validate the complete certificate chain when accessing OCS. Valid values : - True - Include complete certificate chain in ObjectStorageCfg - False - Include only the leaf certificate (default) Impact : - When True : ObjectStorageCfg includes full certificate chain for proper validation - When False : Only the leaf certificate is included - Primarily relevant for OCS ( cos_type=ocs ) with custom certificates - Helps with certificate validation in environments with intermediate CAs Related variables : - cos_type - Most relevant for ocs - ocp_ingress_tls_secret_name - Source of certificates Notes : - Default False is sufficient for most deployments - Enable if you experience certificate validation issues with OCS - Not typically needed for IBM Cloud COS ( cos_type=ibm ) - The certificate chain is extracted from the cluster's ingress configuration Example Playbook \u00a4 Create the Ceph Object store on the existing OCS cluster and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ocs mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos Create the IBM Cloud Object storage Instance and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm ibmcloud_apikey: <Your IBM Cloud API Key> mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos License \u00a4 EPL-2.0","title":"cos"},{"location":"roles/cos/#cos","text":"This role provides support for: Provisioning and Configuring Cloud Object Storage in MAS. It currently supports two providers: In-cluster Ceph Object Storage leveraging OpenShift Container Storage IBM Cloud Object Storage Deprovision Cloud Object Store. It currently supports one provider: IBM Cloud Object Storage Currently this role only supports generating a system-scoped ObjectStorageCfg resource, but the generated file can be modified if you wish to use other scopes.","title":"cos"},{"location":"roles/cos/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/cos/#role-variables-general_1","text":"","title":"Role Variables - General"},{"location":"roles/cos/#cos_type","text":"The Cloud Object Storage provider to use for MAS. Required Environment Variable: COS_TYPE Default Value: None Purpose : Determines which object storage backend will be provisioned and configured for MAS. Different providers have different capabilities, costs, and operational characteristics. When to use : Always required when provisioning object storage. Choose based on your deployment platform, cost requirements, and operational preferences. Valid values : - ibm - IBM Cloud Object Storage (managed cloud service) - ocs - OpenShift Container Storage / OpenShift Data Foundation (in-cluster Ceph-based storage) Impact : - ibm : Provisions IBM Cloud COS instance, requires IBM Cloud account and API key, incurs cloud costs, provides unlimited scalability - ocs : Creates Ceph object store within OpenShift cluster, requires OCS/ODF operator installed, uses cluster storage capacity, no external costs - Determines which set of configuration variables are required - Affects the generated ObjectStorageCfg resource format Related variables : - When cos_type=ibm : Requires ibmcloud_apikey , cos_instance_name , cos_url - When cos_type=ocs : Requires OCS/ODF operator to be installed in the cluster - cos_action - Whether to provision or deprovision Notes : - IBM COS is recommended for production deployments requiring high availability and unlimited capacity - OCS is suitable for on-premises or air-gapped deployments where external cloud services are not available - OCS requires sufficient cluster storage capacity and OCS/ODF operator installation - IBM COS provides better separation of concerns (storage managed separately from compute) - Consider data residency and compliance requirements when choosing provider","title":"cos_type"},{"location":"roles/cos/#cos_action","text":"The action to perform on the Cloud Object Storage instance. Required Environment Variable: COS_ACTION Default Value: provision Purpose : Controls whether to create a new COS instance or delete an existing one. This allows the same role to handle both lifecycle operations. When to use : - Use provision (default) when setting up object storage for a new MAS deployment - Use deprovision when cleaning up COS resources after MAS uninstallation Valid values : - provision - Create and configure COS instance, generate ObjectStorageCfg - deprovision - Delete COS instance (IBM COS only, not supported for OCS) Impact : - Provision : Creates COS instance, generates credentials, creates ObjectStorageCfg resource - Deprovision : Permanently deletes IBM Cloud COS instance and all data (irreversible) - Deprovisioning is only supported for cos_type=ibm , not for OCS - All data stored in the COS instance will be lost during deprovisioning Related variables : - cos_type - Determines which provider to provision/deprovision - cos_instance_name - Identifies which instance to deprovision Notes : - Deprovisioning is irreversible - ensure all data is backed up first - For OCS, deprovisioning must be done manually through OpenShift console - Verify no applications are using the COS instance before deprovisioning - IBM Cloud COS deprovisioning may fail if buckets contain data","title":"cos_action"},{"location":"roles/cos/#ocp_ingress_tls_secret_name","text":"The name of the OpenShift cluster's default router certificate secret. Optional Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default Purpose : Specifies the secret containing the cluster's ingress TLS certificate. This is used when configuring OCS to ensure proper certificate validation for S3 API access. When to use : Only needed in rare cases where the cluster uses a non-standard ingress certificate secret name, or when the role cannot automatically determine the correct secret name. Valid values : - Any valid secret name in the openshift-ingress namespace - Default router-certs-default works for most OpenShift clusters - Must contain TLS certificate and key Impact : - Used to extract the cluster's ingress certificate for OCS configuration - Incorrect value will cause certificate validation failures when accessing OCS - Only relevant when cos_type=ocs Related variables : - cos_type - Only used when set to ocs - include_cluster_ingress_cert_chain - Controls certificate chain inclusion Notes : - The default value works for standard OpenShift installations - Only override if you have customized your cluster's ingress certificates - The secret must exist in the openshift-ingress namespace - Not used for IBM Cloud COS ( cos_type=ibm )","title":"ocp_ingress_tls_secret_name"},{"location":"roles/cos/#custom_labels","text":"Custom key-value labels to apply to provisioned resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Purpose : Allows adding custom labels to COS-related resources for organization, cost tracking, or automation purposes. Labels are applied to instance-specific resources. When to use : When you need to tag resources for cost allocation, environment identification, team ownership, or integration with other automation tools. Valid values : - Comma-separated key=value pairs - Example: environment=production,team=platform,cost-center=engineering - Keys and values must follow Kubernetes label naming conventions - Maximum 63 characters per key/value Impact : - Labels are applied to COS instance and related resources - Useful for cost tracking and resource organization in IBM Cloud - Can be used by automation tools for resource discovery - Does not affect functionality, only metadata Related variables : - Applied to resources created by this role regardless of cos_type Notes : - Labels are metadata only and do not affect resource behavior - Use consistent labeling across all MAS resources for better management - IBM Cloud supports label-based cost allocation and reporting - Labels can be modified after resource creation","title":"custom_labels"},{"location":"roles/cos/#role-variables-ibm-cos","text":"","title":"Role Variables - IBM COS"},{"location":"roles/cos/#cos_instance_name","text":"The name for the IBM Cloud Object Storage instance. Optional (IBM COS only) Environment Variable: COS_INSTANCE_NAME Default Value: Object Storage for MAS (appended with MAS instance ID if set) Purpose : Specifies a custom name for the IBM Cloud COS instance. This name appears in the IBM Cloud console and is used for identification and management. When to use : Only relevant when cos_type=ibm . Provide a descriptive name that follows your organization's naming conventions. Valid values : - Any string following IBM Cloud resource naming conventions - If not specified, defaults to Object Storage for MAS or Object Storage for MAS <mas_instance_id> - Recommended format: <environment>-<purpose>-cos (e.g., prod-mas-cos , dev-maximo-cos ) Impact : - Used as the display name in IBM Cloud console - Helps identify the COS instance among other cloud resources - Used during deprovisioning to locate the instance - Does not affect functionality, only identification Related variables : - cos_type - Must be ibm for this to be used - mas_instance_id - Appended to default name if set - cos_resourcegroup - Resource group containing this instance Notes : - Not used for OCS ( cos_type=ocs ) - Use descriptive names for easier management in multi-instance environments - The name can include the MAS instance ID for clarity - IBM Cloud allows renaming instances after creation","title":"cos_instance_name"},{"location":"roles/cos/#cos_location_info","text":"The geographic location where the IBM Cloud COS instance will be available. Required (IBM COS only) Environment Variable: COS_LOCATION Default Value: global Purpose : Specifies the geographic scope of the IBM Cloud COS instance. This determines data residency and availability characteristics. When to use : Only relevant when cos_type=ibm . Use global for maximum flexibility, or specify a region for data residency compliance. Valid values : - global - Instance available globally with automatic region selection - Specific region codes: us-south , us-east , eu-gb , eu-de , jp-tok , au-syd , etc. - Cross-region: us , eu , ap for multi-region redundancy Impact : - global provides maximum flexibility and automatic failover - Regional instances ensure data stays within specific geographic boundaries - Affects data residency compliance and latency - Cannot be changed after instance creation Related variables : - cos_type - Must be ibm for this to be used - cos_url - Should match the location for optimal performance Notes : - global is recommended for most deployments unless data residency is required - Regional instances may have lower latency for local access - Consider compliance requirements (GDPR, data sovereignty) when choosing location - Not applicable for OCS ( cos_type=ocs )","title":"cos_location_info"},{"location":"roles/cos/#cos_plan_type","text":"The IBM Cloud COS service plan tier. Required (IBM COS only, for provisioning) Environment Variable: COS_PLAN Default Value: standard Purpose : Specifies the IBM Cloud COS service plan, which determines pricing and available features. When to use : Only relevant when cos_type=ibm and cos_action=provision . The standard plan is suitable for most MAS deployments. Valid values : - standard - Standard plan with pay-as-you-go pricing - lite - Free tier with limited capacity (not recommended for production) Impact : - Determines pricing model and billing - Standard plan provides unlimited capacity with usage-based pricing - Lite plan has capacity limits and is not suitable for production MAS - Cannot be changed after provisioning (requires recreation) Related variables : - cos_type - Must be ibm for this to be used - cos_action - Only used during provisioning Notes : - Always use standard plan for production MAS deployments - Lite plan is only suitable for development/testing with minimal data - Pricing is based on storage capacity, requests, and data transfer - Not applicable for OCS ( cos_type=ocs )","title":"cos_plan_type"},{"location":"roles/cos/#cos_url","text":"The IBM Cloud COS regional endpoint URL for S3 API access. Required (IBM COS only, for provisioning) Environment Variable: COS_REGION_LOCATION_URL Default Value: https://s3.us.cloud-object-storage.appdomain.cloud Purpose : Specifies the S3-compatible API endpoint URL for accessing the IBM Cloud COS instance. This URL is used in the generated ObjectStorageCfg resource for MAS to connect to COS. When to use : Only relevant when cos_type=ibm . Should match the region where your COS instance is located for optimal performance. Valid values : - Regional endpoints: https://s3.<region>.cloud-object-storage.appdomain.cloud - Examples: - US: https://s3.us.cloud-object-storage.appdomain.cloud - EU: https://s3.eu.cloud-object-storage.appdomain.cloud - AP: https://s3.ap.cloud-object-storage.appdomain.cloud - Private endpoints: https://s3.private.<region>.cloud-object-storage.appdomain.cloud - Direct endpoints: https://s3.direct.<region>.cloud-object-storage.appdomain.cloud Impact : - Determines which IBM Cloud COS endpoint MAS will use for object storage operations - Affects network latency and data transfer costs - Private endpoints provide better security and lower costs for in-cloud access - Must be accessible from the OpenShift cluster Related variables : - cos_type - Must be ibm for this to be used - cos_location_info - Should align with the endpoint region Notes : - Use regional endpoints matching your COS instance location for best performance - Private endpoints recommended for OpenShift clusters running in IBM Cloud - Direct endpoints provide better performance for high-throughput workloads - Ensure network connectivity from OpenShift cluster to the endpoint - Not applicable for OCS ( cos_type=ocs )","title":"cos_url"},{"location":"roles/cos/#cos_resource_key_iam_role","text":"The IAM role to assign to the COS service credentials. Optional (IBM COS only) Environment Variable: COS_RESOURCE_KEY_IAM_ROLE Default Value: Manager Purpose : Specifies the IAM role level for the service credentials (resource key) created during COS provisioning. This determines the permissions granted to MAS for accessing the COS instance. When to use : Only relevant when cos_type=ibm . The default Manager role provides full access required by MAS. Valid values : - Manager - Full access to COS instance (create/read/update/delete buckets and objects) - Writer - Read and write access to objects, limited bucket operations - Reader - Read-only access to objects - Content Reader - Read-only access without listing capabilities Impact : - Determines what operations MAS can perform on the COS instance - Manager role is required for MAS to create and manage buckets - Lower privilege roles will cause MAS functionality to fail - Credentials are generated with this role during provisioning Related variables : - cos_type - Must be ibm for this to be used - cos_use_hmac - Controls credential format Notes : - Always use Manager role for MAS deployments (default) - Lower privilege roles are not sufficient for MAS operations - The role is assigned to the service credentials, not the instance itself - Not applicable for OCS ( cos_type=ocs )","title":"cos_resource_key_iam_role"},{"location":"roles/cos/#cos_use_hmac","text":"Controls whether to use HMAC-style credentials for COS access. Optional (IBM COS only) Environment Variable: COS_USE_HMAC Default Value: true Purpose : Determines whether to generate HMAC (AWS S3-compatible) credentials or IAM-based credentials for accessing IBM Cloud COS. MAS requires HMAC credentials for S3 API compatibility. When to use : Should always be true for MAS deployments. Setting to false will prevent MAS from accessing the COS instance. Valid values : - true - Generate HMAC credentials (AWS S3-compatible access key and secret key) - false - Use IAM API key authentication (not compatible with MAS) Impact : - When true : Generates S3-compatible access key ID and secret access key - When false : MAS will not be able to access the COS instance - HMAC credentials are required for S3 API compatibility - Cannot be changed after credential creation (requires new credentials) Related variables : - cos_type - Must be ibm for this to be used - cos_resource_key_iam_role - Role assigned to HMAC credentials Notes : - Always use true for MAS deployments - this is critical - HMAC credentials provide S3 API compatibility required by MAS - Setting to false will cause MAS object storage configuration to fail - Not applicable for OCS ( cos_type=ocs )","title":"cos_use_hmac"},{"location":"roles/cos/#cos_apikey","text":"The IBM Cloud API key specifically for COS operations. Required (IBM COS only) Environment Variable: COS_APIKEY Default Value: Falls back to ibmcloud_apikey if not set Purpose : Provides an optional dedicated API key for COS operations. This allows using a less privileged API key specifically for COS management, following the principle of least privilege. When to use : Only relevant when cos_type=ibm . Set this if you want to use a different API key for COS than for other IBM Cloud operations. Valid values : - A valid IBM Cloud API key with COS service permissions - Must have permissions to create and manage COS instances - Format: 44-character alphanumeric string Impact : - If set, this API key is used instead of ibmcloud_apikey for COS operations - Allows separation of permissions between COS and other IBM Cloud services - Falls back to ibmcloud_apikey if not specified Related variables : - ibmcloud_apikey - Fallback if this is not set - cos_type - Must be ibm for this to be used Notes : - Optional - defaults to ibmcloud_apikey if not provided - Use a dedicated key for better security and permission isolation - The key must have COS service management permissions - Not applicable for OCS ( cos_type=ocs )","title":"cos_apikey"},{"location":"roles/cos/#ibmcloud_apikey","text":"The default IBM Cloud API key for all IBM Cloud operations. Required (IBM COS only) Environment Variable: IBMCLOUD_APIKEY Default Value: None Purpose : Provides the primary IBM Cloud API key used across multiple roles in the collection. This is the default authentication method for IBM Cloud services. When to use : Always required when cos_type=ibm . This API key must have permissions to create and manage COS instances. Valid values : - A valid IBM Cloud API key with appropriate permissions - Must have Editor or Administrator role on COS service - Must have access to the specified resource group - Format: 44-character alphanumeric string Impact : - Used for all IBM Cloud operations unless cos_apikey is specified - Determines which IBM Cloud account owns the COS instance - Account will be billed for COS usage - Required permissions: COS service management, resource group access Related variables : - cos_apikey - Can override this for COS-specific operations - ibmcloud_resourcegroup - Must have access to this resource group - cos_type - Must be ibm for this to be used Notes : - Store API keys securely using environment variables or secrets management - Never commit API keys to version control - Consider using service IDs with restricted permissions - Used by multiple roles in the collection for consistency - Not applicable for OCS ( cos_type=ocs )","title":"ibmcloud_apikey"},{"location":"roles/cos/#cos_resourcegroup","text":"The IBM Cloud resource group for the COS instance. Optional (IBM COS only) Environment Variable: COS_RESOURCEGROUP Default Value: Falls back to ibmcloud_resourcegroup Purpose : Specifies the resource group in IBM Cloud where the COS instance will be created. This allows using a different resource group for COS than for other IBM Cloud resources. When to use : Only relevant when cos_type=ibm . Set this if you want to organize COS in a different resource group than other IBM Cloud resources. Valid values : - Any existing resource group name in your IBM Cloud account - Common examples: Default , Storage , MAS-Resources - Must exist before provisioning - Case-sensitive Impact : - Determines billing and cost allocation for the COS instance - Controls IAM access policies - Falls back to ibmcloud_resourcegroup if not specified - Cannot be changed after instance creation Related variables : - ibmcloud_resourcegroup - Fallback if this is not set - ibmcloud_apikey - Must have access to this resource group - cos_type - Must be ibm for this to be used Notes : - Optional - defaults to ibmcloud_resourcegroup if not provided - Use for organizing COS separately from other resources - Ensure the API key has access to the specified resource group - Not applicable for OCS ( cos_type=ocs )","title":"cos_resourcegroup"},{"location":"roles/cos/#ibmcloud_resourcegroup","text":"The default IBM Cloud resource group for all IBM Cloud resources. Optional (IBM COS only) Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default Purpose : Specifies the default resource group used across multiple roles in the collection. This provides consistency in resource organization. When to use : Only relevant when cos_type=ibm . Set this to organize all IBM Cloud resources in a specific resource group. Valid values : - Any existing resource group name in your IBM Cloud account - Common examples: Default , Production , Development - Must exist before provisioning - Case-sensitive Impact : - Used as the default resource group for COS and other IBM Cloud resources - Determines billing and cost allocation - Controls IAM access policies - Can be overridden by cos_resourcegroup for COS-specific placement Related variables : - cos_resourcegroup - Can override this for COS - ibmcloud_apikey - Must have access to this resource group - cos_type - Must be ibm for this to be used Notes : - Defaults to Default resource group if not specified - Use consistent resource groups across all MAS-related services - Helps with cost tracking and access management - Not applicable for OCS ( cos_type=ocs )","title":"ibmcloud_resourcegroup"},{"location":"roles/cos/#role-variables-mas-configuration","text":"","title":"Role Variables - MAS Configuration"},{"location":"roles/cos/#mas_instance_id","text":"The MAS instance ID that the ObjectStorageCfg will target. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None Purpose : Identifies the specific MAS instance for which the ObjectStorageCfg resource will be generated. This ensures the object storage configuration is associated with the correct MAS deployment. When to use : Required if you want the role to generate an ObjectStorageCfg resource. If not set (along with mas_config_dir ), the role will only provision the storage but not generate MAS configuration. Valid values : - Alphanumeric string, typically 3-12 characters - Common examples: prod , dev , test , masinst1 - Should match the MAS instance ID used throughout your deployment - Lowercase recommended Impact : - Used to generate the ObjectStorageCfg resource name and namespace - If not set, no ObjectStorageCfg will be generated (storage only) - Must match the actual MAS instance ID for configuration to work - Both this and mas_config_dir must be set for config generation Related variables : - mas_config_dir - Both must be set for config generation - Used consistently across all MAS DevOps roles Notes : - Use the same mas_instance_id across all MAS components - If you only want to provision storage without MAS config, leave this unset - The ObjectStorageCfg will be created in the mas-<instance-id>-core namespace - Required for automated MAS configuration via suite_config role","title":"mas_instance_id"},{"location":"roles/cos/#mas_config_dir","text":"Local directory where the ObjectStorageCfg resource will be saved. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None Purpose : Specifies the local directory path where the generated ObjectStorageCfg YAML file will be saved. This file can be used to manually configure MAS or as input to the suite_config role. When to use : Required if you want the role to generate an ObjectStorageCfg resource. If not set (along with mas_instance_id ), the role will only provision the storage but not generate MAS configuration. Valid values : - Any valid local filesystem path (e.g., ~/masconfig , /opt/mas/config ) - Directory will be created if it doesn't exist - Should have write permissions - Recommended to use an absolute path Impact : - ObjectStorageCfg YAML file is saved in this directory - If not set, no ObjectStorageCfg will be generated (storage only) - Required for subsequent MAS configuration steps - Both this and mas_instance_id must be set for config generation Related variables : - mas_instance_id - Both must be set for config generation - Used consistently across all MAS DevOps roles for configuration storage Notes : - Use the same mas_config_dir across all MAS DevOps roles - The generated file can be applied manually with oc apply -f - Can be used as input to the suite_config role for automated configuration - If you only want to provision storage without MAS config, leave this unset","title":"mas_config_dir"},{"location":"roles/cos/#include_cluster_ingress_cert_chain","text":"Controls whether to include the complete certificate chain in the ObjectStorageCfg. Optional Environment Variable: INCLUDE_CLUSTER_INGRESS_CERT_CHAIN Default Value: False Purpose : When enabled, includes the complete TLS certificate chain from the cluster's ingress in the generated ObjectStorageCfg resource. This is useful when the cluster uses certificates from a trusted certificate authority. When to use : Set to True when your OpenShift cluster's ingress uses certificates signed by a trusted CA and you want MAS to validate the complete certificate chain when accessing OCS. Valid values : - True - Include complete certificate chain in ObjectStorageCfg - False - Include only the leaf certificate (default) Impact : - When True : ObjectStorageCfg includes full certificate chain for proper validation - When False : Only the leaf certificate is included - Primarily relevant for OCS ( cos_type=ocs ) with custom certificates - Helps with certificate validation in environments with intermediate CAs Related variables : - cos_type - Most relevant for ocs - ocp_ingress_tls_secret_name - Source of certificates Notes : - Default False is sufficient for most deployments - Enable if you experience certificate validation issues with OCS - Not typically needed for IBM Cloud COS ( cos_type=ibm ) - The certificate chain is extracted from the cluster's ingress configuration","title":"include_cluster_ingress_cert_chain"},{"location":"roles/cos/#example-playbook","text":"Create the Ceph Object store on the existing OCS cluster and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ocs mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos Create the IBM Cloud Object storage Instance and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm ibmcloud_apikey: <Your IBM Cloud API Key> mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos","title":"Example Playbook"},{"location":"roles/cos/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cos_bucket/","text":"cos_bucket \u00a4 This role extends support to create or deprovision Cloud Object Storage buckets. Role Variables - General \u00a4 cos_type \u00a4 The Cloud Object Storage provider for bucket operations. Required Environment Variable: COS_TYPE Default Value: None Purpose : Determines which object storage provider will be used for bucket creation or deletion. Different providers have different capabilities, APIs, and operational characteristics. When to use : Always required when creating or deleting buckets. Choose based on your deployment platform and where your COS instance is hosted. Valid values : - ibm - IBM Cloud Object Storage buckets - aws - AWS S3 buckets Impact : - ibm : Creates buckets in IBM Cloud COS, requires IBM Cloud API key and COS instance name - aws : Creates buckets in AWS S3, requires AWS credentials (access key and secret key) - Determines which set of configuration variables are required - Affects bucket naming conventions, storage classes, and lifecycle policies Related variables : - When cos_type=ibm : Requires ibmcloud_apikey , cos_instance_name , cos_bucket_storage_class - When cos_type=aws : Requires AWS credentials via environment or AWS CLI configuration, aws_region - cos_bucket_action - Whether to create or delete the bucket Notes : - IBM COS buckets support advanced storage classes (smart, vault, cold, flex) - AWS S3 buckets support versioning and encryption configurations - Bucket names must be globally unique within each provider - Choose the provider that matches your COS instance location cos_bucket_action \u00a4 The action to perform on the bucket. Required Environment Variable: COS_BUCKET_ACTION Default Value: create Purpose : Controls whether to create a new bucket or delete an existing one. This allows the same role to handle both lifecycle operations. When to use : - Use create (default) when setting up storage buckets for MAS workspaces or applications - Use delete when cleaning up buckets after workspace or application removal Valid values : - create - Create a new bucket with specified configuration - delete - Delete an existing bucket Impact : - Create : Creates bucket with specified storage class, region, and lifecycle policies - Delete : Permanently removes the bucket and potentially its contents (depending on force deletion settings) - Deletion is irreversible - all data in the bucket will be lost - For AWS, deletion behavior depends on aws_bucket_force_deletion_flag Related variables : - cos_type - Determines which provider's bucket to create/delete - cos_bucket_name or aws_bucket_name - Identifies which bucket to operate on - aws_bucket_force_deletion_flag - Controls AWS deletion behavior Notes : - Always backup data before deleting buckets - IBM COS bucket deletion may fail if the bucket contains objects - AWS bucket deletion can force-delete objects if versioning is disabled - Verify the bucket name before deletion to avoid removing the wrong bucket Role Variables - IBM Cloud Object Storage buckets \u00a4 cos_bucket_name \u00a4 The name for the IBM Cloud Object Storage bucket. Optional (IBM COS only) Environment Variable: COS_BUCKET_NAME Default Value: <mas_instance_id>-<mas_workspace_id>-bucket Purpose : Specifies a custom name for the IBM Cloud COS bucket. Bucket names must be globally unique across all IBM Cloud COS instances. When to use : Only relevant when cos_type=ibm . Provide a descriptive name or use the default which includes MAS instance and workspace IDs for uniqueness. Valid values : - 3-63 characters - Lowercase letters, numbers, hyphens, and periods only - Must start and end with a letter or number - Must be globally unique across all IBM Cloud COS - Cannot contain consecutive periods or hyphens - Examples: prod-mas-workspace1-bucket , dev-maximo-data-bucket Impact : - Bucket name must be globally unique or creation will fail - Used to identify the bucket for all operations - Cannot be changed after creation (requires recreation) - Appears in S3 API endpoints and URLs Related variables : - cos_type - Must be ibm for this to be used - mas_instance_id - Used in default name - mas_workspace_id - Used in default name Notes : - Default naming includes instance and workspace IDs for uniqueness - Bucket names are globally visible - avoid including sensitive information - Use consistent naming conventions across all buckets - Not used for AWS buckets (see aws_bucket_name ) cos_bucket_storage_class \u00a4 The IBM Cloud COS storage class for the bucket. Optional (IBM COS only) Environment Variable: COS_BUCKET_STORAGE_CLASS Default Value: smart Purpose : Specifies the storage class tier for the IBM Cloud COS bucket, which determines pricing, performance, and data retrieval characteristics. When to use : Only relevant when cos_type=ibm . Choose based on your data access patterns and cost requirements. Valid values : - smart - Automatically transitions data between hot and cool tiers based on access patterns (recommended) - vault - For data accessed less than once per month, lower storage cost, higher retrieval cost - cold - For data accessed less than once per year, lowest storage cost, highest retrieval cost - flex - For dynamic workloads with unpredictable access patterns Impact : - Determines storage pricing and data retrieval costs - smart provides automatic cost optimization without manual intervention - Lower-tier classes (vault, cold) have higher retrieval costs and latency - Cannot be changed after bucket creation (requires data migration) Related variables : - cos_type - Must be ibm for this to be used - cos_bucket_region_location_type - Storage class availability varies by location type Notes : - smart is recommended for most MAS workloads (default) - Consider data access patterns when choosing storage class - Vault and cold classes suitable for backup and archival data - Review IBM Cloud COS pricing for cost implications - Not applicable for AWS buckets cos_instance_name \u00a4 The name of the IBM Cloud COS instance where the bucket will be created. Required (IBM COS only) Environment Variable: COS_INSTANCE_NAME Default Value: None Purpose : Identifies the specific IBM Cloud COS instance where the bucket will be created or deleted. This must match an existing COS instance in your IBM Cloud account. When to use : Always required when cos_type=ibm . Must match the name of a COS instance provisioned by the cos role or created manually. Valid values : - Any valid IBM Cloud COS instance name in your account - Must exist in the specified resource group - Case-sensitive - Examples: Object Storage for MAS prod , mas-cos-instance Impact : - Determines which COS instance will contain the bucket - Incorrect name will cause bucket creation to fail - All buckets in the same instance share the same service credentials - Used to locate the instance for bucket operations Related variables : - cos_type - Must be ibm for this to be used - ibmcloud_resourcegroup - Resource group containing the instance - ibmcloud_apikey - Must have access to this instance Notes : - The COS instance must be created before running this role - Use the cos role to provision the COS instance first - Verify the instance name in IBM Cloud console if unsure - Not applicable for AWS buckets cos_location_info \u00a4 The geographic location of the IBM Cloud COS instance. Required (IBM COS only) Environment Variable: COS_LOCATION Default Value: global Purpose : Specifies the geographic scope of the IBM Cloud COS instance. This should match the location used when the COS instance was created. When to use : Only relevant when cos_type=ibm . Should match the cos_location_info used in the cos role. Valid values : - global - Instance available globally - Specific region codes: us-south , us-east , eu-gb , eu-de , etc. - Cross-region: us , eu , ap Impact : - Must match the actual COS instance location - Used to locate the correct COS instance - Mismatch will cause bucket creation to fail Related variables : - cos_instance_name - Instance in this location - cos_bucket_region_location - Bucket's specific region Notes : - Default global works for most deployments - Must match the location used when creating the COS instance - Not applicable for AWS buckets cos_bucket_region_location_type \u00a4 The resiliency type for the IBM Cloud COS bucket. Required (IBM COS only) Environment Variable: COS_BUCKET_REGION_LOCATION_TYPE Default Value: cross_region_location Purpose : Defines the geographic distribution and resiliency level of the bucket. This determines data redundancy and availability characteristics. When to use : Only relevant when cos_type=ibm . Choose based on your availability requirements and performance needs. Valid values : - cross_region_location - Data replicated across multiple regions (highest availability, default) - region_location - Data stored in a single region (best performance, lower cost) - single_site_location - Data stored in a single data center (lowest cost, lowest availability) Impact : - cross_region : Highest availability (99.99%), data replicated across 3+ regions, higher cost - region : High availability (99.95%), data replicated within one region, balanced cost/performance - single_site : Standard availability, lowest cost, suitable for non-critical data - Determines which values are valid for cos_bucket_region_location - Cannot be changed after bucket creation Related variables : - cos_bucket_region_location - Specific region(s) for the bucket - cos_bucket_storage_class - Storage class within the location type Notes : - cross_region_location recommended for production MAS workloads (default) - region_location suitable for performance-sensitive workloads - Consider compliance and data residency requirements - Not applicable for AWS buckets cos_bucket_region_location \u00a4 The specific region(s) for the IBM Cloud COS bucket. Required (IBM COS only) Environment Variable: COS_BUCKET_REGION_LOCATION Default Value: Derived from ibmcloud_region (e.g., us , eu , ap ) Purpose : Specifies the exact geographic region(s) where the bucket data will be stored. Valid values depend on the cos_bucket_region_location_type . When to use : Only relevant when cos_type=ibm . Choose based on data residency requirements and proximity to users. Valid values : - For cross_region_location : us , eu , ap - For region_location : us-south , us-east , eu-gb , eu-de , jp-tok , au-syd , ca-tor , jp-osa , br-sao - For single_site_location : Specific data center codes Impact : - Determines physical location of data storage - Affects data residency compliance (GDPR, data sovereignty) - Influences network latency for data access - Cannot be changed after bucket creation - Must be compatible with cos_bucket_region_location_type Related variables : - cos_bucket_region_location_type - Determines valid values - ibmcloud_region - Used to derive default cross-region location - cos_url - Should match the region for optimal performance Notes : - Default automatically selects cross-region based on ibmcloud_region - For cross_region_location , us covers US regions, eu covers Europe, ap covers Asia-Pacific - Consider compliance requirements when selecting region - Not applicable for AWS buckets ibmcloud_region \u00a4 The IBM Cloud region used to derive the default cross-region location. Optional (IBM COS only) Environment Variable: IBMCLOUD_REGION Default Value: us-east Purpose : Provides a hint for automatically determining the appropriate cross-region location when cos_bucket_region_location is not explicitly set. When to use : Only relevant when cos_type=ibm and using cross_region_location type without explicitly setting cos_bucket_region_location . Valid values : - Any valid IBM Cloud region identifier - Examples: us-east , us-south , eu-gb , eu-de , jp-tok - Used to derive cross-region: us-* \u2192 us , eu-* \u2192 eu , jp-* or au-* \u2192 ap Impact : - Automatically determines cross-region location if not explicitly set - us-* regions default to us cross-region - eu-* regions default to eu cross-region - jp-* and au-* regions default to ap cross-region Related variables : - cos_bucket_region_location - Can override the derived value - cos_bucket_region_location_type - Only used for cross-region type Notes : - Only used when cos_bucket_region_location is not explicitly set - Provides convenience for cross-region bucket creation - Not applicable for AWS buckets cos_url \u00a4 The IBM Cloud COS S3 API endpoint URL. Required (IBM COS only, for bucket creation) Environment Variable: COS_REGION_LOCATION_URL Default Value: https://s3.<region>.cloud-object-storage.appdomain.cloud Purpose : Specifies the S3-compatible API endpoint for accessing the IBM Cloud COS bucket. This URL is used for all bucket operations. When to use : Only relevant when cos_type=ibm . Should match the bucket's region location for optimal performance. Valid values : - Regional endpoints: https://s3.<region>.cloud-object-storage.appdomain.cloud - Cross-region endpoints: https://s3.us.cloud-object-storage.appdomain.cloud , https://s3.eu.cloud-object-storage.appdomain.cloud , https://s3.ap.cloud-object-storage.appdomain.cloud - Private endpoints: https://s3.private.<region>.cloud-object-storage.appdomain.cloud - Direct endpoints: https://s3.direct.<region>.cloud-object-storage.appdomain.cloud Impact : - Determines which IBM Cloud COS endpoint is used for bucket operations - Affects network latency and data transfer costs - Private endpoints provide better security and lower costs for in-cloud access - Must be accessible from where the role is executed Related variables : - cos_bucket_region_location - Should align with the endpoint region - cos_type - Must be ibm for this to be used Notes : - Default automatically constructed based on cos_bucket_region_location - Use regional endpoints matching your bucket location for best performance - Private endpoints recommended for OpenShift clusters in IBM Cloud - Not applicable for AWS buckets (see aws_url ) cos_resource_key_iam_role \u00a4 The IAM role for COS service credentials created during bucket setup. Optional (IBM COS only) Environment Variable: COS_RESOURCE_KEY_IAM_ROLE Default Value: Manager Purpose : Specifies the IAM role level for service credentials that may be created during bucket operations. This determines the permissions granted for accessing the bucket. When to use : Only relevant when cos_type=ibm . The default Manager role provides full access required for bucket operations. Valid values : - Manager - Full access to buckets and objects (create/read/update/delete) - Writer - Read and write access to objects - Reader - Read-only access to objects Impact : - Determines permissions for any service credentials created - Manager role required for full bucket management - Lower privilege roles may limit functionality Related variables : - cos_type - Must be ibm for this to be used - cos_instance_name - Instance where credentials are created Notes : - Default Manager role is recommended - Not applicable for AWS buckets ibmcloud_apikey \u00a4 The IBM Cloud API key for bucket operations. Required (IBM COS only) Environment Variable: IBMCLOUD_APIKEY Default Value: None Purpose : Provides authentication credentials for IBM Cloud to create and manage COS buckets. When to use : Always required when cos_type=ibm . Must have permissions to manage buckets in the specified COS instance. Valid values : - A valid IBM Cloud API key with COS permissions - Must have access to the COS instance and resource group - Format: 44-character alphanumeric string Impact : - Authenticates all IBM Cloud COS operations - Determines which account owns the bucket - Required permissions: COS bucket management, resource group access Related variables : - cos_type - Must be ibm for this to be used - ibmcloud_resourcegroup - Must have access to this resource group - cos_instance_name - Must have access to this instance Notes : - Store API keys securely - Never commit API keys to version control - Not applicable for AWS buckets ibmcloud_resourcegroup \u00a4 The IBM Cloud resource group containing the COS instance. Optional (IBM COS only) Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default Purpose : Specifies the resource group where the COS instance is located. When to use : Only relevant when cos_type=ibm . Should match the resource group used when creating the COS instance. Valid values : - Any existing resource group name in your IBM Cloud account - Common examples: Default , Production , MAS-Resources - Case-sensitive Impact : - Used to locate the COS instance - Must match the actual resource group of the COS instance - API key must have access to this resource group Related variables : - ibmcloud_apikey - Must have access to this resource group - cos_instance_name - Instance in this resource group Notes : - Defaults to Default resource group - Must match the resource group used in the cos role - Not applicable for AWS buckets Role Variables - AWS S3 Buckets \u00a4 To run this role successfully for AWS s3 buckets, you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. aws_bucket_name \u00a4 The name for the AWS S3 bucket. Optional (AWS S3 only) Environment Variable: COS_BUCKET_NAME Default Value: <mas_instance_id>-<mas_workspace_id>-bucket Purpose : Specifies a custom name for the AWS S3 bucket. Bucket names must be globally unique across all AWS S3 buckets worldwide. When to use : Only relevant when cos_type=aws . Provide a descriptive name or use the default which includes MAS instance and workspace IDs for uniqueness. Valid values : - 3-63 characters - Lowercase letters, numbers, hyphens, and periods only - Must start and end with a letter or number - Must be globally unique across all AWS S3 - Cannot contain consecutive periods or underscores - Cannot be formatted as an IP address - Examples: prod-mas-workspace1-bucket , dev-maximo-data-bucket Impact : - Bucket name must be globally unique or creation will fail - Used to identify the bucket for all operations - Cannot be changed after creation (requires recreation) - Appears in S3 API endpoints and URLs Related variables : - cos_type - Must be aws for this to be used - mas_instance_id - Used in default name - mas_workspace_id - Used in default name - aws_region - Region where the bucket is created Notes : - Default naming includes instance and workspace IDs for uniqueness - Bucket names are globally visible - avoid including sensitive information - Use consistent naming conventions across all buckets - Not used for IBM COS buckets (see cos_bucket_name ) aws_region \u00a4 The AWS region where the S3 bucket will be created. Required (AWS S3 only) Environment Variable: AWS_REGION Default Value: us-east-2 Purpose : Specifies the AWS region for S3 bucket creation. This determines the physical location of the bucket and affects latency and data residency. When to use : Always required when cos_type=aws . Choose a region close to your OpenShift cluster and users for optimal performance. Valid values : - Any valid AWS region identifier - Examples: us-east-1 , us-east-2 , us-west-2 , eu-west-1 , eu-central-1 , ap-southeast-1 - Must be a region where your AWS account has access Impact : - Determines physical location of bucket storage - Affects data residency compliance (GDPR, data sovereignty) - Influences network latency for data access - Cannot be changed after bucket creation (requires data migration) - Affects pricing (varies by region) Related variables : - cos_type - Must be aws for this to be used - aws_bucket_name - Bucket created in this region - aws_url - Should match the region Notes : - Choose a region close to your OpenShift cluster for best performance - Consider compliance and data residency requirements - Some regions may have capacity constraints - Not applicable for IBM COS buckets aws_bucket_versioning_flag \u00a4 Controls whether to enable versioning for the AWS S3 bucket. Optional (AWS S3 only) Environment Variable: COS_BUCKET_VERSIONING_FLAG Default Value: True Purpose : Determines if S3 versioning should be enabled for the bucket. Versioning keeps multiple variants of an object in the same bucket, providing protection against accidental deletion or overwrite. When to use : Only relevant when cos_type=aws . Enable for production buckets to protect against accidental data loss. Valid values : - True - Enable versioning (recommended for production) - False - Disable versioning Impact : - When True : All object versions are retained, providing rollback capability - When False : Only the latest version of each object is kept - Versioning increases storage costs (all versions consume space) - Once enabled, versioning cannot be fully disabled (only suspended) - Affects aws_bucket_force_deletion_flag behavior Related variables : - cos_type - Must be aws for this to be used - aws_bucket_force_deletion_flag - Cannot force-delete if versioning is enabled Notes : - Recommended to enable for production buckets - Provides protection against accidental deletion - Increases storage costs due to version retention - Consider lifecycle policies to manage old versions - Not applicable for IBM COS buckets aws_bucket_encryption \u00a4 The encryption configuration for the AWS S3 bucket. Optional (AWS S3 only) Environment Variable: COS_BUCKET_ENCRYPTION Default Value: None Purpose : Specifies the default server-side encryption configuration for objects stored in the S3 bucket. This ensures data is encrypted at rest. When to use : Only relevant when cos_type=aws . Set this to enforce encryption for all objects in the bucket. Valid values : - JSON-formatted string defining encryption rules - Example for AES256: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}' - Example for KMS: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"aws:kms\", \"KMSMasterKeyID\": \"arn:aws:kms:region:account:key/key-id\"}}]}' - Supported algorithms: AES256 (S3-managed keys), aws:kms (KMS-managed keys) Impact : - When set: All objects uploaded to the bucket are automatically encrypted - When not set: Encryption must be specified per-object or not used - KMS encryption provides additional key management and audit capabilities - KMS encryption incurs additional costs Related variables : - cos_type - Must be aws for this to be used - aws_bucket_name - Bucket where encryption is applied Notes : - Recommended for production buckets containing sensitive data - AES256 is simpler and has no additional cost - KMS provides better key management and audit trails - Encryption can be changed after bucket creation - Not applicable for IBM COS buckets aws_bucket_force_deletion_flag \u00a4 Controls whether to force-delete bucket contents before deleting the bucket. Optional (AWS S3 only) Environment Variable: COS_BUCKET_FORCE_DELETION_FLAG Default Value: True Purpose : When deleting an AWS S3 bucket, this flag determines whether to automatically delete all objects in the bucket first. This only works if versioning is disabled. When to use : Only relevant when cos_type=aws and cos_bucket_action=delete . Enable to automatically clean up bucket contents during deletion. Valid values : - True - Automatically delete all objects before deleting the bucket (default) - False - Fail if bucket contains objects (safer, requires manual cleanup) Impact : - When True : All objects in the bucket are permanently deleted before bucket deletion - When False : Bucket deletion fails if it contains any objects - Only works if versioning is disabled - versioned buckets cannot be force-deleted - Deletion is irreversible - all data will be lost Related variables : - cos_type - Must be aws for this to be used - cos_bucket_action - Must be delete for this to take effect - aws_bucket_versioning_flag - Must be False for force deletion to work Notes : - WARNING : Force deletion permanently removes all data - Does not work with versioned buckets - disable versioning first - Consider backing up data before force deletion - Setting to False is safer but requires manual object cleanup - Not applicable for IBM COS buckets Example Playbook \u00a4 Create the IBM Cloud Object storage bucket. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm cos_bucket_action: create cos_bucket_name: my-ibm-bucket cos_instance_name: my-ibmcos-instance-name ibmcloud_apikey: my-ibm-cloud-apikey roles: - ibm.mas_devops.cos_bucket Create the AWS S3 storage bucket. - hosts: localhost any_errors_fatal: true vars: cos_type: aws cos_bucket_action: create aws_bucket_name: my-aws-bucket aws_region: us-east-2 aws_bucket_versioning_flag: True aws_bucket_encryption: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}' roles: - ibm.mas_devops.cos_bucket License \u00a4 EPL-2.0","title":"cos_bucket"},{"location":"roles/cos_bucket/#cos_bucket","text":"This role extends support to create or deprovision Cloud Object Storage buckets.","title":"cos_bucket"},{"location":"roles/cos_bucket/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/cos_bucket/#cos_type","text":"The Cloud Object Storage provider for bucket operations. Required Environment Variable: COS_TYPE Default Value: None Purpose : Determines which object storage provider will be used for bucket creation or deletion. Different providers have different capabilities, APIs, and operational characteristics. When to use : Always required when creating or deleting buckets. Choose based on your deployment platform and where your COS instance is hosted. Valid values : - ibm - IBM Cloud Object Storage buckets - aws - AWS S3 buckets Impact : - ibm : Creates buckets in IBM Cloud COS, requires IBM Cloud API key and COS instance name - aws : Creates buckets in AWS S3, requires AWS credentials (access key and secret key) - Determines which set of configuration variables are required - Affects bucket naming conventions, storage classes, and lifecycle policies Related variables : - When cos_type=ibm : Requires ibmcloud_apikey , cos_instance_name , cos_bucket_storage_class - When cos_type=aws : Requires AWS credentials via environment or AWS CLI configuration, aws_region - cos_bucket_action - Whether to create or delete the bucket Notes : - IBM COS buckets support advanced storage classes (smart, vault, cold, flex) - AWS S3 buckets support versioning and encryption configurations - Bucket names must be globally unique within each provider - Choose the provider that matches your COS instance location","title":"cos_type"},{"location":"roles/cos_bucket/#cos_bucket_action","text":"The action to perform on the bucket. Required Environment Variable: COS_BUCKET_ACTION Default Value: create Purpose : Controls whether to create a new bucket or delete an existing one. This allows the same role to handle both lifecycle operations. When to use : - Use create (default) when setting up storage buckets for MAS workspaces or applications - Use delete when cleaning up buckets after workspace or application removal Valid values : - create - Create a new bucket with specified configuration - delete - Delete an existing bucket Impact : - Create : Creates bucket with specified storage class, region, and lifecycle policies - Delete : Permanently removes the bucket and potentially its contents (depending on force deletion settings) - Deletion is irreversible - all data in the bucket will be lost - For AWS, deletion behavior depends on aws_bucket_force_deletion_flag Related variables : - cos_type - Determines which provider's bucket to create/delete - cos_bucket_name or aws_bucket_name - Identifies which bucket to operate on - aws_bucket_force_deletion_flag - Controls AWS deletion behavior Notes : - Always backup data before deleting buckets - IBM COS bucket deletion may fail if the bucket contains objects - AWS bucket deletion can force-delete objects if versioning is disabled - Verify the bucket name before deletion to avoid removing the wrong bucket","title":"cos_bucket_action"},{"location":"roles/cos_bucket/#role-variables-ibm-cloud-object-storage-buckets","text":"","title":"Role Variables - IBM Cloud Object Storage buckets"},{"location":"roles/cos_bucket/#cos_bucket_name","text":"The name for the IBM Cloud Object Storage bucket. Optional (IBM COS only) Environment Variable: COS_BUCKET_NAME Default Value: <mas_instance_id>-<mas_workspace_id>-bucket Purpose : Specifies a custom name for the IBM Cloud COS bucket. Bucket names must be globally unique across all IBM Cloud COS instances. When to use : Only relevant when cos_type=ibm . Provide a descriptive name or use the default which includes MAS instance and workspace IDs for uniqueness. Valid values : - 3-63 characters - Lowercase letters, numbers, hyphens, and periods only - Must start and end with a letter or number - Must be globally unique across all IBM Cloud COS - Cannot contain consecutive periods or hyphens - Examples: prod-mas-workspace1-bucket , dev-maximo-data-bucket Impact : - Bucket name must be globally unique or creation will fail - Used to identify the bucket for all operations - Cannot be changed after creation (requires recreation) - Appears in S3 API endpoints and URLs Related variables : - cos_type - Must be ibm for this to be used - mas_instance_id - Used in default name - mas_workspace_id - Used in default name Notes : - Default naming includes instance and workspace IDs for uniqueness - Bucket names are globally visible - avoid including sensitive information - Use consistent naming conventions across all buckets - Not used for AWS buckets (see aws_bucket_name )","title":"cos_bucket_name"},{"location":"roles/cos_bucket/#cos_bucket_storage_class","text":"The IBM Cloud COS storage class for the bucket. Optional (IBM COS only) Environment Variable: COS_BUCKET_STORAGE_CLASS Default Value: smart Purpose : Specifies the storage class tier for the IBM Cloud COS bucket, which determines pricing, performance, and data retrieval characteristics. When to use : Only relevant when cos_type=ibm . Choose based on your data access patterns and cost requirements. Valid values : - smart - Automatically transitions data between hot and cool tiers based on access patterns (recommended) - vault - For data accessed less than once per month, lower storage cost, higher retrieval cost - cold - For data accessed less than once per year, lowest storage cost, highest retrieval cost - flex - For dynamic workloads with unpredictable access patterns Impact : - Determines storage pricing and data retrieval costs - smart provides automatic cost optimization without manual intervention - Lower-tier classes (vault, cold) have higher retrieval costs and latency - Cannot be changed after bucket creation (requires data migration) Related variables : - cos_type - Must be ibm for this to be used - cos_bucket_region_location_type - Storage class availability varies by location type Notes : - smart is recommended for most MAS workloads (default) - Consider data access patterns when choosing storage class - Vault and cold classes suitable for backup and archival data - Review IBM Cloud COS pricing for cost implications - Not applicable for AWS buckets","title":"cos_bucket_storage_class"},{"location":"roles/cos_bucket/#cos_instance_name","text":"The name of the IBM Cloud COS instance where the bucket will be created. Required (IBM COS only) Environment Variable: COS_INSTANCE_NAME Default Value: None Purpose : Identifies the specific IBM Cloud COS instance where the bucket will be created or deleted. This must match an existing COS instance in your IBM Cloud account. When to use : Always required when cos_type=ibm . Must match the name of a COS instance provisioned by the cos role or created manually. Valid values : - Any valid IBM Cloud COS instance name in your account - Must exist in the specified resource group - Case-sensitive - Examples: Object Storage for MAS prod , mas-cos-instance Impact : - Determines which COS instance will contain the bucket - Incorrect name will cause bucket creation to fail - All buckets in the same instance share the same service credentials - Used to locate the instance for bucket operations Related variables : - cos_type - Must be ibm for this to be used - ibmcloud_resourcegroup - Resource group containing the instance - ibmcloud_apikey - Must have access to this instance Notes : - The COS instance must be created before running this role - Use the cos role to provision the COS instance first - Verify the instance name in IBM Cloud console if unsure - Not applicable for AWS buckets","title":"cos_instance_name"},{"location":"roles/cos_bucket/#cos_location_info","text":"The geographic location of the IBM Cloud COS instance. Required (IBM COS only) Environment Variable: COS_LOCATION Default Value: global Purpose : Specifies the geographic scope of the IBM Cloud COS instance. This should match the location used when the COS instance was created. When to use : Only relevant when cos_type=ibm . Should match the cos_location_info used in the cos role. Valid values : - global - Instance available globally - Specific region codes: us-south , us-east , eu-gb , eu-de , etc. - Cross-region: us , eu , ap Impact : - Must match the actual COS instance location - Used to locate the correct COS instance - Mismatch will cause bucket creation to fail Related variables : - cos_instance_name - Instance in this location - cos_bucket_region_location - Bucket's specific region Notes : - Default global works for most deployments - Must match the location used when creating the COS instance - Not applicable for AWS buckets","title":"cos_location_info"},{"location":"roles/cos_bucket/#cos_bucket_region_location_type","text":"The resiliency type for the IBM Cloud COS bucket. Required (IBM COS only) Environment Variable: COS_BUCKET_REGION_LOCATION_TYPE Default Value: cross_region_location Purpose : Defines the geographic distribution and resiliency level of the bucket. This determines data redundancy and availability characteristics. When to use : Only relevant when cos_type=ibm . Choose based on your availability requirements and performance needs. Valid values : - cross_region_location - Data replicated across multiple regions (highest availability, default) - region_location - Data stored in a single region (best performance, lower cost) - single_site_location - Data stored in a single data center (lowest cost, lowest availability) Impact : - cross_region : Highest availability (99.99%), data replicated across 3+ regions, higher cost - region : High availability (99.95%), data replicated within one region, balanced cost/performance - single_site : Standard availability, lowest cost, suitable for non-critical data - Determines which values are valid for cos_bucket_region_location - Cannot be changed after bucket creation Related variables : - cos_bucket_region_location - Specific region(s) for the bucket - cos_bucket_storage_class - Storage class within the location type Notes : - cross_region_location recommended for production MAS workloads (default) - region_location suitable for performance-sensitive workloads - Consider compliance and data residency requirements - Not applicable for AWS buckets","title":"cos_bucket_region_location_type"},{"location":"roles/cos_bucket/#cos_bucket_region_location","text":"The specific region(s) for the IBM Cloud COS bucket. Required (IBM COS only) Environment Variable: COS_BUCKET_REGION_LOCATION Default Value: Derived from ibmcloud_region (e.g., us , eu , ap ) Purpose : Specifies the exact geographic region(s) where the bucket data will be stored. Valid values depend on the cos_bucket_region_location_type . When to use : Only relevant when cos_type=ibm . Choose based on data residency requirements and proximity to users. Valid values : - For cross_region_location : us , eu , ap - For region_location : us-south , us-east , eu-gb , eu-de , jp-tok , au-syd , ca-tor , jp-osa , br-sao - For single_site_location : Specific data center codes Impact : - Determines physical location of data storage - Affects data residency compliance (GDPR, data sovereignty) - Influences network latency for data access - Cannot be changed after bucket creation - Must be compatible with cos_bucket_region_location_type Related variables : - cos_bucket_region_location_type - Determines valid values - ibmcloud_region - Used to derive default cross-region location - cos_url - Should match the region for optimal performance Notes : - Default automatically selects cross-region based on ibmcloud_region - For cross_region_location , us covers US regions, eu covers Europe, ap covers Asia-Pacific - Consider compliance requirements when selecting region - Not applicable for AWS buckets","title":"cos_bucket_region_location"},{"location":"roles/cos_bucket/#ibmcloud_region","text":"The IBM Cloud region used to derive the default cross-region location. Optional (IBM COS only) Environment Variable: IBMCLOUD_REGION Default Value: us-east Purpose : Provides a hint for automatically determining the appropriate cross-region location when cos_bucket_region_location is not explicitly set. When to use : Only relevant when cos_type=ibm and using cross_region_location type without explicitly setting cos_bucket_region_location . Valid values : - Any valid IBM Cloud region identifier - Examples: us-east , us-south , eu-gb , eu-de , jp-tok - Used to derive cross-region: us-* \u2192 us , eu-* \u2192 eu , jp-* or au-* \u2192 ap Impact : - Automatically determines cross-region location if not explicitly set - us-* regions default to us cross-region - eu-* regions default to eu cross-region - jp-* and au-* regions default to ap cross-region Related variables : - cos_bucket_region_location - Can override the derived value - cos_bucket_region_location_type - Only used for cross-region type Notes : - Only used when cos_bucket_region_location is not explicitly set - Provides convenience for cross-region bucket creation - Not applicable for AWS buckets","title":"ibmcloud_region"},{"location":"roles/cos_bucket/#cos_url","text":"The IBM Cloud COS S3 API endpoint URL. Required (IBM COS only, for bucket creation) Environment Variable: COS_REGION_LOCATION_URL Default Value: https://s3.<region>.cloud-object-storage.appdomain.cloud Purpose : Specifies the S3-compatible API endpoint for accessing the IBM Cloud COS bucket. This URL is used for all bucket operations. When to use : Only relevant when cos_type=ibm . Should match the bucket's region location for optimal performance. Valid values : - Regional endpoints: https://s3.<region>.cloud-object-storage.appdomain.cloud - Cross-region endpoints: https://s3.us.cloud-object-storage.appdomain.cloud , https://s3.eu.cloud-object-storage.appdomain.cloud , https://s3.ap.cloud-object-storage.appdomain.cloud - Private endpoints: https://s3.private.<region>.cloud-object-storage.appdomain.cloud - Direct endpoints: https://s3.direct.<region>.cloud-object-storage.appdomain.cloud Impact : - Determines which IBM Cloud COS endpoint is used for bucket operations - Affects network latency and data transfer costs - Private endpoints provide better security and lower costs for in-cloud access - Must be accessible from where the role is executed Related variables : - cos_bucket_region_location - Should align with the endpoint region - cos_type - Must be ibm for this to be used Notes : - Default automatically constructed based on cos_bucket_region_location - Use regional endpoints matching your bucket location for best performance - Private endpoints recommended for OpenShift clusters in IBM Cloud - Not applicable for AWS buckets (see aws_url )","title":"cos_url"},{"location":"roles/cos_bucket/#cos_resource_key_iam_role","text":"The IAM role for COS service credentials created during bucket setup. Optional (IBM COS only) Environment Variable: COS_RESOURCE_KEY_IAM_ROLE Default Value: Manager Purpose : Specifies the IAM role level for service credentials that may be created during bucket operations. This determines the permissions granted for accessing the bucket. When to use : Only relevant when cos_type=ibm . The default Manager role provides full access required for bucket operations. Valid values : - Manager - Full access to buckets and objects (create/read/update/delete) - Writer - Read and write access to objects - Reader - Read-only access to objects Impact : - Determines permissions for any service credentials created - Manager role required for full bucket management - Lower privilege roles may limit functionality Related variables : - cos_type - Must be ibm for this to be used - cos_instance_name - Instance where credentials are created Notes : - Default Manager role is recommended - Not applicable for AWS buckets","title":"cos_resource_key_iam_role"},{"location":"roles/cos_bucket/#ibmcloud_apikey","text":"The IBM Cloud API key for bucket operations. Required (IBM COS only) Environment Variable: IBMCLOUD_APIKEY Default Value: None Purpose : Provides authentication credentials for IBM Cloud to create and manage COS buckets. When to use : Always required when cos_type=ibm . Must have permissions to manage buckets in the specified COS instance. Valid values : - A valid IBM Cloud API key with COS permissions - Must have access to the COS instance and resource group - Format: 44-character alphanumeric string Impact : - Authenticates all IBM Cloud COS operations - Determines which account owns the bucket - Required permissions: COS bucket management, resource group access Related variables : - cos_type - Must be ibm for this to be used - ibmcloud_resourcegroup - Must have access to this resource group - cos_instance_name - Must have access to this instance Notes : - Store API keys securely - Never commit API keys to version control - Not applicable for AWS buckets","title":"ibmcloud_apikey"},{"location":"roles/cos_bucket/#ibmcloud_resourcegroup","text":"The IBM Cloud resource group containing the COS instance. Optional (IBM COS only) Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default Purpose : Specifies the resource group where the COS instance is located. When to use : Only relevant when cos_type=ibm . Should match the resource group used when creating the COS instance. Valid values : - Any existing resource group name in your IBM Cloud account - Common examples: Default , Production , MAS-Resources - Case-sensitive Impact : - Used to locate the COS instance - Must match the actual resource group of the COS instance - API key must have access to this resource group Related variables : - ibmcloud_apikey - Must have access to this resource group - cos_instance_name - Instance in this resource group Notes : - Defaults to Default resource group - Must match the resource group used in the cos role - Not applicable for AWS buckets","title":"ibmcloud_resourcegroup"},{"location":"roles/cos_bucket/#role-variables-aws-s3-buckets","text":"To run this role successfully for AWS s3 buckets, you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Role Variables - AWS S3 Buckets"},{"location":"roles/cos_bucket/#aws_bucket_name","text":"The name for the AWS S3 bucket. Optional (AWS S3 only) Environment Variable: COS_BUCKET_NAME Default Value: <mas_instance_id>-<mas_workspace_id>-bucket Purpose : Specifies a custom name for the AWS S3 bucket. Bucket names must be globally unique across all AWS S3 buckets worldwide. When to use : Only relevant when cos_type=aws . Provide a descriptive name or use the default which includes MAS instance and workspace IDs for uniqueness. Valid values : - 3-63 characters - Lowercase letters, numbers, hyphens, and periods only - Must start and end with a letter or number - Must be globally unique across all AWS S3 - Cannot contain consecutive periods or underscores - Cannot be formatted as an IP address - Examples: prod-mas-workspace1-bucket , dev-maximo-data-bucket Impact : - Bucket name must be globally unique or creation will fail - Used to identify the bucket for all operations - Cannot be changed after creation (requires recreation) - Appears in S3 API endpoints and URLs Related variables : - cos_type - Must be aws for this to be used - mas_instance_id - Used in default name - mas_workspace_id - Used in default name - aws_region - Region where the bucket is created Notes : - Default naming includes instance and workspace IDs for uniqueness - Bucket names are globally visible - avoid including sensitive information - Use consistent naming conventions across all buckets - Not used for IBM COS buckets (see cos_bucket_name )","title":"aws_bucket_name"},{"location":"roles/cos_bucket/#aws_region","text":"The AWS region where the S3 bucket will be created. Required (AWS S3 only) Environment Variable: AWS_REGION Default Value: us-east-2 Purpose : Specifies the AWS region for S3 bucket creation. This determines the physical location of the bucket and affects latency and data residency. When to use : Always required when cos_type=aws . Choose a region close to your OpenShift cluster and users for optimal performance. Valid values : - Any valid AWS region identifier - Examples: us-east-1 , us-east-2 , us-west-2 , eu-west-1 , eu-central-1 , ap-southeast-1 - Must be a region where your AWS account has access Impact : - Determines physical location of bucket storage - Affects data residency compliance (GDPR, data sovereignty) - Influences network latency for data access - Cannot be changed after bucket creation (requires data migration) - Affects pricing (varies by region) Related variables : - cos_type - Must be aws for this to be used - aws_bucket_name - Bucket created in this region - aws_url - Should match the region Notes : - Choose a region close to your OpenShift cluster for best performance - Consider compliance and data residency requirements - Some regions may have capacity constraints - Not applicable for IBM COS buckets","title":"aws_region"},{"location":"roles/cos_bucket/#aws_bucket_versioning_flag","text":"Controls whether to enable versioning for the AWS S3 bucket. Optional (AWS S3 only) Environment Variable: COS_BUCKET_VERSIONING_FLAG Default Value: True Purpose : Determines if S3 versioning should be enabled for the bucket. Versioning keeps multiple variants of an object in the same bucket, providing protection against accidental deletion or overwrite. When to use : Only relevant when cos_type=aws . Enable for production buckets to protect against accidental data loss. Valid values : - True - Enable versioning (recommended for production) - False - Disable versioning Impact : - When True : All object versions are retained, providing rollback capability - When False : Only the latest version of each object is kept - Versioning increases storage costs (all versions consume space) - Once enabled, versioning cannot be fully disabled (only suspended) - Affects aws_bucket_force_deletion_flag behavior Related variables : - cos_type - Must be aws for this to be used - aws_bucket_force_deletion_flag - Cannot force-delete if versioning is enabled Notes : - Recommended to enable for production buckets - Provides protection against accidental deletion - Increases storage costs due to version retention - Consider lifecycle policies to manage old versions - Not applicable for IBM COS buckets","title":"aws_bucket_versioning_flag"},{"location":"roles/cos_bucket/#aws_bucket_encryption","text":"The encryption configuration for the AWS S3 bucket. Optional (AWS S3 only) Environment Variable: COS_BUCKET_ENCRYPTION Default Value: None Purpose : Specifies the default server-side encryption configuration for objects stored in the S3 bucket. This ensures data is encrypted at rest. When to use : Only relevant when cos_type=aws . Set this to enforce encryption for all objects in the bucket. Valid values : - JSON-formatted string defining encryption rules - Example for AES256: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}' - Example for KMS: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"aws:kms\", \"KMSMasterKeyID\": \"arn:aws:kms:region:account:key/key-id\"}}]}' - Supported algorithms: AES256 (S3-managed keys), aws:kms (KMS-managed keys) Impact : - When set: All objects uploaded to the bucket are automatically encrypted - When not set: Encryption must be specified per-object or not used - KMS encryption provides additional key management and audit capabilities - KMS encryption incurs additional costs Related variables : - cos_type - Must be aws for this to be used - aws_bucket_name - Bucket where encryption is applied Notes : - Recommended for production buckets containing sensitive data - AES256 is simpler and has no additional cost - KMS provides better key management and audit trails - Encryption can be changed after bucket creation - Not applicable for IBM COS buckets","title":"aws_bucket_encryption"},{"location":"roles/cos_bucket/#aws_bucket_force_deletion_flag","text":"Controls whether to force-delete bucket contents before deleting the bucket. Optional (AWS S3 only) Environment Variable: COS_BUCKET_FORCE_DELETION_FLAG Default Value: True Purpose : When deleting an AWS S3 bucket, this flag determines whether to automatically delete all objects in the bucket first. This only works if versioning is disabled. When to use : Only relevant when cos_type=aws and cos_bucket_action=delete . Enable to automatically clean up bucket contents during deletion. Valid values : - True - Automatically delete all objects before deleting the bucket (default) - False - Fail if bucket contains objects (safer, requires manual cleanup) Impact : - When True : All objects in the bucket are permanently deleted before bucket deletion - When False : Bucket deletion fails if it contains any objects - Only works if versioning is disabled - versioned buckets cannot be force-deleted - Deletion is irreversible - all data will be lost Related variables : - cos_type - Must be aws for this to be used - cos_bucket_action - Must be delete for this to take effect - aws_bucket_versioning_flag - Must be False for force deletion to work Notes : - WARNING : Force deletion permanently removes all data - Does not work with versioned buckets - disable versioning first - Consider backing up data before force deletion - Setting to False is safer but requires manual object cleanup - Not applicable for IBM COS buckets","title":"aws_bucket_force_deletion_flag"},{"location":"roles/cos_bucket/#example-playbook","text":"Create the IBM Cloud Object storage bucket. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm cos_bucket_action: create cos_bucket_name: my-ibm-bucket cos_instance_name: my-ibmcos-instance-name ibmcloud_apikey: my-ibm-cloud-apikey roles: - ibm.mas_devops.cos_bucket Create the AWS S3 storage bucket. - hosts: localhost any_errors_fatal: true vars: cos_type: aws cos_bucket_action: create aws_bucket_name: my-aws-bucket aws_region: us-east-2 aws_bucket_versioning_flag: True aws_bucket_encryption: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}' roles: - ibm.mas_devops.cos_bucket","title":"Example Playbook"},{"location":"roles/cos_bucket/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d/","text":"cp4d \u00a4 This role installs or upgrades IBM Cloud Pak for Data Operator in the target cluster. It assumes that you have already installed the IBM Maximo Operator Catalog and configured Certificate Manager in the target cluster. These actions are performed by the ibm_catalogs cert_manager roles in this collection. Cloud Pak for Data will be configured as a specialized installation Info A specialized installation allows a user with project administrator permissions to install the software after a cluster administrator completes the initial cluster setup. A specialized installation also facilitates strict division between Red Hat OpenShift Container Platform projects (Kubernetes namespaces). In a specialized installation, the IBM Cloud Pak foundational services operators are installed in the ibm-common-services project and the Cloud Pak for Data operators are installed in a separate project (typically cpd-operators). Each project has a dedicated: Operator group, which specifies the OwnNamespace installation mode NamespaceScope Operator, which allows the operators in the project to manage operators and service workloads in specific projects In this way, you can specify different settings for the IBM Cloud Pak foundational services and for the Cloud Pak for Data operators. Currently supported Cloud Pak for Data release versions are: 5.1.3 5.2.0 Tip For more information about CPD versioning, see IBM Cloud Pak for data Operator and operand versions 5.1.x Cloud Pak for Data Version Mapping \u00a4 Users can choose to install a specific version of Cloud Pak for Data by setting CPD_PRODUCT_VERSION variable. However, by default, the version of Cloud Pak for Data will be determined by the version of the Maximo Operator Catalog that is installed in the cluster. If CPD_PRODUCT_VERSION variable is not defined, and the role is not able to find the Maximo Operator Catalog, then the role will default to installing the Cloud Pak for Data version supported by the latest released MAS catalog. Upgrade \u00a4 The role will automatically install or upgrade (if targeted to an existing CPD deployment) the corresponding Zen version associated to the chosen Cloud Pak for Data release, for example: Cloud Pak for Data release version 5.1.3 installs Zen/Control Plane version 6.1.1 Cloud Pak for Data release version 5.2.0 installs Zen/Control Plane version 6.2.0 Tip For more information about IBM Cloud Pak for Data upgrade process, refer to the Cloud Pak for Data official documentation . Cloud Pak for Data Deployment Details \u00a4 Cloud Pak for Data 5.x leverages Cloud Pak Foundational Services v4, which runs its deployments in isolated/dedicated scope model, that means that its dependencies will be grouped and installed within the Cloud Pak for Data related projects/namespaces. There are only two namespaces that will be used: CPD instance namespace (e.g ibm-cpd ) and CPD operators namespace (e.g ibm-cpd-operators ). In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 17h ibm-common-service-operator 1/1 1 1 17h ibm-namespace-scope-operator 1/1 1 1 17h ibm-zen-operator 1/1 1 1 17h meta-api-deploy 1/1 1 1 17h operand-deployment-lifecycle-manager 1/1 1 1 17h postgresql-operator-controller-manager-1-18-7 1/1 1 1 17h In the ibm-cpd namespace: oc -n ibm-cpd get zenservice,ibmcpd,deployments,sts,pvc NAME VERSION STATUS AGE zenservice.zen.cpd.ibm.com/lite-cr 6.0.1 Completed 17h NAME AGE ibmcpd.cpd.ibm.com/ibmcpd-cr 17h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ibm-mcs-hubwork 1/1 1 1 17h deployment.apps/ibm-mcs-placement 1/1 1 1 17h deployment.apps/ibm-mcs-storage 1/1 1 1 17h deployment.apps/ibm-nginx 3/3 3 3 16h deployment.apps/ibm-nginx-tester 1/1 1 1 16h deployment.apps/usermgmt 3/3 3 3 16h deployment.apps/zen-audit 2/2 2 2 16h deployment.apps/zen-core 3/3 3 3 16h deployment.apps/zen-core-api 3/3 3 3 16h deployment.apps/zen-watchdog 2/2 2 2 16h deployment.apps/zen-watcher 1/1 1 1 16h NAME READY AGE statefulset.apps/zen-minio 3/3 17h NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/export-zen-minio-0 Bound pvc-b2a2a729-13c1-4e7f-b672-0b5efc6aa40a 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/export-zen-minio-1 Bound pvc-7e772a3a-8849-4291-8e14-501f49e79182 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/export-zen-minio-2 Bound pvc-e0dd31dc-916d-4b15-9d9c-351db0a2b47f 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/ibm-cs-postgres-backup Bound pvc-ef788b99-784f-4531-a1b3-12611f112551 20Gi RWO ibmc-block-gold 16h persistentvolumeclaim/ibm-zen-objectstore-backup-pvc Bound pvc-d5e61dcf-65a3-4930-9cbf-ab80d04dda00 20Gi RWO ibmc-block-gold 16h persistentvolumeclaim/zen-metastore-edb-1 Bound pvc-19d44f17-05ab-4dc0-bb5d-1b5f15ffd201 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/zen-metastore-edb-2 Bound pvc-741ea444-b6f0-44ff-a123-bb4615d97381 20Gi RWO ibmc-block-gold 17h Tip You can retrieve the Initial Cloud Pak for Data password from the admin-user-details secret: oc -n ibm-cpd get secret admin-user-details -o jsonpath=\"{.data.initial_admin_password}\" | base64 -d Role Variables \u00a4 Installation Variables \u00a4 cpd_product_version \u00a4 Cloud Pak for Data release version to install. Required Environment Variable: CPD_PRODUCT_VERSION Default: Determined by installed MAS catalog version Purpose : Specifies which CP4D release version to install or upgrade to. The version determines which Zen/Control Plane version and features are available. When to use : - Set explicitly when you need a specific CP4D version - Leave unset to use version matching the installed MAS catalog - Required for reproducible deployments - Must match supported versions (currently 5.1.3, 5.2.0) Valid values : 5.1.3 , 5.2.0 (or other supported versions) Impact : - 5.1.3 : Installs Zen/Control Plane 6.1.1 - 5.2.0 : Installs Zen/Control Plane 6.2.0 Different versions have different features and compatibility requirements. Related variables : - Determines compatible service versions - Affects cpd_scale_config options Note : If not set and MAS catalog is not found, defaults to CP4D version supported by latest MAS catalog. For version-specific details, see CP4D Operator and operand versions . ibm_entitlement_key \u00a4 IBM entitlement key for accessing container images. Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Provides authentication to IBM Container Registry for pulling CP4D container images. This key grants access to entitled software. When to use : - Always required for CP4D installation - Obtain from IBM Container Library - Must have valid entitlement for CP4D Valid values : Valid IBM entitlement key string from your IBM account Impact : Without a valid key, image pulls will fail and CP4D installation cannot proceed. Key must have CP4D entitlement. Related variables : - cpd_entitlement_key : CP4D-specific override (primarily for development) Note : Keep this key secure and do not commit to source control. The key is tied to your IBM account and entitlements. Can be overridden by cpd_entitlement_key for CP4D-specific scenarios. cpd_entitlement_key \u00a4 CP4D-specific entitlement key override (primarily for development). Optional Environment Variable: CPD_ENTITLEMENT_KEY Default: None (uses ibm_entitlement_key ) Purpose : Provides a CP4D-specific entitlement key that overrides the general ibm_entitlement_key . Primarily used in development scenarios. When to use : - Leave unset for standard deployments (uses ibm_entitlement_key ) - Set when you need a different key specifically for CP4D - Useful in development/testing with separate entitlements Valid values : Valid IBM entitlement key string with CP4D entitlement Impact : When set, this key is used instead of ibm_entitlement_key for CP4D image pulls. If not set, falls back to ibm_entitlement_key . Related variables : - ibm_entitlement_key : General entitlement key (used if this is not set) Note : Most deployments should use ibm_entitlement_key only. This override is primarily for development scenarios where different keys are needed for different components. cpd_primary_storage_class \u00a4 Primary storage class for CP4D (must support ReadWriteMany). Required (if known storage classes not available) Environment Variable: CPD_PRIMARY_STORAGE_CLASS Default: ibmc-file-gold-gid , ocs-storagecluster-cephfs , or azurefiles-premium (if available) Purpose : Specifies the storage class for CP4D primary storage, which requires ReadWriteMany (RWX) access mode for file storage. When to use : - Leave unset for automatic detection if known storage classes exist - Set explicitly when using custom or non-standard storage classes - Must support RWX access mode for shared file storage Valid values : Storage class name supporting ReadWriteMany access mode Impact : CP4D uses this for shared file storage across pods. Incorrect storage class or one not supporting RWX will cause deployment to fail. Related variables : - cpd_metadata_storage_class : Separate storage for metadata (RWO) Note : Known supported classes: ibmc-file-gold-gid (IBM Cloud), ocs-storagecluster-cephfs (OCS), azurefiles-premium (Azure). See CP4D Storage Considerations for details. cpd_metadata_storage_class \u00a4 Storage class for CP4D Zen metadata database (must support ReadWriteOnce). Required (if known storage classes not available) Environment Variable: CPD_METADATA_STORAGE_CLASS Default: ibmc-block-gold , ocs-storagecluster-ceph-rbd , or managed-premium (if available) Purpose : Specifies the storage class for CP4D Zen metadata database, which requires ReadWriteOnce (RWO) access mode for block storage. When to use : - Leave unset for automatic detection if known storage classes exist - Set explicitly when using custom or non-standard storage classes - Must support RWO access mode for block storage Valid values : Storage class name supporting ReadWriteOnce access mode Impact : CP4D Zen metadata database uses this for persistent storage. Incorrect storage class or one not supporting RWO will cause deployment to fail. Related variables : - cpd_primary_storage_class : Separate storage for primary/file storage (RWX) Note : Known supported classes: ibmc-block-gold (IBM Cloud), ocs-storagecluster-ceph-rbd (OCS), managed-premium (Azure). Block storage typically provides better performance for databases. cpd_operators_namespace \u00a4 Namespace for CP4D operators installation. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default: ibm-cpd-operators Purpose : Specifies the namespace where CP4D operators will be installed. This follows the specialized installation model with separate operator and instance namespaces. When to use : - Use default ( ibm-cpd-operators ) for standard deployments - Set custom namespace for specific organizational requirements - Must be different from cpd_instance_namespace Valid values : Valid Kubernetes namespace name Impact : All CP4D operators (platform, zen, common services, etc.) are installed in this namespace. The namespace must not conflict with the instance namespace. Related variables : - cpd_instance_namespace : Separate namespace for CP4D instance workloads Note : CP4D uses a specialized installation model with operators in one namespace ( ibm-cpd-operators ) and instance workloads in another ( ibm-cpd ). This provides better isolation and management. cpd_instance_namespace \u00a4 Namespace for CP4D instance workloads. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default: ibm-cpd Purpose : Specifies the namespace where CP4D instance workloads (Zen, services, databases) will be deployed. Operators in cpd_operators_namespace watch and manage resources in this namespace. When to use : - Use default ( ibm-cpd ) for standard deployments - Set custom namespace for specific organizational requirements - Must be different from cpd_operators_namespace Valid values : Valid Kubernetes namespace name Impact : All CP4D instance workloads (Zen, MinIO, PostgreSQL, services) are deployed in this namespace. Operators watch this namespace for custom resources. Related variables : - cpd_operators_namespace : Separate namespace for CP4D operators Note : CP4D uses a specialized installation model with operators in ibm-cpd-operators and instance workloads in ibm-cpd . This separation provides better isolation and follows CP4D best practices. cpd_scale_config \u00a4 Resource scaling configuration for CP4D instance. Optional Environment Variable: CPD_SCALE_CONFIG Default: medium Purpose : Adjusts resource allocation (CPU, memory, replicas) for CP4D components to match workload requirements and increase processing capacity. When to use : - Use medium (default) for standard production deployments - Use small for development/test environments - Use large for high-capacity production environments - Adjust based on expected workload and performance requirements Valid values : small , medium , large (or other supported scale configurations) Impact : Determines resource requests/limits and replica counts for CP4D components. Larger scales require more cluster resources but provide better performance and capacity. Related variables : - cpd_product_version : Available scale options may vary by version Note : See Managing resources for detailed resource requirements per scale configuration. Ensure your cluster has sufficient resources for the selected scale. cpd_admin_username \u00a4 CP4D admin username for API authentication. Optional Environment Variable: CPD_ADMIN_USERNAME Default: cpadmin Purpose : Specifies the CP4D admin username for authenticating with CP4D APIs. Used when the role needs to interact with CP4D services. When to use : - Leave as default ( cpadmin ) if you haven't changed the initial admin username - Set explicitly if you changed the admin username after CP4D installation - Required for API operations that need admin authentication Valid values : Valid CP4D admin username string Impact : Used for CP4D API authentication. Incorrect username will cause API operations to fail. Related variables : - cpd_admin_password : Password for this admin user Note : The default cpadmin is the standard CP4D admin username. Only change if you've customized the admin username after installation. cpd_admin_password \u00a4 CP4D admin password for API authentication. Optional Environment Variable: CPD_ADMIN_PASSWORD Default: Retrieved from admin-user-details secret in cpd_instance_namespace Purpose : Specifies the CP4D admin password for authenticating with CP4D APIs. Used when the role needs to interact with CP4D services. When to use : - Leave unset (recommended) to auto-retrieve from cluster secret - Set explicitly if you changed the admin password after CP4D installation - Required for API operations that need admin authentication Valid values : Valid CP4D admin password string Impact : Used for CP4D API authentication. If not set, the role retrieves the initial admin password from the cluster. Incorrect password will cause API operations to fail. Related variables : - cpd_admin_username : Username for this admin password - cpd_instance_namespace : Namespace containing the password secret Note : The role automatically retrieves the initial admin password from the admin-user-details secret if not provided. Only set this if you've changed the password after installation. Keep passwords secure and do not commit to source control. Example Playbook \u00a4 Install Cloud Pak for Data 5.1.3 \u00a4 - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.1.3 cpd_primary_storage_class: ibmc-file-gold-gid cpd_metadata_storage_class: ibmc-block-gold roles: - ibm.mas_devops.cp4d Install Cloud Pak for Data 5.2.0 \u00a4 - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.2.0 cpd_primary_storage_class: ibmc-file-gold-gid cpd_metadata_storage_class: ibmc-block-gold roles: - ibm.mas_devops.cp4d Run Role Playbook \u00a4 export CPD_PRODUCT_VERSION=5.2.0 export CPD_PRIMARY_STORAGE_CLASS=ibmc-file-gold-gid export CPD_METADATA_STORAGE_CLASS=ibmc-block-gold export IBM_ENTITLEMENT_KEY=xxxxx ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"cp4d"},{"location":"roles/cp4d/#cp4d","text":"This role installs or upgrades IBM Cloud Pak for Data Operator in the target cluster. It assumes that you have already installed the IBM Maximo Operator Catalog and configured Certificate Manager in the target cluster. These actions are performed by the ibm_catalogs cert_manager roles in this collection. Cloud Pak for Data will be configured as a specialized installation Info A specialized installation allows a user with project administrator permissions to install the software after a cluster administrator completes the initial cluster setup. A specialized installation also facilitates strict division between Red Hat OpenShift Container Platform projects (Kubernetes namespaces). In a specialized installation, the IBM Cloud Pak foundational services operators are installed in the ibm-common-services project and the Cloud Pak for Data operators are installed in a separate project (typically cpd-operators). Each project has a dedicated: Operator group, which specifies the OwnNamespace installation mode NamespaceScope Operator, which allows the operators in the project to manage operators and service workloads in specific projects In this way, you can specify different settings for the IBM Cloud Pak foundational services and for the Cloud Pak for Data operators. Currently supported Cloud Pak for Data release versions are: 5.1.3 5.2.0 Tip For more information about CPD versioning, see IBM Cloud Pak for data Operator and operand versions 5.1.x","title":"cp4d"},{"location":"roles/cp4d/#cloud-pak-for-data-version-mapping","text":"Users can choose to install a specific version of Cloud Pak for Data by setting CPD_PRODUCT_VERSION variable. However, by default, the version of Cloud Pak for Data will be determined by the version of the Maximo Operator Catalog that is installed in the cluster. If CPD_PRODUCT_VERSION variable is not defined, and the role is not able to find the Maximo Operator Catalog, then the role will default to installing the Cloud Pak for Data version supported by the latest released MAS catalog.","title":"Cloud Pak for Data Version Mapping"},{"location":"roles/cp4d/#upgrade","text":"The role will automatically install or upgrade (if targeted to an existing CPD deployment) the corresponding Zen version associated to the chosen Cloud Pak for Data release, for example: Cloud Pak for Data release version 5.1.3 installs Zen/Control Plane version 6.1.1 Cloud Pak for Data release version 5.2.0 installs Zen/Control Plane version 6.2.0 Tip For more information about IBM Cloud Pak for Data upgrade process, refer to the Cloud Pak for Data official documentation .","title":"Upgrade"},{"location":"roles/cp4d/#cloud-pak-for-data-deployment-details","text":"Cloud Pak for Data 5.x leverages Cloud Pak Foundational Services v4, which runs its deployments in isolated/dedicated scope model, that means that its dependencies will be grouped and installed within the Cloud Pak for Data related projects/namespaces. There are only two namespaces that will be used: CPD instance namespace (e.g ibm-cpd ) and CPD operators namespace (e.g ibm-cpd-operators ). In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 17h ibm-common-service-operator 1/1 1 1 17h ibm-namespace-scope-operator 1/1 1 1 17h ibm-zen-operator 1/1 1 1 17h meta-api-deploy 1/1 1 1 17h operand-deployment-lifecycle-manager 1/1 1 1 17h postgresql-operator-controller-manager-1-18-7 1/1 1 1 17h In the ibm-cpd namespace: oc -n ibm-cpd get zenservice,ibmcpd,deployments,sts,pvc NAME VERSION STATUS AGE zenservice.zen.cpd.ibm.com/lite-cr 6.0.1 Completed 17h NAME AGE ibmcpd.cpd.ibm.com/ibmcpd-cr 17h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ibm-mcs-hubwork 1/1 1 1 17h deployment.apps/ibm-mcs-placement 1/1 1 1 17h deployment.apps/ibm-mcs-storage 1/1 1 1 17h deployment.apps/ibm-nginx 3/3 3 3 16h deployment.apps/ibm-nginx-tester 1/1 1 1 16h deployment.apps/usermgmt 3/3 3 3 16h deployment.apps/zen-audit 2/2 2 2 16h deployment.apps/zen-core 3/3 3 3 16h deployment.apps/zen-core-api 3/3 3 3 16h deployment.apps/zen-watchdog 2/2 2 2 16h deployment.apps/zen-watcher 1/1 1 1 16h NAME READY AGE statefulset.apps/zen-minio 3/3 17h NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/export-zen-minio-0 Bound pvc-b2a2a729-13c1-4e7f-b672-0b5efc6aa40a 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/export-zen-minio-1 Bound pvc-7e772a3a-8849-4291-8e14-501f49e79182 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/export-zen-minio-2 Bound pvc-e0dd31dc-916d-4b15-9d9c-351db0a2b47f 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/ibm-cs-postgres-backup Bound pvc-ef788b99-784f-4531-a1b3-12611f112551 20Gi RWO ibmc-block-gold 16h persistentvolumeclaim/ibm-zen-objectstore-backup-pvc Bound pvc-d5e61dcf-65a3-4930-9cbf-ab80d04dda00 20Gi RWO ibmc-block-gold 16h persistentvolumeclaim/zen-metastore-edb-1 Bound pvc-19d44f17-05ab-4dc0-bb5d-1b5f15ffd201 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/zen-metastore-edb-2 Bound pvc-741ea444-b6f0-44ff-a123-bb4615d97381 20Gi RWO ibmc-block-gold 17h Tip You can retrieve the Initial Cloud Pak for Data password from the admin-user-details secret: oc -n ibm-cpd get secret admin-user-details -o jsonpath=\"{.data.initial_admin_password}\" | base64 -d","title":"Cloud Pak for Data Deployment Details"},{"location":"roles/cp4d/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d/#installation-variables","text":"","title":"Installation Variables"},{"location":"roles/cp4d/#cpd_product_version","text":"Cloud Pak for Data release version to install. Required Environment Variable: CPD_PRODUCT_VERSION Default: Determined by installed MAS catalog version Purpose : Specifies which CP4D release version to install or upgrade to. The version determines which Zen/Control Plane version and features are available. When to use : - Set explicitly when you need a specific CP4D version - Leave unset to use version matching the installed MAS catalog - Required for reproducible deployments - Must match supported versions (currently 5.1.3, 5.2.0) Valid values : 5.1.3 , 5.2.0 (or other supported versions) Impact : - 5.1.3 : Installs Zen/Control Plane 6.1.1 - 5.2.0 : Installs Zen/Control Plane 6.2.0 Different versions have different features and compatibility requirements. Related variables : - Determines compatible service versions - Affects cpd_scale_config options Note : If not set and MAS catalog is not found, defaults to CP4D version supported by latest MAS catalog. For version-specific details, see CP4D Operator and operand versions .","title":"cpd_product_version"},{"location":"roles/cp4d/#ibm_entitlement_key","text":"IBM entitlement key for accessing container images. Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Provides authentication to IBM Container Registry for pulling CP4D container images. This key grants access to entitled software. When to use : - Always required for CP4D installation - Obtain from IBM Container Library - Must have valid entitlement for CP4D Valid values : Valid IBM entitlement key string from your IBM account Impact : Without a valid key, image pulls will fail and CP4D installation cannot proceed. Key must have CP4D entitlement. Related variables : - cpd_entitlement_key : CP4D-specific override (primarily for development) Note : Keep this key secure and do not commit to source control. The key is tied to your IBM account and entitlements. Can be overridden by cpd_entitlement_key for CP4D-specific scenarios.","title":"ibm_entitlement_key"},{"location":"roles/cp4d/#cpd_entitlement_key","text":"CP4D-specific entitlement key override (primarily for development). Optional Environment Variable: CPD_ENTITLEMENT_KEY Default: None (uses ibm_entitlement_key ) Purpose : Provides a CP4D-specific entitlement key that overrides the general ibm_entitlement_key . Primarily used in development scenarios. When to use : - Leave unset for standard deployments (uses ibm_entitlement_key ) - Set when you need a different key specifically for CP4D - Useful in development/testing with separate entitlements Valid values : Valid IBM entitlement key string with CP4D entitlement Impact : When set, this key is used instead of ibm_entitlement_key for CP4D image pulls. If not set, falls back to ibm_entitlement_key . Related variables : - ibm_entitlement_key : General entitlement key (used if this is not set) Note : Most deployments should use ibm_entitlement_key only. This override is primarily for development scenarios where different keys are needed for different components.","title":"cpd_entitlement_key"},{"location":"roles/cp4d/#cpd_primary_storage_class","text":"Primary storage class for CP4D (must support ReadWriteMany). Required (if known storage classes not available) Environment Variable: CPD_PRIMARY_STORAGE_CLASS Default: ibmc-file-gold-gid , ocs-storagecluster-cephfs , or azurefiles-premium (if available) Purpose : Specifies the storage class for CP4D primary storage, which requires ReadWriteMany (RWX) access mode for file storage. When to use : - Leave unset for automatic detection if known storage classes exist - Set explicitly when using custom or non-standard storage classes - Must support RWX access mode for shared file storage Valid values : Storage class name supporting ReadWriteMany access mode Impact : CP4D uses this for shared file storage across pods. Incorrect storage class or one not supporting RWX will cause deployment to fail. Related variables : - cpd_metadata_storage_class : Separate storage for metadata (RWO) Note : Known supported classes: ibmc-file-gold-gid (IBM Cloud), ocs-storagecluster-cephfs (OCS), azurefiles-premium (Azure). See CP4D Storage Considerations for details.","title":"cpd_primary_storage_class"},{"location":"roles/cp4d/#cpd_metadata_storage_class","text":"Storage class for CP4D Zen metadata database (must support ReadWriteOnce). Required (if known storage classes not available) Environment Variable: CPD_METADATA_STORAGE_CLASS Default: ibmc-block-gold , ocs-storagecluster-ceph-rbd , or managed-premium (if available) Purpose : Specifies the storage class for CP4D Zen metadata database, which requires ReadWriteOnce (RWO) access mode for block storage. When to use : - Leave unset for automatic detection if known storage classes exist - Set explicitly when using custom or non-standard storage classes - Must support RWO access mode for block storage Valid values : Storage class name supporting ReadWriteOnce access mode Impact : CP4D Zen metadata database uses this for persistent storage. Incorrect storage class or one not supporting RWO will cause deployment to fail. Related variables : - cpd_primary_storage_class : Separate storage for primary/file storage (RWX) Note : Known supported classes: ibmc-block-gold (IBM Cloud), ocs-storagecluster-ceph-rbd (OCS), managed-premium (Azure). Block storage typically provides better performance for databases.","title":"cpd_metadata_storage_class"},{"location":"roles/cp4d/#cpd_operators_namespace","text":"Namespace for CP4D operators installation. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default: ibm-cpd-operators Purpose : Specifies the namespace where CP4D operators will be installed. This follows the specialized installation model with separate operator and instance namespaces. When to use : - Use default ( ibm-cpd-operators ) for standard deployments - Set custom namespace for specific organizational requirements - Must be different from cpd_instance_namespace Valid values : Valid Kubernetes namespace name Impact : All CP4D operators (platform, zen, common services, etc.) are installed in this namespace. The namespace must not conflict with the instance namespace. Related variables : - cpd_instance_namespace : Separate namespace for CP4D instance workloads Note : CP4D uses a specialized installation model with operators in one namespace ( ibm-cpd-operators ) and instance workloads in another ( ibm-cpd ). This provides better isolation and management.","title":"cpd_operators_namespace"},{"location":"roles/cp4d/#cpd_instance_namespace","text":"Namespace for CP4D instance workloads. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default: ibm-cpd Purpose : Specifies the namespace where CP4D instance workloads (Zen, services, databases) will be deployed. Operators in cpd_operators_namespace watch and manage resources in this namespace. When to use : - Use default ( ibm-cpd ) for standard deployments - Set custom namespace for specific organizational requirements - Must be different from cpd_operators_namespace Valid values : Valid Kubernetes namespace name Impact : All CP4D instance workloads (Zen, MinIO, PostgreSQL, services) are deployed in this namespace. Operators watch this namespace for custom resources. Related variables : - cpd_operators_namespace : Separate namespace for CP4D operators Note : CP4D uses a specialized installation model with operators in ibm-cpd-operators and instance workloads in ibm-cpd . This separation provides better isolation and follows CP4D best practices.","title":"cpd_instance_namespace"},{"location":"roles/cp4d/#cpd_scale_config","text":"Resource scaling configuration for CP4D instance. Optional Environment Variable: CPD_SCALE_CONFIG Default: medium Purpose : Adjusts resource allocation (CPU, memory, replicas) for CP4D components to match workload requirements and increase processing capacity. When to use : - Use medium (default) for standard production deployments - Use small for development/test environments - Use large for high-capacity production environments - Adjust based on expected workload and performance requirements Valid values : small , medium , large (or other supported scale configurations) Impact : Determines resource requests/limits and replica counts for CP4D components. Larger scales require more cluster resources but provide better performance and capacity. Related variables : - cpd_product_version : Available scale options may vary by version Note : See Managing resources for detailed resource requirements per scale configuration. Ensure your cluster has sufficient resources for the selected scale.","title":"cpd_scale_config"},{"location":"roles/cp4d/#cpd_admin_username","text":"CP4D admin username for API authentication. Optional Environment Variable: CPD_ADMIN_USERNAME Default: cpadmin Purpose : Specifies the CP4D admin username for authenticating with CP4D APIs. Used when the role needs to interact with CP4D services. When to use : - Leave as default ( cpadmin ) if you haven't changed the initial admin username - Set explicitly if you changed the admin username after CP4D installation - Required for API operations that need admin authentication Valid values : Valid CP4D admin username string Impact : Used for CP4D API authentication. Incorrect username will cause API operations to fail. Related variables : - cpd_admin_password : Password for this admin user Note : The default cpadmin is the standard CP4D admin username. Only change if you've customized the admin username after installation.","title":"cpd_admin_username"},{"location":"roles/cp4d/#cpd_admin_password","text":"CP4D admin password for API authentication. Optional Environment Variable: CPD_ADMIN_PASSWORD Default: Retrieved from admin-user-details secret in cpd_instance_namespace Purpose : Specifies the CP4D admin password for authenticating with CP4D APIs. Used when the role needs to interact with CP4D services. When to use : - Leave unset (recommended) to auto-retrieve from cluster secret - Set explicitly if you changed the admin password after CP4D installation - Required for API operations that need admin authentication Valid values : Valid CP4D admin password string Impact : Used for CP4D API authentication. If not set, the role retrieves the initial admin password from the cluster. Incorrect password will cause API operations to fail. Related variables : - cpd_admin_username : Username for this admin password - cpd_instance_namespace : Namespace containing the password secret Note : The role automatically retrieves the initial admin password from the admin-user-details secret if not provided. Only set this if you've changed the password after installation. Keep passwords secure and do not commit to source control.","title":"cpd_admin_password"},{"location":"roles/cp4d/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/cp4d/#install-cloud-pak-for-data-513","text":"- hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.1.3 cpd_primary_storage_class: ibmc-file-gold-gid cpd_metadata_storage_class: ibmc-block-gold roles: - ibm.mas_devops.cp4d","title":"Install Cloud Pak for Data 5.1.3"},{"location":"roles/cp4d/#install-cloud-pak-for-data-520","text":"- hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.2.0 cpd_primary_storage_class: ibmc-file-gold-gid cpd_metadata_storage_class: ibmc-block-gold roles: - ibm.mas_devops.cp4d","title":"Install Cloud Pak for Data 5.2.0"},{"location":"roles/cp4d/#run-role-playbook","text":"export CPD_PRODUCT_VERSION=5.2.0 export CPD_PRIMARY_STORAGE_CLASS=ibmc-file-gold-gid export CPD_METADATA_STORAGE_CLASS=ibmc-block-gold export IBM_ENTITLEMENT_KEY=xxxxx ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/cp4d/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d_admin_pwd_update/","text":"cp4d_admin_pwd_update \u00a4 This role will update the password on an existing cp4d instance. By default it will update the password to a randomly generated new password only when the instance is still using the 'initial_admin_password' although using the 'cp4d_admin_password_force_update' variable referenced below will override this to update the password regardless of the current one being used. The new password will be added to the same yaml file that the 'initial_admin_password' was generated into - 'admin-user-details' by default. Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier associated with the CP4D deployment. Optional Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Associates the CP4D password update operation with a specific MAS instance for tracking and organization purposes. When to use : Set when CP4D is deployed as part of a MAS installation to maintain the association between CP4D and MAS. Valid values : Valid MAS instance ID (typically 3-12 lowercase alphanumeric characters) Impact : Used for logging and tracking purposes. Does not affect the password update operation itself. Related variables : cp4d_namespace Notes : Optional but recommended for MAS-integrated CP4D deployments to maintain clear associations. cp4d_namespace \u00a4 CP4D instance namespace where password will be updated. Optional Environment Variable: CP4D_NAMESPACE Default: ibm-cpd Purpose : Specifies the OpenShift namespace where the CP4D instance is deployed. When to use : Use default ( ibm-cpd ) for standard CP4D deployments. Override if CP4D is deployed in a custom namespace. Valid values : Valid Kubernetes namespace name Impact : Determines where to find the CP4D admin credentials secret and where to execute the password update. Related variables : cp4d_admin_credentials_secret_name Notes : The default ibm-cpd is the standard namespace for CP4D deployments. cp4d_admin_credentials_secret_name \u00a4 Kubernetes secret name containing CP4D admin credentials. Optional Environment Variable: CP4D_ADMIN_CREDENTIALS_SECRET_NAME Default: admin-user-details Purpose : Identifies the Kubernetes secret that stores the CP4D admin password, used to retrieve the current password and store the new one. When to use : Use default ( admin-user-details ) for standard CP4D deployments. Override if using a custom secret name. Valid values : Valid Kubernetes secret name Impact : The role reads the current password from this secret and updates it with the new password after the change. Related variables : cp4d_namespace , cp4d_admin_password Notes : - The default admin-user-details is the standard secret name for CP4D admin credentials - Secret must exist in the CP4D namespace - New password is written back to this same secret cp4d_admin_username \u00a4 CP4D administrator username. Optional Environment Variable: CP4D_ADMIN_USERNAME Default: admin Purpose : Specifies the CP4D admin user account whose password will be updated. When to use : Use default ( admin ) for standard CP4D deployments. Override if using a custom admin username. Valid values : Valid CP4D username Impact : Determines which user account's password will be changed in CP4D. Related variables : cp4d_admin_password , cp4d_admin_credentials_secret_name Notes : The default admin is the standard administrator username for CP4D. cp4d_admin_password \u00a4 Current CP4D admin password (optional). Optional Environment Variable: CP4D_ADMIN_PASSWORD Default: None Purpose : Provides the current admin password if not retrievable from the credentials secret. When to use : - Leave unset (recommended) to auto-retrieve from the credentials secret - Set explicitly if the secret is not accessible or contains incorrect password - Useful for manual password recovery scenarios Valid values : Valid CP4D admin password string Impact : When set, this password is used instead of retrieving from the secret. Must match the current CP4D admin password. Related variables : cp4d_admin_credentials_secret_name , cp4d_admin_username Notes : - Security : Avoid setting this in plain text; prefer secret-based retrieval - The role will attempt to retrieve the password from the secret if not provided - Only set if you cannot retrieve the password from the secret cp4d_admin_password_force_update \u00a4 Force password update regardless of current password. Optional Environment Variable: CP4D_ADMIN_PASSWORD_FORCE_UPDATE Default: false Purpose : Controls whether to update the password only if it matches the initial password, or to update it regardless of the current value. When to use : - Leave as false (default) for safe updates that only change initial passwords - Set to true to force password update regardless of current password - Use true for password rotation policies or recovery scenarios Valid values : true , false Impact : - false : Only updates password if CP4D is still using the initial password from the secret (safe default) - true : Updates password regardless of current value (use with caution) Related variables : cp4d_admin_password Notes : - Warning : Setting to true will change the password even if it has been customized - Default false is safer as it only updates passwords that haven't been changed from initial value - Use true for scheduled password rotation or when you need to reset a forgotten password - New randomly generated password is stored in the credentials secret Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" cp4d_namespace: ibm-cpd cp4d_admin_credentials_secret_name: admin-user-details cp4d_admin_username: admin cp4d_admin_password: password123 cp4d_admin_password_force_update: True roles: - ibm.mas_devops.cp4d_admin_pwd_update License \u00a4 EPL-2.0","title":"cp4d_admin_pwd_update"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_admin_pwd_update","text":"This role will update the password on an existing cp4d instance. By default it will update the password to a randomly generated new password only when the instance is still using the 'initial_admin_password' although using the 'cp4d_admin_password_force_update' variable referenced below will override this to update the password regardless of the current one being used. The new password will be added to the same yaml file that the 'initial_admin_password' was generated into - 'admin-user-details' by default.","title":"cp4d_admin_pwd_update"},{"location":"roles/cp4d_admin_pwd_update/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d_admin_pwd_update/#mas_instance_id","text":"MAS instance identifier associated with the CP4D deployment. Optional Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Associates the CP4D password update operation with a specific MAS instance for tracking and organization purposes. When to use : Set when CP4D is deployed as part of a MAS installation to maintain the association between CP4D and MAS. Valid values : Valid MAS instance ID (typically 3-12 lowercase alphanumeric characters) Impact : Used for logging and tracking purposes. Does not affect the password update operation itself. Related variables : cp4d_namespace Notes : Optional but recommended for MAS-integrated CP4D deployments to maintain clear associations.","title":"mas_instance_id"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_namespace","text":"CP4D instance namespace where password will be updated. Optional Environment Variable: CP4D_NAMESPACE Default: ibm-cpd Purpose : Specifies the OpenShift namespace where the CP4D instance is deployed. When to use : Use default ( ibm-cpd ) for standard CP4D deployments. Override if CP4D is deployed in a custom namespace. Valid values : Valid Kubernetes namespace name Impact : Determines where to find the CP4D admin credentials secret and where to execute the password update. Related variables : cp4d_admin_credentials_secret_name Notes : The default ibm-cpd is the standard namespace for CP4D deployments.","title":"cp4d_namespace"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_admin_credentials_secret_name","text":"Kubernetes secret name containing CP4D admin credentials. Optional Environment Variable: CP4D_ADMIN_CREDENTIALS_SECRET_NAME Default: admin-user-details Purpose : Identifies the Kubernetes secret that stores the CP4D admin password, used to retrieve the current password and store the new one. When to use : Use default ( admin-user-details ) for standard CP4D deployments. Override if using a custom secret name. Valid values : Valid Kubernetes secret name Impact : The role reads the current password from this secret and updates it with the new password after the change. Related variables : cp4d_namespace , cp4d_admin_password Notes : - The default admin-user-details is the standard secret name for CP4D admin credentials - Secret must exist in the CP4D namespace - New password is written back to this same secret","title":"cp4d_admin_credentials_secret_name"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_admin_username","text":"CP4D administrator username. Optional Environment Variable: CP4D_ADMIN_USERNAME Default: admin Purpose : Specifies the CP4D admin user account whose password will be updated. When to use : Use default ( admin ) for standard CP4D deployments. Override if using a custom admin username. Valid values : Valid CP4D username Impact : Determines which user account's password will be changed in CP4D. Related variables : cp4d_admin_password , cp4d_admin_credentials_secret_name Notes : The default admin is the standard administrator username for CP4D.","title":"cp4d_admin_username"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_admin_password","text":"Current CP4D admin password (optional). Optional Environment Variable: CP4D_ADMIN_PASSWORD Default: None Purpose : Provides the current admin password if not retrievable from the credentials secret. When to use : - Leave unset (recommended) to auto-retrieve from the credentials secret - Set explicitly if the secret is not accessible or contains incorrect password - Useful for manual password recovery scenarios Valid values : Valid CP4D admin password string Impact : When set, this password is used instead of retrieving from the secret. Must match the current CP4D admin password. Related variables : cp4d_admin_credentials_secret_name , cp4d_admin_username Notes : - Security : Avoid setting this in plain text; prefer secret-based retrieval - The role will attempt to retrieve the password from the secret if not provided - Only set if you cannot retrieve the password from the secret","title":"cp4d_admin_password"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_admin_password_force_update","text":"Force password update regardless of current password. Optional Environment Variable: CP4D_ADMIN_PASSWORD_FORCE_UPDATE Default: false Purpose : Controls whether to update the password only if it matches the initial password, or to update it regardless of the current value. When to use : - Leave as false (default) for safe updates that only change initial passwords - Set to true to force password update regardless of current password - Use true for password rotation policies or recovery scenarios Valid values : true , false Impact : - false : Only updates password if CP4D is still using the initial password from the secret (safe default) - true : Updates password regardless of current value (use with caution) Related variables : cp4d_admin_password Notes : - Warning : Setting to true will change the password even if it has been customized - Default false is safer as it only updates passwords that haven't been changed from initial value - Use true for scheduled password rotation or when you need to reset a forgotten password - New randomly generated password is stored in the credentials secret","title":"cp4d_admin_password_force_update"},{"location":"roles/cp4d_admin_pwd_update/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" cp4d_namespace: ibm-cpd cp4d_admin_credentials_secret_name: admin-user-details cp4d_admin_username: admin cp4d_admin_password: password123 cp4d_admin_password_force_update: True roles: - ibm.mas_devops.cp4d_admin_pwd_update","title":"Example Playbook"},{"location":"roles/cp4d_admin_pwd_update/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d_service/","text":"cp4d_service \u00a4 Install or upgrade a chosen CloudPak for Data service. Currently supported Cloud Pak for Data release versions supported are: 5.1.3 5.2.0 The role will automatically install the corresponding CPD service operator channel and custom resource version associated to the chosen Cloud Pak for Data release version. For more information about the specific CPD services channels and versions associated to a particular Cloud Pak for Data release can be found here . Services Supported These services can be deployed and configured using this role: Watson Studio required by Predict Watson Machine Learning required by Predict Analytics Services (Apache Spark) required by Predict Cognos Analytics optional dependency for Manage application Upgrade This role also supports seamlessly CPD services minor version upgrades, as well as patch version upgrades. All you need to do is to define cpd_product_version variable to the version you target to upgrade and run this role for a particular CPD service. It's important that before you upgrade CPD services, the CPD Control Plane/Zen is also upgraded to the same release version. For more information about IBM Cloud Pak for Data upgrade process, refer to the CPD official documentation . Application Support For more information on how Predict and HP Utilities make use of Watson Studio, refer to Predict/HP Utilities documentation Warning The reconcile of many CP4D resources will be marked as Failed multiple times during initial installation, these are misleading status updates , the install is just really slow and the operators can not properly handle this. For example, if you are watching the install of CCS you will see that each rabbitmq-ha pod takes 10-15 minutes to start up and it looks like there is a problem because the pod log will just stop at a certain point. If you see something like this as the last message in the pod log WAL: ra_log_wal init, open tbls: ra_log_open_mem_tables, closed tbls: ra_log_closed_mem_tables be assured that there's nothing wrong, it's just there's a long delay between that message and the next ( starting system coordination ) being logged. Role Variables - General \u00a4 Watson Studio \u00a4 Subscriptions related to Watson Studio: cpd-platform-operator ibm-cpd-wsl ibm-cpd-ccs ibm-cpd-datarefinery ibm-cpd-ws-runtimes Search Engine Dependency CPD 5.1.3 : Uses Elasticsearch operator ( ibm-elasticsearch-operator ) CPD 5.2.0 : Uses OpenSearch operator ( ibm-opensearch-operator ) Watson Studio is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: For CPD 5.1.3: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 83m ibm-cpd-datarefinery-operator 1/1 1 1 83m ibm-cpd-ws-operator 1/1 1 1 83m ibm-cpd-ws-runtimes-operator 1/1 1 1 83m ibm-elasticsearch-operator-ibm-es-controller-manager 1/1 1 1 83m For CPD 5.2.0: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 83m ibm-cpd-datarefinery-operator 1/1 1 1 83m ibm-cpd-ws-operator 1/1 1 1 83m ibm-cpd-ws-runtimes-operator 1/1 1 1 83m ibm-opensearch-operator-controller-manager 1/1 1 1 83m In the ibm-cpd namespace: oc -n ibm-cpd get ccs,ws,datarefinery,notebookruntimes,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 9.0.0 9.0.0 Completed 82m NAME VERSION RECONCILED STATUS AGE ws.ws.cpd.ibm.com/ws-cr 9.0.0 9.0.0 Completed 83m NAME VERSION RECONCILED STATUS AGE datarefinery.datarefinery.cpd.ibm.com/datarefinery-cr 9.0.0 9.0.0 Completed 36m NAME NLP MODELS VERSION RECONCILED STATUS AGE notebookruntime.ws.cpd.ibm.com/ibm-cpd-ws-runtime-241-py 9.0.0 9.0.0 Completed 22m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/asset-files-api 1/1 1 1 60m deployment.apps/ax-cdsx-jupyter-notebooks-converter-deploy 1/1 1 1 15m deployment.apps/ax-cdsx-notebooks-job-manager-deploy 1/1 1 1 15m deployment.apps/ax-environments-api-deploy 1/1 1 1 53m deployment.apps/ax-environments-ui-deploy 1/1 1 1 53m deployment.apps/ax-wdp-notebooks-api-deploy 1/1 1 1 15m deployment.apps/ax-ws-notebooks-ui-deploy 1/1 1 1 15m deployment.apps/catalog-api 2/2 2 2 69m deployment.apps/dataview-api-service 1/1 1 1 48m deployment.apps/dc-main 1/1 1 1 65m deployment.apps/event-logger-api 1/1 1 1 59m deployment.apps/ibm-0100-model-viewer-prod 1/1 1 1 14m deployment.apps/jobs-api 1/1 1 1 49m deployment.apps/jobs-ui 1/1 1 1 49m deployment.apps/ngp-projects-api 1/1 1 1 60m deployment.apps/portal-catalog 1/1 1 1 65m deployment.apps/portal-common-api 1/1 1 1 60m deployment.apps/portal-job-manager 1/1 1 1 60m deployment.apps/portal-main 1/1 1 1 60m deployment.apps/portal-ml-dl 1/1 1 1 14m deployment.apps/portal-notifications 1/1 1 1 60m deployment.apps/portal-projects 1/1 1 1 60m deployment.apps/redis-ha-haproxy 1/1 1 1 78m deployment.apps/runtime-assemblies-operator 1/1 1 1 59m deployment.apps/runtime-manager-api 1/1 1 1 59m deployment.apps/spaces 1/1 1 1 48m deployment.apps/task-credentials 1/1 1 1 48m deployment.apps/wdp-connect-connection 1/1 1 1 66m deployment.apps/wdp-connect-connector 1/1 1 1 66m deployment.apps/wdp-connect-flight 1/1 1 1 66m deployment.apps/wdp-dataprep 1/1 1 1 29m deployment.apps/wdp-dataview 1/1 1 1 48m deployment.apps/wdp-shaper 1/1 1 1 29m deployment.apps/wkc-search 1/1 1 1 66m deployment.apps/wml-main 1/1 1 1 48m NAME READY AGE statefulset.apps/elasticsea-0ac3-ib-6fb9-es-server-esnodes 3/3 74m statefulset.apps/rabbitmq-ha 3/3 79m statefulset.apps/redis-ha-server 3/3 79m statefulset.apps/wdp-couchdb 3/3 79m Watson Machine Learning \u00a4 Subscriptions related to Watson Machine Learning: cpd-platform-operator ibm-cpd-wml ibm-cpd-ccs Search Engine Dependency CPD 5.1.3 : Uses Elasticsearch operator ( ibm-elasticsearch-operator ) CPD 5.2.0 : Uses OpenSearch operator ( ibm-opensearch-operator ) Watson Machine Learning is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: For CPD 5.1.3: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 134m ibm-cpd-datarefinery-operator 1/1 1 1 134m ibm-cpd-wml-operator 1/1 1 1 49m ibm-elasticsearch-operator-ibm-es-controller-manager 1/1 1 1 134m For CPD 5.2.0: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 134m ibm-cpd-datarefinery-operator 1/1 1 1 134m ibm-cpd-wml-operator 1/1 1 1 49m ibm-opensearch-operator-controller-manager 1/1 1 1 134m In the ibm-cpd namespace: oc -n ibm-cpd get ccs,wmlbase,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 9.0.0 9.0.0 Completed 133m NAME VERSION BUILD STATUS RECONCILED AGE wmlbase.wml.cpd.ibm.com/wml-cr 5.0.0 5.0.0-918 Completed 5.0.0 50m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/wml-deployment-envoy 1/1 1 1 23m deployment.apps/wml-deployment-manager 1/1 1 1 19m deployment.apps/wml-main 1/1 1 1 99m deployment.apps/wml-repositoryv4 1/1 1 1 16m deployment.apps/wmltraining 1/1 1 1 15m deployment.apps/wmltrainingorchestrator 1/1 1 1 14m NAME READY AGE statefulset.apps/wml-cpd-etcd 3/3 26m statefulset.apps/wml-deployment-agent 1/1 21m Analytics Engine \u00a4 Subscriptions related to Analytics Engine: cpd-platform-operator analyticsengine-operator Analytics Engine is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ae-operator 1/1 1 1 31m In the ibm-cpd namespace: oc -n ibm-cpd get analyticsengine,deployments NAME VERSION RECONCILED STATUS AGE analyticsengine.ae.cpd.ibm.com/analyticsengine-sample 5.0.0 5.0.0 Completed 31m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/spark-hb-br-recovery 1/1 1 1 11m deployment.apps/spark-hb-control-plane 1/1 1 1 19m deployment.apps/spark-hb-create-trust-store 1/1 1 1 25m deployment.apps/spark-hb-deployer-agent 1/1 1 1 19m deployment.apps/spark-hb-nginx 1/1 1 1 19m deployment.apps/spark-hb-register-hb-dataplane 1/1 1 1 10m deployment.apps/spark-hb-ui 1/1 1 1 19m Cognos Analytics \u00a4 Subscriptions related to Cognos Analytics (in the ibm-cpd-operators namespace): cpd-platform-operator ibm-ca-operator-controller-manager Cognos Analytics is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-ca-operator-controller-manager 1/1 1 1 19m In the ibm-cpd namespace: oc -n ibm-cpd get caservice,deployments NAME AGE caservice.ca.cpd.ibm.com/ca-addon-cr 19m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cognos-analytics-cognos-analytics-addon 1/1 1 1 9m17s Role Variables - Installation \u00a4 cpd_service_name \u00a4 Name of the service to install, supported values are: wsl , wml , spark , and ca Required Environment Variable: CPD_SERVICE_NAME Default Value: None cpd_product_version \u00a4 The product version (also known as operand version) of this service to install. Required Environment Variable: CPD_PRODUCT_VERSION Default Value: Defined by the installed MAS catalog version cpd_service_storage_class \u00a4 This is used to set spec.storageClass in all CPD services that uses file storage class (read-write-many RWX). Required , unless IBMCloud storage classes are available. Environment Variable: CPD_SERVICE_STORAGE_CLASS Default Value: Auto determined if default storage classes are provided and available by your cloud provider. i.e ibmc-file for IBM Cloud, efs for AWS. cpd_service_block_storage_class \u00a4 This is used to set spec.blockStorageClass in all CPD services that uses block storage class (read-write-only RWO). Required , unless IBMCloud storage classes are available. Environment Variable: CPD_SERVICE_BLOCK_STORAGE_CLASS Default Value: Auto determined if default storage classes are provided and available by your cloud provider. i.e ibmc-block for IBM Cloud, gp2 for AWS. cpd_instance_namespace \u00a4 Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default Value: ibm-cpd cpd_operator_namespace \u00a4 Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default Value: ibm-cpd-operators cpd_admin_username \u00a4 The CP4D Admin username to authenticate with CP4D APIs. If you didn't change the initial admin username after installing CP4D then you don't need to provide this. Optional Environment Variable: CPD_ADMIN_USERNAME Default Value: admin (CPD 4.6) cpadmin (CPD 4.8 and newer) cpd_admin_password \u00a4 The CP4D Admin User password to call CP4D API to provision Discovery Instance. If you didn't change the initial admin password after CP4D install, you don't need to provide it. The initial admin user password for admin or cpdamin will be used. Optional Environment Variable: CPD_ADMIN_PASSWORD Default Value: CPD 4.6: Looked up from the admin-user-details secret in the cpd_instance_namespace namespace CPD 4.8 and newer: Looked up from the ibm-iam-bindinfo-platform-auth-idp-credentials secret in the cpd_instance_namespace namespace cpd_service_scale_config \u00a4 Adjust and scale the resources for your Cloud Pak for Data services to increase processing capacity. For more information, refer to Managing resources in IBM Cloud Pak for Data documentation. Optional Environment Variable: CPD_SERVICE_SCALE_CONFIG Default Value: small Role Variables - Watson Studio \u00a4 cpd_wsl_project_name \u00a4 Stores the CP4D Watson Studio Project name that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_NAME Default Value: wsl-mas-${mas_instance_id}-hputilities cpd_wsl_project_description \u00a4 Optional - Stores the CP4D Watson Studio Project description that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_DESCRIPTION Default Value: Watson Studio Project for Maximo Application Suite Role Variables - MAS Configuration Generation \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that a generated configuration will target. If this or mas_config_dir are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated resource definition. This can be used to manually configure a MAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbook \u00a4 Install Watson Studio on CPD 5.1.3 \u00a4 - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.1.3 cpd_service_storage_class: ibmc-file-gold-gid cpd_service_name: wsl roles: - ibm.mas_devops.cp4d_service Install Watson Studio on CPD 5.2.0 \u00a4 - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.2.0 cpd_service_storage_class: ibmc-file-gold-gid cpd_service_name: wsl roles: - ibm.mas_devops.cp4d_service License \u00a4 EPL-2.0","title":"cp4d_service"},{"location":"roles/cp4d_service/#cp4d_service","text":"Install or upgrade a chosen CloudPak for Data service. Currently supported Cloud Pak for Data release versions supported are: 5.1.3 5.2.0 The role will automatically install the corresponding CPD service operator channel and custom resource version associated to the chosen Cloud Pak for Data release version. For more information about the specific CPD services channels and versions associated to a particular Cloud Pak for Data release can be found here . Services Supported These services can be deployed and configured using this role: Watson Studio required by Predict Watson Machine Learning required by Predict Analytics Services (Apache Spark) required by Predict Cognos Analytics optional dependency for Manage application Upgrade This role also supports seamlessly CPD services minor version upgrades, as well as patch version upgrades. All you need to do is to define cpd_product_version variable to the version you target to upgrade and run this role for a particular CPD service. It's important that before you upgrade CPD services, the CPD Control Plane/Zen is also upgraded to the same release version. For more information about IBM Cloud Pak for Data upgrade process, refer to the CPD official documentation . Application Support For more information on how Predict and HP Utilities make use of Watson Studio, refer to Predict/HP Utilities documentation Warning The reconcile of many CP4D resources will be marked as Failed multiple times during initial installation, these are misleading status updates , the install is just really slow and the operators can not properly handle this. For example, if you are watching the install of CCS you will see that each rabbitmq-ha pod takes 10-15 minutes to start up and it looks like there is a problem because the pod log will just stop at a certain point. If you see something like this as the last message in the pod log WAL: ra_log_wal init, open tbls: ra_log_open_mem_tables, closed tbls: ra_log_closed_mem_tables be assured that there's nothing wrong, it's just there's a long delay between that message and the next ( starting system coordination ) being logged.","title":"cp4d_service"},{"location":"roles/cp4d_service/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/cp4d_service/#watson-studio","text":"Subscriptions related to Watson Studio: cpd-platform-operator ibm-cpd-wsl ibm-cpd-ccs ibm-cpd-datarefinery ibm-cpd-ws-runtimes Search Engine Dependency CPD 5.1.3 : Uses Elasticsearch operator ( ibm-elasticsearch-operator ) CPD 5.2.0 : Uses OpenSearch operator ( ibm-opensearch-operator ) Watson Studio is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: For CPD 5.1.3: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 83m ibm-cpd-datarefinery-operator 1/1 1 1 83m ibm-cpd-ws-operator 1/1 1 1 83m ibm-cpd-ws-runtimes-operator 1/1 1 1 83m ibm-elasticsearch-operator-ibm-es-controller-manager 1/1 1 1 83m For CPD 5.2.0: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 83m ibm-cpd-datarefinery-operator 1/1 1 1 83m ibm-cpd-ws-operator 1/1 1 1 83m ibm-cpd-ws-runtimes-operator 1/1 1 1 83m ibm-opensearch-operator-controller-manager 1/1 1 1 83m In the ibm-cpd namespace: oc -n ibm-cpd get ccs,ws,datarefinery,notebookruntimes,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 9.0.0 9.0.0 Completed 82m NAME VERSION RECONCILED STATUS AGE ws.ws.cpd.ibm.com/ws-cr 9.0.0 9.0.0 Completed 83m NAME VERSION RECONCILED STATUS AGE datarefinery.datarefinery.cpd.ibm.com/datarefinery-cr 9.0.0 9.0.0 Completed 36m NAME NLP MODELS VERSION RECONCILED STATUS AGE notebookruntime.ws.cpd.ibm.com/ibm-cpd-ws-runtime-241-py 9.0.0 9.0.0 Completed 22m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/asset-files-api 1/1 1 1 60m deployment.apps/ax-cdsx-jupyter-notebooks-converter-deploy 1/1 1 1 15m deployment.apps/ax-cdsx-notebooks-job-manager-deploy 1/1 1 1 15m deployment.apps/ax-environments-api-deploy 1/1 1 1 53m deployment.apps/ax-environments-ui-deploy 1/1 1 1 53m deployment.apps/ax-wdp-notebooks-api-deploy 1/1 1 1 15m deployment.apps/ax-ws-notebooks-ui-deploy 1/1 1 1 15m deployment.apps/catalog-api 2/2 2 2 69m deployment.apps/dataview-api-service 1/1 1 1 48m deployment.apps/dc-main 1/1 1 1 65m deployment.apps/event-logger-api 1/1 1 1 59m deployment.apps/ibm-0100-model-viewer-prod 1/1 1 1 14m deployment.apps/jobs-api 1/1 1 1 49m deployment.apps/jobs-ui 1/1 1 1 49m deployment.apps/ngp-projects-api 1/1 1 1 60m deployment.apps/portal-catalog 1/1 1 1 65m deployment.apps/portal-common-api 1/1 1 1 60m deployment.apps/portal-job-manager 1/1 1 1 60m deployment.apps/portal-main 1/1 1 1 60m deployment.apps/portal-ml-dl 1/1 1 1 14m deployment.apps/portal-notifications 1/1 1 1 60m deployment.apps/portal-projects 1/1 1 1 60m deployment.apps/redis-ha-haproxy 1/1 1 1 78m deployment.apps/runtime-assemblies-operator 1/1 1 1 59m deployment.apps/runtime-manager-api 1/1 1 1 59m deployment.apps/spaces 1/1 1 1 48m deployment.apps/task-credentials 1/1 1 1 48m deployment.apps/wdp-connect-connection 1/1 1 1 66m deployment.apps/wdp-connect-connector 1/1 1 1 66m deployment.apps/wdp-connect-flight 1/1 1 1 66m deployment.apps/wdp-dataprep 1/1 1 1 29m deployment.apps/wdp-dataview 1/1 1 1 48m deployment.apps/wdp-shaper 1/1 1 1 29m deployment.apps/wkc-search 1/1 1 1 66m deployment.apps/wml-main 1/1 1 1 48m NAME READY AGE statefulset.apps/elasticsea-0ac3-ib-6fb9-es-server-esnodes 3/3 74m statefulset.apps/rabbitmq-ha 3/3 79m statefulset.apps/redis-ha-server 3/3 79m statefulset.apps/wdp-couchdb 3/3 79m","title":"Watson Studio"},{"location":"roles/cp4d_service/#watson-machine-learning","text":"Subscriptions related to Watson Machine Learning: cpd-platform-operator ibm-cpd-wml ibm-cpd-ccs Search Engine Dependency CPD 5.1.3 : Uses Elasticsearch operator ( ibm-elasticsearch-operator ) CPD 5.2.0 : Uses OpenSearch operator ( ibm-opensearch-operator ) Watson Machine Learning is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: For CPD 5.1.3: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 134m ibm-cpd-datarefinery-operator 1/1 1 1 134m ibm-cpd-wml-operator 1/1 1 1 49m ibm-elasticsearch-operator-ibm-es-controller-manager 1/1 1 1 134m For CPD 5.2.0: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 134m ibm-cpd-datarefinery-operator 1/1 1 1 134m ibm-cpd-wml-operator 1/1 1 1 49m ibm-opensearch-operator-controller-manager 1/1 1 1 134m In the ibm-cpd namespace: oc -n ibm-cpd get ccs,wmlbase,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 9.0.0 9.0.0 Completed 133m NAME VERSION BUILD STATUS RECONCILED AGE wmlbase.wml.cpd.ibm.com/wml-cr 5.0.0 5.0.0-918 Completed 5.0.0 50m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/wml-deployment-envoy 1/1 1 1 23m deployment.apps/wml-deployment-manager 1/1 1 1 19m deployment.apps/wml-main 1/1 1 1 99m deployment.apps/wml-repositoryv4 1/1 1 1 16m deployment.apps/wmltraining 1/1 1 1 15m deployment.apps/wmltrainingorchestrator 1/1 1 1 14m NAME READY AGE statefulset.apps/wml-cpd-etcd 3/3 26m statefulset.apps/wml-deployment-agent 1/1 21m","title":"Watson Machine Learning"},{"location":"roles/cp4d_service/#analytics-engine","text":"Subscriptions related to Analytics Engine: cpd-platform-operator analyticsengine-operator Analytics Engine is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ae-operator 1/1 1 1 31m In the ibm-cpd namespace: oc -n ibm-cpd get analyticsengine,deployments NAME VERSION RECONCILED STATUS AGE analyticsengine.ae.cpd.ibm.com/analyticsengine-sample 5.0.0 5.0.0 Completed 31m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/spark-hb-br-recovery 1/1 1 1 11m deployment.apps/spark-hb-control-plane 1/1 1 1 19m deployment.apps/spark-hb-create-trust-store 1/1 1 1 25m deployment.apps/spark-hb-deployer-agent 1/1 1 1 19m deployment.apps/spark-hb-nginx 1/1 1 1 19m deployment.apps/spark-hb-register-hb-dataplane 1/1 1 1 10m deployment.apps/spark-hb-ui 1/1 1 1 19m","title":"Analytics Engine"},{"location":"roles/cp4d_service/#cognos-analytics","text":"Subscriptions related to Cognos Analytics (in the ibm-cpd-operators namespace): cpd-platform-operator ibm-ca-operator-controller-manager Cognos Analytics is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-ca-operator-controller-manager 1/1 1 1 19m In the ibm-cpd namespace: oc -n ibm-cpd get caservice,deployments NAME AGE caservice.ca.cpd.ibm.com/ca-addon-cr 19m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cognos-analytics-cognos-analytics-addon 1/1 1 1 9m17s","title":"Cognos Analytics"},{"location":"roles/cp4d_service/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/cp4d_service/#cpd_service_name","text":"Name of the service to install, supported values are: wsl , wml , spark , and ca Required Environment Variable: CPD_SERVICE_NAME Default Value: None","title":"cpd_service_name"},{"location":"roles/cp4d_service/#cpd_product_version","text":"The product version (also known as operand version) of this service to install. Required Environment Variable: CPD_PRODUCT_VERSION Default Value: Defined by the installed MAS catalog version","title":"cpd_product_version"},{"location":"roles/cp4d_service/#cpd_service_storage_class","text":"This is used to set spec.storageClass in all CPD services that uses file storage class (read-write-many RWX). Required , unless IBMCloud storage classes are available. Environment Variable: CPD_SERVICE_STORAGE_CLASS Default Value: Auto determined if default storage classes are provided and available by your cloud provider. i.e ibmc-file for IBM Cloud, efs for AWS.","title":"cpd_service_storage_class"},{"location":"roles/cp4d_service/#cpd_service_block_storage_class","text":"This is used to set spec.blockStorageClass in all CPD services that uses block storage class (read-write-only RWO). Required , unless IBMCloud storage classes are available. Environment Variable: CPD_SERVICE_BLOCK_STORAGE_CLASS Default Value: Auto determined if default storage classes are provided and available by your cloud provider. i.e ibmc-block for IBM Cloud, gp2 for AWS.","title":"cpd_service_block_storage_class"},{"location":"roles/cp4d_service/#cpd_instance_namespace","text":"Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default Value: ibm-cpd","title":"cpd_instance_namespace"},{"location":"roles/cp4d_service/#cpd_operator_namespace","text":"Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default Value: ibm-cpd-operators","title":"cpd_operator_namespace"},{"location":"roles/cp4d_service/#cpd_admin_username","text":"The CP4D Admin username to authenticate with CP4D APIs. If you didn't change the initial admin username after installing CP4D then you don't need to provide this. Optional Environment Variable: CPD_ADMIN_USERNAME Default Value: admin (CPD 4.6) cpadmin (CPD 4.8 and newer)","title":"cpd_admin_username"},{"location":"roles/cp4d_service/#cpd_admin_password","text":"The CP4D Admin User password to call CP4D API to provision Discovery Instance. If you didn't change the initial admin password after CP4D install, you don't need to provide it. The initial admin user password for admin or cpdamin will be used. Optional Environment Variable: CPD_ADMIN_PASSWORD Default Value: CPD 4.6: Looked up from the admin-user-details secret in the cpd_instance_namespace namespace CPD 4.8 and newer: Looked up from the ibm-iam-bindinfo-platform-auth-idp-credentials secret in the cpd_instance_namespace namespace","title":"cpd_admin_password"},{"location":"roles/cp4d_service/#cpd_service_scale_config","text":"Adjust and scale the resources for your Cloud Pak for Data services to increase processing capacity. For more information, refer to Managing resources in IBM Cloud Pak for Data documentation. Optional Environment Variable: CPD_SERVICE_SCALE_CONFIG Default Value: small","title":"cpd_service_scale_config"},{"location":"roles/cp4d_service/#role-variables-watson-studio","text":"","title":"Role Variables - Watson Studio"},{"location":"roles/cp4d_service/#cpd_wsl_project_name","text":"Stores the CP4D Watson Studio Project name that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_NAME Default Value: wsl-mas-${mas_instance_id}-hputilities","title":"cpd_wsl_project_name"},{"location":"roles/cp4d_service/#cpd_wsl_project_description","text":"Optional - Stores the CP4D Watson Studio Project description that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_DESCRIPTION Default Value: Watson Studio Project for Maximo Application Suite","title":"cpd_wsl_project_description"},{"location":"roles/cp4d_service/#role-variables-mas-configuration-generation","text":"","title":"Role Variables - MAS Configuration Generation"},{"location":"roles/cp4d_service/#mas_instance_id","text":"The instance ID of Maximo Application Suite that a generated configuration will target. If this or mas_config_dir are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cp4d_service/#mas_config_dir","text":"Local directory to save the generated resource definition. This can be used to manually configure a MAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/cp4d_service/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/cp4d_service/#install-watson-studio-on-cpd-513","text":"- hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.1.3 cpd_service_storage_class: ibmc-file-gold-gid cpd_service_name: wsl roles: - ibm.mas_devops.cp4d_service","title":"Install Watson Studio on CPD 5.1.3"},{"location":"roles/cp4d_service/#install-watson-studio-on-cpd-520","text":"- hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.2.0 cpd_service_storage_class: ibmc-file-gold-gid cpd_service_name: wsl roles: - ibm.mas_devops.cp4d_service","title":"Install Watson Studio on CPD 5.2.0"},{"location":"roles/cp4d_service/#license","text":"EPL-2.0","title":"License"},{"location":"roles/db2/","text":"db2 \u00a4 This role creates or upgrades a Db2 instance using the Db2u Operator. When installing db2, the db2u operator will now be installed into the same namespace as the db2 instance ( db2ucluster ). If you already have db2 operator and db2 instances running in separate namespaces, this role will take care of migrating (by deleting & reinstalling) the db2 operators from ibm-common-services to the namespace defined by db2_namespace property (in case of a new role execution for a db2 install or db2 upgrade). A private root CA certificate is created and is used to secure the TLS connections to the database. A Db2 Warehouse cluster will be created along with a public TLS encrypted route to allow external access to the cluster (access is via the ssl-server nodeport port on the -db2u-engn-svc service). Internal access is via the -db2u-engn-svc service and port 50001. Both the external route and the internal service use the same server certificate. The private root CA certificate and the server certificate are available from the db2u-ca and db2u-certificate secrets in the db2 namespace. The default user is db2inst1 and the password is available in the instancepassword secret in the same namespace. You can examine the deployed resources in the db2 namespace. This example assumes the default namespace db2u : oc -n db2u get db2ucluster NAME STATE MAINTENANCESTATE AGE db2u-db01 Ready None 29m It typically takes 20-30 minutes from the db2ucluster being created till it is ready. If the db2ucluster is not ready after that period then check that all the PersistentVolumeClaims in the db2 namespace are ready and that the pods in the namespace are not stuck in init state. If the c-<db2_instance_name>-db2u-0 pod is running then you can exec into the pod and check the /var/log/db2u.log for any issue. If the mas_instance_id and mas_config_dir are provided then the role will generate the JdbcCfg yaml that can be used to configure MAS to connect to this database. It does not apply the yaml to the cluster but does provide you with the yaml files to apply if needed. When upgrading db2, specify the existing namespace where the db2uCluster instances exist. All the instances under that namespace will be upgraded to the db2 version specified. The version of db2 must match the channel of db2 being used for the upgrade. Role Variables - General \u00a4 Installation Variables \u00a4 common_services_namespace \u00a4 OpenShift namespace where IBM Common Services is installed. Optional Environment Variable: COMMON_SERVICES_NAMESPACE Default Value: ibm-common-services Purpose : Specifies the namespace containing IBM Common Services, which provides shared services used by Db2 operator. This is needed for the role to locate and interact with Common Services components. When to use : - Leave as default ( ibm-common-services ) for standard installations - Set only if Common Services is installed in a non-standard namespace - Required when Common Services namespace differs from default Valid values : Any valid Kubernetes namespace name where IBM Common Services is installed Impact : The role uses this to locate Common Services resources. Incorrect namespace will cause the role to fail when trying to access Common Services components. Related variables : Works with db2_namespace to manage operator and instance placement. Note : The default ibm-common-services is the standard namespace for Common Services. Only change if your installation uses a different namespace. db2_action \u00a4 Specifies which operation to perform on the Db2 database. Optional Environment Variable: DB2_ACTION Default: install Purpose : Controls what action the role executes against Db2 instances. This allows the same role to handle installation, upgrades, backups, and restores of Db2 databases. When to use : - Use install (default) for initial Db2 deployment - Use upgrade to upgrade all Db2 instances in the namespace to a new version - Use backup to create a backup of Db2 data - Use restore to restore Db2 from a backup Valid values : install , upgrade , backup , restore Impact : - install : Creates new Db2 operator and instance - upgrade : Upgrades ALL instances in db2_namespace to db2_version (affects all instances in namespace) - backup : Creates backup of Db2 data - restore : Restores Db2 from backup Related variables : - db2_version : Required for upgrade action to specify target version - db2_namespace : All instances in this namespace are affected by upgrade Note : WARNING - When using upgrade , ALL Db2 instances in the specified namespace will be upgraded. Plan accordingly and ensure db2_version matches the operator channel. db2_namespace \u00a4 OpenShift namespace where Db2 operator and instances will be deployed. Optional Environment Variable: DB2_NAMESPACE Default: db2u Purpose : Specifies the namespace for deploying the Db2u operator and Db2 instances (Db2uCluster custom resources). Starting with recent versions, the operator is installed in the same namespace as the instances. When to use : - Use default ( db2u ) for standard single-instance deployments - Set to a custom namespace when organizing multiple Db2 deployments - Must match existing namespace when upgrading Db2 instances Valid values : Any valid Kubernetes namespace name (e.g., db2u , db2-prod , mas-db2 ) Impact : All Db2 resources (operator, instances, secrets, services) are created in this namespace. When upgrading, ALL instances in this namespace will be upgraded together. Related variables : - db2_action : When set to upgrade , affects all instances in this namespace - db2_instance_name : Instance created within this namespace Note : The role handles migration of operators from ibm-common-services to this namespace if needed. All instances in the namespace are upgraded together when using db2_action=upgrade . db2_channel \u00a4 Subscription channel for the Db2 Universal Operator. Optional Environment Variable: DB2_CHANNEL Default: The default channel from the operator package (automatically selected) Purpose : Specifies which operator channel to subscribe to, determining which Db2 operator version stream you receive. Channels typically correspond to major Db2 versions (e.g., v2.2 , v3.0 ). When to use : - Leave unset to use the default channel (recommended for new installations) - Set explicitly when you need a specific operator version stream - Must match db2_version when upgrading (version must be available in the channel) Valid values : Valid Db2 operator channel names (e.g., v2.2 , v3.0 , v3.1 ) Impact : Determines which operator version is installed and which Db2 engine versions are available. Changing channels may require operator migration. Related variables : - db2_version : Must be compatible with the selected channel - When upgrading, version must match channel Note : The operator channel determines available Db2 engine versions. Check the operator package for available channels and their supported versions. db2_instance_name \u00a4 Unique name for the Db2 instance (Db2uCluster resource). Required Environment Variable: DB2_INSTANCE_NAME Default: None Purpose : Defines the name of the Db2uCluster custom resource that will be created. This name is used to identify the Db2 instance and is embedded in resource names (services, pods, secrets). When to use : - Always required for Db2 installation - Use descriptive names that indicate purpose (e.g., maximo-db , iot-db , manage-db ) - Must be unique within the namespace Valid values : Valid Kubernetes resource name (lowercase alphanumeric with hyphens, e.g., db01 , mas-db , manage-db ) Impact : This name is used throughout the Db2 deployment: - Db2uCluster resource name - Service names (e.g., {instance_name}-db2u-engn-svc ) - Pod names (e.g., c-{instance_name}-db2u-0 ) - Secret names Related variables : - db2_dbname : Name of the database within this instance - Used in generated JdbcCfg when mas_instance_id is provided Note : Choose a meaningful name as it appears in many resource names and cannot be easily changed after creation. ibm_entitlement_key \u00a4 IBM Container Library entitlement key for accessing Db2 container images. Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Provides authentication credentials to pull Db2 container images from the IBM Container Registry. This key is associated with your IBM entitlement and grants access to licensed software. When to use : - Always required for Db2 installation - Obtain from IBM Container Library - Same key used across all IBM MAS components Valid values : Valid IBM entitlement key string Impact : Without a valid key, Db2 container images cannot be pulled and installation will fail. The key is stored in an image pull secret in the Db2 namespace. Related variables : Used across multiple roles for accessing IBM container images. Note : Keep this key secure and do not commit to source control. The key is tied to your IBM entitlement and should be treated as a credential. Obtain from the IBM Container Library using your IBM ID. db2_dbname \u00a4 Name of the database to create within the Db2 instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB Purpose : Specifies the name of the database that will be created inside the Db2 instance. A Db2 instance can contain multiple databases, and this defines the primary database name. When to use : - Use default ( BLUDB ) for standard MAS deployments - Set to a custom name when organizational standards require specific naming - Must match the database name expected by applications connecting to Db2 Valid values : Valid Db2 database name (uppercase alphanumeric, up to 8 characters, e.g., BLUDB , MAXDB , MASDB ) Impact : This database name is used in connection strings and JDBC configurations. Applications must use this exact name to connect to the database. Related variables : - db2_instance_name : The instance containing this database - Used in generated JdbcCfg for MAS configuration Note : The default BLUDB is standard for Db2 Warehouse. Db2 database names are typically uppercase and limited to 8 characters. db2_version \u00a4 Db2 engine version to use for installation or upgrade. Optional Environment Variable: DB2_VERSION Default: Latest version supported by the installed Db2 operator (auto-detected from db2u-release configmap) Purpose : Specifies which Db2 engine version to deploy or upgrade to. The version must be compatible with the installed operator channel. When to use : - Leave unset for new installations to use the latest supported version (recommended) - Set explicitly when you need a specific Db2 version - Required when upgrading to specify the target version - Must match the operator channel capabilities Valid values : Valid Db2 engine version supported by the operator (e.g., 11.5.8.0 , 11.5.9.0 ) Impact : Determines which Db2 engine version is deployed. When upgrading, ALL instances in db2_namespace will be upgraded to this version. Version must be available in the operator's supported versions list. Related variables : - db2_channel : Version must be compatible with the operator channel - db2_action : When set to upgrade , this version is the target - db2_namespace : All instances in namespace upgraded to this version Note : Supported versions are listed in the db2u-release configmap in ibm-common-services namespace. Ensure the version matches your operator channel. When upgrading, all instances in the namespace are upgraded together. db2_type \u00a4 Db2 instance type optimized for specific workload patterns. Optional Environment Variable: DB2_TYPE Default: db2wh Purpose : Determines the Db2 instance configuration optimized for either data warehouse (analytical) or online transaction processing workloads. This affects resource allocation and database tuning. When to use : - Use db2wh (default) for MAS Manage and analytical workloads - Use db2oltp for high-transaction workloads requiring OLTP optimization - Choose based on your primary use case Valid values : db2wh , db2oltp Impact : - db2wh : Optimized for data warehouse/analytical queries (recommended for MAS Manage) - db2oltp : Optimized for online transaction processing with high concurrency Related variables : - db2_workload : Further refines workload optimization - db2_table_org : Table organization should align with instance type Note : MAS Manage typically uses db2wh for optimal performance with its analytical workload patterns. db2_timezone \u00a4 Server timezone for the Db2 instance. Optional Environment Variable: DB2_TIMEZONE Default: GMT Purpose : Sets the timezone used by the Db2 server for timestamp operations and scheduling. This affects how Db2 interprets and stores timestamp data. When to use : - Use default ( GMT ) for globally distributed systems - Set to local timezone when all users are in the same timezone - Must match MAS Manage timezone if using Manage with this database Valid values : Valid timezone codes (e.g., GMT , EST , PST , CET , UTC ) Impact : Affects timestamp interpretation and storage in the database. Mismatched timezones between Db2 and applications can cause data inconsistencies. Related variables : - MAS_APP_SETTINGS_SERVER_TIMEZONE : Must be set to the same value for MAS Manage - Affects all timestamp operations in the database Note : IMPORTANT - If using this Db2 instance with MAS Manage, you must also set MAS_APP_SETTINGS_SERVER_TIMEZONE to the same value to ensure consistent timestamp handling. db2_4k_device_support \u00a4 Controls 4K sector device support in Db2. Optional Environment Variable: DB2_4K_DEVICE_SUPPORT Default: ON Purpose : Enables or disables support for storage devices with 4K sector sizes. Modern storage systems often use 4K sectors instead of traditional 512-byte sectors. When to use : - Leave as ON (default) for modern storage systems with 4K sectors - Set to OFF only if using legacy storage with 512-byte sectors - Check your storage specifications if unsure Valid values : ON , OFF Impact : - ON : Enables 4K sector support (required for most modern storage) - OFF : Uses traditional 512-byte sector mode (legacy storage only) Related variables : Works with storage class configuration to ensure compatibility. Note : Most modern storage systems use 4K sectors. Keep this ON unless you have specific legacy storage requirements. db2_workload \u00a4 Workload profile for Db2 instance optimization. Optional Environment Variable: DB2_WORKLOAD Default: ANALYTICS Purpose : Configures Db2 with predefined settings optimized for specific workload patterns. This affects memory allocation, query optimization, and other performance parameters. When to use : - Use ANALYTICS (default) for general analytical and MAS workloads - Use PUREDATA_OLAP for pure data warehouse/OLAP workloads - Choose based on your primary query patterns Valid values : ANALYTICS , PUREDATA_OLAP Impact : - ANALYTICS : Balanced optimization for mixed analytical workloads (recommended for MAS) - PUREDATA_OLAP : Optimized specifically for OLAP/data warehouse queries Related variables : - db2_type : Should align with workload choice - db2_table_org : Table organization should match workload pattern Note : The default ANALYTICS is suitable for most MAS deployments. Only change if you have specific OLAP requirements. db2_table_org \u00a4 Default table organization for database tables. Optional Environment Variable: DB2_TABLE_ORG Default: ROW Purpose : Determines the default storage organization for tables in the database. Row-organized tables are optimized for transactional workloads, while column-organized tables are optimized for analytical queries. When to use : - Use ROW (default) for MAS Manage and mixed workloads - Use COLUMN only for pure analytical/reporting workloads - Choose based on your primary query patterns Valid values : ROW , COLUMN Impact : - ROW : Tables stored row-by-row (better for OLTP, updates, and mixed workloads) - COLUMN : Tables stored column-by-column (better for analytical queries and aggregations) Related variables : - db2_type : Should align with table organization choice - db2_workload : Workload profile should match table organization Note : MAS Manage requires ROW organization. Only use COLUMN for dedicated analytical databases. This sets the default; individual tables can override this setting. db2_ldap_username \u00a4 Username for Db2 LDAP authentication. Optional Environment Variable: DB2_LDAP_USERNAME Default: None (uses default db2inst1 user) Purpose : Defines a custom LDAP username for Db2 authentication instead of the default db2inst1 user. When set, this user is configured in Db2's local LDAP registry and used in MAS JDBC configuration. When to use : - Set when you need a custom database user for MAS connections - Set when organizational policies require specific usernames - Leave unset to use the default db2inst1 user - Must be set together with db2_ldap_password Valid values : Valid LDAP username string Impact : When set, this user is created in Db2's LDAP registry and used in the generated JdbcCfg for MAS. The user will have necessary database permissions configured. Related variables : - db2_ldap_password : Required when this is set - db2_rotate_password : Can auto-generate password for this user - Used in generated JdbcCfg for MAS configuration Note : If not set, MAS will use the default db2inst1 user. When set, you must also provide db2_ldap_password . db2_ldap_password \u00a4 Password for the Db2 LDAP user. Optional Environment Variable: DB2_LDAP_PASSWORD Default: None Purpose : Sets the password for the LDAP user specified in db2_ldap_username . This password is configured in Db2's local LDAP registry and used for database authentication. When to use : - Required when db2_ldap_username is set - Set to a strong password meeting security requirements - Can be omitted if using db2_rotate_password for auto-generation Valid values : Strong password string meeting security requirements Impact : This password is stored in Db2's LDAP registry and used for database authentication. It's also included in the generated JdbcCfg for MAS. Related variables : - db2_ldap_username : Must be set together with this password - db2_rotate_password : Alternative to manually setting password Note : Keep this password secure and do not commit to source control. If using db2_rotate_password=true , this password will be auto-generated and you don't need to provide it. db2_rotate_password \u00a4 Enables automatic password generation and rotation for Db2 LDAP user. Optional Environment Variable: DB2_LDAP_ROTATE_PASSWORD Default: False Purpose : Automates password management by generating a new random password for the Db2 LDAP user and updating both Db2 and MAS configurations. This improves security by enabling regular password rotation. When to use : - Set to True for automated password management - Set to True for regular password rotation as a security practice - Leave as False when manually managing passwords - Useful for automated deployments where manual password management is impractical Valid values : True , False Impact : - When True : Role generates a strong random password, configures it in Db2, and updates MAS JdbcCfg - When False : You must provide db2_ldap_password manually Related variables : - db2_ldap_username : User whose password will be rotated - db2_ldap_password : Not needed when rotation is enabled Note : When enabled, the role handles all password management automatically. The generated password is stored securely in Kubernetes secrets and MAS configuration. Storage Variables \u00a4 We recommend reviewing the Db2 documentation about the certified storage options for Db2 on Red Hat OpenShift. Please ensure your storage class meets the specified deployment requirements for Db2. https://www.ibm.com/docs/en/db2/11.5?topic=storage-certified-options db2_meta_storage_class \u00a4 Storage class for Db2 metadata storage (must support ReadWriteMany). Required Environment Variable: DB2_META_STORAGE_CLASS Default: ibmc-file-gold (if available in cluster) Purpose : Specifies the storage class for Db2 metadata storage, which requires ReadWriteMany (RWX) access mode for shared access across Db2 pods. When to use : - Always required for Db2 installation - Must be a storage class supporting RWX access mode - Typically file-based storage (NFS, IBM Cloud File Storage, etc.) Valid values : Any storage class name supporting ReadWriteMany access mode Impact : Metadata storage is critical for Db2 operation. Incorrect storage class or one not supporting RWX will cause Db2 deployment to fail. Related variables : - db2_meta_storage_size : Size of metadata volume - db2_meta_storage_accessmode : Should be ReadWriteMany Note : See Db2 certified storage options for supported storage. File-based storage classes typically support RWX (e.g., ibmc-file-gold , ocs-storagecluster-cephfs ). db2_meta_storage_size \u00a4 Size of the metadata persistent volume. Optional Environment Variable: DB2_META_STORAGE_SIZE Default: 10Gi Purpose : Specifies the size of the persistent volume for Db2 metadata storage. Metadata includes system catalogs, configuration files, and other operational data. When to use : - Use default ( 10Gi ) for most deployments - Increase for large-scale deployments with many databases or complex configurations - Monitor usage and adjust if metadata volume fills up Valid values : Kubernetes storage size format (e.g., 10Gi , 20Gi , 50Gi ) Impact : Insufficient metadata storage can cause Db2 operational issues. The volume must have enough space for system catalogs and configuration data. Related variables : - db2_meta_storage_class : Storage class for this volume - db2_meta_storage_accessmode : Access mode for this volume Note : The default 10Gi is sufficient for most deployments. Metadata storage requirements are typically much smaller than data storage. db2_meta_storage_accessmode \u00a4 Access mode for metadata storage volume. Optional Environment Variable: DB2_META_STORAGE_ACCESSMODE Default: ReadWriteMany Purpose : Defines the Kubernetes access mode for the metadata persistent volume. ReadWriteMany (RWX) is required for Db2 metadata to be accessible from multiple pods. When to use : - Leave as default ( ReadWriteMany ) for standard Db2 deployments - RWX is required for Db2 high availability and proper operation Valid values : ReadWriteMany (RWX) Impact : Db2 requires RWX access mode for metadata storage. Using a different access mode will cause deployment failures. Related variables : - db2_meta_storage_class : Must support the specified access mode - db2_meta_storage_size : Size of this volume Note : Do not change from the default ReadWriteMany . Db2 metadata requires RWX access mode for proper operation. db2_data_storage_class \u00a4 Storage class for Db2 user data storage (must support ReadWriteOnce). Required Environment Variable: DB2_DATA_STORAGE_CLASS Default: ibmc-block-gold (if available in cluster) Purpose : Specifies the storage class for Db2 user data storage, which requires ReadWriteOnce (RWO) access mode. This is where database tables and user data are stored. When to use : - Always required for Db2 installation - Must be a storage class supporting RWO access mode - Typically block-based storage for performance (SAN, IBM Cloud Block Storage, etc.) Valid values : Any storage class name supporting ReadWriteOnce access mode Impact : Data storage is critical for Db2 performance and capacity. Choose high-performance storage for production workloads. Incorrect storage class or one not supporting RWO will cause Db2 deployment to fail. Related variables : - db2_data_storage_size : Size of data volume - db2_data_storage_accessmode : Should be ReadWriteOnce Note : See Db2 certified storage options for supported storage. Block-based storage classes typically provide better performance for databases (e.g., ibmc-block-gold , ocs-storagecluster-ceph-rbd ). db2_data_storage_size \u00a4 Size of the user data persistent volume. Optional Environment Variable: DB2_DATA_STORAGE_SIZE Default: 50Gi Purpose : Specifies the size of the persistent volume for Db2 user data storage. This is where database tables, indexes, and all user data are stored. When to use : - Use default ( 50Gi ) for development/test environments - Increase significantly for production based on data volume estimates - Plan for data growth over time - Monitor usage and expand as needed Valid values : Kubernetes storage size format (e.g., 50Gi , 100Gi , 500Gi , 1Ti ) Impact : Insufficient data storage will cause database operations to fail when the volume fills up. Size appropriately for your data volume plus growth. Related variables : - db2_data_storage_class : Storage class for this volume - db2_data_storage_accessmode : Access mode for this volume Note : The default 50Gi is suitable for small deployments only. Production MAS Manage deployments typically require 500Gi or more. Plan storage capacity based on expected data volume and growth. db2_data_storage_accessmode \u00a4 Access mode for user data storage volume. Optional Environment Variable: DB2_DATA_STORAGE_ACCESSMODE Default: ReadWriteOnce Purpose : Defines the Kubernetes access mode for the user data persistent volume. ReadWriteOnce (RWO) is standard for Db2 data storage as it's accessed by a single pod at a time. When to use : - Leave as default ( ReadWriteOnce ) for standard Db2 deployments - RWO is the correct mode for Db2 data storage Valid values : ReadWriteOnce (RWO) Impact : Db2 data storage uses RWO access mode. Using a different access mode may cause deployment issues or performance problems. Related variables : - db2_data_storage_class : Must support the specified access mode - db2_data_storage_size : Size of this volume Note : Do not change from the default ReadWriteOnce . Db2 data storage is designed for RWO access mode. db2_backup_storage_class \u00a4 Storage class for Db2 backup storage (must support ReadWriteMany). Optional Environment Variable: DB2_BACKUP_STORAGE_CLASS Default: ibmc-file-gold (if available in cluster) Purpose : Specifies the storage class for Db2 backup storage, which requires ReadWriteMany (RWX) access mode for shared access during backup operations. When to use : - Use default for standard backup storage configuration - Must be a storage class supporting RWX access mode - Set to None to disable backup storage (not recommended for production) - Typically file-based storage (NFS, IBM Cloud File Storage, etc.) Valid values : Any storage class name supporting ReadWriteMany access mode, or None to disable Impact : - When set: Backup storage is configured for Db2 backups - When set to None : Backup storage is dropped from Db2uCluster CR (backups not possible) Related variables : - db2_backup_storage_size : Size of backup volume - db2_backup_storage_accessmode : Should be ReadWriteMany Note : WARNING - Setting to None disables backup capability. Only do this for non-production environments. Production systems should always have backup storage configured. db2_backup_storage_size \u00a4 Size of the backup persistent volume. Optional Environment Variable: DB2_BACKUP_STORAGE_SIZE Default: 50Gi Purpose : Specifies the size of the persistent volume for Db2 backup storage. This volume stores database backups for disaster recovery and point-in-time recovery. When to use : - Use default ( 50Gi ) for small databases or development environments - Increase based on database size and backup retention requirements - Plan for multiple full backups plus transaction logs - Monitor usage and expand as database grows Valid values : Kubernetes storage size format (e.g., 50Gi , 100Gi , 500Gi , 1Ti ) Impact : Insufficient backup storage will cause backup operations to fail. Size should accommodate multiple full backups based on your retention policy. Related variables : - db2_backup_storage_class : Storage class for this volume - db2_backup_storage_accessmode : Access mode for this volume - db2_data_storage_size : Backup storage should be sized relative to data storage Note : Plan backup storage as a multiple of your data storage size. A common practice is 2-3x the data storage size to accommodate multiple full backups and transaction logs. db2_backup_storage_accessmode \u00a4 Access mode for backup storage volume. Optional Environment Variable: DB2_BACKUP_STORAGE_ACCESSMODE Default: ReadWriteMany Purpose : Defines the Kubernetes access mode for the backup persistent volume. ReadWriteMany (RWX) is required for Db2 backup operations to be accessible from multiple pods. When to use : - Leave as default ( ReadWriteMany ) for standard Db2 backup configuration - RWX is required for Db2 backup operations Valid values : ReadWriteMany (RWX) Impact : Db2 backup storage requires RWX access mode. Using a different access mode will cause backup operations to fail. Related variables : - db2_backup_storage_class : Must support the specified access mode - db2_backup_storage_size : Size of this volume Note : Do not change from the default ReadWriteMany . Db2 backup operations require RWX access mode for proper operation. - Default: ReadWriteMany db2_logs_storage_class \u00a4 Storage class used for transaction logs. This must support ReadWriteOnce(RWO) access mode. Optional Environment Variable: DB2_LOGS_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. Set to None will drop the logs storage on DB2ucluster CR. db2_logs_storage_size \u00a4 Size of transaction logs persistent volume. Optional Environment Variable: DB2_LOGS_STORAGE_SIZE Default: 10Gi db2_logs_storage_accessmode \u00a4 The access mode for the storage. Optional Environment Variable: DB2_LOGS_STORAGE_ACCESSMODE Default: ReadWriteOnce db2_temp_storage_class \u00a4 Storage class used for temporary data. This must support ReadWriteMany(RWX) access mode. Optional Environment Variable: DB2_TEMP_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. Set to None will drop the tempts storage on DB2ucluster CR. db2_temp_storage_size \u00a4 Size of temporary persistent volume. Optional Environment Variable: DB2_TEMP_STORAGE_SIZE Default: 10Gi db2_temp_storage_accessmode \u00a4 The access mode for the storage. This must support ReadWriteOnce(RWO) access mode. Optional Environment Variable: DB2_TEMP_STORAGE_ACCESSMODE Default: ReadWriteOnce Resource Request Variables \u00a4 These variables allow you to customize the resources available to the Db2 pod in your cluster. In most circumstances you will want to set these properties because it's impossible for us to provide a default value that will be appropriate for all users. We have set defaults that are suitable for deploying Db2 onto a dedicated worker node with 4cpu and 16gb memory. Tip Note that you must take into account the system overhead on any given node when setting these parameters, if you set the requests equal to the number of CPU or amount of memory on your node then the scheduler will not be able to schedule the Db2 pod because not 100% of the worker nodes' resource will be available to pod on that node, even if there's only a single pod on it. Db2 is sensitive to both CPU and memory issues, particularly memory, we recommend setting requests and limits to the same values, ensuring the scheduler always reserves the resources that Db2 expects to be available to it. db2_cpu_requests \u00a4 Define the Kubernetes CPU request for the Db2 pod. Optional Environment Variable: DB2_CPU_REQUESTS Default: 4000m db2_cpu_limits \u00a4 Define the Kubernetes CPU limit for the Db2 pod. Optional Environment Variable: DB2_CPU_LIMITS Default: 6000m db2_memory_requests \u00a4 Define the Kubernetes memory request for the Db2 pod. Optional Environment Variable: DB2_MEMORY_REQUESTS Default: 8Gi db2_memory_limits \u00a4 Define the Kubernetes memory limit for the Db2 pod. Optional Environment Variable: DB2_MEMORY_LIMITS Default: 16Gi Node Label Affinity Variables \u00a4 Specify both db2_affinity_key and db2_affinity_value to configure requiredDuringSchedulingIgnoredDuringExecution affinity with appropriately labelled nodes. db2_affinity_key \u00a4 Specify the key of a node label to declare affinity with. Optional Environment Variable: DB2_AFFINITY_KEY Default: None db2_affinity_value \u00a4 Specify the value of a node label to declare affinity with. Optional Environment Variable: DB2_AFFINITY_VALUE Default: None Node Taint Toleration Variables \u00a4 Specify db2_tolerate_key , db2_tolerate_value , and db2_tolerate_effect to configure a toleration policy to allow the db2 instance to be scheduled on nodes with the specified taint. db2_tolerate_key \u00a4 Specify the key of the taint that is to be tolerated. Optional Environment Variable: DB2_TOLERATE_KEY Default: None db2_tolerate_value \u00a4 Specify the value of the taint that is to be tolerated. Optional Environment Variable: DB2_TOLERATE_VALUE Default: None db2_tolerate_effect \u00a4 Specify the type of taint effect that will be tolerated ( NoSchedule , PreferNoSchedule , or NoExecute ). Optional Environment Variable: DB2_TOLERATE_EFFECT Default: None Role Variables - DB2UCluster Database Configuration Settings \u00a4 The following variables will overwrite DB2UCluster default properties for the DB2 configuration sections: spec.environment.database.dbConfig spec.environment.instance.dbmConfig spec.environment.instance.registry db2_database_db_config \u00a4 Overwrites the db2ucluster database configuration settings under spec.environment.database.dbConfig section. Optional Environment Variable: DB2_DATABASE_DB_CONFIG Default: None db2_instance_dbm_config \u00a4 Overwrites the db2ucluster instance database configuration settings under spec.environment.instance.dbmConfig section. Important Do not set instance_memory . The Db2 engine does not know Db2 is running inside a container, setting dbmConfig.INSTANCE_MEMORY: automatic will cause it to read the cgroups of the node and potentially go beyond the pod memory limit. Db2U has logic built in to use a normalized percentage that takes into account the memory limit and free memory of the node. Optional Environment Variable: DB2_INSTANCE_DBM_CONFIG Default: None db2_instance_registry \u00a4 Overwrites the db2ucluster instance database configuration settings under spec.environment.instance.registry section. You can define parameters to be included in this section using semicolon separated values. Optional Environment Variable: DB2_INSTANCE_REGISTRY Default: None MPP System Variables \u00a4 Warning Do not use these variables if you intend to use the Db2 instance with IBM Maximo Application Suite; no MAS application supports Db2 MPP db2_mln_count \u00a4 The number of logical nodes (i.e. database partitions to create). Note: ensure that the application using this Db2 can support Db2 MPP (which is created when DB2_MLN_COUNT is greater than 1). Optional Environment Variable: 'DB2_MLN_COUNT Default: 1 db2_num_pods \u00a4 The number of Db2 pods to create in the instance. Note that db2_num_pods must be less than or equal to db2_mln_count . A single db2u pod can contain multiple logical nodes. So be sure to avoid specifying a large number for db2_mln_count while specifying a small number for db2_num_pods . If in doubt, make db2_mln_count = db2_num_pods . For more information refer to the Db2 documentation . Optional Environment Variable: DB2_NUM_PODS Default: 1 MAS Configuration Variables \u00a4 mas_instance_id \u00a4 Providing this and mas_config_dir will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_INSTANCE_ID Default: None mas_config_dir \u00a4 Providing this and mas_instance_id will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_CONFIG_DIR Default: None mas_config_scope \u00a4 Supported values are system , ws , app , or wsapp , this is only used when both mas_config_dir and mas_instance_id are set. Optional Environment Variable: MAS_CONFIG_SCOPE Default: system mas_workspace_id \u00a4 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Optional Environment Variable: MAS_WORKSPACE_ID Default: None mas_application_id \u00a4 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either app or wsapp Optional Environment Variable: 'MAS_APP_ID Default: None Role Variables - Backup and Restore \u00a4 masbr_confirm_cluster \u00a4 Set true or false to indicate the role whether to confirm the currently connected cluster before running the backup or restore job. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false masbr_copy_timeout_sec \u00a4 Set the transfer files timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) masbr_job_timezone \u00a4 Set the time zone for creating scheduled backup job. If not set a value for this variable, this role will use UTC time zone when creating a CronJob for running scheduled backup job. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None masbr_storage_local_folder \u00a4 Set local path to save the backup files. Required Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None masbr_backup_type \u00a4 Set full or incr to indicate the role to create a full backup or incremental backup. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full masbr_backup_from_version \u00a4 Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). This variable is only valid when MASBR_BACKUP_TYPE=incr . If not set a value for this variable, this role will try to find the latest full backup version from the specified storage location. Optional Environment Variable: MASBR_BACKUP_FROM_VERSION Default: None masbr_backup_schedule \u00a4 Set Cron expression to create a scheduled backup. If not set a value for this varialbe, this role will create an on-demand backup. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None masbr_restore_from_version \u00a4 Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) Required only when DB2_ACTION=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Example Playbook \u00a4 Install Db2 \u00a4 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxxx # Configuration for the Db2 cluster db2_instance_name: db2u-db01 db2_meta_storage_class: \"ibmc-file-gold\" db2_data_storage_class: \"ibmc-block-gold\" db2_backup_storage_class: \"ibmc-file-gold\" db2_logs_storage_class: \"ibmc-block-gold\" db2_temp_storage_class: \"ibmc-block-gold\" # Create the MAS JdbcCfg & Secret resource definitions mas_instance_id: inst1 mas_config_dir: /home/david/masconfig roles: - ibm.mas_devops.db2 Backup Db2 \u00a4 - hosts: localhost any_errors_fatal: true vars: db2_action: backup db2_instance_name: db2u-db01 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.db2 Restore Db2 \u00a4 - hosts: localhost any_errors_fatal: true vars: db2_action: restore db2_instance_name: db2u-db01 masbr_restore_from_version: 20240621021316 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.db2 Run Role Playbook \u00a4 export IBM_ENTITLEMENT_KEY=xxxxx export DB2_INSTANCE_NAME=db2u-db01 export DB2_META_STORAGE_CLASS=ibmc-file-gold export DB2_DATA_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/masconfig ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"db2"},{"location":"roles/db2/#db2","text":"This role creates or upgrades a Db2 instance using the Db2u Operator. When installing db2, the db2u operator will now be installed into the same namespace as the db2 instance ( db2ucluster ). If you already have db2 operator and db2 instances running in separate namespaces, this role will take care of migrating (by deleting & reinstalling) the db2 operators from ibm-common-services to the namespace defined by db2_namespace property (in case of a new role execution for a db2 install or db2 upgrade). A private root CA certificate is created and is used to secure the TLS connections to the database. A Db2 Warehouse cluster will be created along with a public TLS encrypted route to allow external access to the cluster (access is via the ssl-server nodeport port on the -db2u-engn-svc service). Internal access is via the -db2u-engn-svc service and port 50001. Both the external route and the internal service use the same server certificate. The private root CA certificate and the server certificate are available from the db2u-ca and db2u-certificate secrets in the db2 namespace. The default user is db2inst1 and the password is available in the instancepassword secret in the same namespace. You can examine the deployed resources in the db2 namespace. This example assumes the default namespace db2u : oc -n db2u get db2ucluster NAME STATE MAINTENANCESTATE AGE db2u-db01 Ready None 29m It typically takes 20-30 minutes from the db2ucluster being created till it is ready. If the db2ucluster is not ready after that period then check that all the PersistentVolumeClaims in the db2 namespace are ready and that the pods in the namespace are not stuck in init state. If the c-<db2_instance_name>-db2u-0 pod is running then you can exec into the pod and check the /var/log/db2u.log for any issue. If the mas_instance_id and mas_config_dir are provided then the role will generate the JdbcCfg yaml that can be used to configure MAS to connect to this database. It does not apply the yaml to the cluster but does provide you with the yaml files to apply if needed. When upgrading db2, specify the existing namespace where the db2uCluster instances exist. All the instances under that namespace will be upgraded to the db2 version specified. The version of db2 must match the channel of db2 being used for the upgrade.","title":"db2"},{"location":"roles/db2/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/db2/#installation-variables","text":"","title":"Installation Variables"},{"location":"roles/db2/#common_services_namespace","text":"OpenShift namespace where IBM Common Services is installed. Optional Environment Variable: COMMON_SERVICES_NAMESPACE Default Value: ibm-common-services Purpose : Specifies the namespace containing IBM Common Services, which provides shared services used by Db2 operator. This is needed for the role to locate and interact with Common Services components. When to use : - Leave as default ( ibm-common-services ) for standard installations - Set only if Common Services is installed in a non-standard namespace - Required when Common Services namespace differs from default Valid values : Any valid Kubernetes namespace name where IBM Common Services is installed Impact : The role uses this to locate Common Services resources. Incorrect namespace will cause the role to fail when trying to access Common Services components. Related variables : Works with db2_namespace to manage operator and instance placement. Note : The default ibm-common-services is the standard namespace for Common Services. Only change if your installation uses a different namespace.","title":"common_services_namespace"},{"location":"roles/db2/#db2_action","text":"Specifies which operation to perform on the Db2 database. Optional Environment Variable: DB2_ACTION Default: install Purpose : Controls what action the role executes against Db2 instances. This allows the same role to handle installation, upgrades, backups, and restores of Db2 databases. When to use : - Use install (default) for initial Db2 deployment - Use upgrade to upgrade all Db2 instances in the namespace to a new version - Use backup to create a backup of Db2 data - Use restore to restore Db2 from a backup Valid values : install , upgrade , backup , restore Impact : - install : Creates new Db2 operator and instance - upgrade : Upgrades ALL instances in db2_namespace to db2_version (affects all instances in namespace) - backup : Creates backup of Db2 data - restore : Restores Db2 from backup Related variables : - db2_version : Required for upgrade action to specify target version - db2_namespace : All instances in this namespace are affected by upgrade Note : WARNING - When using upgrade , ALL Db2 instances in the specified namespace will be upgraded. Plan accordingly and ensure db2_version matches the operator channel.","title":"db2_action"},{"location":"roles/db2/#db2_namespace","text":"OpenShift namespace where Db2 operator and instances will be deployed. Optional Environment Variable: DB2_NAMESPACE Default: db2u Purpose : Specifies the namespace for deploying the Db2u operator and Db2 instances (Db2uCluster custom resources). Starting with recent versions, the operator is installed in the same namespace as the instances. When to use : - Use default ( db2u ) for standard single-instance deployments - Set to a custom namespace when organizing multiple Db2 deployments - Must match existing namespace when upgrading Db2 instances Valid values : Any valid Kubernetes namespace name (e.g., db2u , db2-prod , mas-db2 ) Impact : All Db2 resources (operator, instances, secrets, services) are created in this namespace. When upgrading, ALL instances in this namespace will be upgraded together. Related variables : - db2_action : When set to upgrade , affects all instances in this namespace - db2_instance_name : Instance created within this namespace Note : The role handles migration of operators from ibm-common-services to this namespace if needed. All instances in the namespace are upgraded together when using db2_action=upgrade .","title":"db2_namespace"},{"location":"roles/db2/#db2_channel","text":"Subscription channel for the Db2 Universal Operator. Optional Environment Variable: DB2_CHANNEL Default: The default channel from the operator package (automatically selected) Purpose : Specifies which operator channel to subscribe to, determining which Db2 operator version stream you receive. Channels typically correspond to major Db2 versions (e.g., v2.2 , v3.0 ). When to use : - Leave unset to use the default channel (recommended for new installations) - Set explicitly when you need a specific operator version stream - Must match db2_version when upgrading (version must be available in the channel) Valid values : Valid Db2 operator channel names (e.g., v2.2 , v3.0 , v3.1 ) Impact : Determines which operator version is installed and which Db2 engine versions are available. Changing channels may require operator migration. Related variables : - db2_version : Must be compatible with the selected channel - When upgrading, version must match channel Note : The operator channel determines available Db2 engine versions. Check the operator package for available channels and their supported versions.","title":"db2_channel"},{"location":"roles/db2/#db2_instance_name","text":"Unique name for the Db2 instance (Db2uCluster resource). Required Environment Variable: DB2_INSTANCE_NAME Default: None Purpose : Defines the name of the Db2uCluster custom resource that will be created. This name is used to identify the Db2 instance and is embedded in resource names (services, pods, secrets). When to use : - Always required for Db2 installation - Use descriptive names that indicate purpose (e.g., maximo-db , iot-db , manage-db ) - Must be unique within the namespace Valid values : Valid Kubernetes resource name (lowercase alphanumeric with hyphens, e.g., db01 , mas-db , manage-db ) Impact : This name is used throughout the Db2 deployment: - Db2uCluster resource name - Service names (e.g., {instance_name}-db2u-engn-svc ) - Pod names (e.g., c-{instance_name}-db2u-0 ) - Secret names Related variables : - db2_dbname : Name of the database within this instance - Used in generated JdbcCfg when mas_instance_id is provided Note : Choose a meaningful name as it appears in many resource names and cannot be easily changed after creation.","title":"db2_instance_name"},{"location":"roles/db2/#ibm_entitlement_key","text":"IBM Container Library entitlement key for accessing Db2 container images. Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Provides authentication credentials to pull Db2 container images from the IBM Container Registry. This key is associated with your IBM entitlement and grants access to licensed software. When to use : - Always required for Db2 installation - Obtain from IBM Container Library - Same key used across all IBM MAS components Valid values : Valid IBM entitlement key string Impact : Without a valid key, Db2 container images cannot be pulled and installation will fail. The key is stored in an image pull secret in the Db2 namespace. Related variables : Used across multiple roles for accessing IBM container images. Note : Keep this key secure and do not commit to source control. The key is tied to your IBM entitlement and should be treated as a credential. Obtain from the IBM Container Library using your IBM ID.","title":"ibm_entitlement_key"},{"location":"roles/db2/#db2_dbname","text":"Name of the database to create within the Db2 instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB Purpose : Specifies the name of the database that will be created inside the Db2 instance. A Db2 instance can contain multiple databases, and this defines the primary database name. When to use : - Use default ( BLUDB ) for standard MAS deployments - Set to a custom name when organizational standards require specific naming - Must match the database name expected by applications connecting to Db2 Valid values : Valid Db2 database name (uppercase alphanumeric, up to 8 characters, e.g., BLUDB , MAXDB , MASDB ) Impact : This database name is used in connection strings and JDBC configurations. Applications must use this exact name to connect to the database. Related variables : - db2_instance_name : The instance containing this database - Used in generated JdbcCfg for MAS configuration Note : The default BLUDB is standard for Db2 Warehouse. Db2 database names are typically uppercase and limited to 8 characters.","title":"db2_dbname"},{"location":"roles/db2/#db2_version","text":"Db2 engine version to use for installation or upgrade. Optional Environment Variable: DB2_VERSION Default: Latest version supported by the installed Db2 operator (auto-detected from db2u-release configmap) Purpose : Specifies which Db2 engine version to deploy or upgrade to. The version must be compatible with the installed operator channel. When to use : - Leave unset for new installations to use the latest supported version (recommended) - Set explicitly when you need a specific Db2 version - Required when upgrading to specify the target version - Must match the operator channel capabilities Valid values : Valid Db2 engine version supported by the operator (e.g., 11.5.8.0 , 11.5.9.0 ) Impact : Determines which Db2 engine version is deployed. When upgrading, ALL instances in db2_namespace will be upgraded to this version. Version must be available in the operator's supported versions list. Related variables : - db2_channel : Version must be compatible with the operator channel - db2_action : When set to upgrade , this version is the target - db2_namespace : All instances in namespace upgraded to this version Note : Supported versions are listed in the db2u-release configmap in ibm-common-services namespace. Ensure the version matches your operator channel. When upgrading, all instances in the namespace are upgraded together.","title":"db2_version"},{"location":"roles/db2/#db2_type","text":"Db2 instance type optimized for specific workload patterns. Optional Environment Variable: DB2_TYPE Default: db2wh Purpose : Determines the Db2 instance configuration optimized for either data warehouse (analytical) or online transaction processing workloads. This affects resource allocation and database tuning. When to use : - Use db2wh (default) for MAS Manage and analytical workloads - Use db2oltp for high-transaction workloads requiring OLTP optimization - Choose based on your primary use case Valid values : db2wh , db2oltp Impact : - db2wh : Optimized for data warehouse/analytical queries (recommended for MAS Manage) - db2oltp : Optimized for online transaction processing with high concurrency Related variables : - db2_workload : Further refines workload optimization - db2_table_org : Table organization should align with instance type Note : MAS Manage typically uses db2wh for optimal performance with its analytical workload patterns.","title":"db2_type"},{"location":"roles/db2/#db2_timezone","text":"Server timezone for the Db2 instance. Optional Environment Variable: DB2_TIMEZONE Default: GMT Purpose : Sets the timezone used by the Db2 server for timestamp operations and scheduling. This affects how Db2 interprets and stores timestamp data. When to use : - Use default ( GMT ) for globally distributed systems - Set to local timezone when all users are in the same timezone - Must match MAS Manage timezone if using Manage with this database Valid values : Valid timezone codes (e.g., GMT , EST , PST , CET , UTC ) Impact : Affects timestamp interpretation and storage in the database. Mismatched timezones between Db2 and applications can cause data inconsistencies. Related variables : - MAS_APP_SETTINGS_SERVER_TIMEZONE : Must be set to the same value for MAS Manage - Affects all timestamp operations in the database Note : IMPORTANT - If using this Db2 instance with MAS Manage, you must also set MAS_APP_SETTINGS_SERVER_TIMEZONE to the same value to ensure consistent timestamp handling.","title":"db2_timezone"},{"location":"roles/db2/#db2_4k_device_support","text":"Controls 4K sector device support in Db2. Optional Environment Variable: DB2_4K_DEVICE_SUPPORT Default: ON Purpose : Enables or disables support for storage devices with 4K sector sizes. Modern storage systems often use 4K sectors instead of traditional 512-byte sectors. When to use : - Leave as ON (default) for modern storage systems with 4K sectors - Set to OFF only if using legacy storage with 512-byte sectors - Check your storage specifications if unsure Valid values : ON , OFF Impact : - ON : Enables 4K sector support (required for most modern storage) - OFF : Uses traditional 512-byte sector mode (legacy storage only) Related variables : Works with storage class configuration to ensure compatibility. Note : Most modern storage systems use 4K sectors. Keep this ON unless you have specific legacy storage requirements.","title":"db2_4k_device_support"},{"location":"roles/db2/#db2_workload","text":"Workload profile for Db2 instance optimization. Optional Environment Variable: DB2_WORKLOAD Default: ANALYTICS Purpose : Configures Db2 with predefined settings optimized for specific workload patterns. This affects memory allocation, query optimization, and other performance parameters. When to use : - Use ANALYTICS (default) for general analytical and MAS workloads - Use PUREDATA_OLAP for pure data warehouse/OLAP workloads - Choose based on your primary query patterns Valid values : ANALYTICS , PUREDATA_OLAP Impact : - ANALYTICS : Balanced optimization for mixed analytical workloads (recommended for MAS) - PUREDATA_OLAP : Optimized specifically for OLAP/data warehouse queries Related variables : - db2_type : Should align with workload choice - db2_table_org : Table organization should match workload pattern Note : The default ANALYTICS is suitable for most MAS deployments. Only change if you have specific OLAP requirements.","title":"db2_workload"},{"location":"roles/db2/#db2_table_org","text":"Default table organization for database tables. Optional Environment Variable: DB2_TABLE_ORG Default: ROW Purpose : Determines the default storage organization for tables in the database. Row-organized tables are optimized for transactional workloads, while column-organized tables are optimized for analytical queries. When to use : - Use ROW (default) for MAS Manage and mixed workloads - Use COLUMN only for pure analytical/reporting workloads - Choose based on your primary query patterns Valid values : ROW , COLUMN Impact : - ROW : Tables stored row-by-row (better for OLTP, updates, and mixed workloads) - COLUMN : Tables stored column-by-column (better for analytical queries and aggregations) Related variables : - db2_type : Should align with table organization choice - db2_workload : Workload profile should match table organization Note : MAS Manage requires ROW organization. Only use COLUMN for dedicated analytical databases. This sets the default; individual tables can override this setting.","title":"db2_table_org"},{"location":"roles/db2/#db2_ldap_username","text":"Username for Db2 LDAP authentication. Optional Environment Variable: DB2_LDAP_USERNAME Default: None (uses default db2inst1 user) Purpose : Defines a custom LDAP username for Db2 authentication instead of the default db2inst1 user. When set, this user is configured in Db2's local LDAP registry and used in MAS JDBC configuration. When to use : - Set when you need a custom database user for MAS connections - Set when organizational policies require specific usernames - Leave unset to use the default db2inst1 user - Must be set together with db2_ldap_password Valid values : Valid LDAP username string Impact : When set, this user is created in Db2's LDAP registry and used in the generated JdbcCfg for MAS. The user will have necessary database permissions configured. Related variables : - db2_ldap_password : Required when this is set - db2_rotate_password : Can auto-generate password for this user - Used in generated JdbcCfg for MAS configuration Note : If not set, MAS will use the default db2inst1 user. When set, you must also provide db2_ldap_password .","title":"db2_ldap_username"},{"location":"roles/db2/#db2_ldap_password","text":"Password for the Db2 LDAP user. Optional Environment Variable: DB2_LDAP_PASSWORD Default: None Purpose : Sets the password for the LDAP user specified in db2_ldap_username . This password is configured in Db2's local LDAP registry and used for database authentication. When to use : - Required when db2_ldap_username is set - Set to a strong password meeting security requirements - Can be omitted if using db2_rotate_password for auto-generation Valid values : Strong password string meeting security requirements Impact : This password is stored in Db2's LDAP registry and used for database authentication. It's also included in the generated JdbcCfg for MAS. Related variables : - db2_ldap_username : Must be set together with this password - db2_rotate_password : Alternative to manually setting password Note : Keep this password secure and do not commit to source control. If using db2_rotate_password=true , this password will be auto-generated and you don't need to provide it.","title":"db2_ldap_password"},{"location":"roles/db2/#db2_rotate_password","text":"Enables automatic password generation and rotation for Db2 LDAP user. Optional Environment Variable: DB2_LDAP_ROTATE_PASSWORD Default: False Purpose : Automates password management by generating a new random password for the Db2 LDAP user and updating both Db2 and MAS configurations. This improves security by enabling regular password rotation. When to use : - Set to True for automated password management - Set to True for regular password rotation as a security practice - Leave as False when manually managing passwords - Useful for automated deployments where manual password management is impractical Valid values : True , False Impact : - When True : Role generates a strong random password, configures it in Db2, and updates MAS JdbcCfg - When False : You must provide db2_ldap_password manually Related variables : - db2_ldap_username : User whose password will be rotated - db2_ldap_password : Not needed when rotation is enabled Note : When enabled, the role handles all password management automatically. The generated password is stored securely in Kubernetes secrets and MAS configuration.","title":"db2_rotate_password"},{"location":"roles/db2/#storage-variables","text":"We recommend reviewing the Db2 documentation about the certified storage options for Db2 on Red Hat OpenShift. Please ensure your storage class meets the specified deployment requirements for Db2. https://www.ibm.com/docs/en/db2/11.5?topic=storage-certified-options","title":"Storage Variables"},{"location":"roles/db2/#db2_meta_storage_class","text":"Storage class for Db2 metadata storage (must support ReadWriteMany). Required Environment Variable: DB2_META_STORAGE_CLASS Default: ibmc-file-gold (if available in cluster) Purpose : Specifies the storage class for Db2 metadata storage, which requires ReadWriteMany (RWX) access mode for shared access across Db2 pods. When to use : - Always required for Db2 installation - Must be a storage class supporting RWX access mode - Typically file-based storage (NFS, IBM Cloud File Storage, etc.) Valid values : Any storage class name supporting ReadWriteMany access mode Impact : Metadata storage is critical for Db2 operation. Incorrect storage class or one not supporting RWX will cause Db2 deployment to fail. Related variables : - db2_meta_storage_size : Size of metadata volume - db2_meta_storage_accessmode : Should be ReadWriteMany Note : See Db2 certified storage options for supported storage. File-based storage classes typically support RWX (e.g., ibmc-file-gold , ocs-storagecluster-cephfs ).","title":"db2_meta_storage_class"},{"location":"roles/db2/#db2_meta_storage_size","text":"Size of the metadata persistent volume. Optional Environment Variable: DB2_META_STORAGE_SIZE Default: 10Gi Purpose : Specifies the size of the persistent volume for Db2 metadata storage. Metadata includes system catalogs, configuration files, and other operational data. When to use : - Use default ( 10Gi ) for most deployments - Increase for large-scale deployments with many databases or complex configurations - Monitor usage and adjust if metadata volume fills up Valid values : Kubernetes storage size format (e.g., 10Gi , 20Gi , 50Gi ) Impact : Insufficient metadata storage can cause Db2 operational issues. The volume must have enough space for system catalogs and configuration data. Related variables : - db2_meta_storage_class : Storage class for this volume - db2_meta_storage_accessmode : Access mode for this volume Note : The default 10Gi is sufficient for most deployments. Metadata storage requirements are typically much smaller than data storage.","title":"db2_meta_storage_size"},{"location":"roles/db2/#db2_meta_storage_accessmode","text":"Access mode for metadata storage volume. Optional Environment Variable: DB2_META_STORAGE_ACCESSMODE Default: ReadWriteMany Purpose : Defines the Kubernetes access mode for the metadata persistent volume. ReadWriteMany (RWX) is required for Db2 metadata to be accessible from multiple pods. When to use : - Leave as default ( ReadWriteMany ) for standard Db2 deployments - RWX is required for Db2 high availability and proper operation Valid values : ReadWriteMany (RWX) Impact : Db2 requires RWX access mode for metadata storage. Using a different access mode will cause deployment failures. Related variables : - db2_meta_storage_class : Must support the specified access mode - db2_meta_storage_size : Size of this volume Note : Do not change from the default ReadWriteMany . Db2 metadata requires RWX access mode for proper operation.","title":"db2_meta_storage_accessmode"},{"location":"roles/db2/#db2_data_storage_class","text":"Storage class for Db2 user data storage (must support ReadWriteOnce). Required Environment Variable: DB2_DATA_STORAGE_CLASS Default: ibmc-block-gold (if available in cluster) Purpose : Specifies the storage class for Db2 user data storage, which requires ReadWriteOnce (RWO) access mode. This is where database tables and user data are stored. When to use : - Always required for Db2 installation - Must be a storage class supporting RWO access mode - Typically block-based storage for performance (SAN, IBM Cloud Block Storage, etc.) Valid values : Any storage class name supporting ReadWriteOnce access mode Impact : Data storage is critical for Db2 performance and capacity. Choose high-performance storage for production workloads. Incorrect storage class or one not supporting RWO will cause Db2 deployment to fail. Related variables : - db2_data_storage_size : Size of data volume - db2_data_storage_accessmode : Should be ReadWriteOnce Note : See Db2 certified storage options for supported storage. Block-based storage classes typically provide better performance for databases (e.g., ibmc-block-gold , ocs-storagecluster-ceph-rbd ).","title":"db2_data_storage_class"},{"location":"roles/db2/#db2_data_storage_size","text":"Size of the user data persistent volume. Optional Environment Variable: DB2_DATA_STORAGE_SIZE Default: 50Gi Purpose : Specifies the size of the persistent volume for Db2 user data storage. This is where database tables, indexes, and all user data are stored. When to use : - Use default ( 50Gi ) for development/test environments - Increase significantly for production based on data volume estimates - Plan for data growth over time - Monitor usage and expand as needed Valid values : Kubernetes storage size format (e.g., 50Gi , 100Gi , 500Gi , 1Ti ) Impact : Insufficient data storage will cause database operations to fail when the volume fills up. Size appropriately for your data volume plus growth. Related variables : - db2_data_storage_class : Storage class for this volume - db2_data_storage_accessmode : Access mode for this volume Note : The default 50Gi is suitable for small deployments only. Production MAS Manage deployments typically require 500Gi or more. Plan storage capacity based on expected data volume and growth.","title":"db2_data_storage_size"},{"location":"roles/db2/#db2_data_storage_accessmode","text":"Access mode for user data storage volume. Optional Environment Variable: DB2_DATA_STORAGE_ACCESSMODE Default: ReadWriteOnce Purpose : Defines the Kubernetes access mode for the user data persistent volume. ReadWriteOnce (RWO) is standard for Db2 data storage as it's accessed by a single pod at a time. When to use : - Leave as default ( ReadWriteOnce ) for standard Db2 deployments - RWO is the correct mode for Db2 data storage Valid values : ReadWriteOnce (RWO) Impact : Db2 data storage uses RWO access mode. Using a different access mode may cause deployment issues or performance problems. Related variables : - db2_data_storage_class : Must support the specified access mode - db2_data_storage_size : Size of this volume Note : Do not change from the default ReadWriteOnce . Db2 data storage is designed for RWO access mode.","title":"db2_data_storage_accessmode"},{"location":"roles/db2/#db2_backup_storage_class","text":"Storage class for Db2 backup storage (must support ReadWriteMany). Optional Environment Variable: DB2_BACKUP_STORAGE_CLASS Default: ibmc-file-gold (if available in cluster) Purpose : Specifies the storage class for Db2 backup storage, which requires ReadWriteMany (RWX) access mode for shared access during backup operations. When to use : - Use default for standard backup storage configuration - Must be a storage class supporting RWX access mode - Set to None to disable backup storage (not recommended for production) - Typically file-based storage (NFS, IBM Cloud File Storage, etc.) Valid values : Any storage class name supporting ReadWriteMany access mode, or None to disable Impact : - When set: Backup storage is configured for Db2 backups - When set to None : Backup storage is dropped from Db2uCluster CR (backups not possible) Related variables : - db2_backup_storage_size : Size of backup volume - db2_backup_storage_accessmode : Should be ReadWriteMany Note : WARNING - Setting to None disables backup capability. Only do this for non-production environments. Production systems should always have backup storage configured.","title":"db2_backup_storage_class"},{"location":"roles/db2/#db2_backup_storage_size","text":"Size of the backup persistent volume. Optional Environment Variable: DB2_BACKUP_STORAGE_SIZE Default: 50Gi Purpose : Specifies the size of the persistent volume for Db2 backup storage. This volume stores database backups for disaster recovery and point-in-time recovery. When to use : - Use default ( 50Gi ) for small databases or development environments - Increase based on database size and backup retention requirements - Plan for multiple full backups plus transaction logs - Monitor usage and expand as database grows Valid values : Kubernetes storage size format (e.g., 50Gi , 100Gi , 500Gi , 1Ti ) Impact : Insufficient backup storage will cause backup operations to fail. Size should accommodate multiple full backups based on your retention policy. Related variables : - db2_backup_storage_class : Storage class for this volume - db2_backup_storage_accessmode : Access mode for this volume - db2_data_storage_size : Backup storage should be sized relative to data storage Note : Plan backup storage as a multiple of your data storage size. A common practice is 2-3x the data storage size to accommodate multiple full backups and transaction logs.","title":"db2_backup_storage_size"},{"location":"roles/db2/#db2_backup_storage_accessmode","text":"Access mode for backup storage volume. Optional Environment Variable: DB2_BACKUP_STORAGE_ACCESSMODE Default: ReadWriteMany Purpose : Defines the Kubernetes access mode for the backup persistent volume. ReadWriteMany (RWX) is required for Db2 backup operations to be accessible from multiple pods. When to use : - Leave as default ( ReadWriteMany ) for standard Db2 backup configuration - RWX is required for Db2 backup operations Valid values : ReadWriteMany (RWX) Impact : Db2 backup storage requires RWX access mode. Using a different access mode will cause backup operations to fail. Related variables : - db2_backup_storage_class : Must support the specified access mode - db2_backup_storage_size : Size of this volume Note : Do not change from the default ReadWriteMany . Db2 backup operations require RWX access mode for proper operation. - Default: ReadWriteMany","title":"db2_backup_storage_accessmode"},{"location":"roles/db2/#db2_logs_storage_class","text":"Storage class used for transaction logs. This must support ReadWriteOnce(RWO) access mode. Optional Environment Variable: DB2_LOGS_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. Set to None will drop the logs storage on DB2ucluster CR.","title":"db2_logs_storage_class"},{"location":"roles/db2/#db2_logs_storage_size","text":"Size of transaction logs persistent volume. Optional Environment Variable: DB2_LOGS_STORAGE_SIZE Default: 10Gi","title":"db2_logs_storage_size"},{"location":"roles/db2/#db2_logs_storage_accessmode","text":"The access mode for the storage. Optional Environment Variable: DB2_LOGS_STORAGE_ACCESSMODE Default: ReadWriteOnce","title":"db2_logs_storage_accessmode"},{"location":"roles/db2/#db2_temp_storage_class","text":"Storage class used for temporary data. This must support ReadWriteMany(RWX) access mode. Optional Environment Variable: DB2_TEMP_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. Set to None will drop the tempts storage on DB2ucluster CR.","title":"db2_temp_storage_class"},{"location":"roles/db2/#db2_temp_storage_size","text":"Size of temporary persistent volume. Optional Environment Variable: DB2_TEMP_STORAGE_SIZE Default: 10Gi","title":"db2_temp_storage_size"},{"location":"roles/db2/#db2_temp_storage_accessmode","text":"The access mode for the storage. This must support ReadWriteOnce(RWO) access mode. Optional Environment Variable: DB2_TEMP_STORAGE_ACCESSMODE Default: ReadWriteOnce","title":"db2_temp_storage_accessmode"},{"location":"roles/db2/#resource-request-variables","text":"These variables allow you to customize the resources available to the Db2 pod in your cluster. In most circumstances you will want to set these properties because it's impossible for us to provide a default value that will be appropriate for all users. We have set defaults that are suitable for deploying Db2 onto a dedicated worker node with 4cpu and 16gb memory. Tip Note that you must take into account the system overhead on any given node when setting these parameters, if you set the requests equal to the number of CPU or amount of memory on your node then the scheduler will not be able to schedule the Db2 pod because not 100% of the worker nodes' resource will be available to pod on that node, even if there's only a single pod on it. Db2 is sensitive to both CPU and memory issues, particularly memory, we recommend setting requests and limits to the same values, ensuring the scheduler always reserves the resources that Db2 expects to be available to it.","title":"Resource Request Variables"},{"location":"roles/db2/#db2_cpu_requests","text":"Define the Kubernetes CPU request for the Db2 pod. Optional Environment Variable: DB2_CPU_REQUESTS Default: 4000m","title":"db2_cpu_requests"},{"location":"roles/db2/#db2_cpu_limits","text":"Define the Kubernetes CPU limit for the Db2 pod. Optional Environment Variable: DB2_CPU_LIMITS Default: 6000m","title":"db2_cpu_limits"},{"location":"roles/db2/#db2_memory_requests","text":"Define the Kubernetes memory request for the Db2 pod. Optional Environment Variable: DB2_MEMORY_REQUESTS Default: 8Gi","title":"db2_memory_requests"},{"location":"roles/db2/#db2_memory_limits","text":"Define the Kubernetes memory limit for the Db2 pod. Optional Environment Variable: DB2_MEMORY_LIMITS Default: 16Gi","title":"db2_memory_limits"},{"location":"roles/db2/#node-label-affinity-variables","text":"Specify both db2_affinity_key and db2_affinity_value to configure requiredDuringSchedulingIgnoredDuringExecution affinity with appropriately labelled nodes.","title":"Node Label Affinity Variables"},{"location":"roles/db2/#db2_affinity_key","text":"Specify the key of a node label to declare affinity with. Optional Environment Variable: DB2_AFFINITY_KEY Default: None","title":"db2_affinity_key"},{"location":"roles/db2/#db2_affinity_value","text":"Specify the value of a node label to declare affinity with. Optional Environment Variable: DB2_AFFINITY_VALUE Default: None","title":"db2_affinity_value"},{"location":"roles/db2/#node-taint-toleration-variables","text":"Specify db2_tolerate_key , db2_tolerate_value , and db2_tolerate_effect to configure a toleration policy to allow the db2 instance to be scheduled on nodes with the specified taint.","title":"Node Taint Toleration Variables"},{"location":"roles/db2/#db2_tolerate_key","text":"Specify the key of the taint that is to be tolerated. Optional Environment Variable: DB2_TOLERATE_KEY Default: None","title":"db2_tolerate_key"},{"location":"roles/db2/#db2_tolerate_value","text":"Specify the value of the taint that is to be tolerated. Optional Environment Variable: DB2_TOLERATE_VALUE Default: None","title":"db2_tolerate_value"},{"location":"roles/db2/#db2_tolerate_effect","text":"Specify the type of taint effect that will be tolerated ( NoSchedule , PreferNoSchedule , or NoExecute ). Optional Environment Variable: DB2_TOLERATE_EFFECT Default: None","title":"db2_tolerate_effect"},{"location":"roles/db2/#role-variables-db2ucluster-database-configuration-settings","text":"The following variables will overwrite DB2UCluster default properties for the DB2 configuration sections: spec.environment.database.dbConfig spec.environment.instance.dbmConfig spec.environment.instance.registry","title":"Role Variables - DB2UCluster Database Configuration Settings"},{"location":"roles/db2/#db2_database_db_config","text":"Overwrites the db2ucluster database configuration settings under spec.environment.database.dbConfig section. Optional Environment Variable: DB2_DATABASE_DB_CONFIG Default: None","title":"db2_database_db_config"},{"location":"roles/db2/#db2_instance_dbm_config","text":"Overwrites the db2ucluster instance database configuration settings under spec.environment.instance.dbmConfig section. Important Do not set instance_memory . The Db2 engine does not know Db2 is running inside a container, setting dbmConfig.INSTANCE_MEMORY: automatic will cause it to read the cgroups of the node and potentially go beyond the pod memory limit. Db2U has logic built in to use a normalized percentage that takes into account the memory limit and free memory of the node. Optional Environment Variable: DB2_INSTANCE_DBM_CONFIG Default: None","title":"db2_instance_dbm_config"},{"location":"roles/db2/#db2_instance_registry","text":"Overwrites the db2ucluster instance database configuration settings under spec.environment.instance.registry section. You can define parameters to be included in this section using semicolon separated values. Optional Environment Variable: DB2_INSTANCE_REGISTRY Default: None","title":"db2_instance_registry"},{"location":"roles/db2/#mpp-system-variables","text":"Warning Do not use these variables if you intend to use the Db2 instance with IBM Maximo Application Suite; no MAS application supports Db2 MPP","title":"MPP System Variables"},{"location":"roles/db2/#db2_mln_count","text":"The number of logical nodes (i.e. database partitions to create). Note: ensure that the application using this Db2 can support Db2 MPP (which is created when DB2_MLN_COUNT is greater than 1). Optional Environment Variable: 'DB2_MLN_COUNT Default: 1","title":"db2_mln_count"},{"location":"roles/db2/#db2_num_pods","text":"The number of Db2 pods to create in the instance. Note that db2_num_pods must be less than or equal to db2_mln_count . A single db2u pod can contain multiple logical nodes. So be sure to avoid specifying a large number for db2_mln_count while specifying a small number for db2_num_pods . If in doubt, make db2_mln_count = db2_num_pods . For more information refer to the Db2 documentation . Optional Environment Variable: DB2_NUM_PODS Default: 1","title":"db2_num_pods"},{"location":"roles/db2/#mas-configuration-variables","text":"","title":"MAS Configuration Variables"},{"location":"roles/db2/#mas_instance_id","text":"Providing this and mas_config_dir will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/db2/#mas_config_dir","text":"Providing this and mas_instance_id will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_CONFIG_DIR Default: None","title":"mas_config_dir"},{"location":"roles/db2/#mas_config_scope","text":"Supported values are system , ws , app , or wsapp , this is only used when both mas_config_dir and mas_instance_id are set. Optional Environment Variable: MAS_CONFIG_SCOPE Default: system","title":"mas_config_scope"},{"location":"roles/db2/#mas_workspace_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Optional Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/db2/#mas_application_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either app or wsapp Optional Environment Variable: 'MAS_APP_ID Default: None","title":"mas_application_id"},{"location":"roles/db2/#role-variables-backup-and-restore","text":"","title":"Role Variables - Backup and Restore"},{"location":"roles/db2/#masbr_confirm_cluster","text":"Set true or false to indicate the role whether to confirm the currently connected cluster before running the backup or restore job. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false","title":"masbr_confirm_cluster"},{"location":"roles/db2/#masbr_copy_timeout_sec","text":"Set the transfer files timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours)","title":"masbr_copy_timeout_sec"},{"location":"roles/db2/#masbr_job_timezone","text":"Set the time zone for creating scheduled backup job. If not set a value for this variable, this role will use UTC time zone when creating a CronJob for running scheduled backup job. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None","title":"masbr_job_timezone"},{"location":"roles/db2/#masbr_storage_local_folder","text":"Set local path to save the backup files. Required Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None","title":"masbr_storage_local_folder"},{"location":"roles/db2/#masbr_backup_type","text":"Set full or incr to indicate the role to create a full backup or incremental backup. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full","title":"masbr_backup_type"},{"location":"roles/db2/#masbr_backup_from_version","text":"Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). This variable is only valid when MASBR_BACKUP_TYPE=incr . If not set a value for this variable, this role will try to find the latest full backup version from the specified storage location. Optional Environment Variable: MASBR_BACKUP_FROM_VERSION Default: None","title":"masbr_backup_from_version"},{"location":"roles/db2/#masbr_backup_schedule","text":"Set Cron expression to create a scheduled backup. If not set a value for this varialbe, this role will create an on-demand backup. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None","title":"masbr_backup_schedule"},{"location":"roles/db2/#masbr_restore_from_version","text":"Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) Required only when DB2_ACTION=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None","title":"masbr_restore_from_version"},{"location":"roles/db2/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/db2/#install-db2","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxxx # Configuration for the Db2 cluster db2_instance_name: db2u-db01 db2_meta_storage_class: \"ibmc-file-gold\" db2_data_storage_class: \"ibmc-block-gold\" db2_backup_storage_class: \"ibmc-file-gold\" db2_logs_storage_class: \"ibmc-block-gold\" db2_temp_storage_class: \"ibmc-block-gold\" # Create the MAS JdbcCfg & Secret resource definitions mas_instance_id: inst1 mas_config_dir: /home/david/masconfig roles: - ibm.mas_devops.db2","title":"Install Db2"},{"location":"roles/db2/#backup-db2","text":"- hosts: localhost any_errors_fatal: true vars: db2_action: backup db2_instance_name: db2u-db01 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.db2","title":"Backup Db2"},{"location":"roles/db2/#restore-db2","text":"- hosts: localhost any_errors_fatal: true vars: db2_action: restore db2_instance_name: db2u-db01 masbr_restore_from_version: 20240621021316 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.db2","title":"Restore Db2"},{"location":"roles/db2/#run-role-playbook","text":"export IBM_ENTITLEMENT_KEY=xxxxx export DB2_INSTANCE_NAME=db2u-db01 export DB2_META_STORAGE_CLASS=ibmc-file-gold export DB2_DATA_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/masconfig ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/db2/#license","text":"EPL-2.0","title":"License"},{"location":"roles/dro/","text":"dro \u00a4 DRO will be supported on the following MAS versions - MAS 8.10.6 + - MAS 8.11.2 + - MAS 9.0 + Installs Data Reporter Operator in the redhat-marketplace namespace. If mas_instance_id and the others associated parameters are provided then the role will also generate a configuration file that can be directly applied to IBM Maximo Application Suite. Role Variables - Installation \u00a4 Role Variables - General \u00a4 dro_action \u00a4 Action to perform with the Data Reporter Operator deployment. Optional Environment Variable: DRO_ACTION Default: install Purpose : Controls whether to install or uninstall the Data Reporter Operator. When to use : Set to uninstall when removing DRO from the cluster. Use default install for normal deployment. Valid values : - install - Deploy and configure DRO - uninstall - Remove DRO from the cluster Impact : The uninstall action removes all DRO resources from the specified namespace. Related variables : dro_namespace Notes : DRO is supported on MAS 8.10.6+, 8.11.2+, and 9.0+. dro_namespace \u00a4 Namespace where the Data Reporter Operator will be deployed. Optional Environment Variable: DRO_NAMESPACE Default: redhat-marketplace Purpose : Allows DRO installation in a custom namespace when the default redhat-marketplace namespace has restricted access. When to use : Override the default on OCP clusters where redhat* namespaces have restricted access or when organizational policies require custom namespace naming. Valid values : Any valid Kubernetes namespace name (lowercase alphanumeric and hyphens) Impact : All DRO resources will be created in this namespace. The namespace must be specified during uninstall operations. Related variables : dro_action Notes : Ensure the namespace has appropriate permissions for DRO operator deployment. ibm_entitlement_key \u00a4 IBM entitlement key for accessing container images. Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Authenticates access to IBM container registry for pulling DRO operator images. When to use : Required for all DRO installations. Obtain from IBM Container Library . Valid values : Valid IBM entitlement key string from your IBM account. Impact : Without a valid key, the DRO operator images cannot be pulled and installation will fail. Related variables : None Notes : - Keep the entitlement key secure and do not commit it to version control - The key is associated with your IBM ID and product entitlements - Verify key validity before deployment to avoid installation failures dro_storage_class \u00a4 Storage class for DRO persistent volumes. Optional (auto-detected if not provided) Environment Variable: DRO_STORAGE_CLASS Default: None (automatically determined) Purpose : Provides persistent storage for DRO data and metrics. When to use : Specify explicitly when using customized storage solutions or when automatic detection fails. The playbooks will attempt to auto-detect a suitable RWO storage class if not provided. Valid values : Any storage class name available in your cluster that supports ReadWriteOnce (RWO) access mode. Common examples: - IBM Cloud ROKS: ibmc-block-gold , ibmc-block-silver - AWS: gp2 , gp3 - Azure: managed-premium - On-premises: Depends on your storage provider Impact : Determines where DRO stores its operational data. Must support RWO access mode. Related variables : None Notes : - Verify the storage class exists: oc get storageclass - The storage class must support RWO (Read Write Once) access mode - Auto-detection works for most standard cluster configurations Role Variables - BASCfg Generation \u00a4 mas_instance_id \u00a4 MAS instance identifier for BasCfg generation. Optional Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance the DRO BasCfg configuration will target. When to use : Required when generating MAS configuration files. If not set (along with mas_config_dir ), no BasCfg template will be generated. Valid values : Valid MAS instance ID (lowercase alphanumeric, max 12 characters) Impact : Determines the target MAS instance for the generated DRO configuration. Related variables : mas_config_dir , dro_contact.email , dro_contact.first_name , dro_contact.last_name Notes : Both mas_instance_id and mas_config_dir must be set to generate BasCfg templates. mas_config_dir \u00a4 Local directory for saving generated BasCfg resource definitions. Optional Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies where to save the generated DRO BasCfg configuration file for MAS. When to use : Required when generating MAS configuration files. The generated file can be manually applied or used as input to the suite_config role. Valid values : Any valid local filesystem path with write permissions Impact : If not set (along with mas_instance_id ), no BasCfg template will be generated. Related variables : mas_instance_id Notes : Ensure the directory exists and has appropriate write permissions. dro_endpoint_url \u00a4 URL of an existing DRO instance to connect to. Optional Environment Variable: DRO_ENDPOINT_URL Default: None Purpose : Enables connection to an existing DRO deployment instead of installing a new one. When to use : When you want to configure MAS to use an already-deployed DRO instance. Obtain from the ibm-data-reporter route in the DRO namespace. Valid values : Valid HTTPS URL to the DRO endpoint (e.g., https://ibm-data-reporter.apps.cluster.domain.com ) Impact : When set, the role will not install DRO but will generate configuration to connect to the existing instance. Related variables : dro_api_key , dro_crt_path Notes : All three variables ( dro_endpoint_url , dro_api_key , dro_crt_path ) must be provided to connect to an existing DRO instance. dro_api_key \u00a4 API token for authenticating to an existing DRO instance. Optional Environment Variable: DRO_APIKEY Default: None Purpose : Provides authentication credentials for connecting to an existing DRO deployment. When to use : Required when connecting to an existing DRO instance. Obtain from the ibm-data-reporter-operator-api-token secret in the DRO namespace. Valid values : Valid DRO API token string Impact : Without a valid API key, MAS cannot authenticate to the existing DRO instance. Related variables : dro_endpoint_url , dro_crt_path Notes : - Extract from secret: oc get secret ibm-data-reporter-operator-api-token -n <dro-namespace> -o jsonpath='{.data.token}' | base64 -d - Keep the API key secure dro_crt_path \u00a4 Path to DRO certificate file for TLS verification. Optional Environment Variable: DRO_CERTIFICATE_PATH Default: None Purpose : Provides the certificate for secure TLS communication with an existing DRO instance. When to use : Required when connecting to an existing DRO instance. DRO uses default OCP cluster ingress certificates. Valid values : Valid filesystem path to a .pem certificate file Impact : Without the certificate, TLS verification will fail when connecting to the existing DRO instance. Related variables : dro_endpoint_url , dro_api_key Notes : - Obtain certificate from router-certs-default secret in openshift-ingress namespace or trustedCA configmap in openshift-config namespace - Extract and save tls.crt contents to a .pem file - Example: oc get secret router-certs-default -n openshift-ingress -o jsonpath='{.data.tls\\.crt}' | base64 -d > /tmp/dro-cert.pem dro_contact.email \u00a4 Contact email address for DRO configuration. Required when generating BasCfg (when mas_instance_id and mas_config_dir are set) Environment Variable: DRO_CONTACT_EMAIL Default: None Purpose : Provides contact information for the DRO deployment in MAS configuration. When to use : Required for BasCfg generation. Used for administrative and support purposes. Valid values : Valid email address format Impact : This email will be associated with the DRO configuration in MAS. Related variables : dro_contact.first_name , dro_contact.last_name , mas_instance_id , mas_config_dir Notes : Use a monitored email address for receiving DRO-related notifications. dro_contact.first_name \u00a4 Contact first name for DRO configuration. Required when generating BasCfg (when mas_instance_id and mas_config_dir are set) Environment Variable: DRO_CONTACT_FIRSTNAME Default: None Purpose : Provides contact information for the DRO deployment in MAS configuration. When to use : Required for BasCfg generation along with email and last name. Valid values : Any string representing a first name Impact : This name will be associated with the DRO configuration in MAS. Related variables : dro_contact.email , dro_contact.last_name Notes : Used for administrative identification purposes. dro_contact.last_name \u00a4 Contact last name for DRO configuration. Required when generating BasCfg (when mas_instance_id and mas_config_dir are set) Environment Variable: DRO_CONTACT_LASTNAME Default: None Purpose : Provides contact information for the DRO deployment in MAS configuration. When to use : Required for BasCfg generation along with email and first name. Valid values : Any string representing a last name Impact : This name will be associated with the DRO configuration in MAS. Related variables : dro_contact.email , dro_contact.first_name Notes : Used for administrative identification purposes. custom_labels \u00a4 Custom labels to apply to DRO instance resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Enables tagging of DRO resources with custom metadata for organization, tracking, or automation purposes. When to use : When you need to apply organizational labels for cost tracking, environment identification, or resource management. Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,cost-center=12345 ) Impact : Labels are applied to instance-specific DRO resources for identification and filtering. Related variables : None Notes : - Labels must follow Kubernetes label syntax (alphanumeric, hyphens, underscores, dots) - Useful for cost allocation, resource queries, and automation scripts mas_pod_templates_dir \u00a4 Directory containing pod template configurations for DRO. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Allows customization of DRO pod specifications through template files. When to use : When you need to customize resource limits, node selectors, tolerations, or other pod-level configurations for DRO. Valid values : Valid filesystem path to a directory containing ibm-mas-bascfg.yml Impact : The pod template configuration will be inserted into the BasCfg spec under the podTemplates element, customizing DRO workload behavior. Related variables : None Notes : - The configuration file must be named ibm-mas-bascfg.yml - Content should be a YAML block for the podTemplates element - See BestEfforts reference configuration for examples - Full documentation: Customizing Pod Templates include_cluster_ingress_cert_chain \u00a4 Include complete certificate chain in MAS configuration. Optional Environment Variable: INCLUDE_CLUSTER_INGRESS_CERT_CHAIN Default: False Purpose : Controls whether to include the full certificate chain from the cluster ingress in the generated MAS configuration. When to use : Set to True when your cluster uses a trusted certificate authority and you need the complete certificate chain for proper TLS validation. Valid values : True or False Impact : When enabled, the complete certificate chain is included in the BasCfg, ensuring proper TLS trust validation for DRO connections. Related variables : dro_crt_path Notes : Only applicable when a trusted CA is found in your cluster's ingress configuration. Example Playbook \u00a4 Install in-cluster and generate MAS configuration \u00a4 To install DRO export IBM_ENTITLEMENT_KEY=<valid ibm entitlement key> export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export DRO_ACTION=install export MAS_CONFIG_DIR=<valid local path to the config folder> export MAS_INSTANCE_ID=<valid mas instance id> export DRO_STORAGE_CLASS=<valid storage class name> export ROLE_NAME='dro' export DRO_NAMESPACE=ibm-dro ansible-playbook playbooks/run_role.yml To connect to an existing DRO export DRO_ENDPOINT_URL=<valid DRO url> export DRO_APIKEY=<valid DRO apikey> export DRO_CERTIFICATE_PATH=/temp/cert.pem export IBM_ENTITLEMENT_KEY=<valid ibm entitlement key> export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export MAS_CONFIG_DIR=<valid local path to the config folder> export MAS_INSTANCE_ID=<valid mas instance id> export DRO_ACTION=install export ROLE_NAME='dro' ansible-playbook playbooks/run_role.yml To uninstall DRO export DRO_ACTION=uninstall export ROLE_NAME='dro' export DRO_NAMESPACE=ibm-dro ansible-playbook playbooks/run_role.yml - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig dro_contact: email: 'john@email.com' first_name: 'john' last_name: 'winter' roles: - ibm.mas_devops.dro License \u00a4 EPL-2.0","title":"dro"},{"location":"roles/dro/#dro","text":"DRO will be supported on the following MAS versions - MAS 8.10.6 + - MAS 8.11.2 + - MAS 9.0 + Installs Data Reporter Operator in the redhat-marketplace namespace. If mas_instance_id and the others associated parameters are provided then the role will also generate a configuration file that can be directly applied to IBM Maximo Application Suite.","title":"dro"},{"location":"roles/dro/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/dro/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/dro/#dro_action","text":"Action to perform with the Data Reporter Operator deployment. Optional Environment Variable: DRO_ACTION Default: install Purpose : Controls whether to install or uninstall the Data Reporter Operator. When to use : Set to uninstall when removing DRO from the cluster. Use default install for normal deployment. Valid values : - install - Deploy and configure DRO - uninstall - Remove DRO from the cluster Impact : The uninstall action removes all DRO resources from the specified namespace. Related variables : dro_namespace Notes : DRO is supported on MAS 8.10.6+, 8.11.2+, and 9.0+.","title":"dro_action"},{"location":"roles/dro/#dro_namespace","text":"Namespace where the Data Reporter Operator will be deployed. Optional Environment Variable: DRO_NAMESPACE Default: redhat-marketplace Purpose : Allows DRO installation in a custom namespace when the default redhat-marketplace namespace has restricted access. When to use : Override the default on OCP clusters where redhat* namespaces have restricted access or when organizational policies require custom namespace naming. Valid values : Any valid Kubernetes namespace name (lowercase alphanumeric and hyphens) Impact : All DRO resources will be created in this namespace. The namespace must be specified during uninstall operations. Related variables : dro_action Notes : Ensure the namespace has appropriate permissions for DRO operator deployment.","title":"dro_namespace"},{"location":"roles/dro/#ibm_entitlement_key","text":"IBM entitlement key for accessing container images. Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Authenticates access to IBM container registry for pulling DRO operator images. When to use : Required for all DRO installations. Obtain from IBM Container Library . Valid values : Valid IBM entitlement key string from your IBM account. Impact : Without a valid key, the DRO operator images cannot be pulled and installation will fail. Related variables : None Notes : - Keep the entitlement key secure and do not commit it to version control - The key is associated with your IBM ID and product entitlements - Verify key validity before deployment to avoid installation failures","title":"ibm_entitlement_key"},{"location":"roles/dro/#dro_storage_class","text":"Storage class for DRO persistent volumes. Optional (auto-detected if not provided) Environment Variable: DRO_STORAGE_CLASS Default: None (automatically determined) Purpose : Provides persistent storage for DRO data and metrics. When to use : Specify explicitly when using customized storage solutions or when automatic detection fails. The playbooks will attempt to auto-detect a suitable RWO storage class if not provided. Valid values : Any storage class name available in your cluster that supports ReadWriteOnce (RWO) access mode. Common examples: - IBM Cloud ROKS: ibmc-block-gold , ibmc-block-silver - AWS: gp2 , gp3 - Azure: managed-premium - On-premises: Depends on your storage provider Impact : Determines where DRO stores its operational data. Must support RWO access mode. Related variables : None Notes : - Verify the storage class exists: oc get storageclass - The storage class must support RWO (Read Write Once) access mode - Auto-detection works for most standard cluster configurations","title":"dro_storage_class"},{"location":"roles/dro/#role-variables-bascfg-generation","text":"","title":"Role Variables - BASCfg Generation"},{"location":"roles/dro/#mas_instance_id","text":"MAS instance identifier for BasCfg generation. Optional Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance the DRO BasCfg configuration will target. When to use : Required when generating MAS configuration files. If not set (along with mas_config_dir ), no BasCfg template will be generated. Valid values : Valid MAS instance ID (lowercase alphanumeric, max 12 characters) Impact : Determines the target MAS instance for the generated DRO configuration. Related variables : mas_config_dir , dro_contact.email , dro_contact.first_name , dro_contact.last_name Notes : Both mas_instance_id and mas_config_dir must be set to generate BasCfg templates.","title":"mas_instance_id"},{"location":"roles/dro/#mas_config_dir","text":"Local directory for saving generated BasCfg resource definitions. Optional Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies where to save the generated DRO BasCfg configuration file for MAS. When to use : Required when generating MAS configuration files. The generated file can be manually applied or used as input to the suite_config role. Valid values : Any valid local filesystem path with write permissions Impact : If not set (along with mas_instance_id ), no BasCfg template will be generated. Related variables : mas_instance_id Notes : Ensure the directory exists and has appropriate write permissions.","title":"mas_config_dir"},{"location":"roles/dro/#dro_endpoint_url","text":"URL of an existing DRO instance to connect to. Optional Environment Variable: DRO_ENDPOINT_URL Default: None Purpose : Enables connection to an existing DRO deployment instead of installing a new one. When to use : When you want to configure MAS to use an already-deployed DRO instance. Obtain from the ibm-data-reporter route in the DRO namespace. Valid values : Valid HTTPS URL to the DRO endpoint (e.g., https://ibm-data-reporter.apps.cluster.domain.com ) Impact : When set, the role will not install DRO but will generate configuration to connect to the existing instance. Related variables : dro_api_key , dro_crt_path Notes : All three variables ( dro_endpoint_url , dro_api_key , dro_crt_path ) must be provided to connect to an existing DRO instance.","title":"dro_endpoint_url"},{"location":"roles/dro/#dro_api_key","text":"API token for authenticating to an existing DRO instance. Optional Environment Variable: DRO_APIKEY Default: None Purpose : Provides authentication credentials for connecting to an existing DRO deployment. When to use : Required when connecting to an existing DRO instance. Obtain from the ibm-data-reporter-operator-api-token secret in the DRO namespace. Valid values : Valid DRO API token string Impact : Without a valid API key, MAS cannot authenticate to the existing DRO instance. Related variables : dro_endpoint_url , dro_crt_path Notes : - Extract from secret: oc get secret ibm-data-reporter-operator-api-token -n <dro-namespace> -o jsonpath='{.data.token}' | base64 -d - Keep the API key secure","title":"dro_api_key"},{"location":"roles/dro/#dro_crt_path","text":"Path to DRO certificate file for TLS verification. Optional Environment Variable: DRO_CERTIFICATE_PATH Default: None Purpose : Provides the certificate for secure TLS communication with an existing DRO instance. When to use : Required when connecting to an existing DRO instance. DRO uses default OCP cluster ingress certificates. Valid values : Valid filesystem path to a .pem certificate file Impact : Without the certificate, TLS verification will fail when connecting to the existing DRO instance. Related variables : dro_endpoint_url , dro_api_key Notes : - Obtain certificate from router-certs-default secret in openshift-ingress namespace or trustedCA configmap in openshift-config namespace - Extract and save tls.crt contents to a .pem file - Example: oc get secret router-certs-default -n openshift-ingress -o jsonpath='{.data.tls\\.crt}' | base64 -d > /tmp/dro-cert.pem","title":"dro_crt_path"},{"location":"roles/dro/#dro_contactemail","text":"Contact email address for DRO configuration. Required when generating BasCfg (when mas_instance_id and mas_config_dir are set) Environment Variable: DRO_CONTACT_EMAIL Default: None Purpose : Provides contact information for the DRO deployment in MAS configuration. When to use : Required for BasCfg generation. Used for administrative and support purposes. Valid values : Valid email address format Impact : This email will be associated with the DRO configuration in MAS. Related variables : dro_contact.first_name , dro_contact.last_name , mas_instance_id , mas_config_dir Notes : Use a monitored email address for receiving DRO-related notifications.","title":"dro_contact.email"},{"location":"roles/dro/#dro_contactfirst_name","text":"Contact first name for DRO configuration. Required when generating BasCfg (when mas_instance_id and mas_config_dir are set) Environment Variable: DRO_CONTACT_FIRSTNAME Default: None Purpose : Provides contact information for the DRO deployment in MAS configuration. When to use : Required for BasCfg generation along with email and last name. Valid values : Any string representing a first name Impact : This name will be associated with the DRO configuration in MAS. Related variables : dro_contact.email , dro_contact.last_name Notes : Used for administrative identification purposes.","title":"dro_contact.first_name"},{"location":"roles/dro/#dro_contactlast_name","text":"Contact last name for DRO configuration. Required when generating BasCfg (when mas_instance_id and mas_config_dir are set) Environment Variable: DRO_CONTACT_LASTNAME Default: None Purpose : Provides contact information for the DRO deployment in MAS configuration. When to use : Required for BasCfg generation along with email and first name. Valid values : Any string representing a last name Impact : This name will be associated with the DRO configuration in MAS. Related variables : dro_contact.email , dro_contact.first_name Notes : Used for administrative identification purposes.","title":"dro_contact.last_name"},{"location":"roles/dro/#custom_labels","text":"Custom labels to apply to DRO instance resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Enables tagging of DRO resources with custom metadata for organization, tracking, or automation purposes. When to use : When you need to apply organizational labels for cost tracking, environment identification, or resource management. Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,cost-center=12345 ) Impact : Labels are applied to instance-specific DRO resources for identification and filtering. Related variables : None Notes : - Labels must follow Kubernetes label syntax (alphanumeric, hyphens, underscores, dots) - Useful for cost allocation, resource queries, and automation scripts","title":"custom_labels"},{"location":"roles/dro/#mas_pod_templates_dir","text":"Directory containing pod template configurations for DRO. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Allows customization of DRO pod specifications through template files. When to use : When you need to customize resource limits, node selectors, tolerations, or other pod-level configurations for DRO. Valid values : Valid filesystem path to a directory containing ibm-mas-bascfg.yml Impact : The pod template configuration will be inserted into the BasCfg spec under the podTemplates element, customizing DRO workload behavior. Related variables : None Notes : - The configuration file must be named ibm-mas-bascfg.yml - Content should be a YAML block for the podTemplates element - See BestEfforts reference configuration for examples - Full documentation: Customizing Pod Templates","title":"mas_pod_templates_dir"},{"location":"roles/dro/#include_cluster_ingress_cert_chain","text":"Include complete certificate chain in MAS configuration. Optional Environment Variable: INCLUDE_CLUSTER_INGRESS_CERT_CHAIN Default: False Purpose : Controls whether to include the full certificate chain from the cluster ingress in the generated MAS configuration. When to use : Set to True when your cluster uses a trusted certificate authority and you need the complete certificate chain for proper TLS validation. Valid values : True or False Impact : When enabled, the complete certificate chain is included in the BasCfg, ensuring proper TLS trust validation for DRO connections. Related variables : dro_crt_path Notes : Only applicable when a trusted CA is found in your cluster's ingress configuration.","title":"include_cluster_ingress_cert_chain"},{"location":"roles/dro/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/dro/#install-in-cluster-and-generate-mas-configuration","text":"To install DRO export IBM_ENTITLEMENT_KEY=<valid ibm entitlement key> export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export DRO_ACTION=install export MAS_CONFIG_DIR=<valid local path to the config folder> export MAS_INSTANCE_ID=<valid mas instance id> export DRO_STORAGE_CLASS=<valid storage class name> export ROLE_NAME='dro' export DRO_NAMESPACE=ibm-dro ansible-playbook playbooks/run_role.yml To connect to an existing DRO export DRO_ENDPOINT_URL=<valid DRO url> export DRO_APIKEY=<valid DRO apikey> export DRO_CERTIFICATE_PATH=/temp/cert.pem export IBM_ENTITLEMENT_KEY=<valid ibm entitlement key> export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export MAS_CONFIG_DIR=<valid local path to the config folder> export MAS_INSTANCE_ID=<valid mas instance id> export DRO_ACTION=install export ROLE_NAME='dro' ansible-playbook playbooks/run_role.yml To uninstall DRO export DRO_ACTION=uninstall export ROLE_NAME='dro' export DRO_NAMESPACE=ibm-dro ansible-playbook playbooks/run_role.yml - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig dro_contact: email: 'john@email.com' first_name: 'john' last_name: 'winter' roles: - ibm.mas_devops.dro","title":"Install in-cluster and generate MAS configuration"},{"location":"roles/dro/#license","text":"EPL-2.0","title":"License"},{"location":"roles/eck/","text":"eck \u00a4 This role provides support to install Elastic Cloud on Kubernetes (ECK). Elasticsearch is configured with a default user named elastic , you can obtain the password for this user by running the following command: oc -n eck get secret mas-es-elastic-user -o go-template='{{.data.elastic | base64decode}}'; echo Role Variables - General \u00a4 eck_action \u00a4 Action to perform on ECK installation. Optional Environment Variable: ECK_ACTION Default: install Purpose : Specifies the action to perform on the Elastic Cloud on Kubernetes (ECK) deployment. When to use : - Use install (default and only supported value) to deploy ECK - Future versions may support additional actions Valid values : install Impact : Determines the operation performed on ECK. Currently only installation is supported. Related variables : - eck_enable_elasticsearch : Enable Elasticsearch component - eck_enable_kibana : Enable Kibana component - eck_enable_logstash : Enable Logstash component - eck_enable_filebeat : Enable Filebeat component Note : This role installs ECK operator and optionally deploys Elasticsearch, Kibana, Logstash, and Filebeat components based on enable flags. eck_enable_elasticsearch \u00a4 Enable Elasticsearch deployment. Optional Environment Variable: ECK_ENABLE_ELASTICSEARCH Default: false Purpose : Controls whether Elasticsearch is deployed as part of the ECK installation. When to use : - Set to true to deploy Elasticsearch for log storage and search - Leave as false if using external Elasticsearch or not needed - Required for Kibana and Logstash functionality Valid values : true , false Impact : - true : Deploys Elasticsearch cluster in ECK namespace - false : Skips Elasticsearch deployment Related variables : - eck_enable_kibana : Kibana requires Elasticsearch - eck_enable_logstash : Logstash can send to local or remote Elasticsearch - es_domain : Custom domain for Elasticsearch access Note : Default user elastic is created. Retrieve password with: oc -n eck get secret mas-es-elastic-user -o go-template='{{.data.elastic | base64decode}}' eck_enable_kibana \u00a4 Enable Kibana deployment. Optional Environment Variable: ECK_ENABLE_KIBANA Default: false Purpose : Controls whether Kibana is deployed as part of the ECK installation for log visualization and analysis. When to use : - Set to true to deploy Kibana for log visualization - Leave as false if not using Kibana UI - Requires Elasticsearch to be enabled Valid values : true , false Impact : - true : Deploys Kibana instance connected to Elasticsearch - false : Skips Kibana deployment Related variables : - eck_enable_elasticsearch : Must be true for Kibana to function - kibana_domain : Custom domain for Kibana access - letsencrypt_email : For LetsEncrypt certificate Note : Kibana requires Elasticsearch. Ensure eck_enable_elasticsearch=true when enabling Kibana. eck_enable_logstash \u00a4 Enable Logstash deployment. Optional Environment Variable: ECK_ENABLE_LOGSTASH Default: false Purpose : Controls whether Logstash is deployed as part of the ECK installation for log processing and forwarding. When to use : - Set to true to deploy Logstash for log processing - Leave as false if not using Logstash pipeline - Can send logs to local or remote Elasticsearch Valid values : true , false Impact : - true : Deploys Logstash instance for log processing - false : Skips Logstash deployment Related variables : - eck_remote_es_hosts : Remote Elasticsearch hosts for log forwarding - eck_remote_es_username : Username for remote Elasticsearch - eck_remote_es_password : Password for remote Elasticsearch Note : When remote Elasticsearch variables are set, Logstash forwards logs to the remote instance. Otherwise, logs are sent to local Elasticsearch (if enabled). eck_enable_filebeat \u00a4 Enable Filebeat deployment. Optional Environment Variable: ECK_ENABLE_FILEBEAT Default: false Purpose : Controls whether Filebeat is deployed as part of the ECK installation for log collection from cluster nodes. When to use : - Set to true to deploy Filebeat for log collection - Leave as false if not collecting node logs - Filebeat collects logs from Kubernetes nodes Valid values : true , false Impact : - true : Deploys Filebeat DaemonSet for log collection - false : Skips Filebeat deployment Related variables : - eck_enable_elasticsearch : Filebeat sends logs to Elasticsearch - eck_enable_logstash : Alternative log processing pipeline Note : Filebeat runs as a DaemonSet on cluster nodes to collect logs and forward them to Elasticsearch or Logstash. Role Variables - Remote Elasticsearch \u00a4 When eck_remote_es_hosts , eck_remote_es_username , and eck_remote_es_password are all set, and eck_enable_logstash is true , the Logstash server will be configured to send log messages to the remote Elasticsearch instance defined. eck_remote_es_hosts \u00a4 Remote Elasticsearch host list. Optional (required for remote Elasticsearch) Environment Variable: ECK_REMOTE_ES_HOSTS Default: None Purpose : Specifies one or more remote Elasticsearch hosts for Logstash to forward logs to instead of local Elasticsearch. When to use : - Set when forwarding logs to external Elasticsearch cluster - Required along with eck_remote_es_username and eck_remote_es_password - Only applies when eck_enable_logstash=true Valid values : Comma-separated list of Elasticsearch hosts (e.g., https://es1.example.com:9200,https://es2.example.com:9200 ) Impact : When set with credentials, Logstash forwards logs to remote Elasticsearch instead of local instance. Related variables : - eck_remote_es_username : Username for remote Elasticsearch - eck_remote_es_password : Password for remote Elasticsearch - eck_enable_logstash : Must be true for remote forwarding Note : All three remote Elasticsearch variables must be set together for remote forwarding to work. eck_remote_es_username \u00a4 Remote Elasticsearch username. Optional (required for remote Elasticsearch) Environment Variable: ECK_REMOTE_ES_USERNAME Default: None Purpose : Specifies the username for authenticating with remote Elasticsearch when forwarding logs via Logstash. When to use : - Set when forwarding logs to external Elasticsearch cluster - Required along with eck_remote_es_hosts and eck_remote_es_password - Only applies when eck_enable_logstash=true Valid values : Valid Elasticsearch username string Impact : Used by Logstash to authenticate with remote Elasticsearch. Without valid credentials, log forwarding will fail. Related variables : - eck_remote_es_hosts : Remote Elasticsearch hosts - eck_remote_es_password : Password for authentication - eck_enable_logstash : Must be true for remote forwarding Note : SECURITY - All three remote Elasticsearch variables must be set together. Credentials are stored securely in Kubernetes secrets. eck_remote_es_password \u00a4 Remote Elasticsearch password. Optional (required for remote Elasticsearch) Environment Variable: ECK_REMOTE_ES_PASSWORD Default: None Purpose : Specifies the password for authenticating with remote Elasticsearch when forwarding logs via Logstash. When to use : - Set when forwarding logs to external Elasticsearch cluster - Required along with eck_remote_es_hosts and eck_remote_es_username - Only applies when eck_enable_logstash=true Valid values : Valid Elasticsearch password string Impact : Used by Logstash to authenticate with remote Elasticsearch. Without valid credentials, log forwarding will fail. Related variables : - eck_remote_es_hosts : Remote Elasticsearch hosts - eck_remote_es_username : Username for authentication - eck_enable_logstash : Must be true for remote forwarding Note : SECURITY - Store password securely. All three remote Elasticsearch variables must be set together. Credentials are stored in Kubernetes secrets. Role Variables - Domains and Certificates \u00a4 Elasticsearch and Kibana can be configured with a custom domain and a certificate signed by LetsEncrypt . es_domain \u00a4 Custom domain for Elasticsearch access. Optional Environment Variable: ECK_ELASTICSEARCH_DOMAIN Default: None Purpose : Specifies a custom domain for accessing Elasticsearch, enabling external access with proper DNS routing. When to use : - Set when external access to Elasticsearch is required - Must be routable to the target OCP cluster - Used with LetsEncrypt for automatic certificate generation Valid values : Valid domain name routable to the cluster (e.g., es.example.com ) Impact : When set, creates a route with custom domain for Elasticsearch access. Without it, uses default cluster route. Related variables : - letsencrypt_email : Required for automatic certificate generation - eck_enable_elasticsearch : Must be true Note : Domain must be routable to the cluster. When combined with letsencrypt_email , automatically provisions LetsEncrypt certificate using HTTP solver. kibana_domain \u00a4 Custom domain for Kibana access. Optional Environment Variable: ECK_KIBANA_DOMAIN Default: None Purpose : Specifies a custom domain for accessing Kibana UI, enabling external access with proper DNS routing. When to use : - Set when external access to Kibana is required - Must be routable to the target OCP cluster - Used with LetsEncrypt for automatic certificate generation Valid values : Valid domain name routable to the cluster (e.g., kibana.example.com ) Impact : When set, creates a route with custom domain for Kibana access. Without it, uses default cluster route. Related variables : - letsencrypt_email : Required for automatic certificate generation - eck_enable_kibana : Must be true Note : Domain must be routable to the cluster. When combined with letsencrypt_email , automatically provisions LetsEncrypt certificate using HTTP solver. letsencrypt_email \u00a4 Email for LetsEncrypt certificate registration. Optional Environment Variable: LETSENCRYPT_EMAIL Default: None Purpose : Specifies the email address for registering LetsEncrypt certificates when using custom domains for Elasticsearch or Kibana. When to use : - Set when using custom domains ( es_domain or kibana_domain ) - Required for automatic LetsEncrypt certificate provisioning - Email receives certificate expiration notifications Valid values : Valid email address Impact : When set with custom domains, automatically configures LetsEncrypt Issuer and provisions certificates using HTTP solver via Cert-Manager. Related variables : - es_domain : Elasticsearch custom domain - kibana_domain : Kibana custom domain Note : Requires Cert-Manager to be installed in the cluster. The Issuer uses LetsEncrypt production environment with HTTP-01 challenge solver. Email receives important certificate notifications. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: eck_action: install eck_enable_elasticsearch: true eck_enable_kibana: true eck_enable_logstash: true roles: - ibm.mas_devops.eck License \u00a4 EPL-2.0","title":"eck"},{"location":"roles/eck/#eck","text":"This role provides support to install Elastic Cloud on Kubernetes (ECK). Elasticsearch is configured with a default user named elastic , you can obtain the password for this user by running the following command: oc -n eck get secret mas-es-elastic-user -o go-template='{{.data.elastic | base64decode}}'; echo","title":"eck"},{"location":"roles/eck/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/eck/#eck_action","text":"Action to perform on ECK installation. Optional Environment Variable: ECK_ACTION Default: install Purpose : Specifies the action to perform on the Elastic Cloud on Kubernetes (ECK) deployment. When to use : - Use install (default and only supported value) to deploy ECK - Future versions may support additional actions Valid values : install Impact : Determines the operation performed on ECK. Currently only installation is supported. Related variables : - eck_enable_elasticsearch : Enable Elasticsearch component - eck_enable_kibana : Enable Kibana component - eck_enable_logstash : Enable Logstash component - eck_enable_filebeat : Enable Filebeat component Note : This role installs ECK operator and optionally deploys Elasticsearch, Kibana, Logstash, and Filebeat components based on enable flags.","title":"eck_action"},{"location":"roles/eck/#eck_enable_elasticsearch","text":"Enable Elasticsearch deployment. Optional Environment Variable: ECK_ENABLE_ELASTICSEARCH Default: false Purpose : Controls whether Elasticsearch is deployed as part of the ECK installation. When to use : - Set to true to deploy Elasticsearch for log storage and search - Leave as false if using external Elasticsearch or not needed - Required for Kibana and Logstash functionality Valid values : true , false Impact : - true : Deploys Elasticsearch cluster in ECK namespace - false : Skips Elasticsearch deployment Related variables : - eck_enable_kibana : Kibana requires Elasticsearch - eck_enable_logstash : Logstash can send to local or remote Elasticsearch - es_domain : Custom domain for Elasticsearch access Note : Default user elastic is created. Retrieve password with: oc -n eck get secret mas-es-elastic-user -o go-template='{{.data.elastic | base64decode}}'","title":"eck_enable_elasticsearch"},{"location":"roles/eck/#eck_enable_kibana","text":"Enable Kibana deployment. Optional Environment Variable: ECK_ENABLE_KIBANA Default: false Purpose : Controls whether Kibana is deployed as part of the ECK installation for log visualization and analysis. When to use : - Set to true to deploy Kibana for log visualization - Leave as false if not using Kibana UI - Requires Elasticsearch to be enabled Valid values : true , false Impact : - true : Deploys Kibana instance connected to Elasticsearch - false : Skips Kibana deployment Related variables : - eck_enable_elasticsearch : Must be true for Kibana to function - kibana_domain : Custom domain for Kibana access - letsencrypt_email : For LetsEncrypt certificate Note : Kibana requires Elasticsearch. Ensure eck_enable_elasticsearch=true when enabling Kibana.","title":"eck_enable_kibana"},{"location":"roles/eck/#eck_enable_logstash","text":"Enable Logstash deployment. Optional Environment Variable: ECK_ENABLE_LOGSTASH Default: false Purpose : Controls whether Logstash is deployed as part of the ECK installation for log processing and forwarding. When to use : - Set to true to deploy Logstash for log processing - Leave as false if not using Logstash pipeline - Can send logs to local or remote Elasticsearch Valid values : true , false Impact : - true : Deploys Logstash instance for log processing - false : Skips Logstash deployment Related variables : - eck_remote_es_hosts : Remote Elasticsearch hosts for log forwarding - eck_remote_es_username : Username for remote Elasticsearch - eck_remote_es_password : Password for remote Elasticsearch Note : When remote Elasticsearch variables are set, Logstash forwards logs to the remote instance. Otherwise, logs are sent to local Elasticsearch (if enabled).","title":"eck_enable_logstash"},{"location":"roles/eck/#eck_enable_filebeat","text":"Enable Filebeat deployment. Optional Environment Variable: ECK_ENABLE_FILEBEAT Default: false Purpose : Controls whether Filebeat is deployed as part of the ECK installation for log collection from cluster nodes. When to use : - Set to true to deploy Filebeat for log collection - Leave as false if not collecting node logs - Filebeat collects logs from Kubernetes nodes Valid values : true , false Impact : - true : Deploys Filebeat DaemonSet for log collection - false : Skips Filebeat deployment Related variables : - eck_enable_elasticsearch : Filebeat sends logs to Elasticsearch - eck_enable_logstash : Alternative log processing pipeline Note : Filebeat runs as a DaemonSet on cluster nodes to collect logs and forward them to Elasticsearch or Logstash.","title":"eck_enable_filebeat"},{"location":"roles/eck/#role-variables-remote-elasticsearch","text":"When eck_remote_es_hosts , eck_remote_es_username , and eck_remote_es_password are all set, and eck_enable_logstash is true , the Logstash server will be configured to send log messages to the remote Elasticsearch instance defined.","title":"Role Variables - Remote Elasticsearch"},{"location":"roles/eck/#eck_remote_es_hosts","text":"Remote Elasticsearch host list. Optional (required for remote Elasticsearch) Environment Variable: ECK_REMOTE_ES_HOSTS Default: None Purpose : Specifies one or more remote Elasticsearch hosts for Logstash to forward logs to instead of local Elasticsearch. When to use : - Set when forwarding logs to external Elasticsearch cluster - Required along with eck_remote_es_username and eck_remote_es_password - Only applies when eck_enable_logstash=true Valid values : Comma-separated list of Elasticsearch hosts (e.g., https://es1.example.com:9200,https://es2.example.com:9200 ) Impact : When set with credentials, Logstash forwards logs to remote Elasticsearch instead of local instance. Related variables : - eck_remote_es_username : Username for remote Elasticsearch - eck_remote_es_password : Password for remote Elasticsearch - eck_enable_logstash : Must be true for remote forwarding Note : All three remote Elasticsearch variables must be set together for remote forwarding to work.","title":"eck_remote_es_hosts"},{"location":"roles/eck/#eck_remote_es_username","text":"Remote Elasticsearch username. Optional (required for remote Elasticsearch) Environment Variable: ECK_REMOTE_ES_USERNAME Default: None Purpose : Specifies the username for authenticating with remote Elasticsearch when forwarding logs via Logstash. When to use : - Set when forwarding logs to external Elasticsearch cluster - Required along with eck_remote_es_hosts and eck_remote_es_password - Only applies when eck_enable_logstash=true Valid values : Valid Elasticsearch username string Impact : Used by Logstash to authenticate with remote Elasticsearch. Without valid credentials, log forwarding will fail. Related variables : - eck_remote_es_hosts : Remote Elasticsearch hosts - eck_remote_es_password : Password for authentication - eck_enable_logstash : Must be true for remote forwarding Note : SECURITY - All three remote Elasticsearch variables must be set together. Credentials are stored securely in Kubernetes secrets.","title":"eck_remote_es_username"},{"location":"roles/eck/#eck_remote_es_password","text":"Remote Elasticsearch password. Optional (required for remote Elasticsearch) Environment Variable: ECK_REMOTE_ES_PASSWORD Default: None Purpose : Specifies the password for authenticating with remote Elasticsearch when forwarding logs via Logstash. When to use : - Set when forwarding logs to external Elasticsearch cluster - Required along with eck_remote_es_hosts and eck_remote_es_username - Only applies when eck_enable_logstash=true Valid values : Valid Elasticsearch password string Impact : Used by Logstash to authenticate with remote Elasticsearch. Without valid credentials, log forwarding will fail. Related variables : - eck_remote_es_hosts : Remote Elasticsearch hosts - eck_remote_es_username : Username for authentication - eck_enable_logstash : Must be true for remote forwarding Note : SECURITY - Store password securely. All three remote Elasticsearch variables must be set together. Credentials are stored in Kubernetes secrets.","title":"eck_remote_es_password"},{"location":"roles/eck/#role-variables-domains-and-certificates","text":"Elasticsearch and Kibana can be configured with a custom domain and a certificate signed by LetsEncrypt .","title":"Role Variables - Domains and Certificates"},{"location":"roles/eck/#es_domain","text":"Custom domain for Elasticsearch access. Optional Environment Variable: ECK_ELASTICSEARCH_DOMAIN Default: None Purpose : Specifies a custom domain for accessing Elasticsearch, enabling external access with proper DNS routing. When to use : - Set when external access to Elasticsearch is required - Must be routable to the target OCP cluster - Used with LetsEncrypt for automatic certificate generation Valid values : Valid domain name routable to the cluster (e.g., es.example.com ) Impact : When set, creates a route with custom domain for Elasticsearch access. Without it, uses default cluster route. Related variables : - letsencrypt_email : Required for automatic certificate generation - eck_enable_elasticsearch : Must be true Note : Domain must be routable to the cluster. When combined with letsencrypt_email , automatically provisions LetsEncrypt certificate using HTTP solver.","title":"es_domain"},{"location":"roles/eck/#kibana_domain","text":"Custom domain for Kibana access. Optional Environment Variable: ECK_KIBANA_DOMAIN Default: None Purpose : Specifies a custom domain for accessing Kibana UI, enabling external access with proper DNS routing. When to use : - Set when external access to Kibana is required - Must be routable to the target OCP cluster - Used with LetsEncrypt for automatic certificate generation Valid values : Valid domain name routable to the cluster (e.g., kibana.example.com ) Impact : When set, creates a route with custom domain for Kibana access. Without it, uses default cluster route. Related variables : - letsencrypt_email : Required for automatic certificate generation - eck_enable_kibana : Must be true Note : Domain must be routable to the cluster. When combined with letsencrypt_email , automatically provisions LetsEncrypt certificate using HTTP solver.","title":"kibana_domain"},{"location":"roles/eck/#letsencrypt_email","text":"Email for LetsEncrypt certificate registration. Optional Environment Variable: LETSENCRYPT_EMAIL Default: None Purpose : Specifies the email address for registering LetsEncrypt certificates when using custom domains for Elasticsearch or Kibana. When to use : - Set when using custom domains ( es_domain or kibana_domain ) - Required for automatic LetsEncrypt certificate provisioning - Email receives certificate expiration notifications Valid values : Valid email address Impact : When set with custom domains, automatically configures LetsEncrypt Issuer and provisions certificates using HTTP solver via Cert-Manager. Related variables : - es_domain : Elasticsearch custom domain - kibana_domain : Kibana custom domain Note : Requires Cert-Manager to be installed in the cluster. The Issuer uses LetsEncrypt production environment with HTTP-01 challenge solver. Email receives important certificate notifications.","title":"letsencrypt_email"},{"location":"roles/eck/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: eck_action: install eck_enable_elasticsearch: true eck_enable_kibana: true eck_enable_logstash: true roles: - ibm.mas_devops.eck","title":"Example Playbook"},{"location":"roles/eck/#license","text":"EPL-2.0","title":"License"},{"location":"roles/entitlement_key_rotation/","text":"entitlement_key_rotation \u00a4 This role creates/updates the entitlement username and password that are stored in the secrets used to pull images throughout all MAS related namespaces for one or multiple clusters. The main secret that is updated by this role is the ibm-entitlement which holds the credentials needed to pull the MAS images used by MAS Core or the MAS applications. By default, this role will search for all MAS related namespaces that might contain the secret that holds the entitlement key to be updated. The list of namespaces to be updated with new username/password credentials are: All namespaces starting with mas- , which means by default it will update the ibm-entitlement secret with the new username/password credentials for all MAS namespaces/instances in the cluster. SLS namespace - holds ibm-entitlement which pulls Suite License Services related images. openshift-marketplace - holds wiot-docker-local which pulls the pre-release/development catalog source image for ibm-operator-catalog . Requires the artifactory_username and artifactory_token to be set. Note This role uses ocp_login to login into the target clusters, therefore make sure you export the corresponding environment variables accordingly to the cluster type you want to target. Role Variables - General \u00a4 artifactory_username \u00a4 Required to rotate the ibm-entitlement and wiotp-docker-local secret credentials which is used to pull images across MAS namespaces for development installs and pre-release catalog sources. Environment Variable: ARTIFACTORY_USERNAME Default Value: None artifactory_token \u00a4 Required to rotate the ibm-entitlement and wiotp-docker-local secret credentials which is used to pull images across MAS namespaces for development installs and pre-release catalog sources. Environment Variable: ARTIFACTORY_TOKEN Default Value: None mas_entitlement_username \u00a4 Required to rotate the ibm-entitlement secret credentials only, which is used to pull images across MAS namespaces for MAS installs using release catalog sources. Environment Variable: MAS_ENTITLEMENT_USERNAME Default Value: None mas_entitlement_key \u00a4 Required to rotate the ibm-entitlement secret credentials only, which is used to pull images across MAS namespaces for MAS installs using release catalog sources. Environment Variable: MAS_ENTITLEMENT_KEY Default Value: None cluster_name \u00a4 Required. The target cluster to rotate the credentials/entitlement key secrets. Environment Variable: CLUSTER_NAME Default Value: None sls_namespace \u00a4 Optional. Defines the SLS namespace that holds ibm-entitlement secret which pulls Suite License Services related images. Environment Variable: SLS_NAMESPACE Default Value: ibm-sls Role Variables - Advanced mode \u00a4 Use the following variables to change the default behavior of this role to only rotate the entitlement key for specific clusters or namespaces, instead of running it for all MAS related namespaces. mas_clusters_entitlement_key_rotation_list \u00a4 Optionally define a list of clusters to loop through the entitlement key rotation. Environment Variable: MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST Default Value: If not set, the cluster_name property will be used to target the cluster while executing this role. Example: export MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST='cluster1,cluster2' mas_namespaces_entitlement_key_rotation_list \u00a4 Optionally define a specific list of namespaces to loop through the entitlement key rotation. Environment Variable: MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST Default Value: If not set, all MAS related namespaces for all MAS instances will be target for entitlement key rotation. Example: export MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST='ibm-sls,openshift-marketplace' Example Playbook \u00a4 Rotate entitlement credentials across all MAS instances for a given target cluster: - hosts: localhost any_errors_fatal: true vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME') }}\" cluster_type: \"{{ lookup('env', 'CLUSTER_TYPE') }}\" ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" artifactory_username: \"{{ lookup('env', 'ARTIFACTORY_USERNAME') }}\" artifactory_token: \"{{ lookup('env', 'ARTIFACTORY_TOKEN') }}\" mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.entitlement_key_rotation Rotate entitlement credentials across a specific list of namespaces, targeting multiple clusters: - hosts: localhost any_errors_fatal: true vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME') }}\" # this is the original cluster that will keep the login session context at the end of the rotation loop. cluster_type: \"{{ lookup('env', 'CLUSTER_TYPE') }}\" ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" artifactory_username: \"{{ lookup('env', 'ARTIFACTORY_USERNAME') }}\" artifactory_token: \"{{ lookup('env', 'ARTIFACTORY_TOKEN') }}\" mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" mas_clusters_entitlement_key_rotation_list: \"{{ lookup('env', 'MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST') }}\" mas_namespaces_entitlement_key_rotation_list: \"{{ lookup('env', 'MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST') }}\" roles: - ibm.mas_devops.entitlement_key_rotation License \u00a4 EPL-2.0","title":"entitlement_key_rotation"},{"location":"roles/entitlement_key_rotation/#entitlement_key_rotation","text":"This role creates/updates the entitlement username and password that are stored in the secrets used to pull images throughout all MAS related namespaces for one or multiple clusters. The main secret that is updated by this role is the ibm-entitlement which holds the credentials needed to pull the MAS images used by MAS Core or the MAS applications. By default, this role will search for all MAS related namespaces that might contain the secret that holds the entitlement key to be updated. The list of namespaces to be updated with new username/password credentials are: All namespaces starting with mas- , which means by default it will update the ibm-entitlement secret with the new username/password credentials for all MAS namespaces/instances in the cluster. SLS namespace - holds ibm-entitlement which pulls Suite License Services related images. openshift-marketplace - holds wiot-docker-local which pulls the pre-release/development catalog source image for ibm-operator-catalog . Requires the artifactory_username and artifactory_token to be set. Note This role uses ocp_login to login into the target clusters, therefore make sure you export the corresponding environment variables accordingly to the cluster type you want to target.","title":"entitlement_key_rotation"},{"location":"roles/entitlement_key_rotation/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/entitlement_key_rotation/#artifactory_username","text":"Required to rotate the ibm-entitlement and wiotp-docker-local secret credentials which is used to pull images across MAS namespaces for development installs and pre-release catalog sources. Environment Variable: ARTIFACTORY_USERNAME Default Value: None","title":"artifactory_username"},{"location":"roles/entitlement_key_rotation/#artifactory_token","text":"Required to rotate the ibm-entitlement and wiotp-docker-local secret credentials which is used to pull images across MAS namespaces for development installs and pre-release catalog sources. Environment Variable: ARTIFACTORY_TOKEN Default Value: None","title":"artifactory_token"},{"location":"roles/entitlement_key_rotation/#mas_entitlement_username","text":"Required to rotate the ibm-entitlement secret credentials only, which is used to pull images across MAS namespaces for MAS installs using release catalog sources. Environment Variable: MAS_ENTITLEMENT_USERNAME Default Value: None","title":"mas_entitlement_username"},{"location":"roles/entitlement_key_rotation/#mas_entitlement_key","text":"Required to rotate the ibm-entitlement secret credentials only, which is used to pull images across MAS namespaces for MAS installs using release catalog sources. Environment Variable: MAS_ENTITLEMENT_KEY Default Value: None","title":"mas_entitlement_key"},{"location":"roles/entitlement_key_rotation/#cluster_name","text":"Required. The target cluster to rotate the credentials/entitlement key secrets. Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/entitlement_key_rotation/#sls_namespace","text":"Optional. Defines the SLS namespace that holds ibm-entitlement secret which pulls Suite License Services related images. Environment Variable: SLS_NAMESPACE Default Value: ibm-sls","title":"sls_namespace"},{"location":"roles/entitlement_key_rotation/#role-variables-advanced-mode","text":"Use the following variables to change the default behavior of this role to only rotate the entitlement key for specific clusters or namespaces, instead of running it for all MAS related namespaces.","title":"Role Variables - Advanced mode"},{"location":"roles/entitlement_key_rotation/#mas_clusters_entitlement_key_rotation_list","text":"Optionally define a list of clusters to loop through the entitlement key rotation. Environment Variable: MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST Default Value: If not set, the cluster_name property will be used to target the cluster while executing this role. Example: export MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST='cluster1,cluster2'","title":"mas_clusters_entitlement_key_rotation_list"},{"location":"roles/entitlement_key_rotation/#mas_namespaces_entitlement_key_rotation_list","text":"Optionally define a specific list of namespaces to loop through the entitlement key rotation. Environment Variable: MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST Default Value: If not set, all MAS related namespaces for all MAS instances will be target for entitlement key rotation. Example: export MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST='ibm-sls,openshift-marketplace'","title":"mas_namespaces_entitlement_key_rotation_list"},{"location":"roles/entitlement_key_rotation/#example-playbook","text":"Rotate entitlement credentials across all MAS instances for a given target cluster: - hosts: localhost any_errors_fatal: true vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME') }}\" cluster_type: \"{{ lookup('env', 'CLUSTER_TYPE') }}\" ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" artifactory_username: \"{{ lookup('env', 'ARTIFACTORY_USERNAME') }}\" artifactory_token: \"{{ lookup('env', 'ARTIFACTORY_TOKEN') }}\" mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.entitlement_key_rotation Rotate entitlement credentials across a specific list of namespaces, targeting multiple clusters: - hosts: localhost any_errors_fatal: true vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME') }}\" # this is the original cluster that will keep the login session context at the end of the rotation loop. cluster_type: \"{{ lookup('env', 'CLUSTER_TYPE') }}\" ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" artifactory_username: \"{{ lookup('env', 'ARTIFACTORY_USERNAME') }}\" artifactory_token: \"{{ lookup('env', 'ARTIFACTORY_TOKEN') }}\" mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" mas_clusters_entitlement_key_rotation_list: \"{{ lookup('env', 'MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST') }}\" mas_namespaces_entitlement_key_rotation_list: \"{{ lookup('env', 'MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST') }}\" roles: - ibm.mas_devops.entitlement_key_rotation","title":"Example Playbook"},{"location":"roles/entitlement_key_rotation/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_jdbc/","text":"gencfg_jdbc \u00a4 Generate JDBC configuration files for connecting Maximo Application Suite to external databases. This role creates JdbcCfg custom resources that can be applied to MAS instances to configure database connectivity for various applications including Manage, Health, Predict, and others. The role supports multiple database types (Db2, Oracle, SQL Server) with both secure and insecure connection options, and can generate configurations scoped at system, workspace, application, or workspace-application levels. Role Variables - Data Source \u00a4 Role Variables - General \u00a4 db_instance_id \u00a4 Database instance identifier used in the generated configuration. Optional Environment Variable: DB_INSTANCE_ID Default: dbinst Purpose : Provides a unique identifier for the database instance within MAS configuration. This ID is used to reference the database connection in application configurations. When to use : Always set this to a meaningful name that identifies the database instance, especially when managing multiple database connections. Valid values : Alphanumeric string, typically lowercase with no spaces (e.g., maxdb01 , manage-prod , oracle-dev ). Impact : This identifier appears in MAS configuration resources and must be unique within the scope of the MAS instance. Related variables : mas_config_scope , mas_instance_id Notes : - Default value dbinst is suitable for simple single-database deployments - Use descriptive names in multi-database environments to avoid confusion db_username \u00a4 Database username for authentication. Required Environment Variable: MAS_JDBC_USER Default: None Purpose : Specifies the database user account that MAS applications will use to connect to the database. When to use : Always required. Must be a valid database user with appropriate permissions for the target MAS application. Valid values : Valid database username according to the database system's naming rules. Impact : This user must have the necessary privileges to access and modify the application schema. Insufficient permissions will cause application failures. Related variables : jdbc_instance_password , jdbc_url Notes : - For Manage: User needs full access to the MAXIMO schema - For Oracle: May need to include schema prefix in some configurations - Ensure the user has appropriate connection limits configured in the database jdbc_instance_password \u00a4 Database password for authentication. Required Environment Variable: MAS_JDBC_PASSWORD Default: None Purpose : Provides the password for the database user account specified in db_username . When to use : Always required for database authentication. Valid values : Valid password string. Should meet your organization's password complexity requirements. Impact : Stored as a Kubernetes secret in the MAS namespace. Incorrect password will prevent database connectivity. Related variables : db_username , jdbc_url Notes : - Password is stored securely in Kubernetes secrets - Consider using strong passwords and regular rotation policies - Ensure password doesn't contain characters that require URL encoding if embedded in connection strings jdbc_url \u00a4 JDBC connection URL for the database. Required Environment Variable: MAS_JDBC_URL Default: None Purpose : Defines the complete JDBC connection string including host, port, database name, and connection parameters. When to use : Always required. Must be formatted correctly for the specific database type. Valid values : Valid JDBC URL string for the target database type. Examples: IBM Db2 (insecure) : jdbc:db2://dbserver.example.com:50000/maxdb IBM Db2 (secure) : jdbc:db2://dbserver.example.com:50000/maxdb:sslConnection=true; Oracle Database : jdbc:oracle:thin:@dbserver.example.com:1521:maximo SQL Server (insecure) : jdbc:sqlserver://;serverName=dbserver.example.com;portNumber=1433;databaseName=maxdb;integratedSecurity=false;sendStringParametersAsUnicode=false;selectMethod=cursor;encrypt=false;trustServerCertificate=false; SQL Server (secure) : jdbc:sqlserver://;serverName=dbserver.example.com;portNumber=1433;databaseName=maxdb;integratedSecurity=false;sendStringParametersAsUnicode=false;selectMethod=cursor;encrypt=true;trustServerCertificate=true; Impact : Incorrect URL format will prevent database connectivity. SSL/TLS settings in the URL must match the ssl_enabled variable and database server configuration. Related variables : ssl_enabled , db_pem_file , db_username Notes : - Always test the JDBC URL with a database client before using in MAS - For SSL connections, ensure the URL includes appropriate SSL parameters - Some databases require specific JDBC driver parameters for optimal performance - Port numbers and database names must match your actual database configuration db_pem_file \u00a4 Local file path to the database SSL/TLS certificate in PEM format. Optional Environment Variable: MAS_JDBC_CERT_LOCAL_FILE Default: None Purpose : Provides the SSL/TLS certificate for secure database connections. The certificate is embedded in the generated JdbcCfg resource. When to use : Required when ssl_enabled is true and the database uses SSL/TLS encryption. Not needed for insecure connections. Valid values : Absolute or relative file path to a valid PEM-encoded certificate file (e.g., /tmp/db-ca.pem , ./certs/database-cert.pem ). Impact : Without the correct certificate, SSL connections will fail with certificate validation errors. Related variables : ssl_enabled , jdbc_url Notes : - The file must be accessible from the Ansible controller - For self-signed certificates, this should be the CA certificate - For commercial certificates, may need to include the full certificate chain - Certificate must be in PEM format (Base64-encoded with BEGIN/END markers) Role Variables - MAS Configuration \u00a4 mas_config_scope \u00a4 Configuration scope level for the generated JDBC configuration. Required Environment Variable: MAS_CONFIG_SCOPE Default: None Purpose : Determines at what level the JDBC configuration will be available within MAS - system-wide, workspace-specific, application-specific, or workspace-application combination. When to use : Always required. Choose based on how the database will be shared across MAS components. Valid values : - system - Available to all workspaces and applications - ws - Available only to a specific workspace - app - Available only to a specific application across all workspaces - wsapp - Available only to a specific application in a specific workspace Impact : Determines which MAS components can access this database configuration. Incorrect scope may prevent applications from finding the database configuration. Related variables : mas_workspace_id (required for ws and wsapp ), mas_application_id (required for app and wsapp ) Notes : - Use system scope for shared databases used by multiple applications - Use wsapp scope for dedicated databases per workspace-application combination - Scope cannot be changed after creation; requires recreation of the configuration mas_config_dir \u00a4 Local directory path where the generated YAML configuration file will be saved. Required Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the output directory for the generated JdbcCfg YAML file that can be applied to the MAS instance. When to use : Always required. Directory must exist or be creatable by the Ansible user. Valid values : Valid directory path (absolute or relative) where the Ansible controller has write permissions. Impact : The generated YAML file will be created in this directory and can be applied using kubectl apply or the suite_config role. Related variables : mas_instance_id , db_instance_id Notes : - Directory will be created if it doesn't exist - Generated filename format: jdbccfg-<db_instance_id>.yml - Keep generated files for documentation and disaster recovery purposes mas_instance_id \u00a4 MAS instance identifier for which the configuration is being generated. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies the target MAS instance where this JDBC configuration will be applied. When to use : Always required. Must match an existing MAS instance ID. Valid values : Valid MAS instance ID (typically 3-12 lowercase alphanumeric characters). Impact : The generated configuration will be namespace-scoped to mas-<instance-id>-core . Incorrect instance ID will cause the configuration to be created in the wrong namespace. Related variables : mas_config_scope , mas_workspace_id Notes : - Must match the instance ID used during MAS installation - Case-sensitive value - Cannot be changed after MAS installation mas_workspace_id \u00a4 MAS workspace identifier for workspace-scoped configurations. Required if mas_config_scope is ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Specifies the target workspace when generating workspace-scoped or workspace-application-scoped JDBC configurations. When to use : Required when mas_config_scope is set to ws or wsapp . Not used for system or app scopes. Valid values : Valid MAS workspace ID (typically lowercase alphanumeric, e.g., masdev , prod , test ). Impact : The JDBC configuration will only be available to the specified workspace. Applications in other workspaces cannot access this database configuration. Related variables : mas_config_scope , mas_application_id Notes : - Workspace must exist before applying the configuration - Use workspace-scoped configs for tenant isolation in multi-tenant deployments mas_application_id \u00a4 MAS application identifier for application-scoped configurations. Required if mas_config_scope is app or wsapp Environment Variable: MAS_APP_ID Default: None Purpose : Specifies the target application when generating application-scoped or workspace-application-scoped JDBC configurations. When to use : Required when mas_config_scope is set to app or wsapp . Not used for system or ws scopes. Valid values : Valid MAS application ID: manage , health , predict , monitor , assist , visualinspection , iot , optimizer , safety . Impact : The JDBC configuration will only be available to the specified application. Other applications cannot access this database configuration. Related variables : mas_config_scope , mas_workspace_id Notes : - Application must be installed before applying the configuration - Most commonly used with manage for Manage-specific databases ssl_enabled \u00a4 Indicates whether SSL/TLS encryption is enabled for the database connection. Required Environment Variable: SSL_ENABLED Default: None Purpose : Explicitly declares SSL/TLS status for applications that cannot determine this from the JDBC URL alone. Must match the SSL configuration in jdbc_url . When to use : Always required. Set to true for encrypted connections, false for unencrypted connections. Valid values : - true - SSL/TLS encryption is enabled - false - No encryption (insecure connection) Impact : Mismatch between this setting and the actual JDBC URL SSL parameters will cause connection failures in some MAS applications. Related variables : jdbc_url , db_pem_file Notes : - Always use true for production environments - When true , ensure db_pem_file is provided - Must match the SSL settings in the jdbc_url parameter - Some MAS applications rely on this flag rather than parsing the JDBC URL custom_labels \u00a4 Custom Kubernetes labels to apply to the generated JdbcCfg resource. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds custom labels to the JdbcCfg resource for organization, filtering, or automation purposes. When to use : Use when you need to tag configurations for specific purposes like environment identification, cost tracking, or automated management. Valid values : Comma-separated key=value pairs (e.g., env=prod,team=platform,cost-center=12345 ). Impact : Labels are applied to the Kubernetes resource and can be used for selection, filtering, and automation workflows. Related variables : None Notes : - Labels must follow Kubernetes naming conventions - Useful for GitOps workflows and resource management - Can be used with label selectors in automation scripts Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_jdbc License \u00a4 EPL-2.0","title":"gencfg_jdbc"},{"location":"roles/gencfg_jdbc/#gencfg_jdbc","text":"Generate JDBC configuration files for connecting Maximo Application Suite to external databases. This role creates JdbcCfg custom resources that can be applied to MAS instances to configure database connectivity for various applications including Manage, Health, Predict, and others. The role supports multiple database types (Db2, Oracle, SQL Server) with both secure and insecure connection options, and can generate configurations scoped at system, workspace, application, or workspace-application levels.","title":"gencfg_jdbc"},{"location":"roles/gencfg_jdbc/#role-variables-data-source","text":"","title":"Role Variables - Data Source"},{"location":"roles/gencfg_jdbc/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/gencfg_jdbc/#db_instance_id","text":"Database instance identifier used in the generated configuration. Optional Environment Variable: DB_INSTANCE_ID Default: dbinst Purpose : Provides a unique identifier for the database instance within MAS configuration. This ID is used to reference the database connection in application configurations. When to use : Always set this to a meaningful name that identifies the database instance, especially when managing multiple database connections. Valid values : Alphanumeric string, typically lowercase with no spaces (e.g., maxdb01 , manage-prod , oracle-dev ). Impact : This identifier appears in MAS configuration resources and must be unique within the scope of the MAS instance. Related variables : mas_config_scope , mas_instance_id Notes : - Default value dbinst is suitable for simple single-database deployments - Use descriptive names in multi-database environments to avoid confusion","title":"db_instance_id"},{"location":"roles/gencfg_jdbc/#db_username","text":"Database username for authentication. Required Environment Variable: MAS_JDBC_USER Default: None Purpose : Specifies the database user account that MAS applications will use to connect to the database. When to use : Always required. Must be a valid database user with appropriate permissions for the target MAS application. Valid values : Valid database username according to the database system's naming rules. Impact : This user must have the necessary privileges to access and modify the application schema. Insufficient permissions will cause application failures. Related variables : jdbc_instance_password , jdbc_url Notes : - For Manage: User needs full access to the MAXIMO schema - For Oracle: May need to include schema prefix in some configurations - Ensure the user has appropriate connection limits configured in the database","title":"db_username"},{"location":"roles/gencfg_jdbc/#jdbc_instance_password","text":"Database password for authentication. Required Environment Variable: MAS_JDBC_PASSWORD Default: None Purpose : Provides the password for the database user account specified in db_username . When to use : Always required for database authentication. Valid values : Valid password string. Should meet your organization's password complexity requirements. Impact : Stored as a Kubernetes secret in the MAS namespace. Incorrect password will prevent database connectivity. Related variables : db_username , jdbc_url Notes : - Password is stored securely in Kubernetes secrets - Consider using strong passwords and regular rotation policies - Ensure password doesn't contain characters that require URL encoding if embedded in connection strings","title":"jdbc_instance_password"},{"location":"roles/gencfg_jdbc/#jdbc_url","text":"JDBC connection URL for the database. Required Environment Variable: MAS_JDBC_URL Default: None Purpose : Defines the complete JDBC connection string including host, port, database name, and connection parameters. When to use : Always required. Must be formatted correctly for the specific database type. Valid values : Valid JDBC URL string for the target database type. Examples: IBM Db2 (insecure) : jdbc:db2://dbserver.example.com:50000/maxdb IBM Db2 (secure) : jdbc:db2://dbserver.example.com:50000/maxdb:sslConnection=true; Oracle Database : jdbc:oracle:thin:@dbserver.example.com:1521:maximo SQL Server (insecure) : jdbc:sqlserver://;serverName=dbserver.example.com;portNumber=1433;databaseName=maxdb;integratedSecurity=false;sendStringParametersAsUnicode=false;selectMethod=cursor;encrypt=false;trustServerCertificate=false; SQL Server (secure) : jdbc:sqlserver://;serverName=dbserver.example.com;portNumber=1433;databaseName=maxdb;integratedSecurity=false;sendStringParametersAsUnicode=false;selectMethod=cursor;encrypt=true;trustServerCertificate=true; Impact : Incorrect URL format will prevent database connectivity. SSL/TLS settings in the URL must match the ssl_enabled variable and database server configuration. Related variables : ssl_enabled , db_pem_file , db_username Notes : - Always test the JDBC URL with a database client before using in MAS - For SSL connections, ensure the URL includes appropriate SSL parameters - Some databases require specific JDBC driver parameters for optimal performance - Port numbers and database names must match your actual database configuration","title":"jdbc_url"},{"location":"roles/gencfg_jdbc/#db_pem_file","text":"Local file path to the database SSL/TLS certificate in PEM format. Optional Environment Variable: MAS_JDBC_CERT_LOCAL_FILE Default: None Purpose : Provides the SSL/TLS certificate for secure database connections. The certificate is embedded in the generated JdbcCfg resource. When to use : Required when ssl_enabled is true and the database uses SSL/TLS encryption. Not needed for insecure connections. Valid values : Absolute or relative file path to a valid PEM-encoded certificate file (e.g., /tmp/db-ca.pem , ./certs/database-cert.pem ). Impact : Without the correct certificate, SSL connections will fail with certificate validation errors. Related variables : ssl_enabled , jdbc_url Notes : - The file must be accessible from the Ansible controller - For self-signed certificates, this should be the CA certificate - For commercial certificates, may need to include the full certificate chain - Certificate must be in PEM format (Base64-encoded with BEGIN/END markers)","title":"db_pem_file"},{"location":"roles/gencfg_jdbc/#role-variables-mas-configuration","text":"","title":"Role Variables - MAS Configuration"},{"location":"roles/gencfg_jdbc/#mas_config_scope","text":"Configuration scope level for the generated JDBC configuration. Required Environment Variable: MAS_CONFIG_SCOPE Default: None Purpose : Determines at what level the JDBC configuration will be available within MAS - system-wide, workspace-specific, application-specific, or workspace-application combination. When to use : Always required. Choose based on how the database will be shared across MAS components. Valid values : - system - Available to all workspaces and applications - ws - Available only to a specific workspace - app - Available only to a specific application across all workspaces - wsapp - Available only to a specific application in a specific workspace Impact : Determines which MAS components can access this database configuration. Incorrect scope may prevent applications from finding the database configuration. Related variables : mas_workspace_id (required for ws and wsapp ), mas_application_id (required for app and wsapp ) Notes : - Use system scope for shared databases used by multiple applications - Use wsapp scope for dedicated databases per workspace-application combination - Scope cannot be changed after creation; requires recreation of the configuration","title":"mas_config_scope"},{"location":"roles/gencfg_jdbc/#mas_config_dir","text":"Local directory path where the generated YAML configuration file will be saved. Required Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the output directory for the generated JdbcCfg YAML file that can be applied to the MAS instance. When to use : Always required. Directory must exist or be creatable by the Ansible user. Valid values : Valid directory path (absolute or relative) where the Ansible controller has write permissions. Impact : The generated YAML file will be created in this directory and can be applied using kubectl apply or the suite_config role. Related variables : mas_instance_id , db_instance_id Notes : - Directory will be created if it doesn't exist - Generated filename format: jdbccfg-<db_instance_id>.yml - Keep generated files for documentation and disaster recovery purposes","title":"mas_config_dir"},{"location":"roles/gencfg_jdbc/#mas_instance_id","text":"MAS instance identifier for which the configuration is being generated. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies the target MAS instance where this JDBC configuration will be applied. When to use : Always required. Must match an existing MAS instance ID. Valid values : Valid MAS instance ID (typically 3-12 lowercase alphanumeric characters). Impact : The generated configuration will be namespace-scoped to mas-<instance-id>-core . Incorrect instance ID will cause the configuration to be created in the wrong namespace. Related variables : mas_config_scope , mas_workspace_id Notes : - Must match the instance ID used during MAS installation - Case-sensitive value - Cannot be changed after MAS installation","title":"mas_instance_id"},{"location":"roles/gencfg_jdbc/#mas_workspace_id","text":"MAS workspace identifier for workspace-scoped configurations. Required if mas_config_scope is ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Specifies the target workspace when generating workspace-scoped or workspace-application-scoped JDBC configurations. When to use : Required when mas_config_scope is set to ws or wsapp . Not used for system or app scopes. Valid values : Valid MAS workspace ID (typically lowercase alphanumeric, e.g., masdev , prod , test ). Impact : The JDBC configuration will only be available to the specified workspace. Applications in other workspaces cannot access this database configuration. Related variables : mas_config_scope , mas_application_id Notes : - Workspace must exist before applying the configuration - Use workspace-scoped configs for tenant isolation in multi-tenant deployments","title":"mas_workspace_id"},{"location":"roles/gencfg_jdbc/#mas_application_id","text":"MAS application identifier for application-scoped configurations. Required if mas_config_scope is app or wsapp Environment Variable: MAS_APP_ID Default: None Purpose : Specifies the target application when generating application-scoped or workspace-application-scoped JDBC configurations. When to use : Required when mas_config_scope is set to app or wsapp . Not used for system or ws scopes. Valid values : Valid MAS application ID: manage , health , predict , monitor , assist , visualinspection , iot , optimizer , safety . Impact : The JDBC configuration will only be available to the specified application. Other applications cannot access this database configuration. Related variables : mas_config_scope , mas_workspace_id Notes : - Application must be installed before applying the configuration - Most commonly used with manage for Manage-specific databases","title":"mas_application_id"},{"location":"roles/gencfg_jdbc/#ssl_enabled","text":"Indicates whether SSL/TLS encryption is enabled for the database connection. Required Environment Variable: SSL_ENABLED Default: None Purpose : Explicitly declares SSL/TLS status for applications that cannot determine this from the JDBC URL alone. Must match the SSL configuration in jdbc_url . When to use : Always required. Set to true for encrypted connections, false for unencrypted connections. Valid values : - true - SSL/TLS encryption is enabled - false - No encryption (insecure connection) Impact : Mismatch between this setting and the actual JDBC URL SSL parameters will cause connection failures in some MAS applications. Related variables : jdbc_url , db_pem_file Notes : - Always use true for production environments - When true , ensure db_pem_file is provided - Must match the SSL settings in the jdbc_url parameter - Some MAS applications rely on this flag rather than parsing the JDBC URL","title":"ssl_enabled"},{"location":"roles/gencfg_jdbc/#custom_labels","text":"Custom Kubernetes labels to apply to the generated JdbcCfg resource. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds custom labels to the JdbcCfg resource for organization, filtering, or automation purposes. When to use : Use when you need to tag configurations for specific purposes like environment identification, cost tracking, or automated management. Valid values : Comma-separated key=value pairs (e.g., env=prod,team=platform,cost-center=12345 ). Impact : Labels are applied to the Kubernetes resource and can be used for selection, filtering, and automation workflows. Related variables : None Notes : - Labels must follow Kubernetes naming conventions - Useful for GitOps workflows and resource management - Can be used with label selectors in automation scripts","title":"custom_labels"},{"location":"roles/gencfg_jdbc/#example-playbook","text":"- hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_jdbc","title":"Example Playbook"},{"location":"roles/gencfg_jdbc/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_mongo/","text":"gencfg_mongo \u00a4 Generate MongoDB configuration files for connecting Maximo Application Suite to external MongoDB or MongoDB-compatible databases. This role creates MongoCfg custom resources that can be applied to MAS instances to configure the system database connectivity. The role supports various MongoDB deployments including MongoDB Community Edition, MongoDB Enterprise, IBM Cloud Databases for MongoDB, and Amazon DocumentDB, with flexible authentication mechanisms and SSL/TLS support. Role Variables \u00a4 mongodb_namespace \u00a4 Namespace identifier used in the generated configuration filename. Optional Environment Variable: MONGODB_NAMESPACE Default: mongoce Purpose : Provides a suffix for the generated configuration filename to help identify the MongoDB deployment source or environment. When to use : Customize when managing multiple MongoDB configurations or to clearly identify the MongoDB deployment type (e.g., mongoce , docdb , ibmcloud ). Valid values : Alphanumeric string, typically lowercase (e.g., mongoce , docdb , prod-mongo , dev-mongodb ). Impact : Only affects the generated filename ( mongo-<namespace>.yml ). Does not impact the actual MongoDB connection or MAS configuration. Related variables : mas_config_dir Notes : - Default mongoce indicates MongoDB Community Edition - Use descriptive names when managing multiple MongoDB instances - Filename helps with organization but doesn't affect functionality mongodb_admin_username \u00a4 MongoDB administrative username for authentication. Required Environment Variable: MONGODB_ADMIN_USERNAME Default: None Purpose : Specifies the MongoDB user account that MAS will use to connect to the MongoDB database for system data storage. When to use : Always required. Must be a valid MongoDB user with appropriate permissions for MAS operations. Valid values : Valid MongoDB username according to the authentication mechanism being used. Impact : This user must have read/write access to the MAS databases. Insufficient permissions will cause MAS core services to fail. Related variables : mongodb_admin_password , mongodb_authentication_mechanism , mongodb_authentication_database Notes : - For SCRAM authentication: Standard MongoDB username - For LDAP authentication: LDAP distinguished name or username - User must have readWrite role on MAS databases - Consider using a dedicated service account rather than the MongoDB admin user mongodb_admin_password \u00a4 MongoDB password for authentication. Required Environment Variable: MONGODB_ADMIN_PASSWORD Default: None Purpose : Provides the password for the MongoDB user account specified in mongodb_admin_username . When to use : Always required for MongoDB authentication. Valid values : Valid password string meeting your MongoDB security requirements. Impact : Stored as a Kubernetes secret in the MAS namespace. Incorrect password will prevent MAS from connecting to MongoDB. Related variables : mongodb_admin_username , mongodb_authentication_mechanism Notes : - Password is stored securely in Kubernetes secrets - Use strong passwords meeting your organization's security policies - For Amazon DocumentDB, use the master user password or IAM authentication - Consider implementing password rotation policies mongodb_authentication_mechanism \u00a4 MongoDB authentication method to use for connections. Optional Environment Variable: MONGODB_AUTHENTICATION_MECHANISM Default: DEFAULT Purpose : Specifies the authentication mechanism for connecting to MongoDB, supporting both standard MongoDB authentication and LDAP integration. When to use : Set to DEFAULT for standard MongoDB authentication (SCRAM-SHA-256/SCRAM-SHA-1), or PLAIN for LDAP authentication. Valid values : - DEFAULT - Use MongoDB's default SCRAM authentication (SCRAM-SHA-256 or SCRAM-SHA-1) - PLAIN - Use LDAP authentication (requires LDAP-enabled MongoDB) Impact : Must match the authentication configuration of your MongoDB deployment. Mismatch will cause authentication failures. Related variables : mongodb_authentication_database (must be $external when using PLAIN ) Notes : - DEFAULT is suitable for most MongoDB deployments - PLAIN requires MongoDB Enterprise with LDAP integration - Amazon DocumentDB uses SCRAM authentication (use DEFAULT ) - When using PLAIN , ensure mongodb_authentication_database is set to $external mongodb_authentication_database \u00a4 MongoDB database used for authentication. Optional Environment Variable: MONGODB_AUTHENTICATION_DATABASE Default: admin Purpose : Specifies which MongoDB database contains the user credentials for authentication. When to use : Use admin for standard MongoDB authentication, or $external for LDAP authentication. Valid values : - admin - Standard MongoDB authentication database - $external - External authentication (LDAP) - Custom database name if users are stored elsewhere Impact : Must match where the user credentials are stored in MongoDB. Incorrect value will cause authentication failures. Related variables : mongodb_authentication_mechanism (must be PLAIN when using $external ) Notes : - Default admin is correct for most deployments - Must be $external when mongodb_authentication_mechanism is PLAIN - For Amazon DocumentDB, use admin - Rarely needs to be changed from default unless using custom user databases mongodb_hosts \u00a4 MongoDB connection endpoints including hostnames and ports. Required Environment Variable: MONGODB_HOSTS Default: None Purpose : Defines the MongoDB server addresses that MAS will connect to, supporting both single-node and replica set deployments. When to use : Always required. Provide all replica set members for high availability deployments. Valid values : Comma-separated list of hostname:port pairs. Examples: - Single node: mongodb.example.com:27017 - Replica set: mongo-1.example.com:27017,mongo-2.example.com:27017,mongo-3.example.com:27017 - Amazon DocumentDB: docdb-cluster.abc123.us-east-1.docdb.amazonaws.com:27017 Impact : MAS will attempt to connect to these hosts in order. For replica sets, providing all members ensures high availability and automatic failover. Related variables : mongodb_retry_writes , mongodb_ca_pem_local_file Notes : - Always include port numbers (typically 27017) - For replica sets, list all members for redundancy - For Amazon DocumentDB, use the cluster endpoint or individual instance endpoints - Ensure hostnames are resolvable from the OpenShift cluster - For cloud databases, verify network connectivity and firewall rules mongodb_retry_writes \u00a4 Enable MongoDB retryable writes feature. Optional Environment Variable: MONGODB_RETRY_WRITES Default: true Purpose : Controls whether MongoDB driver will automatically retry write operations that fail due to transient network errors or replica set elections. When to use : Set to true for MongoDB 3.6+ with replica sets. Set to false for Amazon DocumentDB or standalone MongoDB instances. Valid values : - true - Enable retryable writes (recommended for MongoDB replica sets) - false - Disable retryable writes (required for Amazon DocumentDB) Impact : When enabled, improves reliability by automatically retrying failed writes. Amazon DocumentDB does not support this feature and will fail if enabled. Related variables : mongodb_hosts Notes : - Critical : Must be false for Amazon DocumentDB - Recommended to be true for MongoDB replica sets (3.6+) - Standalone MongoDB instances may not support retryable writes - Improves application resilience in replica set environments mongodb_ca_pem_local_file \u00a4 Local file path to the MongoDB SSL/TLS CA certificate in PEM format. Required Environment Variable: MONGODB_CA_PEM_LOCAL_FILE Default: None Purpose : Provides the SSL/TLS certificate authority certificate for secure MongoDB connections. The certificate is embedded in the generated MongoCfg resource. When to use : Always required for production deployments. MongoDB connections should always use SSL/TLS encryption. Valid values : Absolute or relative file path to a valid PEM-encoded CA certificate file (e.g., /tmp/mongo-ca.pem , ./certs/mongodb-cert.pem ). Impact : Without the correct CA certificate, SSL/TLS connections will fail with certificate validation errors. MAS will not be able to connect to MongoDB. Related variables : mongodb_hosts Notes : - The file must be accessible from the Ansible controller - For Amazon DocumentDB, download the CA certificate from AWS - For IBM Cloud Databases, download from the IBM Cloud console - Certificate must be in PEM format (Base64-encoded with BEGIN/END markers) - For self-signed certificates, this should be the CA that signed the MongoDB server certificate mas_instance_id \u00a4 MAS instance identifier for which the MongoDB configuration is being generated. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies the target MAS instance where this MongoDB configuration will be applied. When to use : Always required. Must match an existing MAS instance ID. Valid values : Valid MAS instance ID (typically 3-12 lowercase alphanumeric characters). Impact : The generated configuration will be namespace-scoped to mas-<instance-id>-core . Incorrect instance ID will cause the configuration to be created in the wrong namespace. Related variables : mas_config_dir Notes : - Must match the instance ID used during MAS installation - Case-sensitive value - Cannot be changed after MAS installation - MongoDB is the system database for MAS core services mas_config_dir \u00a4 Local directory path where the generated YAML configuration file will be saved. Required Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the output directory for the generated MongoCfg YAML file that can be applied to the MAS instance. When to use : Always required. Directory must exist or be creatable by the Ansible user. Valid values : Valid directory path (absolute or relative) where the Ansible controller has write permissions. Impact : The generated YAML file will be created in this directory and can be applied using kubectl apply or the suite_config role. Related variables : mas_instance_id , mongodb_namespace Notes : - Directory will be created if it doesn't exist - Generated filename format: mongo-<mongodb_namespace>.yml - Keep generated files for documentation and disaster recovery purposes - Can be used as input to the suite_config role custom_labels \u00a4 Custom Kubernetes labels to apply to the generated MongoCfg resource. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds custom labels to the MongoCfg resource for organization, filtering, or automation purposes. When to use : Use when you need to tag configurations for specific purposes like environment identification, cost tracking, or automated management. Valid values : Comma-separated key=value pairs (e.g., env=prod,team=platform,cost-center=12345 ). Impact : Labels are applied to the Kubernetes resource and can be used for selection, filtering, and automation workflows. Related variables : None Notes : - Labels must follow Kubernetes naming conventions - Useful for GitOps workflows and resource management - Can be used with label selectors in automation scripts Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mongodb_namespace: mongoce mongodb_admin_username: mongoadmin mongodb_admin_password: mongo-strong-password mongodb_hosts: docdb-1.abc.ca-central-1.docdb.amazonaws.com:27017,docdb-2.def.ca-central-1.docdb.amazonaws.com:27017 mongodb_retry_writes: false mongodb_ca_pem_local_file: /tmp/mongo-ca.pem mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.gencfg_mongo License \u00a4 EPL-2.0","title":"gencfg_mongo"},{"location":"roles/gencfg_mongo/#gencfg_mongo","text":"Generate MongoDB configuration files for connecting Maximo Application Suite to external MongoDB or MongoDB-compatible databases. This role creates MongoCfg custom resources that can be applied to MAS instances to configure the system database connectivity. The role supports various MongoDB deployments including MongoDB Community Edition, MongoDB Enterprise, IBM Cloud Databases for MongoDB, and Amazon DocumentDB, with flexible authentication mechanisms and SSL/TLS support.","title":"gencfg_mongo"},{"location":"roles/gencfg_mongo/#role-variables","text":"","title":"Role Variables"},{"location":"roles/gencfg_mongo/#mongodb_namespace","text":"Namespace identifier used in the generated configuration filename. Optional Environment Variable: MONGODB_NAMESPACE Default: mongoce Purpose : Provides a suffix for the generated configuration filename to help identify the MongoDB deployment source or environment. When to use : Customize when managing multiple MongoDB configurations or to clearly identify the MongoDB deployment type (e.g., mongoce , docdb , ibmcloud ). Valid values : Alphanumeric string, typically lowercase (e.g., mongoce , docdb , prod-mongo , dev-mongodb ). Impact : Only affects the generated filename ( mongo-<namespace>.yml ). Does not impact the actual MongoDB connection or MAS configuration. Related variables : mas_config_dir Notes : - Default mongoce indicates MongoDB Community Edition - Use descriptive names when managing multiple MongoDB instances - Filename helps with organization but doesn't affect functionality","title":"mongodb_namespace"},{"location":"roles/gencfg_mongo/#mongodb_admin_username","text":"MongoDB administrative username for authentication. Required Environment Variable: MONGODB_ADMIN_USERNAME Default: None Purpose : Specifies the MongoDB user account that MAS will use to connect to the MongoDB database for system data storage. When to use : Always required. Must be a valid MongoDB user with appropriate permissions for MAS operations. Valid values : Valid MongoDB username according to the authentication mechanism being used. Impact : This user must have read/write access to the MAS databases. Insufficient permissions will cause MAS core services to fail. Related variables : mongodb_admin_password , mongodb_authentication_mechanism , mongodb_authentication_database Notes : - For SCRAM authentication: Standard MongoDB username - For LDAP authentication: LDAP distinguished name or username - User must have readWrite role on MAS databases - Consider using a dedicated service account rather than the MongoDB admin user","title":"mongodb_admin_username"},{"location":"roles/gencfg_mongo/#mongodb_admin_password","text":"MongoDB password for authentication. Required Environment Variable: MONGODB_ADMIN_PASSWORD Default: None Purpose : Provides the password for the MongoDB user account specified in mongodb_admin_username . When to use : Always required for MongoDB authentication. Valid values : Valid password string meeting your MongoDB security requirements. Impact : Stored as a Kubernetes secret in the MAS namespace. Incorrect password will prevent MAS from connecting to MongoDB. Related variables : mongodb_admin_username , mongodb_authentication_mechanism Notes : - Password is stored securely in Kubernetes secrets - Use strong passwords meeting your organization's security policies - For Amazon DocumentDB, use the master user password or IAM authentication - Consider implementing password rotation policies","title":"mongodb_admin_password"},{"location":"roles/gencfg_mongo/#mongodb_authentication_mechanism","text":"MongoDB authentication method to use for connections. Optional Environment Variable: MONGODB_AUTHENTICATION_MECHANISM Default: DEFAULT Purpose : Specifies the authentication mechanism for connecting to MongoDB, supporting both standard MongoDB authentication and LDAP integration. When to use : Set to DEFAULT for standard MongoDB authentication (SCRAM-SHA-256/SCRAM-SHA-1), or PLAIN for LDAP authentication. Valid values : - DEFAULT - Use MongoDB's default SCRAM authentication (SCRAM-SHA-256 or SCRAM-SHA-1) - PLAIN - Use LDAP authentication (requires LDAP-enabled MongoDB) Impact : Must match the authentication configuration of your MongoDB deployment. Mismatch will cause authentication failures. Related variables : mongodb_authentication_database (must be $external when using PLAIN ) Notes : - DEFAULT is suitable for most MongoDB deployments - PLAIN requires MongoDB Enterprise with LDAP integration - Amazon DocumentDB uses SCRAM authentication (use DEFAULT ) - When using PLAIN , ensure mongodb_authentication_database is set to $external","title":"mongodb_authentication_mechanism"},{"location":"roles/gencfg_mongo/#mongodb_authentication_database","text":"MongoDB database used for authentication. Optional Environment Variable: MONGODB_AUTHENTICATION_DATABASE Default: admin Purpose : Specifies which MongoDB database contains the user credentials for authentication. When to use : Use admin for standard MongoDB authentication, or $external for LDAP authentication. Valid values : - admin - Standard MongoDB authentication database - $external - External authentication (LDAP) - Custom database name if users are stored elsewhere Impact : Must match where the user credentials are stored in MongoDB. Incorrect value will cause authentication failures. Related variables : mongodb_authentication_mechanism (must be PLAIN when using $external ) Notes : - Default admin is correct for most deployments - Must be $external when mongodb_authentication_mechanism is PLAIN - For Amazon DocumentDB, use admin - Rarely needs to be changed from default unless using custom user databases","title":"mongodb_authentication_database"},{"location":"roles/gencfg_mongo/#mongodb_hosts","text":"MongoDB connection endpoints including hostnames and ports. Required Environment Variable: MONGODB_HOSTS Default: None Purpose : Defines the MongoDB server addresses that MAS will connect to, supporting both single-node and replica set deployments. When to use : Always required. Provide all replica set members for high availability deployments. Valid values : Comma-separated list of hostname:port pairs. Examples: - Single node: mongodb.example.com:27017 - Replica set: mongo-1.example.com:27017,mongo-2.example.com:27017,mongo-3.example.com:27017 - Amazon DocumentDB: docdb-cluster.abc123.us-east-1.docdb.amazonaws.com:27017 Impact : MAS will attempt to connect to these hosts in order. For replica sets, providing all members ensures high availability and automatic failover. Related variables : mongodb_retry_writes , mongodb_ca_pem_local_file Notes : - Always include port numbers (typically 27017) - For replica sets, list all members for redundancy - For Amazon DocumentDB, use the cluster endpoint or individual instance endpoints - Ensure hostnames are resolvable from the OpenShift cluster - For cloud databases, verify network connectivity and firewall rules","title":"mongodb_hosts"},{"location":"roles/gencfg_mongo/#mongodb_retry_writes","text":"Enable MongoDB retryable writes feature. Optional Environment Variable: MONGODB_RETRY_WRITES Default: true Purpose : Controls whether MongoDB driver will automatically retry write operations that fail due to transient network errors or replica set elections. When to use : Set to true for MongoDB 3.6+ with replica sets. Set to false for Amazon DocumentDB or standalone MongoDB instances. Valid values : - true - Enable retryable writes (recommended for MongoDB replica sets) - false - Disable retryable writes (required for Amazon DocumentDB) Impact : When enabled, improves reliability by automatically retrying failed writes. Amazon DocumentDB does not support this feature and will fail if enabled. Related variables : mongodb_hosts Notes : - Critical : Must be false for Amazon DocumentDB - Recommended to be true for MongoDB replica sets (3.6+) - Standalone MongoDB instances may not support retryable writes - Improves application resilience in replica set environments","title":"mongodb_retry_writes"},{"location":"roles/gencfg_mongo/#mongodb_ca_pem_local_file","text":"Local file path to the MongoDB SSL/TLS CA certificate in PEM format. Required Environment Variable: MONGODB_CA_PEM_LOCAL_FILE Default: None Purpose : Provides the SSL/TLS certificate authority certificate for secure MongoDB connections. The certificate is embedded in the generated MongoCfg resource. When to use : Always required for production deployments. MongoDB connections should always use SSL/TLS encryption. Valid values : Absolute or relative file path to a valid PEM-encoded CA certificate file (e.g., /tmp/mongo-ca.pem , ./certs/mongodb-cert.pem ). Impact : Without the correct CA certificate, SSL/TLS connections will fail with certificate validation errors. MAS will not be able to connect to MongoDB. Related variables : mongodb_hosts Notes : - The file must be accessible from the Ansible controller - For Amazon DocumentDB, download the CA certificate from AWS - For IBM Cloud Databases, download from the IBM Cloud console - Certificate must be in PEM format (Base64-encoded with BEGIN/END markers) - For self-signed certificates, this should be the CA that signed the MongoDB server certificate","title":"mongodb_ca_pem_local_file"},{"location":"roles/gencfg_mongo/#mas_instance_id","text":"MAS instance identifier for which the MongoDB configuration is being generated. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies the target MAS instance where this MongoDB configuration will be applied. When to use : Always required. Must match an existing MAS instance ID. Valid values : Valid MAS instance ID (typically 3-12 lowercase alphanumeric characters). Impact : The generated configuration will be namespace-scoped to mas-<instance-id>-core . Incorrect instance ID will cause the configuration to be created in the wrong namespace. Related variables : mas_config_dir Notes : - Must match the instance ID used during MAS installation - Case-sensitive value - Cannot be changed after MAS installation - MongoDB is the system database for MAS core services","title":"mas_instance_id"},{"location":"roles/gencfg_mongo/#mas_config_dir","text":"Local directory path where the generated YAML configuration file will be saved. Required Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the output directory for the generated MongoCfg YAML file that can be applied to the MAS instance. When to use : Always required. Directory must exist or be creatable by the Ansible user. Valid values : Valid directory path (absolute or relative) where the Ansible controller has write permissions. Impact : The generated YAML file will be created in this directory and can be applied using kubectl apply or the suite_config role. Related variables : mas_instance_id , mongodb_namespace Notes : - Directory will be created if it doesn't exist - Generated filename format: mongo-<mongodb_namespace>.yml - Keep generated files for documentation and disaster recovery purposes - Can be used as input to the suite_config role","title":"mas_config_dir"},{"location":"roles/gencfg_mongo/#custom_labels","text":"Custom Kubernetes labels to apply to the generated MongoCfg resource. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds custom labels to the MongoCfg resource for organization, filtering, or automation purposes. When to use : Use when you need to tag configurations for specific purposes like environment identification, cost tracking, or automated management. Valid values : Comma-separated key=value pairs (e.g., env=prod,team=platform,cost-center=12345 ). Impact : Labels are applied to the Kubernetes resource and can be used for selection, filtering, and automation workflows. Related variables : None Notes : - Labels must follow Kubernetes naming conventions - Useful for GitOps workflows and resource management - Can be used with label selectors in automation scripts","title":"custom_labels"},{"location":"roles/gencfg_mongo/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mongodb_namespace: mongoce mongodb_admin_username: mongoadmin mongodb_admin_password: mongo-strong-password mongodb_hosts: docdb-1.abc.ca-central-1.docdb.amazonaws.com:27017,docdb-2.def.ca-central-1.docdb.amazonaws.com:27017 mongodb_retry_writes: false mongodb_ca_pem_local_file: /tmp/mongo-ca.pem mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.gencfg_mongo","title":"Example Playbook"},{"location":"roles/gencfg_mongo/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_watsonstudio/","text":"gencfg_watsonstudio \u00a4 Generate Watson Studio configuration files for connecting Maximo Application Suite to IBM Cloud Pak for Data Watson Studio instances. This role creates WatsonStudioCfg custom resources that enable MAS applications (particularly Predict and Health) to leverage Watson Studio's machine learning and analytics capabilities. The role supports flexible configuration scoping to make Watson Studio available at system, workspace, application, or workspace-application levels within MAS. Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier for which the Watson Studio configuration is being generated. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies the target MAS instance where this Watson Studio configuration will be applied. When to use : Always required when generating Watson Studio configuration. Must match an existing MAS instance ID. Valid values : Valid MAS instance ID (typically 3-12 lowercase alphanumeric characters). Impact : The generated configuration will be namespace-scoped to mas-<instance-id>-core . Incorrect instance ID will cause the configuration to be created in the wrong namespace. Related variables : mas_config_dir , mas_config_scope Notes : - Must match the instance ID used during MAS installation - Case-sensitive value - Watson Studio integration is primarily used by Predict and Health applications - Required for AI/ML model training and deployment in MAS mas_workspace_id \u00a4 MAS workspace identifier for workspace-scoped configurations. Required if mas_config_scope is ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Specifies the target workspace when generating workspace-scoped or workspace-application-scoped Watson Studio configurations. When to use : Required when mas_config_scope is set to ws or wsapp . Not used for system or app scopes. Valid values : Valid MAS workspace ID (typically lowercase alphanumeric, e.g., masdev , prod , test ). Impact : The Watson Studio configuration will only be available to the specified workspace. Applications in other workspaces cannot access this Watson Studio instance. Related variables : mas_config_scope , mas_application_id Notes : - Workspace must exist before applying the configuration - Use workspace-scoped configs for tenant isolation in multi-tenant deployments - Each workspace can have its own Watson Studio instance for data isolation mas_config_scope \u00a4 Configuration scope level for the generated Watson Studio configuration. Optional Environment Variable: MAS_CONFIG_SCOPE Default: system Purpose : Determines at what level the Watson Studio configuration will be available within MAS - system-wide, workspace-specific, application-specific, or workspace-application combination. When to use : Set based on how Watson Studio will be shared across MAS components. Default system scope makes it available to all workspaces and applications. Valid values : - system - Available to all workspaces and applications (default) - ws - Available only to a specific workspace - app - Available only to a specific application across all workspaces - wsapp - Available only to a specific application in a specific workspace Impact : Determines which MAS components can access this Watson Studio configuration. Incorrect scope may prevent applications from finding the Watson Studio connection. Related variables : mas_workspace_id (required for ws and wsapp ), mas_application_id (required for app and wsapp ) Notes : - Default system scope is suitable for most deployments - Use wsapp scope for dedicated Watson Studio instances per workspace-application - Scope cannot be changed after creation; requires recreation of the configuration - Predict and Health are the primary consumers of Watson Studio mas_config_dir \u00a4 Local directory path where the generated YAML configuration file will be saved. Required Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the output directory for the generated WatsonStudioCfg YAML file that can be applied to the MAS instance. When to use : Always required. Directory must exist or be creatable by the Ansible user. Valid values : Valid directory path (absolute or relative) where the Ansible controller has write permissions. Impact : The generated YAML file will be created in this directory and can be applied using kubectl apply or the suite_config role. Related variables : mas_instance_id Notes : - Directory will be created if it doesn't exist - Generated filename format: watsonstudiocfg-<scope>.yml - Keep generated files for documentation and disaster recovery purposes - Can be used as input to the suite_config role mas_application_id \u00a4 MAS application identifier for application-scoped configurations. Required if mas_config_scope is app or wsapp Environment Variable: MAS_APP_ID Default: None Purpose : Specifies the target application when generating application-scoped or workspace-application-scoped Watson Studio configurations. When to use : Required when mas_config_scope is set to app or wsapp . Not used for system or ws scopes. Valid values : Valid MAS application ID, typically predict or health (the primary Watson Studio consumers). Impact : The Watson Studio configuration will only be available to the specified application. Other applications cannot access this Watson Studio instance. Related variables : mas_config_scope , mas_workspace_id Notes : - Application must be installed before applying the configuration - Predict and Health are the main applications that use Watson Studio - Use application-scoped configs when different applications need different Watson Studio instances cpd_admin_username \u00a4 Cloud Pak for Data administrative username for Watson Studio access. Required Environment Variable: CPD_ADMIN_USERNAME Default: None Purpose : Specifies the Cloud Pak for Data user account that MAS will use to connect to Watson Studio for AI/ML operations. When to use : Always required. Must be a valid CP4D user with appropriate Watson Studio permissions. Valid values : Valid Cloud Pak for Data username with Watson Studio access rights. Impact : This user must have permissions to create and manage Watson Studio projects, deployments, and models. Insufficient permissions will cause Predict/Health AI features to fail. Related variables : cpd_admin_password , cpd_admin_url Notes : - User must have Watson Studio service access - Requires permissions to create projects and deploy models - Consider using a dedicated service account rather than a personal admin account - User should have appropriate resource quotas for ML workloads cpd_admin_password \u00a4 Cloud Pak for Data password for authentication. Required Environment Variable: CPD_ADMIN_PASSWORD Default: None Purpose : Provides the password for the Cloud Pak for Data user account specified in cpd_admin_username . When to use : Always required for CP4D authentication. Valid values : Valid password string meeting Cloud Pak for Data security requirements. Impact : Stored as a Kubernetes secret in the MAS namespace. Incorrect password will prevent MAS from connecting to Watson Studio. Related variables : cpd_admin_username , cpd_admin_url Notes : - Password is stored securely in Kubernetes secrets - Use strong passwords meeting your organization's security policies - Consider implementing password rotation policies - Ensure password doesn't expire or require periodic changes that could break integration cpd_admin_url \u00a4 Cloud Pak for Data console URL for Watson Studio access. Required Environment Variable: CPD_ADMIN_URL Default: None Purpose : Defines the Cloud Pak for Data web console URL that MAS will use to connect to Watson Studio services. When to use : Always required. Must be the accessible URL to the CP4D console. Valid values : Valid HTTPS URL to the Cloud Pak for Data console (e.g., https://cpd-cpd.apps.ocp.example.com ). Impact : MAS applications will use this URL to access Watson Studio APIs. Incorrect or inaccessible URL will prevent AI/ML functionality. Related variables : cpd_admin_username , cpd_admin_password Notes : - Must be the full HTTPS URL including protocol - URL must be accessible from the OpenShift cluster where MAS is running - Verify network connectivity and firewall rules - For multi-cluster deployments, ensure proper routing is configured - URL should point to the CP4D console route, not individual service routes custom_labels \u00a4 Custom Kubernetes labels to apply to the generated WatsonStudioCfg resource. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds custom labels to the WatsonStudioCfg resource for organization, filtering, or automation purposes. When to use : Use when you need to tag configurations for specific purposes like environment identification, cost tracking, or automated management. Valid values : Comma-separated key=value pairs (e.g., env=prod,team=ai-ml,cost-center=12345 ). Impact : Labels are applied to the Kubernetes resource and can be used for selection, filtering, and automation workflows. Related variables : None Notes : - Labels must follow Kubernetes naming conventions - Useful for GitOps workflows and resource management - Can be used with label selectors in automation scripts - Helpful for tracking Watson Studio configurations across multiple environments Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_watsonstudio License \u00a4 EPL-2.0","title":"gencfg_watsonstudio"},{"location":"roles/gencfg_watsonstudio/#gencfg_watsonstudio","text":"Generate Watson Studio configuration files for connecting Maximo Application Suite to IBM Cloud Pak for Data Watson Studio instances. This role creates WatsonStudioCfg custom resources that enable MAS applications (particularly Predict and Health) to leverage Watson Studio's machine learning and analytics capabilities. The role supports flexible configuration scoping to make Watson Studio available at system, workspace, application, or workspace-application levels within MAS.","title":"gencfg_watsonstudio"},{"location":"roles/gencfg_watsonstudio/#role-variables","text":"","title":"Role Variables"},{"location":"roles/gencfg_watsonstudio/#mas_instance_id","text":"MAS instance identifier for which the Watson Studio configuration is being generated. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies the target MAS instance where this Watson Studio configuration will be applied. When to use : Always required when generating Watson Studio configuration. Must match an existing MAS instance ID. Valid values : Valid MAS instance ID (typically 3-12 lowercase alphanumeric characters). Impact : The generated configuration will be namespace-scoped to mas-<instance-id>-core . Incorrect instance ID will cause the configuration to be created in the wrong namespace. Related variables : mas_config_dir , mas_config_scope Notes : - Must match the instance ID used during MAS installation - Case-sensitive value - Watson Studio integration is primarily used by Predict and Health applications - Required for AI/ML model training and deployment in MAS","title":"mas_instance_id"},{"location":"roles/gencfg_watsonstudio/#mas_workspace_id","text":"MAS workspace identifier for workspace-scoped configurations. Required if mas_config_scope is ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Specifies the target workspace when generating workspace-scoped or workspace-application-scoped Watson Studio configurations. When to use : Required when mas_config_scope is set to ws or wsapp . Not used for system or app scopes. Valid values : Valid MAS workspace ID (typically lowercase alphanumeric, e.g., masdev , prod , test ). Impact : The Watson Studio configuration will only be available to the specified workspace. Applications in other workspaces cannot access this Watson Studio instance. Related variables : mas_config_scope , mas_application_id Notes : - Workspace must exist before applying the configuration - Use workspace-scoped configs for tenant isolation in multi-tenant deployments - Each workspace can have its own Watson Studio instance for data isolation","title":"mas_workspace_id"},{"location":"roles/gencfg_watsonstudio/#mas_config_scope","text":"Configuration scope level for the generated Watson Studio configuration. Optional Environment Variable: MAS_CONFIG_SCOPE Default: system Purpose : Determines at what level the Watson Studio configuration will be available within MAS - system-wide, workspace-specific, application-specific, or workspace-application combination. When to use : Set based on how Watson Studio will be shared across MAS components. Default system scope makes it available to all workspaces and applications. Valid values : - system - Available to all workspaces and applications (default) - ws - Available only to a specific workspace - app - Available only to a specific application across all workspaces - wsapp - Available only to a specific application in a specific workspace Impact : Determines which MAS components can access this Watson Studio configuration. Incorrect scope may prevent applications from finding the Watson Studio connection. Related variables : mas_workspace_id (required for ws and wsapp ), mas_application_id (required for app and wsapp ) Notes : - Default system scope is suitable for most deployments - Use wsapp scope for dedicated Watson Studio instances per workspace-application - Scope cannot be changed after creation; requires recreation of the configuration - Predict and Health are the primary consumers of Watson Studio","title":"mas_config_scope"},{"location":"roles/gencfg_watsonstudio/#mas_config_dir","text":"Local directory path where the generated YAML configuration file will be saved. Required Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the output directory for the generated WatsonStudioCfg YAML file that can be applied to the MAS instance. When to use : Always required. Directory must exist or be creatable by the Ansible user. Valid values : Valid directory path (absolute or relative) where the Ansible controller has write permissions. Impact : The generated YAML file will be created in this directory and can be applied using kubectl apply or the suite_config role. Related variables : mas_instance_id Notes : - Directory will be created if it doesn't exist - Generated filename format: watsonstudiocfg-<scope>.yml - Keep generated files for documentation and disaster recovery purposes - Can be used as input to the suite_config role","title":"mas_config_dir"},{"location":"roles/gencfg_watsonstudio/#mas_application_id","text":"MAS application identifier for application-scoped configurations. Required if mas_config_scope is app or wsapp Environment Variable: MAS_APP_ID Default: None Purpose : Specifies the target application when generating application-scoped or workspace-application-scoped Watson Studio configurations. When to use : Required when mas_config_scope is set to app or wsapp . Not used for system or ws scopes. Valid values : Valid MAS application ID, typically predict or health (the primary Watson Studio consumers). Impact : The Watson Studio configuration will only be available to the specified application. Other applications cannot access this Watson Studio instance. Related variables : mas_config_scope , mas_workspace_id Notes : - Application must be installed before applying the configuration - Predict and Health are the main applications that use Watson Studio - Use application-scoped configs when different applications need different Watson Studio instances","title":"mas_application_id"},{"location":"roles/gencfg_watsonstudio/#cpd_admin_username","text":"Cloud Pak for Data administrative username for Watson Studio access. Required Environment Variable: CPD_ADMIN_USERNAME Default: None Purpose : Specifies the Cloud Pak for Data user account that MAS will use to connect to Watson Studio for AI/ML operations. When to use : Always required. Must be a valid CP4D user with appropriate Watson Studio permissions. Valid values : Valid Cloud Pak for Data username with Watson Studio access rights. Impact : This user must have permissions to create and manage Watson Studio projects, deployments, and models. Insufficient permissions will cause Predict/Health AI features to fail. Related variables : cpd_admin_password , cpd_admin_url Notes : - User must have Watson Studio service access - Requires permissions to create projects and deploy models - Consider using a dedicated service account rather than a personal admin account - User should have appropriate resource quotas for ML workloads","title":"cpd_admin_username"},{"location":"roles/gencfg_watsonstudio/#cpd_admin_password","text":"Cloud Pak for Data password for authentication. Required Environment Variable: CPD_ADMIN_PASSWORD Default: None Purpose : Provides the password for the Cloud Pak for Data user account specified in cpd_admin_username . When to use : Always required for CP4D authentication. Valid values : Valid password string meeting Cloud Pak for Data security requirements. Impact : Stored as a Kubernetes secret in the MAS namespace. Incorrect password will prevent MAS from connecting to Watson Studio. Related variables : cpd_admin_username , cpd_admin_url Notes : - Password is stored securely in Kubernetes secrets - Use strong passwords meeting your organization's security policies - Consider implementing password rotation policies - Ensure password doesn't expire or require periodic changes that could break integration","title":"cpd_admin_password"},{"location":"roles/gencfg_watsonstudio/#cpd_admin_url","text":"Cloud Pak for Data console URL for Watson Studio access. Required Environment Variable: CPD_ADMIN_URL Default: None Purpose : Defines the Cloud Pak for Data web console URL that MAS will use to connect to Watson Studio services. When to use : Always required. Must be the accessible URL to the CP4D console. Valid values : Valid HTTPS URL to the Cloud Pak for Data console (e.g., https://cpd-cpd.apps.ocp.example.com ). Impact : MAS applications will use this URL to access Watson Studio APIs. Incorrect or inaccessible URL will prevent AI/ML functionality. Related variables : cpd_admin_username , cpd_admin_password Notes : - Must be the full HTTPS URL including protocol - URL must be accessible from the OpenShift cluster where MAS is running - Verify network connectivity and firewall rules - For multi-cluster deployments, ensure proper routing is configured - URL should point to the CP4D console route, not individual service routes","title":"cpd_admin_url"},{"location":"roles/gencfg_watsonstudio/#custom_labels","text":"Custom Kubernetes labels to apply to the generated WatsonStudioCfg resource. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds custom labels to the WatsonStudioCfg resource for organization, filtering, or automation purposes. When to use : Use when you need to tag configurations for specific purposes like environment identification, cost tracking, or automated management. Valid values : Comma-separated key=value pairs (e.g., env=prod,team=ai-ml,cost-center=12345 ). Impact : Labels are applied to the Kubernetes resource and can be used for selection, filtering, and automation workflows. Related variables : None Notes : - Labels must follow Kubernetes naming conventions - Useful for GitOps workflows and resource management - Can be used with label selectors in automation scripts - Helpful for tracking Watson Studio configurations across multiple environments","title":"custom_labels"},{"location":"roles/gencfg_watsonstudio/#example-playbook","text":"- hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_watsonstudio","title":"Example Playbook"},{"location":"roles/gencfg_watsonstudio/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_workspace/","text":"gencfg_workspace \u00a4 This role generates a Workspace custom resource configuration file for Maximo Application Suite. The generated configuration can be applied manually or automatically using the suite_config role. The configuration file is saved to local disk in the directory specified by mas_config_dir . Tip Workspaces are logical containers in MAS that isolate data and configurations for different business units, environments, or use cases. Each workspace can have its own applications, users, and data. Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier for the workspace. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance the workspace configuration will be generated for. The workspace will be associated with this instance. When to use : - Always required for workspace configuration generation - Must match the instance ID from MAS installation - Used to create instance-specific workspace configurations Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance the workspace belongs to. The generated configuration file will be named and structured for this specific instance. Related variables : - mas_workspace_id : Workspace identifier within this instance - mas_config_dir : Directory where instance-specific config is saved Note : Multiple workspaces can exist within a single MAS instance, each with its own ID and configuration. mas_workspace_id \u00a4 Workspace identifier. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Defines the unique identifier for the workspace within the MAS instance. This ID is used in URLs, API calls, and resource names. When to use : - Always required for workspace configuration - Must be unique within the MAS instance - Used as the technical identifier for the workspace Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , masdev , qa ) Impact : Determines the workspace's technical identifier. This ID appears in URLs (e.g., https://instance.mas.com/masdev ) and Kubernetes resource names. Related variables : - mas_instance_id : Parent instance for this workspace - mas_workspace_name : Human-readable display name Note : Choose a meaningful ID that reflects the workspace purpose (e.g., prod , dev , qa ). The ID cannot be changed after workspace creation. mas_workspace_name \u00a4 Workspace display name. Required Environment Variable: MAS_WORKSPACE_NAME Default: None Purpose : Defines the human-readable display name for the workspace shown in the MAS user interface. When to use : - Always required for workspace configuration - Should be descriptive and user-friendly - Displayed in MAS UI and documentation Valid values : Any string, typically 3-50 characters (e.g., Production , Development , MAS Development , QA Environment ) Impact : Determines how the workspace appears to users in the MAS interface. Unlike the workspace ID, this name can include spaces and special characters. Related variables : - mas_workspace_id : Technical identifier for this workspace - mas_instance_id : Parent instance Note : Use a clear, descriptive name that helps users identify the workspace purpose. This name can be changed after creation if needed. mas_config_dir \u00a4 Configuration output directory. Required Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the local directory path where the generated workspace configuration file will be saved. When to use : - Always required for configuration generation - Should be a writable directory path - Typically organized by instance ID Valid values : Valid local filesystem path (e.g., /home/user/masconfig , ~/masconfig/inst1 ) Impact : Determines where the workspace configuration YAML file is written. The file can then be applied manually with oc apply or automatically with the suite_config role. Related variables : - mas_instance_id : Used to organize configs by instance - mas_workspace_id : Used in the generated filename Note : The directory will be created if it doesn't exist. Organize configs by instance ID for clarity (e.g., /masconfig/inst1/ , /masconfig/inst2/ ). custom_labels \u00a4 Custom Kubernetes labels for workspace resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds custom labels to workspace-specific Kubernetes resources for organization, tracking, or automation purposes. When to use : - Optional for most deployments - Use for resource organization and filtering - Helpful for cost tracking, ownership, or automation - Common in multi-tenant or managed environments Valid values : Comma-separated key=value pairs (e.g., env=prod,team=operations,cost-center=12345 ) Impact : Adds the specified labels to workspace resources. Labels can be used for: - Resource filtering and selection - Cost allocation and tracking - Automation and policy enforcement - Organizational categorization Related variables : - mas_workspace_id : Workspace these labels apply to Note : Labels must follow Kubernetes naming conventions (alphanumeric, hyphens, dots, max 63 chars per segment). Common uses include environment tags, team ownership, and cost center tracking. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_workspace_id: \"masdev\" mas_workspace_name: \"MAS Development\" mas_config_dir: \"/home/david/masconfig/inst1\" roles: - ibm.mas_devops.gencfg_workspace License \u00a4 EPL-2.0","title":"gencfg_workspace"},{"location":"roles/gencfg_workspace/#gencfg_workspace","text":"This role generates a Workspace custom resource configuration file for Maximo Application Suite. The generated configuration can be applied manually or automatically using the suite_config role. The configuration file is saved to local disk in the directory specified by mas_config_dir . Tip Workspaces are logical containers in MAS that isolate data and configurations for different business units, environments, or use cases. Each workspace can have its own applications, users, and data.","title":"gencfg_workspace"},{"location":"roles/gencfg_workspace/#role-variables","text":"","title":"Role Variables"},{"location":"roles/gencfg_workspace/#mas_instance_id","text":"MAS instance identifier for the workspace. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance the workspace configuration will be generated for. The workspace will be associated with this instance. When to use : - Always required for workspace configuration generation - Must match the instance ID from MAS installation - Used to create instance-specific workspace configurations Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance the workspace belongs to. The generated configuration file will be named and structured for this specific instance. Related variables : - mas_workspace_id : Workspace identifier within this instance - mas_config_dir : Directory where instance-specific config is saved Note : Multiple workspaces can exist within a single MAS instance, each with its own ID and configuration.","title":"mas_instance_id"},{"location":"roles/gencfg_workspace/#mas_workspace_id","text":"Workspace identifier. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Defines the unique identifier for the workspace within the MAS instance. This ID is used in URLs, API calls, and resource names. When to use : - Always required for workspace configuration - Must be unique within the MAS instance - Used as the technical identifier for the workspace Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , masdev , qa ) Impact : Determines the workspace's technical identifier. This ID appears in URLs (e.g., https://instance.mas.com/masdev ) and Kubernetes resource names. Related variables : - mas_instance_id : Parent instance for this workspace - mas_workspace_name : Human-readable display name Note : Choose a meaningful ID that reflects the workspace purpose (e.g., prod , dev , qa ). The ID cannot be changed after workspace creation.","title":"mas_workspace_id"},{"location":"roles/gencfg_workspace/#mas_workspace_name","text":"Workspace display name. Required Environment Variable: MAS_WORKSPACE_NAME Default: None Purpose : Defines the human-readable display name for the workspace shown in the MAS user interface. When to use : - Always required for workspace configuration - Should be descriptive and user-friendly - Displayed in MAS UI and documentation Valid values : Any string, typically 3-50 characters (e.g., Production , Development , MAS Development , QA Environment ) Impact : Determines how the workspace appears to users in the MAS interface. Unlike the workspace ID, this name can include spaces and special characters. Related variables : - mas_workspace_id : Technical identifier for this workspace - mas_instance_id : Parent instance Note : Use a clear, descriptive name that helps users identify the workspace purpose. This name can be changed after creation if needed.","title":"mas_workspace_name"},{"location":"roles/gencfg_workspace/#mas_config_dir","text":"Configuration output directory. Required Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the local directory path where the generated workspace configuration file will be saved. When to use : - Always required for configuration generation - Should be a writable directory path - Typically organized by instance ID Valid values : Valid local filesystem path (e.g., /home/user/masconfig , ~/masconfig/inst1 ) Impact : Determines where the workspace configuration YAML file is written. The file can then be applied manually with oc apply or automatically with the suite_config role. Related variables : - mas_instance_id : Used to organize configs by instance - mas_workspace_id : Used in the generated filename Note : The directory will be created if it doesn't exist. Organize configs by instance ID for clarity (e.g., /masconfig/inst1/ , /masconfig/inst2/ ).","title":"mas_config_dir"},{"location":"roles/gencfg_workspace/#custom_labels","text":"Custom Kubernetes labels for workspace resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds custom labels to workspace-specific Kubernetes resources for organization, tracking, or automation purposes. When to use : - Optional for most deployments - Use for resource organization and filtering - Helpful for cost tracking, ownership, or automation - Common in multi-tenant or managed environments Valid values : Comma-separated key=value pairs (e.g., env=prod,team=operations,cost-center=12345 ) Impact : Adds the specified labels to workspace resources. Labels can be used for: - Resource filtering and selection - Cost allocation and tracking - Automation and policy enforcement - Organizational categorization Related variables : - mas_workspace_id : Workspace these labels apply to Note : Labels must follow Kubernetes naming conventions (alphanumeric, hyphens, dots, max 63 chars per segment). Common uses include environment tags, team ownership, and cost center tracking.","title":"custom_labels"},{"location":"roles/gencfg_workspace/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_workspace_id: \"masdev\" mas_workspace_name: \"MAS Development\" mas_config_dir: \"/home/david/masconfig/inst1\" roles: - ibm.mas_devops.gencfg_workspace","title":"Example Playbook"},{"location":"roles/gencfg_workspace/#license","text":"EPL-2.0","title":"License"},{"location":"roles/grafana/","text":"grafana \u00a4 Installs and configures an instance of Grafana for use with IBM Maximo Application Suite, using the community grafana operator Note The credentials for the grafana admin user are stored in grafana-admin-credentials secret in the grafana namespace. A route is created in the grafana namespace to allow access to the grafana UI. Role Variables \u00a4 grafana_action \u00a4 Action to perform on Grafana installation. Optional Environment Variable: GRAFANA_ACTION Default: install Purpose : Specifies whether to install, uninstall, or upgrade Grafana for use with MAS monitoring. When to use : - Use install (default) for new Grafana deployments - Use update to upgrade from Grafana v4 to v5 - Use uninstall to remove Grafana Valid values : install , uninstall , update Impact : - install : Deploys Grafana operator and instance - update : Upgrades from v4 to v5 (creates new instance with new URL) - uninstall : Removes Grafana operator and instance Related variables : - grafana_major_version : Version to install/upgrade to - grafana_v4_namespace / grafana_v5_namespace : Namespaces for different versions Note : IMPORTANT - When upgrading from v4 to v5, the new instance will have a different URL and will NOT inherit the user database. Admin password will be reset, and user accounts must be recreated. The v4 instance remains until manually removed. grafana_major_version \u00a4 Grafana operator major version. Optional Environment Variable: GRAFANA_MAJOR_VERSION Default: 5 Purpose : Specifies which major version of the Grafana operator to install. When to use : - Use default ( 5 ) for new installations (recommended) - Set to 4 only for legacy deployments - Version 5 is the current supported version Valid values : 4 , 5 Impact : Determines which Grafana operator version is deployed. Version 5 uses a different namespace and has breaking changes from version 4. Related variables : - grafana_action : Use update to upgrade from v4 to v5 - grafana_v4_namespace : Namespace for v4 installation - grafana_v5_namespace : Namespace for v5 installation Note : Version 5 is recommended for all new deployments. Upgrading from v4 to v5 requires using grafana_action=update and results in a new instance with different URL and no user database migration. grafana_v4_namespace \u00a4 Namespace for Grafana v4 installation. Optional Environment Variable: GRAFANA_NAMESPACE Default: grafana Purpose : Specifies the Kubernetes namespace where Grafana operator v4 and instance are installed. When to use : - Only applies when grafana_major_version=4 - Use default ( grafana ) for standard deployments - Override only if namespace conflicts exist Valid values : Valid Kubernetes namespace name Impact : Determines where Grafana v4 resources are created. Namespace must not conflict with v5 installation. Related variables : - grafana_major_version : Must be 4 for this to apply - grafana_v5_namespace : Separate namespace for v5 Note : When upgrading from v4 to v5, both namespaces coexist. The v4 instance remains in this namespace until manually removed. grafana_v5_namespace \u00a4 Namespace for Grafana v5 installation. Optional Environment Variable: GRAFANA_V5_NAMESPACE Default: grafana5 Purpose : Specifies the Kubernetes namespace where Grafana operator v5 and instance are installed. When to use : - Applies when grafana_major_version=5 (default) - Use default ( grafana5 ) for standard deployments - Override only if namespace conflicts exist Valid values : Valid Kubernetes namespace name Impact : Determines where Grafana v5 resources are created. Uses separate namespace from v4 to allow coexistence during upgrades. Related variables : - grafana_major_version : Must be 5 for this to apply - grafana_v4_namespace : Separate namespace for v4 Note : The separate namespace allows v4 and v5 to coexist during upgrades. After upgrading, the v4 instance remains in its namespace until manually removed. grafana_instance_storage_class \u00a4 Storage class for Grafana user data. Required (if supported storage class not available) Environment Variable: GRAFANA_INSTANCE_STORAGE_CLASS Default: Auto-detected from ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium Purpose : Specifies the storage class for Grafana's persistent volume that stores user data, dashboards, and configurations. When to use : - Leave unset to auto-detect from supported storage classes - Set explicitly if none of the default classes are available - Required if cluster has no supported storage classes Valid values : Any storage class supporting ReadWriteOnce (RWO) access mode. ReadWriteMany (RWX) classes also work. Impact : Determines where Grafana user data is stored. Incorrect or unavailable storage class will cause deployment to fail. Related variables : - grafana_instance_storage_size : Size of the storage volume Note : RWO access mode is sufficient for Grafana. RWX classes (like ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium ) also work but are not required. The role auto-detects these common classes if available. grafana_instance_storage_size \u00a4 Storage volume size for Grafana user data. Optional Environment Variable: GRAFANA_INSTANCE_STORAGE_SIZE Default: 10Gi Purpose : Specifies the size of the persistent volume used to store Grafana user data, dashboards, and configurations. When to use : - Use default ( 10Gi ) for most deployments - Increase for environments with many dashboards or extensive data - Consider growth over time for dashboard storage Valid values : Kubernetes storage size format (e.g., 10Gi , 20Gi , 50Gi ) Impact : Determines available storage for Grafana data. Insufficient size may limit dashboard creation. Excessive size wastes storage resources. Related variables : - grafana_instance_storage_class : Storage class for the volume Note : The default 10Gi is sufficient for typical MAS monitoring deployments. When upgrading from v4 to v5, the new instance inherits the v4 storage size unless explicitly overridden. Example Playbook \u00a4 - hosts: localhost vars: grafana_instance_storage_class: \"ibmc-file-gold-gid\" grafana_instance_storage_class: \"15Gi\" roles: - ibm.mas_devops.grafana To Upgrade from Grafana Operator from V4 to V5 - hosts: localhost vars: grafana_action: \"update\" roles: - ibm.mas_devops.grafana Note note that the upgraded v5 grafana inherits the storage class and size from the v4 configuration unless they are defined as environment variables. License \u00a4 EPL-2.0","title":"grafana"},{"location":"roles/grafana/#grafana","text":"Installs and configures an instance of Grafana for use with IBM Maximo Application Suite, using the community grafana operator Note The credentials for the grafana admin user are stored in grafana-admin-credentials secret in the grafana namespace. A route is created in the grafana namespace to allow access to the grafana UI.","title":"grafana"},{"location":"roles/grafana/#role-variables","text":"","title":"Role Variables"},{"location":"roles/grafana/#grafana_action","text":"Action to perform on Grafana installation. Optional Environment Variable: GRAFANA_ACTION Default: install Purpose : Specifies whether to install, uninstall, or upgrade Grafana for use with MAS monitoring. When to use : - Use install (default) for new Grafana deployments - Use update to upgrade from Grafana v4 to v5 - Use uninstall to remove Grafana Valid values : install , uninstall , update Impact : - install : Deploys Grafana operator and instance - update : Upgrades from v4 to v5 (creates new instance with new URL) - uninstall : Removes Grafana operator and instance Related variables : - grafana_major_version : Version to install/upgrade to - grafana_v4_namespace / grafana_v5_namespace : Namespaces for different versions Note : IMPORTANT - When upgrading from v4 to v5, the new instance will have a different URL and will NOT inherit the user database. Admin password will be reset, and user accounts must be recreated. The v4 instance remains until manually removed.","title":"grafana_action"},{"location":"roles/grafana/#grafana_major_version","text":"Grafana operator major version. Optional Environment Variable: GRAFANA_MAJOR_VERSION Default: 5 Purpose : Specifies which major version of the Grafana operator to install. When to use : - Use default ( 5 ) for new installations (recommended) - Set to 4 only for legacy deployments - Version 5 is the current supported version Valid values : 4 , 5 Impact : Determines which Grafana operator version is deployed. Version 5 uses a different namespace and has breaking changes from version 4. Related variables : - grafana_action : Use update to upgrade from v4 to v5 - grafana_v4_namespace : Namespace for v4 installation - grafana_v5_namespace : Namespace for v5 installation Note : Version 5 is recommended for all new deployments. Upgrading from v4 to v5 requires using grafana_action=update and results in a new instance with different URL and no user database migration.","title":"grafana_major_version"},{"location":"roles/grafana/#grafana_v4_namespace","text":"Namespace for Grafana v4 installation. Optional Environment Variable: GRAFANA_NAMESPACE Default: grafana Purpose : Specifies the Kubernetes namespace where Grafana operator v4 and instance are installed. When to use : - Only applies when grafana_major_version=4 - Use default ( grafana ) for standard deployments - Override only if namespace conflicts exist Valid values : Valid Kubernetes namespace name Impact : Determines where Grafana v4 resources are created. Namespace must not conflict with v5 installation. Related variables : - grafana_major_version : Must be 4 for this to apply - grafana_v5_namespace : Separate namespace for v5 Note : When upgrading from v4 to v5, both namespaces coexist. The v4 instance remains in this namespace until manually removed.","title":"grafana_v4_namespace"},{"location":"roles/grafana/#grafana_v5_namespace","text":"Namespace for Grafana v5 installation. Optional Environment Variable: GRAFANA_V5_NAMESPACE Default: grafana5 Purpose : Specifies the Kubernetes namespace where Grafana operator v5 and instance are installed. When to use : - Applies when grafana_major_version=5 (default) - Use default ( grafana5 ) for standard deployments - Override only if namespace conflicts exist Valid values : Valid Kubernetes namespace name Impact : Determines where Grafana v5 resources are created. Uses separate namespace from v4 to allow coexistence during upgrades. Related variables : - grafana_major_version : Must be 5 for this to apply - grafana_v4_namespace : Separate namespace for v4 Note : The separate namespace allows v4 and v5 to coexist during upgrades. After upgrading, the v4 instance remains in its namespace until manually removed.","title":"grafana_v5_namespace"},{"location":"roles/grafana/#grafana_instance_storage_class","text":"Storage class for Grafana user data. Required (if supported storage class not available) Environment Variable: GRAFANA_INSTANCE_STORAGE_CLASS Default: Auto-detected from ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium Purpose : Specifies the storage class for Grafana's persistent volume that stores user data, dashboards, and configurations. When to use : - Leave unset to auto-detect from supported storage classes - Set explicitly if none of the default classes are available - Required if cluster has no supported storage classes Valid values : Any storage class supporting ReadWriteOnce (RWO) access mode. ReadWriteMany (RWX) classes also work. Impact : Determines where Grafana user data is stored. Incorrect or unavailable storage class will cause deployment to fail. Related variables : - grafana_instance_storage_size : Size of the storage volume Note : RWO access mode is sufficient for Grafana. RWX classes (like ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium ) also work but are not required. The role auto-detects these common classes if available.","title":"grafana_instance_storage_class"},{"location":"roles/grafana/#grafana_instance_storage_size","text":"Storage volume size for Grafana user data. Optional Environment Variable: GRAFANA_INSTANCE_STORAGE_SIZE Default: 10Gi Purpose : Specifies the size of the persistent volume used to store Grafana user data, dashboards, and configurations. When to use : - Use default ( 10Gi ) for most deployments - Increase for environments with many dashboards or extensive data - Consider growth over time for dashboard storage Valid values : Kubernetes storage size format (e.g., 10Gi , 20Gi , 50Gi ) Impact : Determines available storage for Grafana data. Insufficient size may limit dashboard creation. Excessive size wastes storage resources. Related variables : - grafana_instance_storage_class : Storage class for the volume Note : The default 10Gi is sufficient for typical MAS monitoring deployments. When upgrading from v4 to v5, the new instance inherits the v4 storage size unless explicitly overridden.","title":"grafana_instance_storage_size"},{"location":"roles/grafana/#example-playbook","text":"- hosts: localhost vars: grafana_instance_storage_class: \"ibmc-file-gold-gid\" grafana_instance_storage_class: \"15Gi\" roles: - ibm.mas_devops.grafana To Upgrade from Grafana Operator from V4 to V5 - hosts: localhost vars: grafana_action: \"update\" roles: - ibm.mas_devops.grafana Note note that the upgraded v5 grafana inherits the storage class and size from the v4 configuration unless they are defined as environment variables.","title":"Example Playbook"},{"location":"roles/grafana/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ibm_catalogs/","text":"ibm_catalogs \u00a4 This role installs the IBM Maximo Operator Catalog , which is a curated Operator Catalog derived from the IBM Operator Catalog , with all content certified compatible with IBM Maximo Application Suite: Additional, for IBM employees only, the pre-release development operator catalog can be installed, this is achieved by setting both the artifactory_username and artifactory_token variables. Role Variables \u00a4 General Variables \u00a4 mas_catalog_version \u00a4 Version of the IBM Maximo Operator Catalog to install. Optional Environment Variable: MAS_CATALOG_VERSION Default Value: @@MAS_LATEST_CATALOG@@ (latest stable version) Purpose : Specifies which version of the IBM Maximo Operator Catalog to install. The catalog provides certified operators compatible with MAS, including MAS Core, applications, and dependencies. When to use : - Leave as default to install the latest stable catalog version (recommended) - Set explicitly when you need a specific catalog version - Set to match your MAS version requirements - Use specific version for reproducible deployments Valid values : Valid catalog version string (e.g., v8-240625-amd64 , v9-250115-amd64 ) Impact : Determines which operator versions are available for installation. The catalog version must be compatible with your target MAS version. Using an incompatible catalog version may prevent MAS installation or upgrades. Related variables : The catalog version affects which MAS and application versions can be installed. Note : The default value is automatically updated to the latest stable catalog version. For production deployments, consider pinning to a specific version for consistency and reproducibility. Development Variables \u00a4 artifactory_username \u00a4 Artifactory username for accessing pre-release development catalogs (IBM employees only). Optional Environment Variable: ARTIFACTORY_USERNAME Default Value: None Purpose : Provides authentication to IBM Artifactory for installing development catalog sources containing pre-release MAS operators. This enables testing of upcoming MAS versions before general availability. When to use : - Only for IBM employees with Artifactory access - Only for development/testing of pre-release MAS versions - Must be set together with artifactory_token - Never use in production environments Valid values : Valid IBM Artifactory username Impact : When set with artifactory_token , enables installation of development catalog sources. Without both credentials, only production catalogs are available. Related variables : - artifactory_token : Required together with this username - Both must be set to enable development catalog access Note : IBM EMPLOYEES ONLY - This is for pre-release testing only. Never use development catalogs in production. Keep credentials secure and do not commit to source control. artifactory_token \u00a4 Artifactory API token for accessing pre-release development catalogs (IBM employees only). Optional Environment Variable: ARTIFACTORY_TOKEN Default Value: None Purpose : Provides API token authentication to IBM Artifactory for installing development catalog sources containing pre-release MAS operators. This enables testing of upcoming MAS versions before general availability. When to use : - Only for IBM employees with Artifactory access - Only for development/testing of pre-release MAS versions - Must be set together with artifactory_username - Never use in production environments Valid values : Valid IBM Artifactory API token string Impact : When set with artifactory_username , enables installation of development catalog sources. Without both credentials, only production catalogs are available. Related variables : - artifactory_username : Required together with this token - Both must be set to enable development catalog access Note : IBM EMPLOYEES ONLY - This is for pre-release testing only. Never use development catalogs in production. Keep this token secure and do not commit to source control. Generate tokens from IBM Artifactory. Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost roles: - ibm.mas_devops.ibm_catalogs Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=ibm_catalogs ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"ibm_catalogs"},{"location":"roles/ibm_catalogs/#ibm_catalogs","text":"This role installs the IBM Maximo Operator Catalog , which is a curated Operator Catalog derived from the IBM Operator Catalog , with all content certified compatible with IBM Maximo Application Suite: Additional, for IBM employees only, the pre-release development operator catalog can be installed, this is achieved by setting both the artifactory_username and artifactory_token variables.","title":"ibm_catalogs"},{"location":"roles/ibm_catalogs/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ibm_catalogs/#general-variables","text":"","title":"General Variables"},{"location":"roles/ibm_catalogs/#mas_catalog_version","text":"Version of the IBM Maximo Operator Catalog to install. Optional Environment Variable: MAS_CATALOG_VERSION Default Value: @@MAS_LATEST_CATALOG@@ (latest stable version) Purpose : Specifies which version of the IBM Maximo Operator Catalog to install. The catalog provides certified operators compatible with MAS, including MAS Core, applications, and dependencies. When to use : - Leave as default to install the latest stable catalog version (recommended) - Set explicitly when you need a specific catalog version - Set to match your MAS version requirements - Use specific version for reproducible deployments Valid values : Valid catalog version string (e.g., v8-240625-amd64 , v9-250115-amd64 ) Impact : Determines which operator versions are available for installation. The catalog version must be compatible with your target MAS version. Using an incompatible catalog version may prevent MAS installation or upgrades. Related variables : The catalog version affects which MAS and application versions can be installed. Note : The default value is automatically updated to the latest stable catalog version. For production deployments, consider pinning to a specific version for consistency and reproducibility.","title":"mas_catalog_version"},{"location":"roles/ibm_catalogs/#development-variables","text":"","title":"Development Variables"},{"location":"roles/ibm_catalogs/#artifactory_username","text":"Artifactory username for accessing pre-release development catalogs (IBM employees only). Optional Environment Variable: ARTIFACTORY_USERNAME Default Value: None Purpose : Provides authentication to IBM Artifactory for installing development catalog sources containing pre-release MAS operators. This enables testing of upcoming MAS versions before general availability. When to use : - Only for IBM employees with Artifactory access - Only for development/testing of pre-release MAS versions - Must be set together with artifactory_token - Never use in production environments Valid values : Valid IBM Artifactory username Impact : When set with artifactory_token , enables installation of development catalog sources. Without both credentials, only production catalogs are available. Related variables : - artifactory_token : Required together with this username - Both must be set to enable development catalog access Note : IBM EMPLOYEES ONLY - This is for pre-release testing only. Never use development catalogs in production. Keep credentials secure and do not commit to source control.","title":"artifactory_username"},{"location":"roles/ibm_catalogs/#artifactory_token","text":"Artifactory API token for accessing pre-release development catalogs (IBM employees only). Optional Environment Variable: ARTIFACTORY_TOKEN Default Value: None Purpose : Provides API token authentication to IBM Artifactory for installing development catalog sources containing pre-release MAS operators. This enables testing of upcoming MAS versions before general availability. When to use : - Only for IBM employees with Artifactory access - Only for development/testing of pre-release MAS versions - Must be set together with artifactory_username - Never use in production environments Valid values : Valid IBM Artifactory API token string Impact : When set with artifactory_username , enables installation of development catalog sources. Without both credentials, only production catalogs are available. Related variables : - artifactory_username : Required together with this token - Both must be set to enable development catalog access Note : IBM EMPLOYEES ONLY - This is for pre-release testing only. Never use development catalogs in production. Keep this token secure and do not commit to source control. Generate tokens from IBM Artifactory.","title":"artifactory_token"},{"location":"roles/ibm_catalogs/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost roles: - ibm.mas_devops.ibm_catalogs","title":"Example Playbook"},{"location":"roles/ibm_catalogs/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=ibm_catalogs ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/ibm_catalogs/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ibmcloud_resource_key/","text":"ibmcloud_resource_key \u00a4 Create IBM Cloud resource keys (apikeys for specific services associated to the account) Role Variables \u00a4 service_instance \u00a4 The name of the IBM Cloud service instance for which to create or manage resource keys. Required Environment Variable: SERVICE_INSTANCE Default Value: None Purpose : Identifies the IBM Cloud service instance (such as Db2, MongoDB, Cloud Object Storage, or Cloud Pak for Data) for which a resource key (service credentials) will be created or deleted. When to use : Always required when creating or deleting service keys. The service instance must already exist in your IBM Cloud account. Valid values : - Any valid IBM Cloud service instance name in your account - Common examples: mas-db2-instance , mas-mongodb , mas-cos-bucket , cp4d-instance - Must match the exact name of an existing service instance - Case-sensitive Impact : - Determines which service instance the resource key will provide access to - The generated credentials will be scoped to this specific service instance - Different service types (Db2, COS, etc.) will generate different credential formats - Used to derive the default resource key name if not explicitly specified Related variables : - service_resource_key_name - Name of the key to create (defaults to <service_instance>_resource-key ) - output_service_key_details_to_file - Whether to save credentials to file Notes : - Ensure the service instance exists before running this role - You must have appropriate IAM permissions to create resource keys for the service - Resource keys provide full access to the service instance - protect them carefully - Each service instance can have multiple resource keys for different purposes service_resource_key_name \u00a4 The name to assign to the resource key (service credentials). Optional Environment Variable: SERVICE_RESOURCE_KEY_NAME Default Value: <service_instance>_resource-key (derived from service_instance) Purpose : Specifies a custom name for the resource key. If not provided, the role automatically generates a name by appending _resource-key to the service instance name. When to use : - When you need multiple resource keys for the same service instance - When you want a more descriptive name than the default - When following specific naming conventions in your organization Valid values : - Any string that follows IBM Cloud resource naming conventions - Alphanumeric characters, hyphens, and underscores - Should be unique within the service instance - Recommended format: <purpose>-<service>-key (e.g., mas-db2-key , backup-cos-key ) Impact : - Used to identify the resource key in IBM Cloud console and CLI - Required when deleting a specific resource key - Appears in the output filename if output_service_key_details_to_file is enabled - Cannot be changed after creation (requires deletion and recreation) Related variables : - service_instance - The service for which this key is created - delete_service_key - Uses this name to identify which key to delete - output_service_key_details_to_file - Uses this name in the output filename Notes : - If not specified, defaults to <service_instance>_resource-key - Use descriptive names to identify the purpose of each key - Keep track of key names for future deletion or rotation - Multiple keys can exist for the same service instance with different names delete_service_key \u00a4 Controls whether to delete the specified resource key instead of creating it. Optional Environment Variable: DELETE_SERVICE_KEY Default Value: False Purpose : When set to True , the role will delete the specified resource key instead of creating a new one. This is used for credential rotation or cleanup operations. When to use : - When rotating service credentials and removing old keys - During cleanup or deprovisioning operations - When a resource key is no longer needed - Before recreating a key with the same name Valid values : - True - Delete the resource key specified by service_resource_key_name - False - Create a new resource key (default behavior) Impact : - When True : Permanently deletes the specified resource key and its credentials - Applications using the deleted credentials will immediately lose access - Deletion is irreversible - credentials cannot be recovered - Does not affect the service instance itself, only the access credentials Related variables : - service_resource_key_name - Identifies which key to delete - service_instance - The service instance containing the key Notes : - Ensure no applications are using the credentials before deletion - Consider creating a new key before deleting the old one for zero-downtime rotation - Deletion will fail if the key doesn't exist (this is not an error condition) - Always verify the key name before deletion to avoid removing the wrong credentials output_service_key_details_to_file \u00a4 Controls whether to save the resource key credentials to a JSON file. Optional Environment Variable: OUTPUT_SERVICE_KEY_DETAILS_TO_FILE Default Value: False Purpose : When set to True , the role will export the complete resource key details and credentials to a JSON file for later use or reference. When to use : - When you need to store credentials for manual configuration - For backup or documentation purposes - When integrating with other automation tools that consume JSON credentials - During initial setup to review credential structure Valid values : - True - Export credentials to JSON file - False - Do not create output file (default) Impact : - When True : Creates a JSON file named service-key_<service_resource_key_name>.json - File contains complete credential details including connection strings, passwords, and API keys - File is created in the directory specified by output_dir - Credentials in the file are sensitive and should be protected Related variables : - output_dir - Directory where the JSON file will be created - service_resource_key_name - Used in the output filename Notes : - The output file contains sensitive credentials - protect it appropriately - Do not commit credential files to version control - Consider encrypting the output file or storing it in a secrets manager - The JSON structure varies by service type (Db2, COS, MongoDB, etc.) - Useful for troubleshooting connection issues or manual configuration output_dir \u00a4 The directory path where the resource key JSON file will be saved. Optional Environment Variable: OUTPUT_DIR Default Value: . (current directory, typically ibm/mas_devops ) Purpose : Specifies the local filesystem directory where the resource key credentials JSON file will be written when output_service_key_details_to_file is enabled. When to use : Set this when you want to save credentials to a specific location other than the current directory, such as a secure credentials directory or a mounted volume. Valid values : - Any valid local filesystem path (e.g., ~/credentials , /opt/mas/keys , ./config ) - Can be relative or absolute path - Directory will be created if it doesn't exist - Must have write permissions Impact : - Determines where the service-key_<service_resource_key_name>.json file is created - Only used when output_service_key_details_to_file is True - Default of . places the file in the Ansible collection directory Related variables : - output_service_key_details_to_file - Must be True for this to take effect - service_resource_key_name - Used in the output filename Notes : - Ensure the directory has appropriate permissions for writing - Consider using a dedicated credentials directory with restricted access - The default . location is relative to where Ansible is executed - For production use, specify an absolute path to avoid confusion - Back up this directory if it contains important credentials Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: ibmcloud_apikey: xxx service_instance: xxx service_resource_key_name: xxx output_service_key_details_to_file: True OR False delete_service_key: True OR False roles: - ibmcloud_resource_key License \u00a4 EPL-2.0","title":"ibmcloud_resource_key"},{"location":"roles/ibmcloud_resource_key/#ibmcloud_resource_key","text":"Create IBM Cloud resource keys (apikeys for specific services associated to the account)","title":"ibmcloud_resource_key"},{"location":"roles/ibmcloud_resource_key/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ibmcloud_resource_key/#service_instance","text":"The name of the IBM Cloud service instance for which to create or manage resource keys. Required Environment Variable: SERVICE_INSTANCE Default Value: None Purpose : Identifies the IBM Cloud service instance (such as Db2, MongoDB, Cloud Object Storage, or Cloud Pak for Data) for which a resource key (service credentials) will be created or deleted. When to use : Always required when creating or deleting service keys. The service instance must already exist in your IBM Cloud account. Valid values : - Any valid IBM Cloud service instance name in your account - Common examples: mas-db2-instance , mas-mongodb , mas-cos-bucket , cp4d-instance - Must match the exact name of an existing service instance - Case-sensitive Impact : - Determines which service instance the resource key will provide access to - The generated credentials will be scoped to this specific service instance - Different service types (Db2, COS, etc.) will generate different credential formats - Used to derive the default resource key name if not explicitly specified Related variables : - service_resource_key_name - Name of the key to create (defaults to <service_instance>_resource-key ) - output_service_key_details_to_file - Whether to save credentials to file Notes : - Ensure the service instance exists before running this role - You must have appropriate IAM permissions to create resource keys for the service - Resource keys provide full access to the service instance - protect them carefully - Each service instance can have multiple resource keys for different purposes","title":"service_instance"},{"location":"roles/ibmcloud_resource_key/#service_resource_key_name","text":"The name to assign to the resource key (service credentials). Optional Environment Variable: SERVICE_RESOURCE_KEY_NAME Default Value: <service_instance>_resource-key (derived from service_instance) Purpose : Specifies a custom name for the resource key. If not provided, the role automatically generates a name by appending _resource-key to the service instance name. When to use : - When you need multiple resource keys for the same service instance - When you want a more descriptive name than the default - When following specific naming conventions in your organization Valid values : - Any string that follows IBM Cloud resource naming conventions - Alphanumeric characters, hyphens, and underscores - Should be unique within the service instance - Recommended format: <purpose>-<service>-key (e.g., mas-db2-key , backup-cos-key ) Impact : - Used to identify the resource key in IBM Cloud console and CLI - Required when deleting a specific resource key - Appears in the output filename if output_service_key_details_to_file is enabled - Cannot be changed after creation (requires deletion and recreation) Related variables : - service_instance - The service for which this key is created - delete_service_key - Uses this name to identify which key to delete - output_service_key_details_to_file - Uses this name in the output filename Notes : - If not specified, defaults to <service_instance>_resource-key - Use descriptive names to identify the purpose of each key - Keep track of key names for future deletion or rotation - Multiple keys can exist for the same service instance with different names","title":"service_resource_key_name"},{"location":"roles/ibmcloud_resource_key/#delete_service_key","text":"Controls whether to delete the specified resource key instead of creating it. Optional Environment Variable: DELETE_SERVICE_KEY Default Value: False Purpose : When set to True , the role will delete the specified resource key instead of creating a new one. This is used for credential rotation or cleanup operations. When to use : - When rotating service credentials and removing old keys - During cleanup or deprovisioning operations - When a resource key is no longer needed - Before recreating a key with the same name Valid values : - True - Delete the resource key specified by service_resource_key_name - False - Create a new resource key (default behavior) Impact : - When True : Permanently deletes the specified resource key and its credentials - Applications using the deleted credentials will immediately lose access - Deletion is irreversible - credentials cannot be recovered - Does not affect the service instance itself, only the access credentials Related variables : - service_resource_key_name - Identifies which key to delete - service_instance - The service instance containing the key Notes : - Ensure no applications are using the credentials before deletion - Consider creating a new key before deleting the old one for zero-downtime rotation - Deletion will fail if the key doesn't exist (this is not an error condition) - Always verify the key name before deletion to avoid removing the wrong credentials","title":"delete_service_key"},{"location":"roles/ibmcloud_resource_key/#output_service_key_details_to_file","text":"Controls whether to save the resource key credentials to a JSON file. Optional Environment Variable: OUTPUT_SERVICE_KEY_DETAILS_TO_FILE Default Value: False Purpose : When set to True , the role will export the complete resource key details and credentials to a JSON file for later use or reference. When to use : - When you need to store credentials for manual configuration - For backup or documentation purposes - When integrating with other automation tools that consume JSON credentials - During initial setup to review credential structure Valid values : - True - Export credentials to JSON file - False - Do not create output file (default) Impact : - When True : Creates a JSON file named service-key_<service_resource_key_name>.json - File contains complete credential details including connection strings, passwords, and API keys - File is created in the directory specified by output_dir - Credentials in the file are sensitive and should be protected Related variables : - output_dir - Directory where the JSON file will be created - service_resource_key_name - Used in the output filename Notes : - The output file contains sensitive credentials - protect it appropriately - Do not commit credential files to version control - Consider encrypting the output file or storing it in a secrets manager - The JSON structure varies by service type (Db2, COS, MongoDB, etc.) - Useful for troubleshooting connection issues or manual configuration","title":"output_service_key_details_to_file"},{"location":"roles/ibmcloud_resource_key/#output_dir","text":"The directory path where the resource key JSON file will be saved. Optional Environment Variable: OUTPUT_DIR Default Value: . (current directory, typically ibm/mas_devops ) Purpose : Specifies the local filesystem directory where the resource key credentials JSON file will be written when output_service_key_details_to_file is enabled. When to use : Set this when you want to save credentials to a specific location other than the current directory, such as a secure credentials directory or a mounted volume. Valid values : - Any valid local filesystem path (e.g., ~/credentials , /opt/mas/keys , ./config ) - Can be relative or absolute path - Directory will be created if it doesn't exist - Must have write permissions Impact : - Determines where the service-key_<service_resource_key_name>.json file is created - Only used when output_service_key_details_to_file is True - Default of . places the file in the Ansible collection directory Related variables : - output_service_key_details_to_file - Must be True for this to take effect - service_resource_key_name - Used in the output filename Notes : - Ensure the directory has appropriate permissions for writing - Consider using a dedicated credentials directory with restricted access - The default . location is relative to where Ansible is executed - For production use, specify an absolute path to avoid confusion - Back up this directory if it contains important credentials","title":"output_dir"},{"location":"roles/ibmcloud_resource_key/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: ibmcloud_apikey: xxx service_instance: xxx service_resource_key_name: xxx output_service_key_details_to_file: True OR False delete_service_key: True OR False roles: - ibmcloud_resource_key","title":"Example Playbook"},{"location":"roles/ibmcloud_resource_key/#license","text":"EPL-2.0","title":"License"},{"location":"roles/kafka/","text":"kafka \u00a4 This role provides support to install a Kafka Cluster using Strimzi , Red Hat AMQ Streams , IBM Event Streams or AWS MSK and generate configuration that can be directly applied to Maximo Application Suite. Both Strimzi and Red Hat AMQ streams component are massively scalable, distributed, and high-performance data streaming platform based on the Apache Kafka project. Both offer a distributed backbone that allows microservices and other applications to share data with high throughput and low latency. As more applications move to Kubernetes and Red Hat OpenShift, it is increasingly important to be able to run the communication infrastructure on the same platform. Red Hat OpenShift, as a highly scalable platform, is a natural fit for messaging technologies such as Kafka. The AMQ streams component makes running and managing Apache Kafka OpenShift native through the use of powerful operators that simplify the deployment, configuration, management, and use of Apache Kafka on Red Hat OpenShift. The AMQ streams component is part of the Red Hat AMQ family, which also includes the AMQ broker, a longtime innovation leader in Java\u2122 Message Service (JMS) and polyglot messaging, as well as the AMQ interconnect router, a wide-area, peer-to-peer messaging solution. Under the covers, AMQ streams leverages Strimzi's architecture, resources and configurations. Note: The MAS license does not include entitlement for AMQ streams. The MAS Devops Collection supports this Kafka deployment as an example only. Therefore, we recommend the use of Strimzi for an opensource Kafka provider. Tip The role will generate a yaml file containing the definition of a Secret and KafkaCfg resource that can be used to configure the deployed cluster as the MAS system Kafka. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/kafkacfg-amqstreams-system.yaml or used in conjunction with the suite_config role. Role Variables \u00a4 General Variables \u00a4 kafka_action \u00a4 Specifies which operation to perform on the Kafka cluster. Optional Environment Variable: KAFKA_ACTION Default Value: install Purpose : Controls what action the role executes against Kafka deployments. This allows the same role to handle installation, upgrades, and removal of Kafka clusters. When to use : - Use install (default) for initial Kafka deployment - Use upgrade to upgrade existing Kafka cluster (Strimzi and Red Hat AMQ Streams only) - Use uninstall to remove Kafka cluster and operator Valid values : install , upgrade , uninstall Impact : - install : Deploys Kafka operator and creates cluster - upgrade : Upgrades existing Kafka cluster (only for Strimzi and Red Hat providers) - uninstall : Removes Kafka cluster and operator Related variables : - kafka_provider : Upgrade action only supported for strimzi and redhat providers - kafka_version : Target version when upgrading Note : The upgrade action is only available for Strimzi and Red Hat AMQ Streams providers. IBM Event Streams and AWS MSK do not support in-place upgrades through this role. kafka_provider \u00a4 Specifies which Kafka provider to use for deployment. Optional Environment Variable: KAFKA_PROVIDER Default Value: strimzi Purpose : Determines which Kafka implementation to deploy. Different providers offer different features, licensing models, and deployment targets (on-cluster vs cloud-managed). When to use : - Use strimzi (default) for open-source Kafka on OpenShift (recommended, no additional license required) - Use redhat for Red Hat AMQ Streams (requires separate license not included with MAS) - Use ibm for IBM Event Streams on IBM Cloud (managed service, additional cost) - Use aws for AWS MSK (managed service, additional cost) Valid values : strimzi , redhat , ibm , aws Impact : - strimzi : Deploys open-source Kafka using Strimzi operator (no additional license) - redhat : Deploys AMQ Streams (requires Red Hat AMQ license) - ibm : Provisions IBM Event Streams in IBM Cloud (requires IBM Cloud account and incurs costs) - aws : Provisions AWS MSK in AWS account (requires AWS account and incurs costs) Related variables : - Different providers require different additional variables - kafka_action=upgrade only supported for strimzi and redhat Note : IMPORTANT - MAS license does NOT include entitlement for Red Hat AMQ Streams. Strimzi is recommended for open-source Kafka. IBM and AWS providers provision managed cloud services with additional costs. Red Hat AMQ Streams & Strimzi Variables \u00a4 kafka_version \u00a4 Kafka version to deploy (Strimzi and Red Hat AMQ Streams only). Optional Environment Variable: KAFKA_VERSION Default Value: 3.8.0 for AMQ Streams, 3.9.0 for Strimzi Purpose : Specifies which Apache Kafka version to deploy when using Strimzi or Red Hat AMQ Streams providers. The version must be supported by the installed operator. When to use : - Leave as default for standard deployments - Set explicitly when you need a specific Kafka version - Verify version compatibility with operator before changing Valid values : Valid Kafka version supported by the operator (e.g., 3.8.0 , 3.9.0 ) Impact : Determines which Kafka version is deployed. Incompatible versions will cause deployment failures. Related variables : - kafka_provider : Only applies to strimzi and redhat providers - Operator version determines available Kafka versions Note : Before changing this value, verify the version is supported by your AMQ Streams operator or Strimzi operator . This variable does not apply to IBM Event Streams or AWS MSK providers. kafka_namespace \u00a4 OpenShift namespace where Kafka operator and cluster will be deployed. Optional Environment Variable: KAFKA_NAMESPACE Default Value: amq-streams for AMQ Streams, strimzi for Strimzi Purpose : Specifies the namespace for deploying the Kafka operator and Kafka cluster resources. This isolates Kafka resources from other applications. When to use : - Use default for standard single-cluster deployments - Set to custom namespace when organizing multiple Kafka deployments - Must be unique if deploying multiple Kafka clusters Valid values : Any valid Kubernetes namespace name (e.g., strimzi , amq-streams , kafka-prod ) Impact : All Kafka resources (operator, cluster, topics, users) are created in this namespace. Related variables : - kafka_provider : Default namespace depends on provider ( strimzi or amq-streams ) - kafka_cluster_name : Cluster created within this namespace Note : The default namespace differs by provider: strimzi for Strimzi provider, amq-streams for Red Hat AMQ Streams. This variable only applies to Strimzi and Red Hat providers. kafka_cluster_name \u00a4 Name for the Kafka cluster resource. Optional Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka Purpose : Defines the name of the Kafka custom resource that will be created. This name is used to identify the Kafka cluster and is embedded in resource names. When to use : - Use default ( maskafka ) for standard MAS deployments - Set to custom name when deploying multiple Kafka clusters - Use descriptive names for multi-cluster environments Valid values : Valid Kubernetes resource name (lowercase alphanumeric with hyphens, e.g., maskafka , kafka-prod , mas-kafka ) Impact : This name is used throughout the Kafka deployment in resource names (services, pods, secrets). It appears in the generated KafkaCfg for MAS. Related variables : - kafka_namespace : Cluster created within this namespace - Used in generated KafkaCfg when mas_instance_id is provided Note : Choose a meaningful name as it appears in many resource names. The default maskafka is suitable for single-cluster MAS deployments. kafka_cluster_size \u00a4 Predefined configuration size for the Kafka cluster. Optional Environment Variable: KAFKA_CLUSTER_SIZE Default Value: small Purpose : Selects a predefined resource configuration for the Kafka cluster. Different sizes allocate different amounts of CPU, memory, and replicas for Kafka brokers and Zookeeper nodes. When to use : - Use small (default) for development, test, or small production environments - Use large for production environments with higher throughput requirements - Choose based on expected message volume and performance needs Valid values : small , large Impact : - small : Fewer resources, suitable for dev/test or small workloads - large : More resources (CPU, memory, replicas) for production workloads Related variables : - kafka_storage_size : Storage size should align with cluster size - zookeeper_storage_size : Zookeeper storage should align with cluster size Note : The small configuration is suitable for development and testing. Production environments typically require the large configuration for adequate performance and reliability. kafka_storage_class \u00a4 Storage class for Kafka broker persistent storage (must support ReadWriteOnce). Optional Environment Variable: KAFKA_STORAGE_CLASS Default Value: Auto-detected from available storage classes in cluster Purpose : Specifies the storage class for Kafka broker persistent storage, which requires ReadWriteOnce (RWO) access mode. This is where Kafka stores message logs and data. When to use : - Leave unset for automatic detection (recommended) - Set explicitly when you need a specific storage class - Must be a storage class supporting RWO access mode - Typically block-based storage for performance Valid values : Any storage class name supporting ReadWriteOnce access mode Impact : Kafka broker performance depends heavily on storage performance. Choose high-performance storage for production workloads. Incorrect storage class or one not supporting RWO will cause deployment to fail. Related variables : - kafka_storage_size : Size of storage for Kafka brokers - zookeeper_storage_class : Separate storage class for Zookeeper Note : Block-based storage classes typically provide better performance for Kafka (e.g., ibmc-block-gold , ocs-storagecluster-ceph-rbd ). This variable only applies to Strimzi and Red Hat AMQ Streams providers. kafka_storage_size \u00a4 Size of persistent storage for Kafka brokers. Optional Environment Variable: KAFKA_STORAGE_SIZE Default Value: 100Gi Purpose : Specifies the size of persistent volumes for Kafka broker storage. This is where Kafka stores all message logs and topic data. When to use : - Use default ( 100Gi ) for development/test environments - Increase significantly for production based on message volume and retention - Plan for message throughput and retention period - Monitor usage and expand as needed Valid values : Kubernetes storage size format (e.g., 100Gi , 500Gi , 1Ti , 2Ti ) Impact : Insufficient storage will cause Kafka to stop accepting messages when volumes fill up. Size appropriately for your message volume and retention requirements. Related variables : - kafka_storage_class : Storage class for these volumes - kafka_cluster_size : Larger clusters may need more storage - Message retention settings affect storage requirements Note : The default 100Gi is suitable for small deployments only. Production environments typically require 500Gi or more depending on message volume and retention. Plan storage based on daily message volume \u00d7 retention days. zookeeper_storage_class \u00a4 Storage class for Zookeeper persistent storage (must support ReadWriteOnce). Optional Environment Variable: ZOOKEEPER_STORAGE_CLASS Default Value: Auto-detected from available storage classes in cluster Purpose : Specifies the storage class for Zookeeper persistent storage, which requires ReadWriteOnce (RWO) access mode. Zookeeper stores cluster metadata and coordination data. When to use : - Leave unset for automatic detection (recommended) - Set explicitly when you need a specific storage class - Must be a storage class supporting RWO access mode - Can be same as or different from kafka_storage_class Valid values : Any storage class name supporting ReadWriteOnce access mode Impact : Zookeeper is critical for Kafka cluster coordination. Reliable storage is essential. Incorrect storage class or one not supporting RWO will cause deployment to fail. Related variables : - zookeeper_storage_size : Size of storage for Zookeeper - kafka_storage_class : Separate storage class for Kafka brokers Note : Zookeeper storage requirements are typically much smaller than Kafka broker storage. This variable only applies to Strimzi and Red Hat AMQ Streams providers. zookeeper_storage_size \u00a4 Size of persistent storage for Zookeeper nodes. Optional Environment Variable: ZOOKEEPER_STORAGE_SIZE Default Value: 10Gi Purpose : Specifies the size of persistent volumes for Zookeeper storage. Zookeeper stores cluster metadata, configuration, and coordination data. When to use : - Use default ( 10Gi ) for most deployments - Increase only for very large Kafka clusters with many topics/partitions - Zookeeper storage needs are typically much smaller than Kafka broker storage Valid values : Kubernetes storage size format (e.g., 10Gi , 20Gi , 50Gi ) Impact : Insufficient Zookeeper storage can cause cluster coordination issues. However, Zookeeper storage requirements are typically modest. Related variables : - zookeeper_storage_class : Storage class for these volumes - kafka_storage_size : Kafka broker storage (typically much larger) Note : The default 10Gi is sufficient for most deployments. Zookeeper storage requirements grow slowly with cluster size. Only increase if you have a very large number of topics and partitions. kafka_user_name \u00a4 Username for MAS to authenticate with Kafka. Optional Environment Variable: KAFKA_USER_NAME Default Value: masuser Purpose : Defines the Kafka user that will be created for MAS to authenticate with the Kafka cluster. This user is configured with appropriate permissions for MAS operations. When to use : - Use default ( masuser ) for standard MAS deployments - Set to custom name when organizational policies require specific usernames - Must be unique within the Kafka cluster Valid values : Valid Kafka username string (e.g., masuser , mas-kafka-user ) Impact : This user is created in Kafka with necessary permissions and credentials are included in the generated KafkaCfg for MAS. Related variables : - kafka_user_password : Password for this user (Strimzi 0.25.0+/AMQ Streams 2.x+) - Used in generated KafkaCfg when mas_instance_id is provided Note : The default masuser is suitable for most deployments. This variable only applies to Strimzi and Red Hat AMQ Streams providers. kafka_user_password \u00a4 Password for the Kafka user (Strimzi 0.25.0+/AMQ Streams 2.x+). Optional Environment Variable: KAFKA_USER_PASSWORD Default Value: Randomly generated if not specified Purpose : Sets the password for the Kafka user specified in kafka_user_name . This password is used for SCRAM-SHA authentication. When to use : - Leave unset for automatic random password generation (recommended) - Set explicitly when you need a specific password - Requires Strimzi operator 0.25.0+ or AMQ Streams 2.x+ Valid values : Strong password string meeting security requirements Impact : This password is stored in Kafka secrets and included in the generated KafkaCfg for MAS. If not set, a secure random password is generated automatically. Related variables : - kafka_user_name : User for which this password is set - Used in generated KafkaCfg when mas_instance_id is provided Note : Automatic password generation is recommended for security. This feature requires Strimzi operator version 0.25.0 or AMQ Streams 2.x+. Keep passwords secure and do not commit to source control. mas_instance_id \u00a4 MAS instance ID for generating KafkaCfg configuration. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None Purpose : Identifies which MAS instance the generated KafkaCfg will target. When set (along with mas_config_dir ), the role generates a KafkaCfg resource for configuring MAS to use this Kafka cluster. When to use : - Set when you want to generate MAS configuration automatically - Must match the instance ID of your MAS installation - Required together with mas_config_dir for KafkaCfg generation Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : When set with mas_config_dir , generates a KafkaCfg YAML file that can be applied to configure MAS. Without this, no MAS configuration is generated. Related variables : - mas_config_dir : Required together with this for KafkaCfg generation - Generated KafkaCfg targets this MAS instance Note : If either mas_instance_id or mas_config_dir is not set, the role will not generate a KafkaCfg template. You'll need to configure MAS manually. mas_config_dir \u00a4 Local directory path where generated KafkaCfg will be saved. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None Purpose : Specifies where to save the generated KafkaCfg YAML file. This file can be manually applied to configure MAS or used with the suite_config role for automated configuration. When to use : - Set when you want to generate MAS configuration automatically - Use the same directory across all MAS setup roles for consistency - Required together with mas_instance_id for KafkaCfg generation Valid values : Any valid local filesystem path (e.g., /home/user/masconfig , ~/masconfig , ./config ) Impact : When set with mas_instance_id , generates kafkacfg-{provider}-system.yaml in this directory. The file can be applied with oc apply or used with suite_config role. Related variables : - mas_instance_id : Required together with this for KafkaCfg generation - kafka_provider : Affects the generated filename Note : If either mas_instance_id or mas_config_dir is not set, no KafkaCfg template is generated. Ensure the directory exists and is writable. custom_labels \u00a4 Comma-separated list of key=value labels to apply to Kafka resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Purpose : Adds Kubernetes labels to Kafka-related resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,component=kafka ) Impact : Labels are applied to Kafka-related resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect Kafka functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management. Note : This variable applies to Strimzi and Red Hat AMQ Streams providers. Labels help with resource organization and are especially useful in multi-tenant environments. IBM Cloud Evenstreams Role Variables ibmcloud_apikey \u00a4 Defines IBM Cloud API Key. This API Key needs to have access to manage (provision/deprovision) IBM Cloud Event Streams. Required Environment Variable: IBMCLOUD_APIKEY Default Value: None eventstreams_resourcegroup \u00a4 Defines the IBM Cloud Resource Group to target the Event Streams instance. Optional Environment Variable: EVENTSTREAMS_RESOURCEGROUP Default Value: Default or value defined by IBMCLOUD_RESOURCEGROUP eventstreams_name \u00a4 Event Streams instance name. Required Environment Variable: EVENTSTREAMS_NAME Default Value: None eventstreams_plan \u00a4 Event Streams instance plan. Optional Environment Variable: EVENTSTREAMS_PLAN Default Value: standard eventstreams_location \u00a4 Optional Environment Variable: EVENTSTREAMS_LOCATION Default Value: us-east or value defined by IBMCLOUD_REGION eventstreams_retention \u00a4 Event Streams topic retention period (in miliseconds). Optional Environment Variable: EVENTSTREAMS_RETENTION Default Value: 1209600000 eventstreams_create_manage_jms_topic \u00a4 Defines whether to create specific Manage application JMS topics by default. Optional Environment Variable: EVENTSTREAMS_CREATE_MANAGE_JMS_TOPICS Default Value: True mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # Set storage class suitable for use on IBM Cloud ROKS kafka_storage_class: ibmc-block-gold # Generate a KafkaCfg template mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.kafka AWS MSK Variables \u00a4 Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. kafka_version \u00a4 The version of Kafka to deploy for AWS MSK. Environment Variable: KAFKA_VERSION Default Value: 3.3.1 kafka_cluster_name \u00a4 The name of the Kafka cluster that will be created Required Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka aws_region \u00a4 Required Environment Variable: AWS_REGION Default Value: None vpc_id \u00a4 The AWS Virtual Private Cloud identifier (VPC ID) where the MSK instance will be hosted. Required Environment Variable: VPC_ID Default Value: None aws_msk_cidr_az1 \u00a4 The CIDR address for the first Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ1 Default Value: None aws_msk_cidr_az2 \u00a4 The CIDR address for the second Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ2 Default Value: None aws_msk_cidr_az3 \u00a4 The CIDR address for the third Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ3 Default Value: None aws_msk_ingress_cidr \u00a4 The IPv4 CIDR address for ingress connection. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_INGRESS_CIDR Default Value: None aws_msk_egress_cidr \u00a4 The IPv4 CIDR address for egress connection. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_EGRESS_CIDR Default Value: None aws_kafka_user_name \u00a4 The name of the user to setup in the cluster for MAS. Required Environment Variable: AWS_KAFKA_USER_NAME Default Value: None aws_kafka_user_password \u00a4 The password of the user to setup in the cluster for MAS. Optional Environment Variable: AWS_KAFKA_USER_PASSWORD Default Value: None aws_msk_instance_type \u00a4 The type/flavor of your MSK instance. Optional Environment Variable: AWS_MSK_INSTANCE_TYPE Default Value: kafka.m5.large aws_msk_volume_size \u00a4 The storage/volume size of your MSK instance. Optional Environment Variable: AWS_MSK_VOLUME_SIZE Default Value: 100 aws_msk_instance_number \u00a4 The number of broker/instances of your MSK instance. Optional Environment Variable: AWS_MSK_INSTANCE_NUMBER Default Value: 3 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None aws_msk_secret \u00a4 AWS MSK Secret name. The secret name must begin with the prefix AmazonMSK_. If this is not set, then default secret name will be AmazonMSK_SECRET_{{kafka_cluster_name}} Optional Environment Variable: AWS_MSK_SECRET Default Value: AmazonMSK_SECRET_{{kafka_cluster_name}}' Install AWS MSK \u00a4 - hosts: localhost any_errors_fatal: true vars: aws_region: ca-central-1 aws_access_key_id: ***** aws_secret_access_key: ***** kafka_version: 3.3.1 kafka_provider: aws kafka_action: install kafka_cluster_name: msk-abcd0zyxw kafka_namespace: msk-abcd0zyxw vpc_id: vpc-07088da510b3c35c5 aws_kafka_user_name: mskuser-abcd0zyxw aws_msk_instance_type: kafka.t3.small aws_msk_volume_size: 100 aws_msk_instance_number: 3 aws_msk_cidr_az1: \"10.0.128.0/20\" aws_msk_cidr_az2: \"10.0.144.0/20\" aws_msk_cidr_az3: \"10.0.160.0/20\" aws_msk_ingress_cidr: \"10.0.0.0/16\" aws_msk_egress_cidr: \"10.0.0.0/16\" # Generate a KafkaCfg template mas_config_dir: /var/tmp/masconfigdir mas_instance_id: abcd0zyxw roles: - ibm.mas_devops.kafka Uninstall AWS MSK \u00a4 - hosts: localhost any_errors_fatal: true vars: aws_region: ca-central-1 aws_access_key_id: ***** aws_secret_access_key: ***** vpc_id: vpc-07088da510b3c35c5 kafka_provider: aws kafka_action: uninstall kafka_cluster_name: msk-abcd0zyxw aws_msk_cidr_az1: \"10.0.128.0/20\" aws_msk_cidr_az2: \"10.0.144.0/20\" aws_msk_cidr_az3: \"10.0.160.0/20\" roles: - ibm.mas_devops.kafka Run Role Playbook \u00a4 export KAFKA_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=masinst1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"kafka"},{"location":"roles/kafka/#kafka","text":"This role provides support to install a Kafka Cluster using Strimzi , Red Hat AMQ Streams , IBM Event Streams or AWS MSK and generate configuration that can be directly applied to Maximo Application Suite. Both Strimzi and Red Hat AMQ streams component are massively scalable, distributed, and high-performance data streaming platform based on the Apache Kafka project. Both offer a distributed backbone that allows microservices and other applications to share data with high throughput and low latency. As more applications move to Kubernetes and Red Hat OpenShift, it is increasingly important to be able to run the communication infrastructure on the same platform. Red Hat OpenShift, as a highly scalable platform, is a natural fit for messaging technologies such as Kafka. The AMQ streams component makes running and managing Apache Kafka OpenShift native through the use of powerful operators that simplify the deployment, configuration, management, and use of Apache Kafka on Red Hat OpenShift. The AMQ streams component is part of the Red Hat AMQ family, which also includes the AMQ broker, a longtime innovation leader in Java\u2122 Message Service (JMS) and polyglot messaging, as well as the AMQ interconnect router, a wide-area, peer-to-peer messaging solution. Under the covers, AMQ streams leverages Strimzi's architecture, resources and configurations. Note: The MAS license does not include entitlement for AMQ streams. The MAS Devops Collection supports this Kafka deployment as an example only. Therefore, we recommend the use of Strimzi for an opensource Kafka provider. Tip The role will generate a yaml file containing the definition of a Secret and KafkaCfg resource that can be used to configure the deployed cluster as the MAS system Kafka. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/kafkacfg-amqstreams-system.yaml or used in conjunction with the suite_config role.","title":"kafka"},{"location":"roles/kafka/#role-variables","text":"","title":"Role Variables"},{"location":"roles/kafka/#general-variables","text":"","title":"General Variables"},{"location":"roles/kafka/#kafka_action","text":"Specifies which operation to perform on the Kafka cluster. Optional Environment Variable: KAFKA_ACTION Default Value: install Purpose : Controls what action the role executes against Kafka deployments. This allows the same role to handle installation, upgrades, and removal of Kafka clusters. When to use : - Use install (default) for initial Kafka deployment - Use upgrade to upgrade existing Kafka cluster (Strimzi and Red Hat AMQ Streams only) - Use uninstall to remove Kafka cluster and operator Valid values : install , upgrade , uninstall Impact : - install : Deploys Kafka operator and creates cluster - upgrade : Upgrades existing Kafka cluster (only for Strimzi and Red Hat providers) - uninstall : Removes Kafka cluster and operator Related variables : - kafka_provider : Upgrade action only supported for strimzi and redhat providers - kafka_version : Target version when upgrading Note : The upgrade action is only available for Strimzi and Red Hat AMQ Streams providers. IBM Event Streams and AWS MSK do not support in-place upgrades through this role.","title":"kafka_action"},{"location":"roles/kafka/#kafka_provider","text":"Specifies which Kafka provider to use for deployment. Optional Environment Variable: KAFKA_PROVIDER Default Value: strimzi Purpose : Determines which Kafka implementation to deploy. Different providers offer different features, licensing models, and deployment targets (on-cluster vs cloud-managed). When to use : - Use strimzi (default) for open-source Kafka on OpenShift (recommended, no additional license required) - Use redhat for Red Hat AMQ Streams (requires separate license not included with MAS) - Use ibm for IBM Event Streams on IBM Cloud (managed service, additional cost) - Use aws for AWS MSK (managed service, additional cost) Valid values : strimzi , redhat , ibm , aws Impact : - strimzi : Deploys open-source Kafka using Strimzi operator (no additional license) - redhat : Deploys AMQ Streams (requires Red Hat AMQ license) - ibm : Provisions IBM Event Streams in IBM Cloud (requires IBM Cloud account and incurs costs) - aws : Provisions AWS MSK in AWS account (requires AWS account and incurs costs) Related variables : - Different providers require different additional variables - kafka_action=upgrade only supported for strimzi and redhat Note : IMPORTANT - MAS license does NOT include entitlement for Red Hat AMQ Streams. Strimzi is recommended for open-source Kafka. IBM and AWS providers provision managed cloud services with additional costs.","title":"kafka_provider"},{"location":"roles/kafka/#red-hat-amq-streams-strimzi-variables","text":"","title":"Red Hat AMQ Streams &amp; Strimzi Variables"},{"location":"roles/kafka/#kafka_version","text":"Kafka version to deploy (Strimzi and Red Hat AMQ Streams only). Optional Environment Variable: KAFKA_VERSION Default Value: 3.8.0 for AMQ Streams, 3.9.0 for Strimzi Purpose : Specifies which Apache Kafka version to deploy when using Strimzi or Red Hat AMQ Streams providers. The version must be supported by the installed operator. When to use : - Leave as default for standard deployments - Set explicitly when you need a specific Kafka version - Verify version compatibility with operator before changing Valid values : Valid Kafka version supported by the operator (e.g., 3.8.0 , 3.9.0 ) Impact : Determines which Kafka version is deployed. Incompatible versions will cause deployment failures. Related variables : - kafka_provider : Only applies to strimzi and redhat providers - Operator version determines available Kafka versions Note : Before changing this value, verify the version is supported by your AMQ Streams operator or Strimzi operator . This variable does not apply to IBM Event Streams or AWS MSK providers.","title":"kafka_version"},{"location":"roles/kafka/#kafka_namespace","text":"OpenShift namespace where Kafka operator and cluster will be deployed. Optional Environment Variable: KAFKA_NAMESPACE Default Value: amq-streams for AMQ Streams, strimzi for Strimzi Purpose : Specifies the namespace for deploying the Kafka operator and Kafka cluster resources. This isolates Kafka resources from other applications. When to use : - Use default for standard single-cluster deployments - Set to custom namespace when organizing multiple Kafka deployments - Must be unique if deploying multiple Kafka clusters Valid values : Any valid Kubernetes namespace name (e.g., strimzi , amq-streams , kafka-prod ) Impact : All Kafka resources (operator, cluster, topics, users) are created in this namespace. Related variables : - kafka_provider : Default namespace depends on provider ( strimzi or amq-streams ) - kafka_cluster_name : Cluster created within this namespace Note : The default namespace differs by provider: strimzi for Strimzi provider, amq-streams for Red Hat AMQ Streams. This variable only applies to Strimzi and Red Hat providers.","title":"kafka_namespace"},{"location":"roles/kafka/#kafka_cluster_name","text":"Name for the Kafka cluster resource. Optional Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka Purpose : Defines the name of the Kafka custom resource that will be created. This name is used to identify the Kafka cluster and is embedded in resource names. When to use : - Use default ( maskafka ) for standard MAS deployments - Set to custom name when deploying multiple Kafka clusters - Use descriptive names for multi-cluster environments Valid values : Valid Kubernetes resource name (lowercase alphanumeric with hyphens, e.g., maskafka , kafka-prod , mas-kafka ) Impact : This name is used throughout the Kafka deployment in resource names (services, pods, secrets). It appears in the generated KafkaCfg for MAS. Related variables : - kafka_namespace : Cluster created within this namespace - Used in generated KafkaCfg when mas_instance_id is provided Note : Choose a meaningful name as it appears in many resource names. The default maskafka is suitable for single-cluster MAS deployments.","title":"kafka_cluster_name"},{"location":"roles/kafka/#kafka_cluster_size","text":"Predefined configuration size for the Kafka cluster. Optional Environment Variable: KAFKA_CLUSTER_SIZE Default Value: small Purpose : Selects a predefined resource configuration for the Kafka cluster. Different sizes allocate different amounts of CPU, memory, and replicas for Kafka brokers and Zookeeper nodes. When to use : - Use small (default) for development, test, or small production environments - Use large for production environments with higher throughput requirements - Choose based on expected message volume and performance needs Valid values : small , large Impact : - small : Fewer resources, suitable for dev/test or small workloads - large : More resources (CPU, memory, replicas) for production workloads Related variables : - kafka_storage_size : Storage size should align with cluster size - zookeeper_storage_size : Zookeeper storage should align with cluster size Note : The small configuration is suitable for development and testing. Production environments typically require the large configuration for adequate performance and reliability.","title":"kafka_cluster_size"},{"location":"roles/kafka/#kafka_storage_class","text":"Storage class for Kafka broker persistent storage (must support ReadWriteOnce). Optional Environment Variable: KAFKA_STORAGE_CLASS Default Value: Auto-detected from available storage classes in cluster Purpose : Specifies the storage class for Kafka broker persistent storage, which requires ReadWriteOnce (RWO) access mode. This is where Kafka stores message logs and data. When to use : - Leave unset for automatic detection (recommended) - Set explicitly when you need a specific storage class - Must be a storage class supporting RWO access mode - Typically block-based storage for performance Valid values : Any storage class name supporting ReadWriteOnce access mode Impact : Kafka broker performance depends heavily on storage performance. Choose high-performance storage for production workloads. Incorrect storage class or one not supporting RWO will cause deployment to fail. Related variables : - kafka_storage_size : Size of storage for Kafka brokers - zookeeper_storage_class : Separate storage class for Zookeeper Note : Block-based storage classes typically provide better performance for Kafka (e.g., ibmc-block-gold , ocs-storagecluster-ceph-rbd ). This variable only applies to Strimzi and Red Hat AMQ Streams providers.","title":"kafka_storage_class"},{"location":"roles/kafka/#kafka_storage_size","text":"Size of persistent storage for Kafka brokers. Optional Environment Variable: KAFKA_STORAGE_SIZE Default Value: 100Gi Purpose : Specifies the size of persistent volumes for Kafka broker storage. This is where Kafka stores all message logs and topic data. When to use : - Use default ( 100Gi ) for development/test environments - Increase significantly for production based on message volume and retention - Plan for message throughput and retention period - Monitor usage and expand as needed Valid values : Kubernetes storage size format (e.g., 100Gi , 500Gi , 1Ti , 2Ti ) Impact : Insufficient storage will cause Kafka to stop accepting messages when volumes fill up. Size appropriately for your message volume and retention requirements. Related variables : - kafka_storage_class : Storage class for these volumes - kafka_cluster_size : Larger clusters may need more storage - Message retention settings affect storage requirements Note : The default 100Gi is suitable for small deployments only. Production environments typically require 500Gi or more depending on message volume and retention. Plan storage based on daily message volume \u00d7 retention days.","title":"kafka_storage_size"},{"location":"roles/kafka/#zookeeper_storage_class","text":"Storage class for Zookeeper persistent storage (must support ReadWriteOnce). Optional Environment Variable: ZOOKEEPER_STORAGE_CLASS Default Value: Auto-detected from available storage classes in cluster Purpose : Specifies the storage class for Zookeeper persistent storage, which requires ReadWriteOnce (RWO) access mode. Zookeeper stores cluster metadata and coordination data. When to use : - Leave unset for automatic detection (recommended) - Set explicitly when you need a specific storage class - Must be a storage class supporting RWO access mode - Can be same as or different from kafka_storage_class Valid values : Any storage class name supporting ReadWriteOnce access mode Impact : Zookeeper is critical for Kafka cluster coordination. Reliable storage is essential. Incorrect storage class or one not supporting RWO will cause deployment to fail. Related variables : - zookeeper_storage_size : Size of storage for Zookeeper - kafka_storage_class : Separate storage class for Kafka brokers Note : Zookeeper storage requirements are typically much smaller than Kafka broker storage. This variable only applies to Strimzi and Red Hat AMQ Streams providers.","title":"zookeeper_storage_class"},{"location":"roles/kafka/#zookeeper_storage_size","text":"Size of persistent storage for Zookeeper nodes. Optional Environment Variable: ZOOKEEPER_STORAGE_SIZE Default Value: 10Gi Purpose : Specifies the size of persistent volumes for Zookeeper storage. Zookeeper stores cluster metadata, configuration, and coordination data. When to use : - Use default ( 10Gi ) for most deployments - Increase only for very large Kafka clusters with many topics/partitions - Zookeeper storage needs are typically much smaller than Kafka broker storage Valid values : Kubernetes storage size format (e.g., 10Gi , 20Gi , 50Gi ) Impact : Insufficient Zookeeper storage can cause cluster coordination issues. However, Zookeeper storage requirements are typically modest. Related variables : - zookeeper_storage_class : Storage class for these volumes - kafka_storage_size : Kafka broker storage (typically much larger) Note : The default 10Gi is sufficient for most deployments. Zookeeper storage requirements grow slowly with cluster size. Only increase if you have a very large number of topics and partitions.","title":"zookeeper_storage_size"},{"location":"roles/kafka/#kafka_user_name","text":"Username for MAS to authenticate with Kafka. Optional Environment Variable: KAFKA_USER_NAME Default Value: masuser Purpose : Defines the Kafka user that will be created for MAS to authenticate with the Kafka cluster. This user is configured with appropriate permissions for MAS operations. When to use : - Use default ( masuser ) for standard MAS deployments - Set to custom name when organizational policies require specific usernames - Must be unique within the Kafka cluster Valid values : Valid Kafka username string (e.g., masuser , mas-kafka-user ) Impact : This user is created in Kafka with necessary permissions and credentials are included in the generated KafkaCfg for MAS. Related variables : - kafka_user_password : Password for this user (Strimzi 0.25.0+/AMQ Streams 2.x+) - Used in generated KafkaCfg when mas_instance_id is provided Note : The default masuser is suitable for most deployments. This variable only applies to Strimzi and Red Hat AMQ Streams providers.","title":"kafka_user_name"},{"location":"roles/kafka/#kafka_user_password","text":"Password for the Kafka user (Strimzi 0.25.0+/AMQ Streams 2.x+). Optional Environment Variable: KAFKA_USER_PASSWORD Default Value: Randomly generated if not specified Purpose : Sets the password for the Kafka user specified in kafka_user_name . This password is used for SCRAM-SHA authentication. When to use : - Leave unset for automatic random password generation (recommended) - Set explicitly when you need a specific password - Requires Strimzi operator 0.25.0+ or AMQ Streams 2.x+ Valid values : Strong password string meeting security requirements Impact : This password is stored in Kafka secrets and included in the generated KafkaCfg for MAS. If not set, a secure random password is generated automatically. Related variables : - kafka_user_name : User for which this password is set - Used in generated KafkaCfg when mas_instance_id is provided Note : Automatic password generation is recommended for security. This feature requires Strimzi operator version 0.25.0 or AMQ Streams 2.x+. Keep passwords secure and do not commit to source control.","title":"kafka_user_password"},{"location":"roles/kafka/#mas_instance_id","text":"MAS instance ID for generating KafkaCfg configuration. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None Purpose : Identifies which MAS instance the generated KafkaCfg will target. When set (along with mas_config_dir ), the role generates a KafkaCfg resource for configuring MAS to use this Kafka cluster. When to use : - Set when you want to generate MAS configuration automatically - Must match the instance ID of your MAS installation - Required together with mas_config_dir for KafkaCfg generation Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : When set with mas_config_dir , generates a KafkaCfg YAML file that can be applied to configure MAS. Without this, no MAS configuration is generated. Related variables : - mas_config_dir : Required together with this for KafkaCfg generation - Generated KafkaCfg targets this MAS instance Note : If either mas_instance_id or mas_config_dir is not set, the role will not generate a KafkaCfg template. You'll need to configure MAS manually.","title":"mas_instance_id"},{"location":"roles/kafka/#mas_config_dir","text":"Local directory path where generated KafkaCfg will be saved. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None Purpose : Specifies where to save the generated KafkaCfg YAML file. This file can be manually applied to configure MAS or used with the suite_config role for automated configuration. When to use : - Set when you want to generate MAS configuration automatically - Use the same directory across all MAS setup roles for consistency - Required together with mas_instance_id for KafkaCfg generation Valid values : Any valid local filesystem path (e.g., /home/user/masconfig , ~/masconfig , ./config ) Impact : When set with mas_instance_id , generates kafkacfg-{provider}-system.yaml in this directory. The file can be applied with oc apply or used with suite_config role. Related variables : - mas_instance_id : Required together with this for KafkaCfg generation - kafka_provider : Affects the generated filename Note : If either mas_instance_id or mas_config_dir is not set, no KafkaCfg template is generated. Ensure the directory exists and is writable.","title":"mas_config_dir"},{"location":"roles/kafka/#custom_labels","text":"Comma-separated list of key=value labels to apply to Kafka resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Purpose : Adds Kubernetes labels to Kafka-related resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,component=kafka ) Impact : Labels are applied to Kafka-related resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect Kafka functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management. Note : This variable applies to Strimzi and Red Hat AMQ Streams providers. Labels help with resource organization and are especially useful in multi-tenant environments. IBM Cloud Evenstreams Role Variables","title":"custom_labels"},{"location":"roles/kafka/#ibmcloud_apikey","text":"Defines IBM Cloud API Key. This API Key needs to have access to manage (provision/deprovision) IBM Cloud Event Streams. Required Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/kafka/#eventstreams_resourcegroup","text":"Defines the IBM Cloud Resource Group to target the Event Streams instance. Optional Environment Variable: EVENTSTREAMS_RESOURCEGROUP Default Value: Default or value defined by IBMCLOUD_RESOURCEGROUP","title":"eventstreams_resourcegroup"},{"location":"roles/kafka/#eventstreams_name","text":"Event Streams instance name. Required Environment Variable: EVENTSTREAMS_NAME Default Value: None","title":"eventstreams_name"},{"location":"roles/kafka/#eventstreams_plan","text":"Event Streams instance plan. Optional Environment Variable: EVENTSTREAMS_PLAN Default Value: standard","title":"eventstreams_plan"},{"location":"roles/kafka/#eventstreams_location","text":"Optional Environment Variable: EVENTSTREAMS_LOCATION Default Value: us-east or value defined by IBMCLOUD_REGION","title":"eventstreams_location"},{"location":"roles/kafka/#eventstreams_retention","text":"Event Streams topic retention period (in miliseconds). Optional Environment Variable: EVENTSTREAMS_RETENTION Default Value: 1209600000","title":"eventstreams_retention"},{"location":"roles/kafka/#eventstreams_create_manage_jms_topic","text":"Defines whether to create specific Manage application JMS topics by default. Optional Environment Variable: EVENTSTREAMS_CREATE_MANAGE_JMS_TOPICS Default Value: True","title":"eventstreams_create_manage_jms_topic"},{"location":"roles/kafka/#mas_instance_id_1","text":"The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/kafka/#mas_config_dir_1","text":"Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/kafka/#custom_labels_1","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/kafka/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Set storage class suitable for use on IBM Cloud ROKS kafka_storage_class: ibmc-block-gold # Generate a KafkaCfg template mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.kafka","title":"Example Playbook"},{"location":"roles/kafka/#aws-msk-variables","text":"","title":"AWS MSK Variables"},{"location":"roles/kafka/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/kafka/#kafka_version_1","text":"The version of Kafka to deploy for AWS MSK. Environment Variable: KAFKA_VERSION Default Value: 3.3.1","title":"kafka_version"},{"location":"roles/kafka/#kafka_cluster_name_1","text":"The name of the Kafka cluster that will be created Required Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka","title":"kafka_cluster_name"},{"location":"roles/kafka/#aws_region","text":"Required Environment Variable: AWS_REGION Default Value: None","title":"aws_region"},{"location":"roles/kafka/#vpc_id","text":"The AWS Virtual Private Cloud identifier (VPC ID) where the MSK instance will be hosted. Required Environment Variable: VPC_ID Default Value: None","title":"vpc_id"},{"location":"roles/kafka/#aws_msk_cidr_az1","text":"The CIDR address for the first Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ1 Default Value: None","title":"aws_msk_cidr_az1"},{"location":"roles/kafka/#aws_msk_cidr_az2","text":"The CIDR address for the second Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ2 Default Value: None","title":"aws_msk_cidr_az2"},{"location":"roles/kafka/#aws_msk_cidr_az3","text":"The CIDR address for the third Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ3 Default Value: None","title":"aws_msk_cidr_az3"},{"location":"roles/kafka/#aws_msk_ingress_cidr","text":"The IPv4 CIDR address for ingress connection. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_INGRESS_CIDR Default Value: None","title":"aws_msk_ingress_cidr"},{"location":"roles/kafka/#aws_msk_egress_cidr","text":"The IPv4 CIDR address for egress connection. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_EGRESS_CIDR Default Value: None","title":"aws_msk_egress_cidr"},{"location":"roles/kafka/#aws_kafka_user_name","text":"The name of the user to setup in the cluster for MAS. Required Environment Variable: AWS_KAFKA_USER_NAME Default Value: None","title":"aws_kafka_user_name"},{"location":"roles/kafka/#aws_kafka_user_password","text":"The password of the user to setup in the cluster for MAS. Optional Environment Variable: AWS_KAFKA_USER_PASSWORD Default Value: None","title":"aws_kafka_user_password"},{"location":"roles/kafka/#aws_msk_instance_type","text":"The type/flavor of your MSK instance. Optional Environment Variable: AWS_MSK_INSTANCE_TYPE Default Value: kafka.m5.large","title":"aws_msk_instance_type"},{"location":"roles/kafka/#aws_msk_volume_size","text":"The storage/volume size of your MSK instance. Optional Environment Variable: AWS_MSK_VOLUME_SIZE Default Value: 100","title":"aws_msk_volume_size"},{"location":"roles/kafka/#aws_msk_instance_number","text":"The number of broker/instances of your MSK instance. Optional Environment Variable: AWS_MSK_INSTANCE_NUMBER Default Value: 3","title":"aws_msk_instance_number"},{"location":"roles/kafka/#mas_instance_id_2","text":"The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/kafka/#mas_config_dir_2","text":"Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/kafka/#custom_labels_2","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/kafka/#aws_msk_secret","text":"AWS MSK Secret name. The secret name must begin with the prefix AmazonMSK_. If this is not set, then default secret name will be AmazonMSK_SECRET_{{kafka_cluster_name}} Optional Environment Variable: AWS_MSK_SECRET Default Value: AmazonMSK_SECRET_{{kafka_cluster_name}}'","title":"aws_msk_secret"},{"location":"roles/kafka/#install-aws-msk","text":"- hosts: localhost any_errors_fatal: true vars: aws_region: ca-central-1 aws_access_key_id: ***** aws_secret_access_key: ***** kafka_version: 3.3.1 kafka_provider: aws kafka_action: install kafka_cluster_name: msk-abcd0zyxw kafka_namespace: msk-abcd0zyxw vpc_id: vpc-07088da510b3c35c5 aws_kafka_user_name: mskuser-abcd0zyxw aws_msk_instance_type: kafka.t3.small aws_msk_volume_size: 100 aws_msk_instance_number: 3 aws_msk_cidr_az1: \"10.0.128.0/20\" aws_msk_cidr_az2: \"10.0.144.0/20\" aws_msk_cidr_az3: \"10.0.160.0/20\" aws_msk_ingress_cidr: \"10.0.0.0/16\" aws_msk_egress_cidr: \"10.0.0.0/16\" # Generate a KafkaCfg template mas_config_dir: /var/tmp/masconfigdir mas_instance_id: abcd0zyxw roles: - ibm.mas_devops.kafka","title":"Install AWS MSK"},{"location":"roles/kafka/#uninstall-aws-msk","text":"- hosts: localhost any_errors_fatal: true vars: aws_region: ca-central-1 aws_access_key_id: ***** aws_secret_access_key: ***** vpc_id: vpc-07088da510b3c35c5 kafka_provider: aws kafka_action: uninstall kafka_cluster_name: msk-abcd0zyxw aws_msk_cidr_az1: \"10.0.128.0/20\" aws_msk_cidr_az2: \"10.0.144.0/20\" aws_msk_cidr_az3: \"10.0.160.0/20\" roles: - ibm.mas_devops.kafka","title":"Uninstall AWS MSK"},{"location":"roles/kafka/#run-role-playbook","text":"export KAFKA_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=masinst1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/kafka/#license","text":"EPL-2.0","title":"License"},{"location":"roles/key_rotation/","text":"key_rotation \u00a4 Create new apikey for user in cloud account and delete the existing one. Role Variables - General \u00a4 cluster_type \u00a4 Required. Specify the cluster type, supported values are roks , and rosa . Environment Variable: CLUSTER_TYPE Default Value: None Role Variables - ROKS \u00a4 ibmcloud_apikey \u00a4 Required. A new key will be created and this key will be deleted. Environment Variable: IBMCLOUD_APIKEY Default: None ibmcloud_keyname \u00a4 Required. A new key will be created and this key will be deleted. Environment Variable: IBMCLOUD_KEYNAME Default: None ibmcloud_output_keydir \u00a4 Optional. Environment Variable: IBMCLOUD_OUTPUT_KEYDIR Default: '/tmp' Role Variables - ROSA or IPI/AWS \u00a4 The following variables are used when cluster_type = rosa or cluster_type=ipe and cluster_platform=aws . aws_region \u00a4 Required when cluster_type = rosa or cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_REGION Default Value: us-east-1 aws_username \u00a4 Required. Environment Variable: AWS_USERNAME Default: None aws_access_key_id \u00a4 Required. A new key will be created and this key will be deleted. Environment Variable: AWS_ACCESS_KEY_ID Default: None aws_secret_access_key \u00a4 Required. A new key will be created and this key will be deleted. Environment Variable: AWS_SECRET_ACCESS_KEY Default: None Example Playbook \u00a4 - hosts: localhost vars: cluster_type: roks ibmcloud_apikey: ################ ibmcloud_keyname: myapikeyname roles: - ibm.mas_devops.key_rotation License \u00a4 EPL-2.0","title":"key_rotation"},{"location":"roles/key_rotation/#key_rotation","text":"Create new apikey for user in cloud account and delete the existing one.","title":"key_rotation"},{"location":"roles/key_rotation/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/key_rotation/#cluster_type","text":"Required. Specify the cluster type, supported values are roks , and rosa . Environment Variable: CLUSTER_TYPE Default Value: None","title":"cluster_type"},{"location":"roles/key_rotation/#role-variables-roks","text":"","title":"Role Variables - ROKS"},{"location":"roles/key_rotation/#ibmcloud_apikey","text":"Required. A new key will be created and this key will be deleted. Environment Variable: IBMCLOUD_APIKEY Default: None","title":"ibmcloud_apikey"},{"location":"roles/key_rotation/#ibmcloud_keyname","text":"Required. A new key will be created and this key will be deleted. Environment Variable: IBMCLOUD_KEYNAME Default: None","title":"ibmcloud_keyname"},{"location":"roles/key_rotation/#ibmcloud_output_keydir","text":"Optional. Environment Variable: IBMCLOUD_OUTPUT_KEYDIR Default: '/tmp'","title":"ibmcloud_output_keydir"},{"location":"roles/key_rotation/#role-variables-rosa-or-ipiaws","text":"The following variables are used when cluster_type = rosa or cluster_type=ipe and cluster_platform=aws .","title":"Role Variables - ROSA or IPI/AWS"},{"location":"roles/key_rotation/#aws_region","text":"Required when cluster_type = rosa or cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_REGION Default Value: us-east-1","title":"aws_region"},{"location":"roles/key_rotation/#aws_username","text":"Required. Environment Variable: AWS_USERNAME Default: None","title":"aws_username"},{"location":"roles/key_rotation/#aws_access_key_id","text":"Required. A new key will be created and this key will be deleted. Environment Variable: AWS_ACCESS_KEY_ID Default: None","title":"aws_access_key_id"},{"location":"roles/key_rotation/#aws_secret_access_key","text":"Required. A new key will be created and this key will be deleted. Environment Variable: AWS_SECRET_ACCESS_KEY Default: None","title":"aws_secret_access_key"},{"location":"roles/key_rotation/#example-playbook","text":"- hosts: localhost vars: cluster_type: roks ibmcloud_apikey: ################ ibmcloud_keyname: myapikeyname roles: - ibm.mas_devops.key_rotation","title":"Example Playbook"},{"location":"roles/key_rotation/#license","text":"EPL-2.0","title":"License"},{"location":"roles/longhorn/","text":"longhorn \u00a4 Deploy Longhorn distributed block storage system for Kubernetes, providing both ReadWriteMany (RWX) and ReadWriteOnce (RWO) storage for Maximo Application Suite. Longhorn is a lightweight, reliable, and easy-to-use distributed block storage system for Kubernetes. Originally developed by Rancher Labs, it is now an incubating project of the Cloud Native Computing Foundation (CNCF). Features \u00a4 Unified Storage Solution : Provides both RWX and RWO storage classes for MAS High Availability : Configurable replica count for data redundancy Web UI : Management interface available at https://longhorn-ui-longhorn-system.{clusterdomain} (OpenShift OAuth authentication) Dynamic Provisioning : Automatic volume provisioning for MAS workloads Snapshot Support : Built-in backup and snapshot capabilities Deployed Components \u00a4 After installation, the following deployments will be available: oc -n longhorn-system get deployments NAME READY UP-TO-DATE AVAILABLE AGE csi-attacher 3/3 3 3 38m csi-provisioner 3/3 3 3 38m csi-resizer 3/3 3 3 38m csi-snapshotter 3/3 3 3 38m longhorn-driver-deployer 1/1 1 1 40m longhorn-ui 2/2 2 2 40m Storage Classes \u00a4 Two storage classes are automatically created: oc get storageclass | grep longhorn longhorn (default) driver.longhorn.io Delete Immediate true 40m longhorn-static driver.longhorn.io Delete Immediate true 40m Note : MAS uses dynamic provisioning with the longhorn storage class. The longhorn-static storage class is not used by MAS. Additional Resources \u00a4 What is Longhorn? Longhorn Helm Chart Settings Longhorn on OpenShift Role Variables \u00a4 longhorn_namespace \u00a4 Namespace for Longhorn installation. Optional Environment Variable: LONGHORN_NAMESPACE Default: longhorn-system Purpose : Specifies the OpenShift namespace where Longhorn components will be deployed. When to use : Use default unless you have specific namespace requirements or multiple Longhorn instances. Valid values : Valid Kubernetes namespace name (lowercase alphanumeric with hyphens) Impact : All Longhorn resources (deployments, services, storage classes) will be created in this namespace. Related variables : None Notes : - Default longhorn-system is the standard namespace for Longhorn - Namespace will be created if it doesn't exist - Longhorn UI will be accessible at https://longhorn-ui-{namespace}.{clusterdomain} - Storage classes are cluster-wide regardless of namespace longhorn_replica_count \u00a4 Number of data replicas for Longhorn volumes. Optional Environment Variable: LONGHORN_REPLICA_COUNT Default: 3 Purpose : Determines how many copies of volume data are stored across different nodes in the cluster, directly impacting data availability and resilience. When to use : - Use default 3 for production deployments requiring high availability - Set to 2 for moderate availability with reduced storage overhead - Set to 1 for development/testing environments to minimize storage requirements Valid values : Integer between 1 and the number of worker nodes. Common values: - 3 - Production (tolerates 2 node failures, recommended) - 2 - Moderate availability (tolerates 1 node failure) - 1 - Development only (no redundancy, data loss if node fails) Impact : - Availability : Higher replica count = better fault tolerance - Storage : Each replica consumes storage space (3 replicas = 3x storage usage) - Performance : More replicas may impact write performance slightly - Node Requirements : Replica count cannot exceed number of available nodes Related variables : None Notes : - Default 3 allows system to tolerate up to 2 replica failures while maintaining data integrity - Setting to 1 sacrifices resilience for reduced storage requirements (development only) - Replicas are distributed across different nodes for fault tolerance - Cannot change replica count after volumes are created (requires volume recreation) - Ensure cluster has sufficient nodes for desired replica count - Consider storage capacity when setting replica count (3 replicas = 3x storage consumption) Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: longhorn_namespace: longhorn-system longhorn_replica_count: 3 roles: - ibm.mas_devops.longhorn For development environments with reduced storage requirements: - hosts: localhost any_errors_fatal: true vars: longhorn_namespace: longhorn-system longhorn_replica_count: 1 roles: - ibm.mas_devops.longhorn License \u00a4 EPL-2.0","title":"longhorn"},{"location":"roles/longhorn/#longhorn","text":"Deploy Longhorn distributed block storage system for Kubernetes, providing both ReadWriteMany (RWX) and ReadWriteOnce (RWO) storage for Maximo Application Suite. Longhorn is a lightweight, reliable, and easy-to-use distributed block storage system for Kubernetes. Originally developed by Rancher Labs, it is now an incubating project of the Cloud Native Computing Foundation (CNCF).","title":"longhorn"},{"location":"roles/longhorn/#features","text":"Unified Storage Solution : Provides both RWX and RWO storage classes for MAS High Availability : Configurable replica count for data redundancy Web UI : Management interface available at https://longhorn-ui-longhorn-system.{clusterdomain} (OpenShift OAuth authentication) Dynamic Provisioning : Automatic volume provisioning for MAS workloads Snapshot Support : Built-in backup and snapshot capabilities","title":"Features"},{"location":"roles/longhorn/#deployed-components","text":"After installation, the following deployments will be available: oc -n longhorn-system get deployments NAME READY UP-TO-DATE AVAILABLE AGE csi-attacher 3/3 3 3 38m csi-provisioner 3/3 3 3 38m csi-resizer 3/3 3 3 38m csi-snapshotter 3/3 3 3 38m longhorn-driver-deployer 1/1 1 1 40m longhorn-ui 2/2 2 2 40m","title":"Deployed Components"},{"location":"roles/longhorn/#storage-classes","text":"Two storage classes are automatically created: oc get storageclass | grep longhorn longhorn (default) driver.longhorn.io Delete Immediate true 40m longhorn-static driver.longhorn.io Delete Immediate true 40m Note : MAS uses dynamic provisioning with the longhorn storage class. The longhorn-static storage class is not used by MAS.","title":"Storage Classes"},{"location":"roles/longhorn/#additional-resources","text":"What is Longhorn? Longhorn Helm Chart Settings Longhorn on OpenShift","title":"Additional Resources"},{"location":"roles/longhorn/#role-variables","text":"","title":"Role Variables"},{"location":"roles/longhorn/#longhorn_namespace","text":"Namespace for Longhorn installation. Optional Environment Variable: LONGHORN_NAMESPACE Default: longhorn-system Purpose : Specifies the OpenShift namespace where Longhorn components will be deployed. When to use : Use default unless you have specific namespace requirements or multiple Longhorn instances. Valid values : Valid Kubernetes namespace name (lowercase alphanumeric with hyphens) Impact : All Longhorn resources (deployments, services, storage classes) will be created in this namespace. Related variables : None Notes : - Default longhorn-system is the standard namespace for Longhorn - Namespace will be created if it doesn't exist - Longhorn UI will be accessible at https://longhorn-ui-{namespace}.{clusterdomain} - Storage classes are cluster-wide regardless of namespace","title":"longhorn_namespace"},{"location":"roles/longhorn/#longhorn_replica_count","text":"Number of data replicas for Longhorn volumes. Optional Environment Variable: LONGHORN_REPLICA_COUNT Default: 3 Purpose : Determines how many copies of volume data are stored across different nodes in the cluster, directly impacting data availability and resilience. When to use : - Use default 3 for production deployments requiring high availability - Set to 2 for moderate availability with reduced storage overhead - Set to 1 for development/testing environments to minimize storage requirements Valid values : Integer between 1 and the number of worker nodes. Common values: - 3 - Production (tolerates 2 node failures, recommended) - 2 - Moderate availability (tolerates 1 node failure) - 1 - Development only (no redundancy, data loss if node fails) Impact : - Availability : Higher replica count = better fault tolerance - Storage : Each replica consumes storage space (3 replicas = 3x storage usage) - Performance : More replicas may impact write performance slightly - Node Requirements : Replica count cannot exceed number of available nodes Related variables : None Notes : - Default 3 allows system to tolerate up to 2 replica failures while maintaining data integrity - Setting to 1 sacrifices resilience for reduced storage requirements (development only) - Replicas are distributed across different nodes for fault tolerance - Cannot change replica count after volumes are created (requires volume recreation) - Ensure cluster has sufficient nodes for desired replica count - Consider storage capacity when setting replica count (3 replicas = 3x storage consumption)","title":"longhorn_replica_count"},{"location":"roles/longhorn/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: longhorn_namespace: longhorn-system longhorn_replica_count: 3 roles: - ibm.mas_devops.longhorn For development environments with reduced storage requirements: - hosts: localhost any_errors_fatal: true vars: longhorn_namespace: longhorn-system longhorn_replica_count: 1 roles: - ibm.mas_devops.longhorn","title":"Example Playbook"},{"location":"roles/longhorn/#license","text":"EPL-2.0","title":"License"},{"location":"roles/mirror_case_prepare/","text":"mirror_case_prepare \u00a4 This role generates a mirror manifest file suitable for use with the oc mirror command (or the ibm.mas_devops.mirror_images role) from an IBM CASE bundle. Requirements \u00a4 The ibm-pak plugin must be installed. Role Variables - General \u00a4 case_name \u00a4 Name of the IBM CASE bundle to prepare for mirroring. Required Environment Variable: CASE_NAME Default: None Purpose : Specifies which IBM Cloud Pak Application Software Entitlement (CASE) bundle to download and prepare for mirroring to a disconnected registry. When to use : - Always required for CASE bundle mirroring preparation - Must match an available CASE bundle name - Common values: ibm-mas , ibm-sls , ibm-cp4d Valid values : Valid IBM CASE bundle name (e.g., ibm-mas , ibm-sls , ibm-cp4d ) Impact : Determines which CASE bundle is downloaded and processed. Incorrect name will cause download to fail. Related variables : - case_version : Version of this CASE bundle to download - ibmpak_skip_dependencies : Whether to include CASE dependencies Note : CASE bundles contain metadata about container images and operators. Use oc ibm-pak list to see available CASE bundles. case_version \u00a4 Version of the IBM CASE bundle to prepare for mirroring. Required Environment Variable: CASE_VERSION Default: None Purpose : Specifies which version of the CASE bundle to download and prepare for mirroring. Different versions contain different image sets. When to use : - Always required for CASE bundle mirroring preparation - Must match an available version for the specified CASE bundle - Use specific version for production (e.g., 8.8.1 , 9.0.0 ) Valid values : Valid version number for the CASE bundle (e.g., 8.8.1 , 9.0.0 , 3.8.0 ) Impact : Determines which version of images are included in the mirror manifest. Version must exist for the specified CASE bundle. Related variables : - case_name : CASE bundle for this version Note : Use oc ibm-pak list <case_name> to see available versions. Underscores in version are automatically converted to dots (e.g., 8_8_1 becomes 8.8.1 ). registry_public_host \u00a4 Target registry hostname for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None Purpose : Specifies the hostname of the target container registry where images will be mirrored. Used to generate the mirror manifest with correct destination paths. When to use : - Always required for mirror manifest preparation - Must be the hostname of your disconnected/private registry - Images are not mirrored yet, but manifest needs target destination Valid values : Valid hostname or IP address (e.g., registry.example.com , 10.0.0.100 ) Impact : Determines the target registry in the generated mirror manifest. All image paths will reference this host. Related variables : - registry_public_port : Port for this registry - registry_prefix : Optional path prefix in registry Note : Images are not mirrored during this role execution. This role only prepares the manifest. Use mirror_images role to perform actual mirroring. registry_public_port \u00a4 Target registry port for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None Purpose : Specifies the port of the target container registry where images will be mirrored. Used to generate the mirror manifest with correct destination paths. When to use : - Always required for mirror manifest preparation - Must be the port where your registry is accessible - Common values: 443 (HTTPS), 5000 (HTTP), 32500 (NodePort) Valid values : Valid port number (e.g., 443 , 5000 , 32500 ) Impact : Determines the target registry port in the generated mirror manifest. All image paths will include this port. Related variables : - registry_public_host : Hostname for this registry - registry_prefix : Optional path prefix in registry Note : Images are not mirrored during this role execution. This role only prepares the manifest with the target destination. registry_prefix \u00a4 Path prefix in target registry for mirrored images. Optional Environment Variable: REGISTRY_PREFIX Default: None Purpose : Specifies an optional path prefix in the target registry. Images will be mirrored to {host}:{port}/{prefix}/{reponame} format. When to use : - Leave unset if images should be at registry root - Set to organize images under a specific path (e.g., project name, namespace) - Useful for multi-tenant registries Valid values : Valid registry path (e.g., mas-mirror , production , project-name ) Impact : - When set: Images mirrored to {host}:{port}/{prefix}/{reponame} - When unset: Images mirrored to {host}:{port}/{reponame} Related variables : - registry_public_host : Registry hostname - registry_public_port : Registry port Note : The prefix helps organize images in the registry. Example: with prefix mas-mirror , images go to registry.example.com:5000/mas-mirror/ibm-mas-operator . exclude_images \u00a4 List of child CASE bundles to exclude from mirroring. Optional Environment Variable: None (must be set in playbook) Default: None Purpose : Specifies which child CASE bundles (dependencies) to exclude from the mirror manifest. Reduces mirror size by excluding unwanted components. When to use : - Leave unset to include all CASE bundle dependencies - Set to exclude specific MAS applications or components you don't need - Useful for reducing mirror size and time Valid values : List of CASE bundle names (e.g., ['ibm-sls', 'ibm-mas-assist', 'ibm-mas-iot'] ) Impact : Excluded CASE bundles and their images are not included in the mirror manifest. Reduces mirror size but those components won't be available. Related variables : - case_name : Parent CASE bundle - ibmpak_skip_dependencies : Skip all dependencies (different from selective exclusion) Note : Use this to exclude MAS applications you don't need (e.g., exclude Assist, IoT, Manage if only using Monitor). Cannot be set via environment variable; must be set in playbook vars. Role Variables - IBM Pak \u00a4 ibmpak_skip_verify \u00a4 Skip certificate verification when downloading CASE bundles. Optional Environment Variable: IBMPAK_SKIP_VERIFY Default: false Purpose : Controls whether to skip certificate verification when downloading CASE bundles with oc ibm-pak get . Useful for environments with self-signed certificates. When to use : - Leave as false (default) for production with valid certificates - Set to true only in development/testing with self-signed certificates - Use when certificate verification causes download failures Valid values : true , false Impact : - true : Skips certificate verification (less secure, allows self-signed certs) - false : Verifies certificates (more secure, requires valid certs) Related variables : - ibmpak_insecure : Skip TLS/SSL verification (related but different) Note : Only use true in development/testing environments. Production should use valid certificates and keep this false . ibmpak_skip_dependencies \u00a4 Skip downloading CASE bundle dependencies. Optional Environment Variable: IBMPAK_SKIP_DEPENDENCIES Default: false Purpose : Controls whether to skip downloading dependent CASE bundles. When enabled, only the specified CASE bundle is downloaded, not its dependencies. When to use : - Leave as false (default) to include all dependencies (recommended) - Set to true to download only the specified CASE bundle - Use when dependencies are already downloaded or not needed Valid values : true , false Impact : - true : Only specified CASE bundle is downloaded (may miss required dependencies) - false : All dependent CASE bundles are downloaded (complete set) Related variables : - exclude_images : Selective exclusion of specific dependencies (more granular) Note : Skipping dependencies may result in incomplete mirror. Use exclude_images for selective exclusion instead of skipping all dependencies. ibmpak_insecure \u00a4 Skip TLS/SSL verification when downloading CASE bundles. Optional Environment Variable: IBMPAK_INSECURE Default: false Purpose : Controls whether to skip TLS/SSL verification when downloading CASE bundles with oc ibm-pak get . Useful for environments with self-signed certificates or TLS issues. When to use : - Leave as false (default) for production with valid TLS certificates - Set to true only in development/testing with TLS issues - Use when TLS verification causes download failures Valid values : true , false Impact : - true : Skips TLS/SSL verification (less secure, allows self-signed certs) - false : Verifies TLS/SSL (more secure, requires valid certificates) Related variables : - ibmpak_skip_verify : Skip certificate verification (related but different) Note : Only use true in development/testing environments. Production should use valid TLS certificates and keep this false . Example Playbook \u00a4 - hosts: localhost vars: case_name: ibm-mas case_version: 8.8.1 exclude_images: - ibm-truststore-mgr - ibm-sls - ibm-mas-assist - ibm-mas-iot - ibm-mas-manage registry_public_host: myregistry.com registry_public_port: 32500 registry_prefix: projectName roles: - ibm.mas_devops.mirror_case_prepare License \u00a4 EPL-2.0","title":"mirror_case_prepare"},{"location":"roles/mirror_case_prepare/#mirror_case_prepare","text":"This role generates a mirror manifest file suitable for use with the oc mirror command (or the ibm.mas_devops.mirror_images role) from an IBM CASE bundle.","title":"mirror_case_prepare"},{"location":"roles/mirror_case_prepare/#requirements","text":"The ibm-pak plugin must be installed.","title":"Requirements"},{"location":"roles/mirror_case_prepare/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/mirror_case_prepare/#case_name","text":"Name of the IBM CASE bundle to prepare for mirroring. Required Environment Variable: CASE_NAME Default: None Purpose : Specifies which IBM Cloud Pak Application Software Entitlement (CASE) bundle to download and prepare for mirroring to a disconnected registry. When to use : - Always required for CASE bundle mirroring preparation - Must match an available CASE bundle name - Common values: ibm-mas , ibm-sls , ibm-cp4d Valid values : Valid IBM CASE bundle name (e.g., ibm-mas , ibm-sls , ibm-cp4d ) Impact : Determines which CASE bundle is downloaded and processed. Incorrect name will cause download to fail. Related variables : - case_version : Version of this CASE bundle to download - ibmpak_skip_dependencies : Whether to include CASE dependencies Note : CASE bundles contain metadata about container images and operators. Use oc ibm-pak list to see available CASE bundles.","title":"case_name"},{"location":"roles/mirror_case_prepare/#case_version","text":"Version of the IBM CASE bundle to prepare for mirroring. Required Environment Variable: CASE_VERSION Default: None Purpose : Specifies which version of the CASE bundle to download and prepare for mirroring. Different versions contain different image sets. When to use : - Always required for CASE bundle mirroring preparation - Must match an available version for the specified CASE bundle - Use specific version for production (e.g., 8.8.1 , 9.0.0 ) Valid values : Valid version number for the CASE bundle (e.g., 8.8.1 , 9.0.0 , 3.8.0 ) Impact : Determines which version of images are included in the mirror manifest. Version must exist for the specified CASE bundle. Related variables : - case_name : CASE bundle for this version Note : Use oc ibm-pak list <case_name> to see available versions. Underscores in version are automatically converted to dots (e.g., 8_8_1 becomes 8.8.1 ).","title":"case_version"},{"location":"roles/mirror_case_prepare/#registry_public_host","text":"Target registry hostname for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None Purpose : Specifies the hostname of the target container registry where images will be mirrored. Used to generate the mirror manifest with correct destination paths. When to use : - Always required for mirror manifest preparation - Must be the hostname of your disconnected/private registry - Images are not mirrored yet, but manifest needs target destination Valid values : Valid hostname or IP address (e.g., registry.example.com , 10.0.0.100 ) Impact : Determines the target registry in the generated mirror manifest. All image paths will reference this host. Related variables : - registry_public_port : Port for this registry - registry_prefix : Optional path prefix in registry Note : Images are not mirrored during this role execution. This role only prepares the manifest. Use mirror_images role to perform actual mirroring.","title":"registry_public_host"},{"location":"roles/mirror_case_prepare/#registry_public_port","text":"Target registry port for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None Purpose : Specifies the port of the target container registry where images will be mirrored. Used to generate the mirror manifest with correct destination paths. When to use : - Always required for mirror manifest preparation - Must be the port where your registry is accessible - Common values: 443 (HTTPS), 5000 (HTTP), 32500 (NodePort) Valid values : Valid port number (e.g., 443 , 5000 , 32500 ) Impact : Determines the target registry port in the generated mirror manifest. All image paths will include this port. Related variables : - registry_public_host : Hostname for this registry - registry_prefix : Optional path prefix in registry Note : Images are not mirrored during this role execution. This role only prepares the manifest with the target destination.","title":"registry_public_port"},{"location":"roles/mirror_case_prepare/#registry_prefix","text":"Path prefix in target registry for mirrored images. Optional Environment Variable: REGISTRY_PREFIX Default: None Purpose : Specifies an optional path prefix in the target registry. Images will be mirrored to {host}:{port}/{prefix}/{reponame} format. When to use : - Leave unset if images should be at registry root - Set to organize images under a specific path (e.g., project name, namespace) - Useful for multi-tenant registries Valid values : Valid registry path (e.g., mas-mirror , production , project-name ) Impact : - When set: Images mirrored to {host}:{port}/{prefix}/{reponame} - When unset: Images mirrored to {host}:{port}/{reponame} Related variables : - registry_public_host : Registry hostname - registry_public_port : Registry port Note : The prefix helps organize images in the registry. Example: with prefix mas-mirror , images go to registry.example.com:5000/mas-mirror/ibm-mas-operator .","title":"registry_prefix"},{"location":"roles/mirror_case_prepare/#exclude_images","text":"List of child CASE bundles to exclude from mirroring. Optional Environment Variable: None (must be set in playbook) Default: None Purpose : Specifies which child CASE bundles (dependencies) to exclude from the mirror manifest. Reduces mirror size by excluding unwanted components. When to use : - Leave unset to include all CASE bundle dependencies - Set to exclude specific MAS applications or components you don't need - Useful for reducing mirror size and time Valid values : List of CASE bundle names (e.g., ['ibm-sls', 'ibm-mas-assist', 'ibm-mas-iot'] ) Impact : Excluded CASE bundles and their images are not included in the mirror manifest. Reduces mirror size but those components won't be available. Related variables : - case_name : Parent CASE bundle - ibmpak_skip_dependencies : Skip all dependencies (different from selective exclusion) Note : Use this to exclude MAS applications you don't need (e.g., exclude Assist, IoT, Manage if only using Monitor). Cannot be set via environment variable; must be set in playbook vars.","title":"exclude_images"},{"location":"roles/mirror_case_prepare/#role-variables-ibm-pak","text":"","title":"Role Variables - IBM Pak"},{"location":"roles/mirror_case_prepare/#ibmpak_skip_verify","text":"Skip certificate verification when downloading CASE bundles. Optional Environment Variable: IBMPAK_SKIP_VERIFY Default: false Purpose : Controls whether to skip certificate verification when downloading CASE bundles with oc ibm-pak get . Useful for environments with self-signed certificates. When to use : - Leave as false (default) for production with valid certificates - Set to true only in development/testing with self-signed certificates - Use when certificate verification causes download failures Valid values : true , false Impact : - true : Skips certificate verification (less secure, allows self-signed certs) - false : Verifies certificates (more secure, requires valid certs) Related variables : - ibmpak_insecure : Skip TLS/SSL verification (related but different) Note : Only use true in development/testing environments. Production should use valid certificates and keep this false .","title":"ibmpak_skip_verify"},{"location":"roles/mirror_case_prepare/#ibmpak_skip_dependencies","text":"Skip downloading CASE bundle dependencies. Optional Environment Variable: IBMPAK_SKIP_DEPENDENCIES Default: false Purpose : Controls whether to skip downloading dependent CASE bundles. When enabled, only the specified CASE bundle is downloaded, not its dependencies. When to use : - Leave as false (default) to include all dependencies (recommended) - Set to true to download only the specified CASE bundle - Use when dependencies are already downloaded or not needed Valid values : true , false Impact : - true : Only specified CASE bundle is downloaded (may miss required dependencies) - false : All dependent CASE bundles are downloaded (complete set) Related variables : - exclude_images : Selective exclusion of specific dependencies (more granular) Note : Skipping dependencies may result in incomplete mirror. Use exclude_images for selective exclusion instead of skipping all dependencies.","title":"ibmpak_skip_dependencies"},{"location":"roles/mirror_case_prepare/#ibmpak_insecure","text":"Skip TLS/SSL verification when downloading CASE bundles. Optional Environment Variable: IBMPAK_INSECURE Default: false Purpose : Controls whether to skip TLS/SSL verification when downloading CASE bundles with oc ibm-pak get . Useful for environments with self-signed certificates or TLS issues. When to use : - Leave as false (default) for production with valid TLS certificates - Set to true only in development/testing with TLS issues - Use when TLS verification causes download failures Valid values : true , false Impact : - true : Skips TLS/SSL verification (less secure, allows self-signed certs) - false : Verifies TLS/SSL (more secure, requires valid certificates) Related variables : - ibmpak_skip_verify : Skip certificate verification (related but different) Note : Only use true in development/testing environments. Production should use valid TLS certificates and keep this false .","title":"ibmpak_insecure"},{"location":"roles/mirror_case_prepare/#example-playbook","text":"- hosts: localhost vars: case_name: ibm-mas case_version: 8.8.1 exclude_images: - ibm-truststore-mgr - ibm-sls - ibm-mas-assist - ibm-mas-iot - ibm-mas-manage registry_public_host: myregistry.com registry_public_port: 32500 registry_prefix: projectName roles: - ibm.mas_devops.mirror_case_prepare","title":"Example Playbook"},{"location":"roles/mirror_case_prepare/#license","text":"EPL-2.0","title":"License"},{"location":"roles/mirror_extras_prepare/","text":"mirror_extras_prepare \u00a4 This role generates a mirror manifest file suitable for use with the oc mirror command (or the ibm.mas_devops.mirror_images role) for a specific set of extra images. Available Extras Extra Versions Description catalog N/A Special extra package for mirroring the IBM Maximo Operator Catalog db2u 1.0.0, 1.0.1 Extra container images missing from the ibm-db2operator CASE bundle mongoce 4.2.6, 4.2.23, 4.4.21 Package containing all images required to use MongoCE Operator in the disconnected environment wd 5.3.1 Extra container images missing from the ibm-watson-discovery CASE bundle odf 4.15 Extra images needed for ODF 4.15 Role Variables \u00a4 extras_name \u00a4 Name of the extras package to prepare for mirroring. Required Environment Variable: EXTRAS_NAME Default: None Purpose : Specifies which extras package to prepare for mirroring. Extras packages contain additional container images not included in standard CASE bundles but required for MAS deployments. When to use : - Always required for extras package mirroring preparation - Must match an available extras package name - See \"Available Extras\" table above for valid packages Valid values : catalog , db2u , mongoce , wd , odf - catalog : IBM Maximo Operator Catalog images - db2u : Extra Db2 images missing from ibm-db2operator CASE - mongoce : MongoDB Community Edition Operator images - wd : Extra Watson Discovery images missing from CASE - odf : OpenShift Data Foundation extra images Impact : Determines which extras package is downloaded and processed. Incorrect name will cause preparation to fail. Related variables : - extras_version : Version of this extras package to prepare Note : Extras packages fill gaps in CASE bundles, providing images needed for disconnected environments. Each package addresses specific missing images for different components. extras_version \u00a4 Version of the extras package to prepare for mirroring. Required Environment Variable: EXTRAS_VERSION Default: None Purpose : Specifies which version of the extras package to prepare for mirroring. Different versions contain different image sets or versions. When to use : - Always required for extras package mirroring preparation - Must match an available version for the specified extras package - See \"Available Extras\" table above for valid versions per package Valid values : Version depends on extras package: - catalog : N/A (no version required) - db2u : 1.0.0 , 1.0.1 - mongoce : 4.2.6 , 4.2.23 , 4.4.21 - wd : 5.3.1 - odf : 4.15 Impact : Determines which version of images are included in the mirror manifest. Version must exist for the specified extras package. Related variables : - extras_name : Extras package for this version Note : Not all extras packages have versions (e.g., catalog ). Refer to the \"Available Extras\" table for valid version combinations. registry_public_host \u00a4 Target registry hostname for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None Purpose : Specifies the hostname of the target container registry where extras images will be mirrored. Used to generate the mirror manifest with correct destination paths. When to use : - Always required for mirror manifest preparation - Must be the hostname of your disconnected/private registry - Images are not mirrored yet, but manifest needs target destination Valid values : Valid hostname or IP address (e.g., registry.example.com , 10.0.0.100 ) Impact : Determines the target registry in the generated mirror manifest. All image paths will reference this host. Related variables : - registry_public_port : Port for this registry - registry_prefix : Optional path prefix in registry Note : Images are not mirrored during this role execution. This role only prepares the manifest. Use mirror_images role to perform actual mirroring. registry_public_port \u00a4 Target registry port for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None Purpose : Specifies the port of the target container registry where extras images will be mirrored. Used to generate the mirror manifest with correct destination paths. When to use : - Always required for mirror manifest preparation - Must be the port where your registry is accessible - Common values: 443 (HTTPS), 5000 (HTTP), 32500 (NodePort) Valid values : Valid port number (e.g., 443 , 5000 , 32500 ) Impact : Determines the target registry port in the generated mirror manifest. All image paths will include this port. Related variables : - registry_public_host : Hostname for this registry - registry_prefix : Optional path prefix in registry Note : Images are not mirrored during this role execution. This role only prepares the manifest with the target destination. registry_prefix \u00a4 Path prefix in target registry for mirrored images. Optional Environment Variable: REGISTRY_PREFIX Default: None Purpose : Specifies an optional path prefix in the target registry. Extras images will be mirrored to {host}:{port}/{prefix}/{reponame} format. When to use : - Leave unset if images should be at registry root - Set to organize images under a specific path (e.g., project name, namespace) - Useful for multi-tenant registries or organizing extras separately Valid values : Valid registry path (e.g., mas-extras , production , project-name ) Impact : - When set: Images mirrored to {host}:{port}/{prefix}/{reponame} - When unset: Images mirrored to {host}:{port}/{reponame} Related variables : - registry_public_host : Registry hostname - registry_public_port : Registry port Note : The prefix helps organize images in the registry. Example: with prefix mas-extras , images go to registry.example.com:5000/mas-extras/mongodb-community-operator . Example Playbook \u00a4 - hosts: localhost vars: extras_name: mongoce extras_version: 4.2.6 registry_public_host: myocp-5f1320191125833da1cac8216c06779e-0000.us-south.containers.appdomain.cloud registry_public_port: 32500 registry_prefix: projectName roles: - ibm.mas_devops.mirror_extras_prepare License \u00a4 EPL-2.0","title":"mirror_extras_prepare"},{"location":"roles/mirror_extras_prepare/#mirror_extras_prepare","text":"This role generates a mirror manifest file suitable for use with the oc mirror command (or the ibm.mas_devops.mirror_images role) for a specific set of extra images. Available Extras Extra Versions Description catalog N/A Special extra package for mirroring the IBM Maximo Operator Catalog db2u 1.0.0, 1.0.1 Extra container images missing from the ibm-db2operator CASE bundle mongoce 4.2.6, 4.2.23, 4.4.21 Package containing all images required to use MongoCE Operator in the disconnected environment wd 5.3.1 Extra container images missing from the ibm-watson-discovery CASE bundle odf 4.15 Extra images needed for ODF 4.15","title":"mirror_extras_prepare"},{"location":"roles/mirror_extras_prepare/#role-variables","text":"","title":"Role Variables"},{"location":"roles/mirror_extras_prepare/#extras_name","text":"Name of the extras package to prepare for mirroring. Required Environment Variable: EXTRAS_NAME Default: None Purpose : Specifies which extras package to prepare for mirroring. Extras packages contain additional container images not included in standard CASE bundles but required for MAS deployments. When to use : - Always required for extras package mirroring preparation - Must match an available extras package name - See \"Available Extras\" table above for valid packages Valid values : catalog , db2u , mongoce , wd , odf - catalog : IBM Maximo Operator Catalog images - db2u : Extra Db2 images missing from ibm-db2operator CASE - mongoce : MongoDB Community Edition Operator images - wd : Extra Watson Discovery images missing from CASE - odf : OpenShift Data Foundation extra images Impact : Determines which extras package is downloaded and processed. Incorrect name will cause preparation to fail. Related variables : - extras_version : Version of this extras package to prepare Note : Extras packages fill gaps in CASE bundles, providing images needed for disconnected environments. Each package addresses specific missing images for different components.","title":"extras_name"},{"location":"roles/mirror_extras_prepare/#extras_version","text":"Version of the extras package to prepare for mirroring. Required Environment Variable: EXTRAS_VERSION Default: None Purpose : Specifies which version of the extras package to prepare for mirroring. Different versions contain different image sets or versions. When to use : - Always required for extras package mirroring preparation - Must match an available version for the specified extras package - See \"Available Extras\" table above for valid versions per package Valid values : Version depends on extras package: - catalog : N/A (no version required) - db2u : 1.0.0 , 1.0.1 - mongoce : 4.2.6 , 4.2.23 , 4.4.21 - wd : 5.3.1 - odf : 4.15 Impact : Determines which version of images are included in the mirror manifest. Version must exist for the specified extras package. Related variables : - extras_name : Extras package for this version Note : Not all extras packages have versions (e.g., catalog ). Refer to the \"Available Extras\" table for valid version combinations.","title":"extras_version"},{"location":"roles/mirror_extras_prepare/#registry_public_host","text":"Target registry hostname for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None Purpose : Specifies the hostname of the target container registry where extras images will be mirrored. Used to generate the mirror manifest with correct destination paths. When to use : - Always required for mirror manifest preparation - Must be the hostname of your disconnected/private registry - Images are not mirrored yet, but manifest needs target destination Valid values : Valid hostname or IP address (e.g., registry.example.com , 10.0.0.100 ) Impact : Determines the target registry in the generated mirror manifest. All image paths will reference this host. Related variables : - registry_public_port : Port for this registry - registry_prefix : Optional path prefix in registry Note : Images are not mirrored during this role execution. This role only prepares the manifest. Use mirror_images role to perform actual mirroring.","title":"registry_public_host"},{"location":"roles/mirror_extras_prepare/#registry_public_port","text":"Target registry port for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None Purpose : Specifies the port of the target container registry where extras images will be mirrored. Used to generate the mirror manifest with correct destination paths. When to use : - Always required for mirror manifest preparation - Must be the port where your registry is accessible - Common values: 443 (HTTPS), 5000 (HTTP), 32500 (NodePort) Valid values : Valid port number (e.g., 443 , 5000 , 32500 ) Impact : Determines the target registry port in the generated mirror manifest. All image paths will include this port. Related variables : - registry_public_host : Hostname for this registry - registry_prefix : Optional path prefix in registry Note : Images are not mirrored during this role execution. This role only prepares the manifest with the target destination.","title":"registry_public_port"},{"location":"roles/mirror_extras_prepare/#registry_prefix","text":"Path prefix in target registry for mirrored images. Optional Environment Variable: REGISTRY_PREFIX Default: None Purpose : Specifies an optional path prefix in the target registry. Extras images will be mirrored to {host}:{port}/{prefix}/{reponame} format. When to use : - Leave unset if images should be at registry root - Set to organize images under a specific path (e.g., project name, namespace) - Useful for multi-tenant registries or organizing extras separately Valid values : Valid registry path (e.g., mas-extras , production , project-name ) Impact : - When set: Images mirrored to {host}:{port}/{prefix}/{reponame} - When unset: Images mirrored to {host}:{port}/{reponame} Related variables : - registry_public_host : Registry hostname - registry_public_port : Registry port Note : The prefix helps organize images in the registry. Example: with prefix mas-extras , images go to registry.example.com:5000/mas-extras/mongodb-community-operator .","title":"registry_prefix"},{"location":"roles/mirror_extras_prepare/#example-playbook","text":"- hosts: localhost vars: extras_name: mongoce extras_version: 4.2.6 registry_public_host: myocp-5f1320191125833da1cac8216c06779e-0000.us-south.containers.appdomain.cloud registry_public_port: 32500 registry_prefix: projectName roles: - ibm.mas_devops.mirror_extras_prepare","title":"Example Playbook"},{"location":"roles/mirror_extras_prepare/#license","text":"EPL-2.0","title":"License"},{"location":"roles/mirror_images/","text":"mirror_images \u00a4 This role supports mirroring container images to a target mirror registry using the oc mirror command. It performs the actual image mirroring operation using manifests prepared by mirror_case_prepare or mirror_extras_prepare roles. Role Variables \u00a4 Registry Configuration \u00a4 registry_public_host \u00a4 Target registry hostname for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None Purpose : Specifies the hostname of the target container registry where images will be mirrored. This is the actual destination for the mirroring operation. When to use : - Always required for image mirroring - Must be the hostname of your disconnected/private registry - Must match the registry specified in the mirror manifest Valid values : Valid hostname or IP address (e.g., registry.example.com , 10.0.0.100 ) Impact : Determines the target registry for image mirroring. Images are pushed to this registry. Related variables : - registry_public_port : Port for this registry - registry_prefix : Optional path prefix in registry - registry_username , registry_password : Authentication credentials Note : Must match the registry specified when preparing the mirror manifest. Registry must be accessible and have sufficient storage space. registry_public_port \u00a4 Target registry port for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None Purpose : Specifies the port of the target container registry where images will be mirrored. When to use : - Always required for image mirroring - Must be the port where your registry is accessible - Common values: 443 (HTTPS), 5000 (HTTP), 32500 (NodePort) Valid values : Valid port number (e.g., 443 , 5000 , 32500 ) Impact : Determines the target registry port for image mirroring. Images are pushed to this port. Related variables : - registry_public_host : Hostname for this registry - registry_prefix : Optional path prefix in registry Note : Must match the port specified when preparing the mirror manifest. Ensure the port is accessible from the mirroring system. registry_prefix \u00a4 Path prefix in target registry for mirrored images. Optional Environment Variable: REGISTRY_PREFIX Default: None Purpose : Specifies an optional path prefix in the target registry. Images are mirrored to {host}:{port}/{prefix}/{reponame} format. When to use : - Leave unset if images should be at registry root - Set to organize images under a specific path (e.g., project name, namespace) - Must match the prefix used when preparing the mirror manifest Valid values : Valid registry path (e.g., mas-mirror , production , project-name ) Impact : - When set: Images mirrored to {host}:{port}/{prefix}/{reponame} - When unset: Images mirrored to {host}:{port}/{reponame} Related variables : - registry_public_host : Registry hostname - registry_public_port : Registry port Note : Must match the prefix specified when preparing the mirror manifest. The prefix helps organize images in the registry. registry_is_ecr \u00a4 Enable AWS Elastic Container Registry (ECR) support. Optional Environment Variable: REGISTRY_IS_ECR Default: false Purpose : Indicates whether the target registry is AWS Elastic Container Registry. Enables ECR-specific authentication and configuration. When to use : - Set to true when mirroring to AWS ECR - Leave as false (default) for other registry types - Requires AWS credentials and ECR region configuration Valid values : true , false Impact : - true : Uses ECR-specific authentication and configuration - false : Uses standard registry authentication Related variables : - registry_ecr_aws_region : AWS region for ECR (required when true ) Note : When using ECR, ensure AWS credentials are configured and the ECR region is specified. registry_ecr_aws_region \u00a4 AWS region for Elastic Container Registry. Optional (Required when registry_is_ecr=true ) Environment Variable: REGISTRY_ECR_AWS_REGION Default: None Purpose : Specifies the AWS region where the ECR registry is located. Required for ECR authentication and access. When to use : - Only applies when registry_is_ecr=true - Must match the region where your ECR registry exists - Required for ECR authentication Valid values : Valid AWS region (e.g., us-east-1 , eu-west-1 , ap-southeast-1 ) Impact : Determines which AWS region is used for ECR authentication and access. Related variables : - registry_is_ecr : Must be true for this to apply Note : Ensure AWS credentials have permissions to push images to ECR in the specified region. Authentication \u00a4 registry_username \u00a4 Username for target registry authentication. Required Environment Variable: REGISTRY_USERNAME Default: None Purpose : Provides the username for authenticating to the target container registry. Required to push images during mirroring. When to use : - Always required for image mirroring (unless using ECR with AWS credentials) - Must have push permissions to the target registry - Obtain from your registry administrator Valid values : Valid username for the target registry Impact : Used to authenticate to the target registry. Without valid credentials, image push will fail. Related variables : - registry_password : Password paired with this username Note : Keep credentials secure. Use environment variables or secure vaults rather than hardcoding in playbooks. registry_password \u00a4 Password for target registry authentication. Required Environment Variable: REGISTRY_PASSWORD Default: None Purpose : Provides the password for authenticating to the target container registry. Required to push images during mirroring. When to use : - Always required for image mirroring (unless using ECR with AWS credentials) - Must correspond to the provided username - Keep secure and rotate regularly Valid values : Valid password for the target registry Impact : Used to authenticate to the target registry. Without valid credentials, image push will fail. Related variables : - registry_username : Username paired with this password Note : Keep passwords secure. Never commit to version control. Use environment variables or secure vaults. ibm_entitlement_key \u00a4 IBM entitlement key for accessing source images. Required (for IBM images) Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Authenticates access to IBM's entitled container registry to pull source images during mirroring. Required when mirroring IBM product images. When to use : - Required when mirroring IBM product images (MAS, Db2, etc.) - Obtain from IBM Container Library at https://myibm.ibm.com/products-services/containerlibrary - Must be a valid, non-expired key with appropriate entitlements Valid values : IBM entitlement key string (typically starts with \"eyJ...\") Impact : Without a valid key, pulling IBM source images will fail and mirroring cannot proceed. Related variables : - None Note : Keep entitlement keys secure. Keys are tied to your IBM Cloud account and product entitlements. Verify key has entitlements for the products being mirrored. Mirroring Configuration \u00a4 mirror_mode \u00a4 Mirroring operation mode. Optional Environment Variable: MIRROR_MODE Default: None Purpose : Specifies the mode of operation for the mirroring process. Controls whether to perform direct mirroring or use disk-based mirroring. When to use : - Set based on your mirroring workflow requirements - Use direct for direct registry-to-registry mirroring - Use to-disk or from-disk for disk-based mirroring workflows Valid values : direct , to-disk , from-disk Impact : Determines the mirroring workflow. Direct mode requires network access to both source and target registries. Disk-based mode allows offline transfer. Related variables : - mirror_working_dir : Working directory for disk-based mirroring Note : Disk-based mirroring is useful for air-gapped environments where direct registry access is not possible. mirror_working_dir \u00a4 Working directory for mirroring operations. Optional Environment Variable: MIRROR_WORKING_DIR Default: None Purpose : Specifies the working directory for mirroring operations. Used to store manifests, temporary files, and disk-based mirror archives. When to use : - Set to specify a custom working directory - Leave unset to use default location - Ensure directory has sufficient disk space for disk-based mirroring Valid values : Absolute filesystem path Impact : Determines where mirroring files are stored. Insufficient space will cause mirroring to fail. Related variables : - mirror_mode : Working directory used for disk-based modes Note : For disk-based mirroring, ensure the directory has sufficient space for all images (can be hundreds of GB). manifest_name \u00a4 Name of the mirror manifest to use. Optional Environment Variable: MANIFEST_NAME Default: None Purpose : Specifies the name of the mirror manifest file to use for the mirroring operation. Manifest defines which images to mirror. When to use : - Set to use a specific manifest file - Leave unset to use default manifest discovery - Manifest should be prepared by mirror_case_prepare or mirror_extras_prepare roles Valid values : Manifest filename (e.g., ibm-mas , mongoce ) Impact : Determines which manifest is used for mirroring. Incorrect name will cause mirroring to fail. Related variables : - manifest_version : Version of the manifest - mirror_working_dir : Directory containing manifests Note : Manifests are typically prepared by the mirror_case_prepare or mirror_extras_prepare roles before running this role. manifest_version \u00a4 Version of the mirror manifest to use. Optional Environment Variable: MANIFEST_VERSION Default: None Purpose : Specifies the version of the mirror manifest file to use for the mirroring operation. When to use : - Set to use a specific manifest version - Leave unset to use default version discovery - Must match a manifest prepared earlier Valid values : Version string (e.g., 8.8.1 , 9.0.0 ) Impact : Determines which manifest version is used for mirroring. Incorrect version will cause mirroring to fail. Related variables : - manifest_name : Name of the manifest Note : Underscores in version are automatically converted to dots (e.g., 8_8_1 becomes 8.8.1 ). mirror_single_arch \u00a4 Mirror only a single architecture. Optional Environment Variable: MIRROR_SINGLE_ARCH Default: amd64 Purpose : Specifies whether to mirror only a single architecture instead of all available architectures. Reduces mirror size and time. When to use : - Use default ( amd64 ) for x86_64 clusters (most common) - Set to arm64 for ARM-based clusters - Set to empty string to mirror all architectures Valid values : amd64 , arm64 , or empty string for all architectures Impact : - amd64 : Mirrors only x86_64 images (smaller, faster) - arm64 : Mirrors only ARM images - Empty: Mirrors all architectures (larger, slower) Related variables : - None Note : Most OpenShift clusters use amd64 architecture. Only change if you have specific architecture requirements. Example Playbook \u00a4 - hosts: localhost vars: # Target registry registry_public_host: registry.example.com registry_public_port: 5000 registry_prefix: mas-mirror # Registry authentication registry_username: admin registry_password: \"{{ lookup('env', 'REGISTRY_PASSWORD') }}\" # IBM authentication ibm_entitlement_key: \"{{ lookup('env', 'IBM_ENTITLEMENT_KEY') }}\" # Mirroring configuration manifest_name: ibm-mas manifest_version: 8.8.1 mirror_mode: direct roles: - ibm.mas_devops.mirror_images License \u00a4 EPL-2.0","title":"mirror_images"},{"location":"roles/mirror_images/#mirror_images","text":"This role supports mirroring container images to a target mirror registry using the oc mirror command. It performs the actual image mirroring operation using manifests prepared by mirror_case_prepare or mirror_extras_prepare roles.","title":"mirror_images"},{"location":"roles/mirror_images/#role-variables","text":"","title":"Role Variables"},{"location":"roles/mirror_images/#registry-configuration","text":"","title":"Registry Configuration"},{"location":"roles/mirror_images/#registry_public_host","text":"Target registry hostname for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None Purpose : Specifies the hostname of the target container registry where images will be mirrored. This is the actual destination for the mirroring operation. When to use : - Always required for image mirroring - Must be the hostname of your disconnected/private registry - Must match the registry specified in the mirror manifest Valid values : Valid hostname or IP address (e.g., registry.example.com , 10.0.0.100 ) Impact : Determines the target registry for image mirroring. Images are pushed to this registry. Related variables : - registry_public_port : Port for this registry - registry_prefix : Optional path prefix in registry - registry_username , registry_password : Authentication credentials Note : Must match the registry specified when preparing the mirror manifest. Registry must be accessible and have sufficient storage space.","title":"registry_public_host"},{"location":"roles/mirror_images/#registry_public_port","text":"Target registry port for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None Purpose : Specifies the port of the target container registry where images will be mirrored. When to use : - Always required for image mirroring - Must be the port where your registry is accessible - Common values: 443 (HTTPS), 5000 (HTTP), 32500 (NodePort) Valid values : Valid port number (e.g., 443 , 5000 , 32500 ) Impact : Determines the target registry port for image mirroring. Images are pushed to this port. Related variables : - registry_public_host : Hostname for this registry - registry_prefix : Optional path prefix in registry Note : Must match the port specified when preparing the mirror manifest. Ensure the port is accessible from the mirroring system.","title":"registry_public_port"},{"location":"roles/mirror_images/#registry_prefix","text":"Path prefix in target registry for mirrored images. Optional Environment Variable: REGISTRY_PREFIX Default: None Purpose : Specifies an optional path prefix in the target registry. Images are mirrored to {host}:{port}/{prefix}/{reponame} format. When to use : - Leave unset if images should be at registry root - Set to organize images under a specific path (e.g., project name, namespace) - Must match the prefix used when preparing the mirror manifest Valid values : Valid registry path (e.g., mas-mirror , production , project-name ) Impact : - When set: Images mirrored to {host}:{port}/{prefix}/{reponame} - When unset: Images mirrored to {host}:{port}/{reponame} Related variables : - registry_public_host : Registry hostname - registry_public_port : Registry port Note : Must match the prefix specified when preparing the mirror manifest. The prefix helps organize images in the registry.","title":"registry_prefix"},{"location":"roles/mirror_images/#registry_is_ecr","text":"Enable AWS Elastic Container Registry (ECR) support. Optional Environment Variable: REGISTRY_IS_ECR Default: false Purpose : Indicates whether the target registry is AWS Elastic Container Registry. Enables ECR-specific authentication and configuration. When to use : - Set to true when mirroring to AWS ECR - Leave as false (default) for other registry types - Requires AWS credentials and ECR region configuration Valid values : true , false Impact : - true : Uses ECR-specific authentication and configuration - false : Uses standard registry authentication Related variables : - registry_ecr_aws_region : AWS region for ECR (required when true ) Note : When using ECR, ensure AWS credentials are configured and the ECR region is specified.","title":"registry_is_ecr"},{"location":"roles/mirror_images/#registry_ecr_aws_region","text":"AWS region for Elastic Container Registry. Optional (Required when registry_is_ecr=true ) Environment Variable: REGISTRY_ECR_AWS_REGION Default: None Purpose : Specifies the AWS region where the ECR registry is located. Required for ECR authentication and access. When to use : - Only applies when registry_is_ecr=true - Must match the region where your ECR registry exists - Required for ECR authentication Valid values : Valid AWS region (e.g., us-east-1 , eu-west-1 , ap-southeast-1 ) Impact : Determines which AWS region is used for ECR authentication and access. Related variables : - registry_is_ecr : Must be true for this to apply Note : Ensure AWS credentials have permissions to push images to ECR in the specified region.","title":"registry_ecr_aws_region"},{"location":"roles/mirror_images/#authentication","text":"","title":"Authentication"},{"location":"roles/mirror_images/#registry_username","text":"Username for target registry authentication. Required Environment Variable: REGISTRY_USERNAME Default: None Purpose : Provides the username for authenticating to the target container registry. Required to push images during mirroring. When to use : - Always required for image mirroring (unless using ECR with AWS credentials) - Must have push permissions to the target registry - Obtain from your registry administrator Valid values : Valid username for the target registry Impact : Used to authenticate to the target registry. Without valid credentials, image push will fail. Related variables : - registry_password : Password paired with this username Note : Keep credentials secure. Use environment variables or secure vaults rather than hardcoding in playbooks.","title":"registry_username"},{"location":"roles/mirror_images/#registry_password","text":"Password for target registry authentication. Required Environment Variable: REGISTRY_PASSWORD Default: None Purpose : Provides the password for authenticating to the target container registry. Required to push images during mirroring. When to use : - Always required for image mirroring (unless using ECR with AWS credentials) - Must correspond to the provided username - Keep secure and rotate regularly Valid values : Valid password for the target registry Impact : Used to authenticate to the target registry. Without valid credentials, image push will fail. Related variables : - registry_username : Username paired with this password Note : Keep passwords secure. Never commit to version control. Use environment variables or secure vaults.","title":"registry_password"},{"location":"roles/mirror_images/#ibm_entitlement_key","text":"IBM entitlement key for accessing source images. Required (for IBM images) Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Authenticates access to IBM's entitled container registry to pull source images during mirroring. Required when mirroring IBM product images. When to use : - Required when mirroring IBM product images (MAS, Db2, etc.) - Obtain from IBM Container Library at https://myibm.ibm.com/products-services/containerlibrary - Must be a valid, non-expired key with appropriate entitlements Valid values : IBM entitlement key string (typically starts with \"eyJ...\") Impact : Without a valid key, pulling IBM source images will fail and mirroring cannot proceed. Related variables : - None Note : Keep entitlement keys secure. Keys are tied to your IBM Cloud account and product entitlements. Verify key has entitlements for the products being mirrored.","title":"ibm_entitlement_key"},{"location":"roles/mirror_images/#mirroring-configuration","text":"","title":"Mirroring Configuration"},{"location":"roles/mirror_images/#mirror_mode","text":"Mirroring operation mode. Optional Environment Variable: MIRROR_MODE Default: None Purpose : Specifies the mode of operation for the mirroring process. Controls whether to perform direct mirroring or use disk-based mirroring. When to use : - Set based on your mirroring workflow requirements - Use direct for direct registry-to-registry mirroring - Use to-disk or from-disk for disk-based mirroring workflows Valid values : direct , to-disk , from-disk Impact : Determines the mirroring workflow. Direct mode requires network access to both source and target registries. Disk-based mode allows offline transfer. Related variables : - mirror_working_dir : Working directory for disk-based mirroring Note : Disk-based mirroring is useful for air-gapped environments where direct registry access is not possible.","title":"mirror_mode"},{"location":"roles/mirror_images/#mirror_working_dir","text":"Working directory for mirroring operations. Optional Environment Variable: MIRROR_WORKING_DIR Default: None Purpose : Specifies the working directory for mirroring operations. Used to store manifests, temporary files, and disk-based mirror archives. When to use : - Set to specify a custom working directory - Leave unset to use default location - Ensure directory has sufficient disk space for disk-based mirroring Valid values : Absolute filesystem path Impact : Determines where mirroring files are stored. Insufficient space will cause mirroring to fail. Related variables : - mirror_mode : Working directory used for disk-based modes Note : For disk-based mirroring, ensure the directory has sufficient space for all images (can be hundreds of GB).","title":"mirror_working_dir"},{"location":"roles/mirror_images/#manifest_name","text":"Name of the mirror manifest to use. Optional Environment Variable: MANIFEST_NAME Default: None Purpose : Specifies the name of the mirror manifest file to use for the mirroring operation. Manifest defines which images to mirror. When to use : - Set to use a specific manifest file - Leave unset to use default manifest discovery - Manifest should be prepared by mirror_case_prepare or mirror_extras_prepare roles Valid values : Manifest filename (e.g., ibm-mas , mongoce ) Impact : Determines which manifest is used for mirroring. Incorrect name will cause mirroring to fail. Related variables : - manifest_version : Version of the manifest - mirror_working_dir : Directory containing manifests Note : Manifests are typically prepared by the mirror_case_prepare or mirror_extras_prepare roles before running this role.","title":"manifest_name"},{"location":"roles/mirror_images/#manifest_version","text":"Version of the mirror manifest to use. Optional Environment Variable: MANIFEST_VERSION Default: None Purpose : Specifies the version of the mirror manifest file to use for the mirroring operation. When to use : - Set to use a specific manifest version - Leave unset to use default version discovery - Must match a manifest prepared earlier Valid values : Version string (e.g., 8.8.1 , 9.0.0 ) Impact : Determines which manifest version is used for mirroring. Incorrect version will cause mirroring to fail. Related variables : - manifest_name : Name of the manifest Note : Underscores in version are automatically converted to dots (e.g., 8_8_1 becomes 8.8.1 ).","title":"manifest_version"},{"location":"roles/mirror_images/#mirror_single_arch","text":"Mirror only a single architecture. Optional Environment Variable: MIRROR_SINGLE_ARCH Default: amd64 Purpose : Specifies whether to mirror only a single architecture instead of all available architectures. Reduces mirror size and time. When to use : - Use default ( amd64 ) for x86_64 clusters (most common) - Set to arm64 for ARM-based clusters - Set to empty string to mirror all architectures Valid values : amd64 , arm64 , or empty string for all architectures Impact : - amd64 : Mirrors only x86_64 images (smaller, faster) - arm64 : Mirrors only ARM images - Empty: Mirrors all architectures (larger, slower) Related variables : - None Note : Most OpenShift clusters use amd64 architecture. Only change if you have specific architecture requirements.","title":"mirror_single_arch"},{"location":"roles/mirror_images/#example-playbook","text":"- hosts: localhost vars: # Target registry registry_public_host: registry.example.com registry_public_port: 5000 registry_prefix: mas-mirror # Registry authentication registry_username: admin registry_password: \"{{ lookup('env', 'REGISTRY_PASSWORD') }}\" # IBM authentication ibm_entitlement_key: \"{{ lookup('env', 'IBM_ENTITLEMENT_KEY') }}\" # Mirroring configuration manifest_name: ibm-mas manifest_version: 8.8.1 mirror_mode: direct roles: - ibm.mas_devops.mirror_images","title":"Example Playbook"},{"location":"roles/mirror_images/#license","text":"EPL-2.0","title":"License"},{"location":"roles/mirror_ocp/","text":"mirror_ocp \u00a4 This role supports mirroring the Red Hat Platform and selected content from the Red Hat operator catalogs . Only content in the Red Hat catalogs directly used by IBM Maximo Application Suite is mirrored. Four actions are supported: direct Directly mirror content to your target registry to-filesystem Mirror content to the local filesystem from-filesystem Mirror content from the local filesystem to your target registry Catalogs \u00a4 Three catalogs are mirrored, containing the following content: certified-operator-index \u00a4 gpu-operator-certified (required by ibm.mas_devops.nvidia_gpu role) kubeturbo-certified (required by ibm.mas_devops.kubeturbo role) ibm-metrics-operator (required by ibm.mas_devops.dro role) ibm-data-reporter-operator (required by ibm.mas_devops.dro role) community-operator-index \u00a4 grafana-operator (required by ibm.mas_devops.grafana role) strimzi-kafka-operator (required by ibm.mas_devops.kafka role) redhat-operator-index \u00a4 amq-streams (required by ibm.mas_devops.kafka role) openshift-pipelines-operator-rh (required by the MAS CLI) nfd (required by ibm.mas_devops.nvidia_gpu role) aws-efs-csi-driver-operator (required by ibm.mas_devops.ocp_efs role) local-storage-operator (required by ibm.mas_devops.ocs role) odf-operator (required by ibm.mas_devops.ocs role) openshift-cert-manager-operator (required by ibm.mas_devops.cert_manager role) lvms-operator (not directly used, but often used in SNO environments) Requirements \u00a4 oc tool must be installed oc-mirror plugin must be installed Role Variables - General \u00a4 mirror_mode \u00a4 Mirroring operation mode for Red Hat content. Required Environment Variable: MIRROR_MODE Default: None Purpose : Specifies the mode of operation for mirroring Red Hat OpenShift platform and operator catalog content. Controls the mirroring workflow. When to use : - Always required for Red Hat content mirroring - Use direct for direct registry-to-registry mirroring (requires network access to both) - Use to-filesystem to mirror to local disk (for air-gapped transfer) - Use from-filesystem to mirror from local disk to target registry Valid values : direct , to-filesystem , from-filesystem Impact : - direct : Mirrors directly from Red Hat registries to target registry (fastest, requires network access) - to-filesystem : Mirrors to local filesystem for offline transfer (enables air-gapped deployment) - from-filesystem : Mirrors from local filesystem to target registry (completes air-gapped deployment) Related variables : - mirror_working_dir : Working directory for all modes - mirror_redhat_platform : Whether to mirror platform images - mirror_redhat_operators : Whether to mirror operator catalogs Note : For air-gapped environments, use to-filesystem on a connected system, transfer files, then use from-filesystem on the disconnected system. Role Variables - Mirror Actions \u00a4 mirror_working_dir \u00a4 Working directory for mirroring operations. Required Environment Variable: MIRROR_WORKING_DIR Default: None Purpose : Specifies the working directory for mirroring operations. Used to store manifests, temporary files, and disk-based mirror archives. When to use : - Always required for Red Hat content mirroring - Must have sufficient disk space (especially for to-filesystem mode) - Directory is created if it doesn't exist Valid values : Absolute filesystem path (e.g., /tmp/mirror , /mnt/mirror-storage ) Impact : Determines where mirroring files are stored. Insufficient space will cause mirroring to fail. Related variables : - mirror_mode : Working directory used for all modes Note : For to-filesystem mode, ensure the directory has sufficient space for all images (can be hundreds of GB for platform + operators). The directory structure is preserved for from-filesystem mode. mirror_redhat_platform \u00a4 Enable mirroring of Red Hat OpenShift platform images. Optional Environment Variable: MIRROR_REDHAT_PLATFORM Default: false Purpose : Controls whether to mirror Red Hat OpenShift Container Platform release images. Required for installing or upgrading OpenShift in disconnected environments. When to use : - Set to true to mirror OpenShift platform images - Leave as false (default) to skip platform mirroring - Enable when preparing for OpenShift installation or upgrades in air-gapped environments Valid values : true , false Impact : - true : Mirrors OpenShift platform images (large download, required for OCP install/upgrade) - false : Skips platform mirroring (only operators are mirrored if enabled) Related variables : - ocp_release : OpenShift version to mirror - ocp_min_version , ocp_max_version : Version range to mirror Note : Platform images are large (tens of GB). Only enable if you need to install or upgrade OpenShift in a disconnected environment. mirror_redhat_operators \u00a4 Enable mirroring of selected Red Hat operator catalog content. Optional Environment Variable: MIRROR_REDHAT_OPERATORS Default: false Purpose : Controls whether to mirror selected content from Red Hat operator catalogs. Only operators directly used by MAS are mirrored (see catalog list above). When to use : - Set to true to mirror Red Hat operator catalogs - Leave as false (default) to skip operator mirroring - Enable when preparing for MAS deployment in air-gapped environments Valid values : true , false Impact : - true : Mirrors selected operators from certified, community, and redhat-operator-index catalogs - false : Skips operator catalog mirroring Related variables : - ocp_release : OpenShift version for operator compatibility Note : Only selected operators used by MAS are mirrored, not entire catalogs. See the catalog list at the top of this README for included operators. redhat_pullsecret \u00a4 Path to Red Hat pull secret file. Required Environment Variable: REDHAT_PULLSECRET Default: None Purpose : Specifies the path to your Red Hat pull secret file. Required to authenticate and pull images from Red Hat registries during mirroring. When to use : - Always required for Red Hat content mirroring - Obtain from Red Hat OpenShift Console - Must be a valid, non-expired pull secret Valid values : Absolute path to pull secret JSON file (e.g., ~/pull-secret.json , /tmp/pull-secret.json ) Impact : Without a valid pull secret, pulling Red Hat images will fail and mirroring cannot proceed. Related variables : - None Note : Download your pull secret from the Red Hat OpenShift Console. Keep the file secure as it contains authentication credentials. The pull secret must be valid and associated with an active Red Hat account. Role Variables - OpenShift Version \u00a4 ocp_release \u00a4 OpenShift release version to mirror. Required Environment Variable: OCP_RELEASE Default: None Purpose : Specifies the major.minor version of OpenShift Container Platform to mirror content for. Determines which platform images and operator versions are mirrored. When to use : - Always required for Red Hat content mirroring - Must match the OpenShift version in your target environment - Use format: 4.19 , 4.18 , 4.17 Valid values : OpenShift major.minor version (e.g., 4.19 , 4.18 , 4.17 , 4.16 ) Impact : Determines which OpenShift version's images and operators are mirrored. Must match your target cluster version. Related variables : - ocp_min_version : Minimum patch version to mirror - ocp_max_version : Maximum patch version to mirror - mirror_redhat_platform : Whether to mirror platform images for this version Note : Use the major.minor version format (e.g., 4.19 ), not full version (e.g., 4.19.10 ). Use ocp_min_version and ocp_max_version to control patch version range. ocp_min_version \u00a4 Minimum OpenShift patch version to mirror. Optional Environment Variable: OCP_MIN_VERSION Default: None (mirrors all versions) Purpose : Specifies the minimum patch version of OpenShift platform images to mirror. Limits the version range to reduce mirror size. When to use : - Leave unset to mirror all available patch versions for the release - Set to mirror only specific patch versions and newer - Only applies when mirror_redhat_platform=true Valid values : Full OpenShift version (e.g., 4.19.10 , 4.19.15 ) Impact : Only platform images for this version and newer are mirrored. Reduces mirror size but limits available versions. Related variables : - ocp_release : Major.minor version (must match) - ocp_max_version : Maximum version to mirror - mirror_redhat_platform : Must be true for this to apply Note : Only affects platform image mirroring, not operators. Use to limit mirror size when you know the specific OpenShift versions you need. ocp_max_version \u00a4 Maximum OpenShift patch version to mirror. Optional Environment Variable: OCP_MAX_VERSION Default: None (mirrors all versions) Purpose : Specifies the maximum patch version of OpenShift platform images to mirror. Limits the version range to reduce mirror size. When to use : - Leave unset to mirror all available patch versions for the release - Set to mirror only specific patch versions and older - Only applies when mirror_redhat_platform=true Valid values : Full OpenShift version (e.g., 4.19.20 , 4.19.25 ) Impact : Only platform images for this version and older are mirrored. Reduces mirror size but limits available versions. Related variables : - ocp_release : Major.minor version (must match) - ocp_min_version : Minimum version to mirror - mirror_redhat_platform : Must be true for this to apply Note : Only affects platform image mirroring, not operators. Use to limit mirror size when you know the specific OpenShift versions you need. Role Variables - Target Registry \u00a4 registry_public_host \u00a4 Target registry hostname for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None Purpose : Specifies the hostname of the target container registry where Red Hat images will be mirrored. When to use : - Always required for Red Hat content mirroring - Must be the hostname of your disconnected/private registry - Used for direct and from-filesystem modes Valid values : Valid hostname or IP address (e.g., registry.example.com , 10.0.0.100 ) Impact : Determines the target registry for image mirroring. Images are pushed to this registry. Related variables : - registry_public_port : Port for this registry - registry_prefix_redhat : Optional path prefix in registry Note : Registry must be accessible and have sufficient storage space for Red Hat content (can be hundreds of GB). registry_public_port \u00a4 Target registry port for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None Purpose : Specifies the port of the target container registry where Red Hat images will be mirrored. When to use : - Always required for Red Hat content mirroring - Must be the port where your registry is accessible - Common values: 443 (HTTPS), 5000 (HTTP), 32500 (NodePort) Valid values : Valid port number (e.g., 443 , 5000 , 32500 ) Impact : Determines the target registry port for image mirroring. Images are pushed to this port. Related variables : - registry_public_host : Hostname for this registry Note : Ensure the port is accessible from the mirroring system. registry_is_ecr \u00a4 Enable AWS Elastic Container Registry (ECR) support. Optional Environment Variable: REGISTRY_IS_ECR Default: false Purpose : Indicates whether the target registry is AWS Elastic Container Registry. Enables ECR-specific authentication and configuration. When to use : - Set to true when mirroring to AWS ECR - Leave as false (default) for other registry types - Requires AWS credentials and ECR region configuration Valid values : true , false Impact : - true : Uses ECR-specific authentication and configuration - false : Uses standard registry authentication Related variables : - registry_ecr_aws_region : AWS region for ECR (required when true ) Note : When using ECR, ensure AWS credentials are configured and the ECR region is specified. registry_ecr_aws_region \u00a4 AWS region for Elastic Container Registry. Optional (Required when registry_is_ecr=true ) Environment Variable: REGISTRY_ECR_AWS_REGION Default: None Purpose : Specifies the AWS region where the ECR registry is located. Required for ECR authentication and access. When to use : - Only applies when registry_is_ecr=true - Must match the region where your ECR registry exists - Required for ECR authentication Valid values : Valid AWS region (e.g., us-east-1 , eu-west-1 , ap-southeast-1 ) Impact : Determines which AWS region is used for ECR authentication and access. Related variables : - registry_is_ecr : Must be true for this to apply Note : Ensure AWS credentials have permissions to push images to ECR in the specified region. registry_prefix_redhat \u00a4 Path prefix in target registry for Red Hat images. Optional Environment Variable: REGISTRY_PREFIX_REDHAT Default: None Purpose : Specifies an optional path prefix in the target registry for Red Hat images. Images are mirrored to {host}[:{port}]/{prefix}/{reponame} format. When to use : - Leave unset if images should be at registry root - Set to organize Red Hat images under a specific path (e.g., ocp419 , redhat ) - Useful for organizing different content types or versions Valid values : Valid registry path (e.g., ocp419 , redhat , openshift ) Impact : - When set: Images mirrored to {host}:{port}/{prefix}/{reponame} - When unset: Images mirrored to {host}:{port}/{reponame} Related variables : - registry_public_host : Registry hostname - registry_public_port : Registry port Note : The prefix helps organize Red Hat images separately from IBM or other content in the registry. registry_username \u00a4 Username for target registry authentication. Required Environment Variable: REGISTRY_USERNAME Default: None Purpose : Provides the username for authenticating to the target container registry. Required to push images during mirroring. When to use : - Always required for Red Hat content mirroring (unless using ECR with AWS credentials) - Must have push permissions to the target registry - Obtain from your registry administrator Valid values : Valid username for the target registry Impact : Used to authenticate to the target registry. Without valid credentials, image push will fail. Related variables : - registry_password : Password paired with this username Note : Keep credentials secure. Use environment variables or secure vaults rather than hardcoding in playbooks. registry_password \u00a4 Password for target registry authentication. Required Environment Variable: REGISTRY_PASSWORD Default: None Purpose : Provides the password for authenticating to the target container registry. Required to push images during mirroring. When to use : - Always required for Red Hat content mirroring (unless using ECR with AWS credentials) - Must correspond to the provided username - Keep secure and rotate regularly Valid values : Valid password for the target registry Impact : Used to authenticate to the target registry. Without valid credentials, image push will fail. Related variables : - registry_username : Username paired with this password Note : Keep passwords secure. Never commit to version control. Use environment variables or secure vaults. Example Playbook \u00a4 - hosts: localhost vars: registry_public_host: myregistry.mycompany.com registry_public_port: 5000 registry_prefix_redhat: \"ocp416\" registry_username: user1 registry_password: 8934jk77s862! # Not a real password, don't worry security folks mirror_mode: direct mirror_working_dir: /tmp/mirror mirror_redhat_platform: false mirror_redhat_operators: true ocp_release: 4.19 redhat_pullsecret: ~/pull-secret.json roles: - ibm.mas_devops.mirror_ocp License \u00a4 EPL-2.0","title":"mirror_ocp"},{"location":"roles/mirror_ocp/#mirror_ocp","text":"This role supports mirroring the Red Hat Platform and selected content from the Red Hat operator catalogs . Only content in the Red Hat catalogs directly used by IBM Maximo Application Suite is mirrored. Four actions are supported: direct Directly mirror content to your target registry to-filesystem Mirror content to the local filesystem from-filesystem Mirror content from the local filesystem to your target registry","title":"mirror_ocp"},{"location":"roles/mirror_ocp/#catalogs","text":"Three catalogs are mirrored, containing the following content:","title":"Catalogs"},{"location":"roles/mirror_ocp/#certified-operator-index","text":"gpu-operator-certified (required by ibm.mas_devops.nvidia_gpu role) kubeturbo-certified (required by ibm.mas_devops.kubeturbo role) ibm-metrics-operator (required by ibm.mas_devops.dro role) ibm-data-reporter-operator (required by ibm.mas_devops.dro role)","title":"certified-operator-index"},{"location":"roles/mirror_ocp/#community-operator-index","text":"grafana-operator (required by ibm.mas_devops.grafana role) strimzi-kafka-operator (required by ibm.mas_devops.kafka role)","title":"community-operator-index"},{"location":"roles/mirror_ocp/#redhat-operator-index","text":"amq-streams (required by ibm.mas_devops.kafka role) openshift-pipelines-operator-rh (required by the MAS CLI) nfd (required by ibm.mas_devops.nvidia_gpu role) aws-efs-csi-driver-operator (required by ibm.mas_devops.ocp_efs role) local-storage-operator (required by ibm.mas_devops.ocs role) odf-operator (required by ibm.mas_devops.ocs role) openshift-cert-manager-operator (required by ibm.mas_devops.cert_manager role) lvms-operator (not directly used, but often used in SNO environments)","title":"redhat-operator-index"},{"location":"roles/mirror_ocp/#requirements","text":"oc tool must be installed oc-mirror plugin must be installed","title":"Requirements"},{"location":"roles/mirror_ocp/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/mirror_ocp/#mirror_mode","text":"Mirroring operation mode for Red Hat content. Required Environment Variable: MIRROR_MODE Default: None Purpose : Specifies the mode of operation for mirroring Red Hat OpenShift platform and operator catalog content. Controls the mirroring workflow. When to use : - Always required for Red Hat content mirroring - Use direct for direct registry-to-registry mirroring (requires network access to both) - Use to-filesystem to mirror to local disk (for air-gapped transfer) - Use from-filesystem to mirror from local disk to target registry Valid values : direct , to-filesystem , from-filesystem Impact : - direct : Mirrors directly from Red Hat registries to target registry (fastest, requires network access) - to-filesystem : Mirrors to local filesystem for offline transfer (enables air-gapped deployment) - from-filesystem : Mirrors from local filesystem to target registry (completes air-gapped deployment) Related variables : - mirror_working_dir : Working directory for all modes - mirror_redhat_platform : Whether to mirror platform images - mirror_redhat_operators : Whether to mirror operator catalogs Note : For air-gapped environments, use to-filesystem on a connected system, transfer files, then use from-filesystem on the disconnected system.","title":"mirror_mode"},{"location":"roles/mirror_ocp/#role-variables-mirror-actions","text":"","title":"Role Variables - Mirror Actions"},{"location":"roles/mirror_ocp/#mirror_working_dir","text":"Working directory for mirroring operations. Required Environment Variable: MIRROR_WORKING_DIR Default: None Purpose : Specifies the working directory for mirroring operations. Used to store manifests, temporary files, and disk-based mirror archives. When to use : - Always required for Red Hat content mirroring - Must have sufficient disk space (especially for to-filesystem mode) - Directory is created if it doesn't exist Valid values : Absolute filesystem path (e.g., /tmp/mirror , /mnt/mirror-storage ) Impact : Determines where mirroring files are stored. Insufficient space will cause mirroring to fail. Related variables : - mirror_mode : Working directory used for all modes Note : For to-filesystem mode, ensure the directory has sufficient space for all images (can be hundreds of GB for platform + operators). The directory structure is preserved for from-filesystem mode.","title":"mirror_working_dir"},{"location":"roles/mirror_ocp/#mirror_redhat_platform","text":"Enable mirroring of Red Hat OpenShift platform images. Optional Environment Variable: MIRROR_REDHAT_PLATFORM Default: false Purpose : Controls whether to mirror Red Hat OpenShift Container Platform release images. Required for installing or upgrading OpenShift in disconnected environments. When to use : - Set to true to mirror OpenShift platform images - Leave as false (default) to skip platform mirroring - Enable when preparing for OpenShift installation or upgrades in air-gapped environments Valid values : true , false Impact : - true : Mirrors OpenShift platform images (large download, required for OCP install/upgrade) - false : Skips platform mirroring (only operators are mirrored if enabled) Related variables : - ocp_release : OpenShift version to mirror - ocp_min_version , ocp_max_version : Version range to mirror Note : Platform images are large (tens of GB). Only enable if you need to install or upgrade OpenShift in a disconnected environment.","title":"mirror_redhat_platform"},{"location":"roles/mirror_ocp/#mirror_redhat_operators","text":"Enable mirroring of selected Red Hat operator catalog content. Optional Environment Variable: MIRROR_REDHAT_OPERATORS Default: false Purpose : Controls whether to mirror selected content from Red Hat operator catalogs. Only operators directly used by MAS are mirrored (see catalog list above). When to use : - Set to true to mirror Red Hat operator catalogs - Leave as false (default) to skip operator mirroring - Enable when preparing for MAS deployment in air-gapped environments Valid values : true , false Impact : - true : Mirrors selected operators from certified, community, and redhat-operator-index catalogs - false : Skips operator catalog mirroring Related variables : - ocp_release : OpenShift version for operator compatibility Note : Only selected operators used by MAS are mirrored, not entire catalogs. See the catalog list at the top of this README for included operators.","title":"mirror_redhat_operators"},{"location":"roles/mirror_ocp/#redhat_pullsecret","text":"Path to Red Hat pull secret file. Required Environment Variable: REDHAT_PULLSECRET Default: None Purpose : Specifies the path to your Red Hat pull secret file. Required to authenticate and pull images from Red Hat registries during mirroring. When to use : - Always required for Red Hat content mirroring - Obtain from Red Hat OpenShift Console - Must be a valid, non-expired pull secret Valid values : Absolute path to pull secret JSON file (e.g., ~/pull-secret.json , /tmp/pull-secret.json ) Impact : Without a valid pull secret, pulling Red Hat images will fail and mirroring cannot proceed. Related variables : - None Note : Download your pull secret from the Red Hat OpenShift Console. Keep the file secure as it contains authentication credentials. The pull secret must be valid and associated with an active Red Hat account.","title":"redhat_pullsecret"},{"location":"roles/mirror_ocp/#role-variables-openshift-version","text":"","title":"Role Variables - OpenShift Version"},{"location":"roles/mirror_ocp/#ocp_release","text":"OpenShift release version to mirror. Required Environment Variable: OCP_RELEASE Default: None Purpose : Specifies the major.minor version of OpenShift Container Platform to mirror content for. Determines which platform images and operator versions are mirrored. When to use : - Always required for Red Hat content mirroring - Must match the OpenShift version in your target environment - Use format: 4.19 , 4.18 , 4.17 Valid values : OpenShift major.minor version (e.g., 4.19 , 4.18 , 4.17 , 4.16 ) Impact : Determines which OpenShift version's images and operators are mirrored. Must match your target cluster version. Related variables : - ocp_min_version : Minimum patch version to mirror - ocp_max_version : Maximum patch version to mirror - mirror_redhat_platform : Whether to mirror platform images for this version Note : Use the major.minor version format (e.g., 4.19 ), not full version (e.g., 4.19.10 ). Use ocp_min_version and ocp_max_version to control patch version range.","title":"ocp_release"},{"location":"roles/mirror_ocp/#ocp_min_version","text":"Minimum OpenShift patch version to mirror. Optional Environment Variable: OCP_MIN_VERSION Default: None (mirrors all versions) Purpose : Specifies the minimum patch version of OpenShift platform images to mirror. Limits the version range to reduce mirror size. When to use : - Leave unset to mirror all available patch versions for the release - Set to mirror only specific patch versions and newer - Only applies when mirror_redhat_platform=true Valid values : Full OpenShift version (e.g., 4.19.10 , 4.19.15 ) Impact : Only platform images for this version and newer are mirrored. Reduces mirror size but limits available versions. Related variables : - ocp_release : Major.minor version (must match) - ocp_max_version : Maximum version to mirror - mirror_redhat_platform : Must be true for this to apply Note : Only affects platform image mirroring, not operators. Use to limit mirror size when you know the specific OpenShift versions you need.","title":"ocp_min_version"},{"location":"roles/mirror_ocp/#ocp_max_version","text":"Maximum OpenShift patch version to mirror. Optional Environment Variable: OCP_MAX_VERSION Default: None (mirrors all versions) Purpose : Specifies the maximum patch version of OpenShift platform images to mirror. Limits the version range to reduce mirror size. When to use : - Leave unset to mirror all available patch versions for the release - Set to mirror only specific patch versions and older - Only applies when mirror_redhat_platform=true Valid values : Full OpenShift version (e.g., 4.19.20 , 4.19.25 ) Impact : Only platform images for this version and older are mirrored. Reduces mirror size but limits available versions. Related variables : - ocp_release : Major.minor version (must match) - ocp_min_version : Minimum version to mirror - mirror_redhat_platform : Must be true for this to apply Note : Only affects platform image mirroring, not operators. Use to limit mirror size when you know the specific OpenShift versions you need.","title":"ocp_max_version"},{"location":"roles/mirror_ocp/#role-variables-target-registry","text":"","title":"Role Variables - Target Registry"},{"location":"roles/mirror_ocp/#registry_public_host","text":"Target registry hostname for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None Purpose : Specifies the hostname of the target container registry where Red Hat images will be mirrored. When to use : - Always required for Red Hat content mirroring - Must be the hostname of your disconnected/private registry - Used for direct and from-filesystem modes Valid values : Valid hostname or IP address (e.g., registry.example.com , 10.0.0.100 ) Impact : Determines the target registry for image mirroring. Images are pushed to this registry. Related variables : - registry_public_port : Port for this registry - registry_prefix_redhat : Optional path prefix in registry Note : Registry must be accessible and have sufficient storage space for Red Hat content (can be hundreds of GB).","title":"registry_public_host"},{"location":"roles/mirror_ocp/#registry_public_port","text":"Target registry port for mirrored images. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None Purpose : Specifies the port of the target container registry where Red Hat images will be mirrored. When to use : - Always required for Red Hat content mirroring - Must be the port where your registry is accessible - Common values: 443 (HTTPS), 5000 (HTTP), 32500 (NodePort) Valid values : Valid port number (e.g., 443 , 5000 , 32500 ) Impact : Determines the target registry port for image mirroring. Images are pushed to this port. Related variables : - registry_public_host : Hostname for this registry Note : Ensure the port is accessible from the mirroring system.","title":"registry_public_port"},{"location":"roles/mirror_ocp/#registry_is_ecr","text":"Enable AWS Elastic Container Registry (ECR) support. Optional Environment Variable: REGISTRY_IS_ECR Default: false Purpose : Indicates whether the target registry is AWS Elastic Container Registry. Enables ECR-specific authentication and configuration. When to use : - Set to true when mirroring to AWS ECR - Leave as false (default) for other registry types - Requires AWS credentials and ECR region configuration Valid values : true , false Impact : - true : Uses ECR-specific authentication and configuration - false : Uses standard registry authentication Related variables : - registry_ecr_aws_region : AWS region for ECR (required when true ) Note : When using ECR, ensure AWS credentials are configured and the ECR region is specified.","title":"registry_is_ecr"},{"location":"roles/mirror_ocp/#registry_ecr_aws_region","text":"AWS region for Elastic Container Registry. Optional (Required when registry_is_ecr=true ) Environment Variable: REGISTRY_ECR_AWS_REGION Default: None Purpose : Specifies the AWS region where the ECR registry is located. Required for ECR authentication and access. When to use : - Only applies when registry_is_ecr=true - Must match the region where your ECR registry exists - Required for ECR authentication Valid values : Valid AWS region (e.g., us-east-1 , eu-west-1 , ap-southeast-1 ) Impact : Determines which AWS region is used for ECR authentication and access. Related variables : - registry_is_ecr : Must be true for this to apply Note : Ensure AWS credentials have permissions to push images to ECR in the specified region.","title":"registry_ecr_aws_region"},{"location":"roles/mirror_ocp/#registry_prefix_redhat","text":"Path prefix in target registry for Red Hat images. Optional Environment Variable: REGISTRY_PREFIX_REDHAT Default: None Purpose : Specifies an optional path prefix in the target registry for Red Hat images. Images are mirrored to {host}[:{port}]/{prefix}/{reponame} format. When to use : - Leave unset if images should be at registry root - Set to organize Red Hat images under a specific path (e.g., ocp419 , redhat ) - Useful for organizing different content types or versions Valid values : Valid registry path (e.g., ocp419 , redhat , openshift ) Impact : - When set: Images mirrored to {host}:{port}/{prefix}/{reponame} - When unset: Images mirrored to {host}:{port}/{reponame} Related variables : - registry_public_host : Registry hostname - registry_public_port : Registry port Note : The prefix helps organize Red Hat images separately from IBM or other content in the registry.","title":"registry_prefix_redhat"},{"location":"roles/mirror_ocp/#registry_username","text":"Username for target registry authentication. Required Environment Variable: REGISTRY_USERNAME Default: None Purpose : Provides the username for authenticating to the target container registry. Required to push images during mirroring. When to use : - Always required for Red Hat content mirroring (unless using ECR with AWS credentials) - Must have push permissions to the target registry - Obtain from your registry administrator Valid values : Valid username for the target registry Impact : Used to authenticate to the target registry. Without valid credentials, image push will fail. Related variables : - registry_password : Password paired with this username Note : Keep credentials secure. Use environment variables or secure vaults rather than hardcoding in playbooks.","title":"registry_username"},{"location":"roles/mirror_ocp/#registry_password","text":"Password for target registry authentication. Required Environment Variable: REGISTRY_PASSWORD Default: None Purpose : Provides the password for authenticating to the target container registry. Required to push images during mirroring. When to use : - Always required for Red Hat content mirroring (unless using ECR with AWS credentials) - Must correspond to the provided username - Keep secure and rotate regularly Valid values : Valid password for the target registry Impact : Used to authenticate to the target registry. Without valid credentials, image push will fail. Related variables : - registry_username : Username paired with this password Note : Keep passwords secure. Never commit to version control. Use environment variables or secure vaults.","title":"registry_password"},{"location":"roles/mirror_ocp/#example-playbook","text":"- hosts: localhost vars: registry_public_host: myregistry.mycompany.com registry_public_port: 5000 registry_prefix_redhat: \"ocp416\" registry_username: user1 registry_password: 8934jk77s862! # Not a real password, don't worry security folks mirror_mode: direct mirror_working_dir: /tmp/mirror mirror_redhat_platform: false mirror_redhat_operators: true ocp_release: 4.19 redhat_pullsecret: ~/pull-secret.json roles: - ibm.mas_devops.mirror_ocp","title":"Example Playbook"},{"location":"roles/mirror_ocp/#license","text":"EPL-2.0","title":"License"},{"location":"roles/mongodb/","text":"mongodb \u00a4 This role currently supports provisioning of mongodb in three different providers: - community - aws (documentdb) - ibm If the selected provider is community then the MongoDB Community Kubernetes Operator will be configured and deployed into the specified namespace. By default a three member MongoDB replica set will be created. The cluster will bind six PVCs, these provide persistence for the data and system logs across the three nodes. Currently there is no support built-in for customizing the cluster beyond this configuration. Tip The role will generate a yaml file containing the definition of a Secret and MongoCfg resource that can be used to configure the deployed instance as the MAS system MongoDb. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/mongocfg-mongoce-system.yaml or used in conjunction with the suite_config role. Prerequisites \u00a4 To run this role with providers as ibm or aws you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role when provider is either ibm or aws . To run the docdb_secret_rotate MONGODB_ACTION when the provider is aws you must have already installed the Mongo Shell . This role will install a GrafanaDashboard used for monitoring the MongoDB instance when the provided is community and you have run the grafana role previously. If you did not run the grafana role then the GrafanaDashboard won't be installed. Role Variables - General \u00a4 Common Variables \u00a4 mas_instance_id \u00a4 Unique identifier for the MAS instance that will use this MongoDB deployment. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None Purpose : Identifies which MAS instance this MongoDB configuration targets. Used to generate the MongoCfg resource that connects MAS to the MongoDB instance. When to use : - Set when you want to automatically generate MongoCfg for MAS integration - Must be set together with mas_config_dir for MongoCfg generation - Leave unset if manually managing MongoDB configuration for MAS Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : When set with mas_config_dir , generates a MongoCfg YAML file at $MAS_CONFIG_DIR/mongocfg-mongoce-system.yaml . Without this, no MAS configuration file is created. Related variables : Must be set together with mas_config_dir for MongoCfg generation. mas_config_dir \u00a4 Local directory path where the generated MongoCfg resource file will be saved. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None Purpose : Specifies where to save the MongoCfg YAML file that configures MAS to connect to this MongoDB instance. This file can be applied manually or used with the suite_config role for automated MAS configuration. When to use : - Set when you want to automatically generate MongoCfg for MAS integration - Must be set together with mas_instance_id for MongoCfg generation - Use the same directory across multiple dependency roles (mongodb, db2, sls) to collect all configurations - Leave unset if manually managing MongoDB configuration for MAS Valid values : Any valid local filesystem path (e.g., /home/user/masconfig , ~/masconfig , ./config ) Impact : When set with mas_instance_id , creates file mongocfg-mongoce-system.yaml in this directory. The file contains Secret and MongoCfg resources ready to apply to MAS. Related variables : - Must be set together with mas_instance_id for MongoCfg generation - Used by suite_config role to apply configurations mongodb_provider \u00a4 Selects which MongoDB deployment option to use for MAS database requirements. Optional Environment Variable: MONGODB_PROVIDER Default Value: community Purpose : Determines the MongoDB infrastructure provider, which affects deployment architecture, management approach, operational requirements, and cost model. Each provider offers different trade-offs between control, convenience, and cost. When to use : - Use community for self-managed deployments on OpenShift with full control - Use ibm for managed IBM Cloud Databases for MongoDB service - Use aws for managed AWS DocumentDB service (MongoDB-compatible) - Consider operational expertise, cloud platform, and management preferences Valid values : - community - MongoDB Community Edition Operator (self-managed on OpenShift) - ibm - IBM Cloud Databases for MongoDB (managed service) - aws - AWS DocumentDB (managed service, MongoDB-compatible) Impact : - community : Requires cluster storage, manual backup management, and operational overhead - ibm : Requires IBM Cloud account, API key, and incurs IBM Cloud service charges - aws : Requires AWS account, VPC configuration, and incurs AWS service charges Related variables : - When community : Requires mongodb_storage_class and related storage/resource variables - When ibm : Requires ibmcloud_apikey , ibm_mongo_region , ibm_mongo_resourcegroup - When aws : Requires aws_access_key_id , aws_secret_access_key , vpc_id , docdb_* variables - Affects which mongodb_action values are supported Note : Provider cannot be changed after initial deployment. Migration between providers requires backup and restore procedures. mongodb_action \u00a4 Specifies which operation to perform on the MongoDB instance. Optional Environment Variable: MONGODB_ACTION Default Value: install Purpose : Controls what action the role executes against the MongoDB instance. Different providers support different sets of actions based on their capabilities and management model. When to use : - Use install for initial deployment or updates - Use uninstall to remove MongoDB instance (use with caution) - Use backup to create MongoDB backups (community/ibm only) - Use restore to restore from backup (community/ibm only) - Use docdb_secret_rotate to rotate DocumentDB credentials (aws only) - Use destroy-data to delete all data from MongoDB (aws only) - Use create-mongo-service-credentials to generate service credentials (ibm only) Valid values (provider-specific): - community : install , uninstall , backup , restore - aws : install , uninstall , docdb_secret_rotate , destroy-data - ibm : install , uninstall , backup , restore , create-mongo-service-credentials Impact : The action determines what the role will do. Destructive actions like uninstall and destroy-data will permanently delete data. Backup/restore actions require additional variables to be set. Related variables : - mongodb_provider determines which actions are available - Backup actions require masbr_* variables - Restore actions require masbr_restore_from_version - AWS secret rotation requires docdb_* credential variables Note : Always backup data before performing destructive operations. Some actions are irreversible. CE Operator Variables \u00a4 mongodb_namespace \u00a4 OpenShift namespace where the MongoDB Community Operator and MongoDB cluster will be deployed. Optional Environment Variable: MONGODB_NAMESPACE Default Value: mongoce Purpose : Defines the Kubernetes namespace for MongoDB resources, providing isolation and organization for the MongoDB deployment within the cluster. When to use : - Use default mongoce for standard deployments - Change only if you need multiple MongoDB instances or have namespace naming requirements - Ensure namespace doesn't conflict with existing deployments Valid values : Any valid Kubernetes namespace name (lowercase alphanumeric and hyphens) Impact : All MongoDB resources (operator, replica set pods, PVCs, secrets, services) will be created in this namespace. Changing this after deployment requires reinstallation. Related variables : Used in MongoCfg generation to reference MongoDB service endpoints. mongodb_version \u00a4 Specifies the MongoDB version to deploy. Optional Environment Variable: MONGODB_VERSION Default Value: Automatically defined by the mongo version specified in the latest MAS case bundle available Purpose : Controls which MongoDB version is deployed, ensuring compatibility with MAS requirements and enabling version-specific features. The default aligns with the tested and supported version for your MAS release. When to use : - Leave as default for standard deployments (recommended) - Override only when specific version requirements exist - Use for testing compatibility with newer MongoDB versions - Never use to downgrade an existing MongoDB instance Valid values : 7.0.12 , 7.0.22 , 7.0.23 , 8.0.13 , 8.0.17 (check MAS compatibility matrix for supported versions) Impact : Determines MongoDB feature set, performance characteristics, and compatibility. Changing versions may require data migration or compatibility testing. Related variables : - Version upgrades require corresponding mongodb_v*_upgrade flags - Must be compatible with MAS version requirements Note : Never downgrade MongoDB versions . Always create scheduled backups before version changes. Use mongodb_v5_upgrade , mongodb_v6_upgrade , mongodb_v7_upgrade , or mongodb_v8_upgrade flags when upgrading between major versions. mongodb_override_spec \u00a4 Forces the role to use environment variables instead of preserving existing MongoDB spec settings. Optional Environment Variable: MONGODB_OVERRIDE_SPEC Default Value: false Purpose : Controls whether the role preserves existing MongoDB configuration during upgrades/reinstalls or applies new values from environment variables. This prevents accidental configuration changes during routine operations. When to use : - Leave as false (default) to preserve existing settings during upgrades - Set to true only when intentionally changing MongoDB configuration - Use with caution - requires setting all environment variables to match desired state Valid values : true , false Impact : - When false : Existing CPU, memory, storage, and replica settings are preserved during reinstall/upgrade - When true : All settings are taken from environment variables; unset variables revert to defaults Related variables : When set to true , affects these settings: - mongodb_cpu_limits - mongodb_mem_limits - mongodb_cpu_requests - mongodb_mem_requests - mongodb_storage_class - mongodb_storage_capacity_data - mongodb_storage_capacity_logs - mongodb_replicas Note : Check existing MongoDB installation before enabling . If environment variables don't match current spec, resources may be reset to defaults, potentially causing disruption. Unknown settings are not preserved. mongodb_storage_class \u00a4 Name of the Kubernetes storage class for MongoDB persistent volumes. Required when mongodb_provider=community Environment Variable: MONGODB_STORAGE_CLASS Default Value: None Purpose : Specifies which storage class provides persistent volumes for MongoDB data and logs. The storage class determines performance characteristics, availability, and cost of MongoDB storage. When to use : - Always required for Community Edition deployments - Choose based on performance requirements (SSD vs HDD) - Consider backup and snapshot capabilities of the storage class - Verify storage class exists in your cluster before deployment Valid values : Any valid storage class name in your cluster that supports ReadWriteOnce (RWO) access mode Impact : Affects MongoDB performance, reliability, and cost. Six PVCs will be created (data + logs for each of 3 replicas by default). Storage class cannot be changed after deployment without data migration. Related variables : - mongodb_storage_capacity_data - Size of data PVCs - mongodb_storage_capacity_logs - Size of log PVCs - mongodb_replicas - Number of replica sets (affects total PVC count) Note : Storage class must support ReadWriteOnce (RWO) access mode. Verify with oc get storageclass before deployment. mongodb_storage_capacity_data \u00a4 Size of the persistent volume claim (PVC) for MongoDB data storage on each replica set member. Optional Environment Variable: MONGODB_STORAGE_CAPACITY_DATA Default Value: 20Gi Purpose : Determines disk space allocated for storing MongoDB databases, collections, and indexes on each replica. Proper sizing prevents storage exhaustion and ensures adequate space for data growth. When to use : - Increase from default for production environments with large data volumes - Increase for environments with high data growth rates - Use default (20Gi) for development, testing, or small deployments - Consider backup strategy when sizing (larger volumes take longer to backup) Valid values : Any valid Kubernetes storage size (e.g., 20Gi , 100Gi , 500Gi , 1Ti ) Impact : - Larger values consume more cluster storage resources - Cannot be decreased after deployment (PVC expansion only) - Total storage = this value \u00d7 number of replicas - Affects backup and restore duration Related variables : - mongodb_replicas : Total storage = capacity \u00d7 replicas - mongodb_storage_class : Must support volume expansion if you plan to increase size later - mongodb_storage_capacity_logs : Consider balancing data and log storage Note : PVCs can be expanded but not shrunk. Plan for growth when setting initial size. Monitor storage usage to avoid running out of space. mongodb_storage_capacity_logs \u00a4 Size of the persistent volume claim (PVC) for MongoDB log storage on each replica set member. Optional Environment Variable: MONGODB_STORAGE_CAPACITY_LOGS Default Value: 20Gi Purpose : Determines disk space allocated for MongoDB operational logs on each replica. Logs are essential for troubleshooting, auditing, and monitoring MongoDB operations. When to use : - Increase for production environments with verbose logging requirements - Increase if log retention policies require longer history - Use default (20Gi) for standard deployments - Consider log rotation and retention policies when sizing Valid values : Any valid Kubernetes storage size (e.g., 10Gi , 20Gi , 50Gi ) Impact : - Larger values consume more cluster storage resources - Cannot be decreased after deployment (PVC expansion only) - Total log storage = this value \u00d7 number of replicas - Insufficient log space can cause MongoDB operational issues Related variables : - mongodb_replicas : Total log storage = capacity \u00d7 replicas - mongodb_storage_class : Must support volume expansion - mongodb_storage_capacity_data : Consider balancing data and log storage Note : Monitor log usage and implement log rotation to prevent filling log volumes. PVCs can be expanded but not shrunk. mongodb_cpu_limits \u00a4 Maximum CPU cores allocated to each MongoDB container. Optional Environment Variable: MONGODB_CPU_LIMITS Default Value: 1 Purpose : Sets the upper bound on CPU usage for MongoDB containers, preventing any single MongoDB instance from consuming excessive cluster CPU resources. When to use : - Increase for production workloads with high query volumes - Increase for large datasets requiring more processing power - Use default (1 core) for development or light workloads - Set higher than mongodb_cpu_requests to allow burst capacity Valid values : CPU units as decimal (e.g., 0.5 , 1 , 2 , 4 ) or millicores (e.g., 500m , 1000m , 2000m ) Impact : - Higher limits allow better performance under load but consume more cluster resources - Limits prevent MongoDB from starving other workloads of CPU - Total CPU limit = this value \u00d7 number of replicas - Setting too low can cause performance degradation Related variables : - mongodb_cpu_requests : Should be set lower than limits for burst capacity - mongodb_replicas : Total CPU = limits \u00d7 replicas - mongodb_mem_limits : Balance CPU and memory allocation Note : Ensure cluster has sufficient CPU capacity for all replicas. Monitor actual CPU usage to right-size limits. mongodb_mem_limits \u00a4 Maximum memory allocated to each MongoDB container. Optional Environment Variable: MONGODB_MEM_LIMITS Default Value: 1Gi Purpose : Sets the upper bound on memory usage for MongoDB containers. MongoDB uses memory for caching data and indexes, so adequate memory is critical for performance. When to use : - Increase significantly for production workloads (4Gi-8Gi recommended) - Increase for large datasets to improve cache hit rates - Default (1Gi) is only suitable for development/testing - Set higher than mongodb_mem_requests to allow burst capacity Valid values : Memory size (e.g., 1Gi , 2Gi , 4Gi , 8Gi , 16Gi ) Impact : - Higher limits improve performance through better caching - Limits prevent MongoDB from consuming all cluster memory - Total memory limit = this value \u00d7 number of replicas - Setting too low causes frequent cache evictions and poor performance Related variables : - mongodb_mem_requests : Should be set lower than limits - mongodb_replicas : Total memory = limits \u00d7 replicas - mongodb_cpu_limits : Balance CPU and memory allocation Note : MongoDB performance heavily depends on available memory. Production deployments typically need 4Gi-8Gi per replica. Monitor memory usage and adjust accordingly. mongodb_cpu_requests \u00a4 Guaranteed CPU cores reserved for each MongoDB container. Optional Environment Variable: MONGODB_CPU_REQUESTS Default Value: 500m Purpose : Defines the minimum CPU resources guaranteed to MongoDB containers. Kubernetes scheduler uses this to place pods on nodes with sufficient available CPU. When to use : - Increase for production workloads requiring consistent performance - Set to match expected baseline CPU usage - Use default (500m) for development or light workloads - Set lower than mongodb_cpu_limits to allow burst capacity Valid values : CPU units as decimal (e.g., 0.5 , 1 , 2 ) or millicores (e.g., 500m , 1000m , 2000m ) Impact : - Higher requests guarantee more CPU but may limit pod scheduling if cluster capacity is constrained - Requests ensure MongoDB has minimum CPU even under cluster load - Total CPU request = this value \u00d7 number of replicas - Affects pod Quality of Service (QoS) class Related variables : - mongodb_cpu_limits : Requests should be lower than limits - mongodb_replicas : Total CPU requests = this value \u00d7 replicas - mongodb_mem_requests : Balance CPU and memory requests Note : Set requests based on baseline usage, not peak. The difference between requests and limits provides burst capacity. mongodb_mem_requests \u00a4 Guaranteed memory reserved for each MongoDB container. Optional Environment Variable: MONGODB_MEM_REQUESTS Default Value: 1Gi Purpose : Defines the minimum memory resources guaranteed to MongoDB containers. Kubernetes scheduler uses this to place pods on nodes with sufficient available memory. When to use : - Increase for production workloads (2Gi-4Gi recommended) - Set to match expected baseline memory usage - Default (1Gi) is only suitable for development/testing - Set lower than mongodb_mem_limits to allow burst capacity Valid values : Memory size (e.g., 1Gi , 2Gi , 4Gi , 8Gi ) Impact : - Higher requests guarantee more memory but may limit pod scheduling if cluster capacity is constrained - Requests ensure MongoDB has minimum memory even under cluster load - Total memory request = this value \u00d7 number of replicas - Affects pod Quality of Service (QoS) class Related variables : - mongodb_mem_limits : Requests should be lower than limits - mongodb_replicas : Total memory requests = this value \u00d7 replicas - mongodb_cpu_requests : Balance CPU and memory requests Note : MongoDB needs adequate memory for good performance. Production deployments typically need 2Gi-4Gi requests. Set based on baseline usage, not peak. mongodb_replicas \u00a4 Number of MongoDB replica set members to deploy. Optional Environment Variable: MONGODB_REPLICAS Default Value: 3 Purpose : Determines the size of the MongoDB replica set, which affects high availability, read scalability, and resource consumption. Replica sets provide data redundancy and automatic failover. When to use : - Use default (3) for production deployments with high availability requirements - Set to 1 only for Single Node OpenShift (SNO) clusters or development environments - Use 5 or more for critical production workloads requiring higher availability - Odd numbers (1, 3, 5) are recommended for proper election quorum Valid values : Positive integers, typically 1, 3, or 5 Impact : - More replicas = higher availability but more resource consumption - Each replica requires its own data and log PVCs - Total resources = (CPU + memory + storage) \u00d7 replicas - Affects election behavior and write acknowledgment Related variables : - mongodb_storage_capacity_data : Total data storage = capacity \u00d7 replicas - mongodb_storage_capacity_logs : Total log storage = capacity \u00d7 replicas - mongodb_cpu_limits and mongodb_mem_limits : Total resources = limits \u00d7 replicas Note : Set to 1 for SNO clusters. Production deployments should use 3 or more for high availability. Changing replica count after deployment requires careful planning. custom_labels \u00a4 Comma-separated list of key=value labels to apply to MongoDB resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Purpose : Adds Kubernetes labels to MongoDB resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,app=mongodb ) Impact : Labels are applied to MongoDB resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect MongoDB functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management. mongodb_v5_upgrade \u00a4 Confirmation flag to upgrade MongoDB from version 4.x to version 5. Optional Environment Variable: MONGODB_V5_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades. Upgrading MongoDB major versions requires careful planning and testing. When to use : - Set to true only when intentionally upgrading from MongoDB 4.2 or 4.4 to version 5 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to a 5.x version, triggers MongoDB upgrade from 4.x to 5.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to a 5.x version for upgrade to proceed - Other upgrade flags: mongodb_v6_upgrade , mongodb_v7_upgrade , mongodb_v8_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 5.0 release notes for breaking changes and new requirements (e.g., AVX instruction set). mongodb_v6_upgrade \u00a4 Confirmation flag to upgrade MongoDB from version 5 to version 6. Optional Environment Variable: MONGODB_V6_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades from version 5 to 6. When to use : - Set to true only when intentionally upgrading from MongoDB 5 to version 6 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to a 6.x version, triggers MongoDB upgrade from 5.x to 6.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to a 6.x version for upgrade to proceed - Other upgrade flags: mongodb_v5_upgrade , mongodb_v7_upgrade , mongodb_v8_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 6.0 release notes for breaking changes. mongodb_v7_upgrade \u00a4 Confirmation flag to upgrade MongoDB from version 6 to version 7. Optional Environment Variable: MONGODB_V7_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades from version 6 to 7. When to use : - Set to true only when intentionally upgrading from MongoDB 6 to version 7 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to a 7.x version, triggers MongoDB upgrade from 6.x to 7.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to a 7.x version for upgrade to proceed - Other upgrade flags: mongodb_v5_upgrade , mongodb_v6_upgrade , mongodb_v8_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 7.0 release notes for breaking changes. mongodb_v8_upgrade \u00a4 Confirmation flag to upgrade MongoDB from version 7 to version 8. Optional Environment Variable: MONGODB_V8_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades from version 7 to 8. When to use : - Set to true only when intentionally upgrading from MongoDB 7 to version 8 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to an 8.x version, triggers MongoDB upgrade from 7.x to 8.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to an 8.x version for upgrade to proceed - Other upgrade flags: mongodb_v5_upgrade , mongodb_v6_upgrade , mongodb_v7_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 8.0 release notes for breaking changes. masbr_confirm_cluster \u00a4 Enables cluster confirmation prompt before executing backup or restore operations. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false Purpose : Provides a safety check to confirm you're connected to the correct cluster before performing backup or restore operations, preventing accidental operations on wrong clusters. When to use : - Set to true in environments with multiple clusters to prevent mistakes - Set to true for production environments as an additional safety measure - Leave as false for automated pipelines where confirmation isn't possible Valid values : true , false Impact : When true , the role will prompt for confirmation of the cluster before proceeding with backup/restore. This adds a manual step but prevents costly mistakes. Related variables : Used with mongodb_action when set to backup or restore . masbr_copy_timeout_sec \u00a4 Timeout in seconds for backup/restore file transfer operations. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) Purpose : Sets the maximum time allowed for copying backup files to/from storage. Prevents operations from hanging indefinitely on slow networks or large datasets. When to use : - Increase for very large databases or slow network connections - Decrease for smaller databases to fail faster on issues - Use default (12 hours) for most deployments Valid values : Positive integer representing seconds (e.g., 3600 = 1 hour, 43200 = 12 hours, 86400 = 24 hours) Impact : Operations exceeding this timeout will fail. Setting too low causes failures on legitimate long-running transfers. Setting too high delays detection of stuck operations. Related variables : Used with mongodb_action when set to backup or restore . Note : Consider database size and network speed when setting timeout. Monitor actual transfer times to optimize this value. masbr_job_timezone \u00a4 Time zone for scheduled backup job execution. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None (uses UTC) Purpose : Specifies the time zone for scheduled backup CronJobs, ensuring backups run at the intended local time rather than UTC. When to use : - Set when scheduling backups to run at specific local times - Use for compliance with backup windows in specific time zones - Leave unset to use UTC (recommended for global deployments) Valid values : Any valid tz database time zone (e.g., America/New_York , Europe/London , Asia/Tokyo ) Impact : Affects when scheduled backups execute. Incorrect time zone can cause backups to run during business hours or miss backup windows. Related variables : - masbr_backup_schedule : Defines the cron schedule - Only applies when masbr_backup_schedule is set Note : When not set, CronJobs use UTC time zone. Consider daylight saving time changes when scheduling backups. masbr_storage_local_folder \u00a4 Local filesystem path where backup files will be stored or retrieved from. Required when mongodb_action is backup or restore Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None Purpose : Specifies the directory for storing MongoDB backup files. This location must have sufficient space and appropriate permissions for backup operations. When to use : - Always required for backup and restore operations - Use a path with adequate storage space for full database backups - Consider using network-attached storage for backup retention - Ensure path is accessible and has proper permissions Valid values : Any valid local filesystem path (e.g., /backup/mongodb , /mnt/nfs/backups , /tmp/masbr ) Impact : Backup files are written to this location. Insufficient space causes backup failures. Path must be accessible during restore operations. Related variables : - masbr_backup_type : Determines if full or incremental backups are stored here - masbr_restore_from_version : Specifies which backup version to restore from this location Note : Ensure adequate disk space (at least 2x database size for full backups). Implement backup retention policies to manage storage usage. masbr_backup_type \u00a4 Type of backup to create: full or incremental. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full Purpose : Determines whether to create a complete database backup or an incremental backup containing only changes since the last full backup. Incremental backups save time and storage. When to use : - Use full for initial backups or periodic complete backups - Use incr for frequent backups between full backups to save time and space - Implement a strategy like weekly full + daily incremental backups Valid values : full , incr Impact : - full : Creates complete backup, takes longer, uses more storage - incr : Creates incremental backup, faster, uses less storage, requires base full backup Related variables : - masbr_backup_from_version : Required when incr is used to specify base full backup - mongodb_action : Must be set to backup Note : Incremental backups require a full backup as base. Restore operations may need to apply multiple incremental backups sequentially. masbr_backup_from_version \u00a4 Timestamp of the full backup to use as base for incremental backup. Optional Environment Variable: MASBR_BACKUP_FROM_VERSION Default: None (automatically uses latest full backup) Purpose : Specifies which full backup serves as the base for an incremental backup. This links the incremental backup to a specific full backup version. When to use : - Set when creating incremental backups and you want to specify a particular full backup - Leave unset to automatically use the most recent full backup - Only valid when masbr_backup_type=incr Valid values : Timestamp in format YYYYMMDDHHMMSS (e.g., 20240621021316 ) Impact : Incremental backup will contain only changes since the specified full backup. Incorrect version can cause backup chain issues. Related variables : - masbr_backup_type : Must be set to incr - masbr_storage_local_folder : Location where full backup exists Note : If not specified, role automatically finds the latest full backup. Ensure the specified full backup exists in the storage location. masbr_backup_schedule \u00a4 Cron expression for scheduling automated backups. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None (creates on-demand backup) Purpose : Defines when automated backups should run using standard cron syntax. Enables regular, unattended backup operations. When to use : - Set to schedule regular automated backups (e.g., daily, weekly) - Leave unset for manual, on-demand backups - Consider backup windows and system load when scheduling Valid values : Standard cron expression (e.g., 0 2 * * * for daily at 2 AM, 0 2 * * 0 for weekly on Sunday at 2 AM) Impact : When set, creates a Kubernetes CronJob that automatically runs backups on schedule. Without this, backups only run when role is manually executed. Related variables : - masbr_job_timezone : Specifies time zone for schedule - masbr_backup_type : Determines if scheduled backups are full or incremental Note : Test cron expressions before deploying. Consider backup duration when scheduling to avoid overlapping backup jobs. masbr_restore_from_version \u00a4 Timestamp of the backup version to restore. Required when mongodb_action=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Purpose : Specifies which backup version to restore from. This allows point-in-time recovery to a specific backup. When to use : - Always required when performing restore operations - Use to restore to a specific point in time - Verify backup version exists before attempting restore Valid values : Timestamp in format YYYYMMDDHHMMSS (e.g., 20240621021316 ) Impact : Restores MongoDB to the state captured in the specified backup. All data after this backup will be lost. This is a destructive operation. Related variables : - mongodb_action : Must be set to restore - masbr_storage_local_folder : Location where backup exists Note : Verify backup version before restoring . List available backups in storage location first. Restore is destructive and cannot be undone without another backup. Role Variables - IBM Cloud \u00a4 ibm_mongo_name \u00a4 Name for the IBM Cloud Databases for MongoDB instance. Required when mongodb_provider=ibm Environment Variable: IBM_MONGO_NAME Default Value: mongo-${MAS_INSTANCE_ID} Purpose : Identifies the MongoDB database instance in IBM Cloud. This name is used for resource identification, billing, and management within IBM Cloud. When to use : - Always required when using IBM Cloud as the MongoDB provider - Use default naming convention for consistency across MAS instances - Customize only if organizational naming standards require it Valid values : Valid IBM Cloud resource name (alphanumeric, hyphens allowed, must start with letter) Impact : This name appears in IBM Cloud console, billing reports, and resource lists. Changing it after creation requires recreating the database instance. Related variables : - mas_instance_id : Default name includes this value - mongodb_provider : Must be set to ibm Note : Choose a descriptive name that identifies the MAS instance and environment. The name cannot be changed after creation. ibm_mongo_admin_password \u00a4 Administrator password for the IBM Cloud MongoDB instance. Optional Environment Variable: IBM_MONGO_ADMIN_PASSWORD Default Value: Auto-generated 20-character string Purpose : Sets the password for the MongoDB administrator user. If not provided, a secure random password is automatically generated. When to use : - Set explicitly if you need to know the password in advance - Leave unset to use auto-generated secure password (recommended) - Set if integrating with external password management systems Valid values : String meeting IBM Cloud password requirements (minimum length, complexity) Impact : This password is used for administrative access to the MongoDB instance. Auto-generated passwords are stored in Kubernetes secrets. Related variables : - ibm_mongo_admin_credentials_secret_name : Secret where credentials are stored Note : Auto-generated passwords are more secure. If setting manually, ensure password meets security requirements and is stored securely. ibm_mongo_admin_credentials_secret_name \u00a4 Name of the Kubernetes secret containing MongoDB admin credentials. Optional Environment Variable: IBM_MONGO_ADMIN_CREDENTIALS_SECRET_NAME Default Value: <mongo-name>-admin-credentials Purpose : Specifies the Kubernetes secret name where MongoDB administrator credentials are stored. This secret is created automatically by the role. When to use : - Customize if organizational standards require specific secret naming - Use default for standard deployments - Reference this secret name in other automation Valid values : Valid Kubernetes secret name Impact : The secret contains admin username and password for MongoDB access. Other roles and applications reference this secret for database connectivity. Related variables : - ibm_mongo_name : Default secret name includes this value - ibm_mongo_admin_password : Password stored in this secret ibm_mongo_service_credentials_secret_name \u00a4 Name of the Kubernetes secret containing MongoDB service credentials. Optional Environment Variable: IBM_MONGO_SERVICE_CREDENTIALS_SECRET_NAME Default Value: <mongo-name>-service-credentials Purpose : Specifies the Kubernetes secret name where MongoDB service-level credentials are stored. These credentials are used by MAS applications to connect to MongoDB. When to use : - Customize if organizational standards require specific secret naming - Use default for standard deployments - Reference this secret name when configuring MAS applications Valid values : Valid Kubernetes secret name Impact : The secret contains connection strings and credentials for application-level MongoDB access. MAS applications use this secret to connect to the database. Related variables : - ibm_mongo_name : Default secret name includes this value ibm_mongo_resourcegroup \u00a4 IBM Cloud resource group for MongoDB instance placement. Required when mongodb_provider=ibm Environment Variable: IBM_MONGO_RESOURCEGROUP Default Value: Default Purpose : Specifies which IBM Cloud resource group will contain the MongoDB instance. Resource groups organize IBM Cloud resources for access control and billing. When to use : - Always required when using IBM Cloud provider - Use Default for simple deployments - Specify custom resource group for organizational resource management - Align with IBM Cloud IAM and billing structure Valid values : Name of an existing IBM Cloud resource group in your account Impact : Determines access control, billing allocation, and resource organization. Users need appropriate IAM permissions for the specified resource group. Related variables : - ibmcloud_apikey : API key must have access to the specified resource group Note : Ensure the resource group exists and your API key has permissions to create resources in it. ibm_mongo_region \u00a4 IBM Cloud region where MongoDB instance will be deployed. Required when mongodb_provider=ibm Environment Variable: IBM_MONGO_REGION Default Value: us-east Purpose : Specifies the geographic region for MongoDB deployment. Region selection affects latency, data residency, and availability. When to use : - Always required when using IBM Cloud provider - Choose region closest to your OpenShift cluster for lowest latency - Consider data residency requirements for compliance - Use default ( us-east ) if no specific requirements Valid values : Valid IBM Cloud region (e.g., us-east , us-south , eu-gb , eu-de , jp-tok , au-syd ) Impact : Affects network latency between OpenShift and MongoDB, data residency compliance, and regional pricing. Cannot be changed after creation. Related variables : - ibmcloud_apikey : API key must have access to the specified region Note : Choose region carefully as it cannot be changed. Consider deploying MongoDB in the same region as your OpenShift cluster for best performance. ibmcloud_apikey \u00a4 IBM Cloud API key for authentication and resource management. Required when mongodb_provider=ibm Environment Variable: IBMCLOUD_APIKEY Default Value: None Purpose : Provides authentication credentials for creating and managing IBM Cloud resources. The API key must have sufficient permissions to create and manage Databases for MongoDB instances. When to use : - Always required when using IBM Cloud provider - Must have permissions for the target resource group and region - Should be stored securely (e.g., in Ansible Vault or external secret management) Valid values : Valid IBM Cloud API key string Impact : This key is used to authenticate all IBM Cloud API calls. Insufficient permissions will cause deployment failures. Related variables : - ibm_mongo_resourcegroup : API key must have access to this resource group - ibm_mongo_region : API key must have access to this region Note : Never commit API keys to source control . Use secure secret management. Ensure the API key has appropriate IAM permissions for Databases for MongoDB service. ibm_mongo_plan \u00a4 IBM Cloud service plan for MongoDB instance. Optional Environment Variable: IBM_MONGO_PLAN Default Value: standard Purpose : Specifies the IBM Cloud service plan tier for MongoDB. Different plans offer different performance, availability, and pricing characteristics. When to use : - Use standard (default) for most production deployments - Consider enterprise plan for critical workloads requiring higher SLA - Review IBM Cloud pricing and plan features before selecting Valid values : standard , enterprise (check IBM Cloud documentation for current plan options) Impact : Affects pricing, performance characteristics, SLA, and available features. Plan cannot be changed after creation without recreating the instance. Related variables : - ibm_mongo_memory , ibm_mongo_disk , ibm_mongo_cpu : Resource allocations vary by plan Note : Review IBM Cloud Databases for MongoDB plan documentation for current offerings and pricing. ibm_mongo_service \u00a4 IBM Cloud service type identifier for MongoDB. Required (Read-only constant) Environment Variable: None (internal constant) Default Value: databases-for-mongodb Purpose : Identifies the IBM Cloud service type. This is a fixed value used internally by the role. When to use : This is set automatically by the role and should not be modified. Valid values : databases-for-mongodb Impact : Used in IBM Cloud API calls to specify the service type. Note : This is a constant value and does not need to be set by users. ibm_mongo_service_endpoints \u00a4 Network endpoint type for MongoDB connectivity. Optional Environment Variable: IBM_MONGO_SERVICE_ENDPOINTS Default Value: public Purpose : Determines whether MongoDB is accessible via public internet or private network only. Private endpoints provide better security and performance for cluster-to-database communication. When to use : - Use public for simple deployments or when OpenShift cluster lacks private network connectivity - Use private for production deployments with private network connectivity (recommended) - Private endpoints require IBM Cloud private network configuration Valid values : public , private Impact : - public : MongoDB accessible over internet (requires firewall rules) - private : MongoDB accessible only via IBM Cloud private network (more secure, lower latency) Related variables : Network configuration must support the chosen endpoint type Note : Private endpoints are recommended for production. Ensure your OpenShift cluster can reach IBM Cloud private network if using private . ibm_mongo_version \u00a4 MongoDB version to deploy in IBM Cloud. Optional Environment Variable: IBM_MONGO_VERSION Default Value: 4.2 Purpose : Specifies which MongoDB version to deploy. Version selection affects available features, performance, and compatibility. When to use : - Use default ( 4.2 ) for compatibility with older MAS versions - Specify newer version (e.g., 5.0 , 6.0 ) for new deployments - Check MAS compatibility matrix before selecting version Valid values : MongoDB versions supported by IBM Cloud Databases (e.g., 4.2 , 4.4 , 5.0 , 6.0 ) Impact : Affects available MongoDB features, performance characteristics, and MAS compatibility. Version cannot be easily downgraded. Related variables : - Check MAS compatibility requirements before selecting version Note : Verify version compatibility with your MAS version. Newer MongoDB versions may require MAS updates. ibm_mongo_memory \u00a4 Memory allocation per MongoDB member in MB. Optional Environment Variable: IBM_MONGO_MEMORY Default Value: 3840 (3.75 GB) Purpose : Specifies memory allocation for each MongoDB replica set member. Memory affects caching performance and query execution. When to use : - Use default (3840 MB) for development or small deployments - Increase for production workloads (8192 MB or higher recommended) - Increase for large datasets to improve cache hit rates Valid values : Integer in MB, minimum varies by plan (typically 1024 MB minimum) Impact : Higher memory improves performance through better caching but increases costs. Total cost = memory \u00d7 number of members. Related variables : - ibm_mongo_plan : Available memory ranges vary by plan - ibm_mongo_disk : Balance memory and disk allocation Note : IBM Cloud charges based on allocated memory. Production deployments typically need 8GB+ per member. ibm_mongo_disk \u00a4 Disk storage allocation per MongoDB member in MB. Optional Environment Variable: IBM_MONGO_DISK Default Value: 30720 (30 GB) Purpose : Specifies disk storage for each MongoDB replica set member. Storage holds databases, indexes, and operational logs. When to use : - Use default (30 GB) for development or small datasets - Increase for production workloads based on data volume - Plan for data growth and backup storage needs Valid values : Integer in MB, minimum varies by plan (typically 5120 MB minimum) Impact : Affects storage capacity and costs. Disk can be expanded but not shrunk. Total storage = disk \u00d7 number of members. Related variables : - ibm_mongo_plan : Available disk ranges vary by plan - ibm_mongo_memory : Balance memory and disk allocation Note : Plan for data growth. Disk can be expanded online but cannot be reduced. Monitor storage usage to avoid running out of space. ibm_mongo_cpu \u00a4 Dedicated CPU cores per MongoDB member. Optional Environment Variable: IBM_MONGO_CPU Default Value: 0 (shared CPU) Purpose : Specifies dedicated CPU cores for each MongoDB member. Dedicated CPUs provide consistent performance but increase costs. When to use : - Use 0 (default) for shared CPU, suitable for development and light workloads - Set to 3 or higher for production workloads requiring consistent performance - Dedicated CPUs recommended for production environments Valid values : 0 (shared), or integer \u2265 3 for dedicated CPUs Impact : - 0 : Shared CPU, lower cost, variable performance - \u22653 : Dedicated CPUs, higher cost, consistent performance Related variables : - ibm_mongo_plan : CPU options vary by plan - ibm_mongo_memory : Balance CPU and memory allocation Note : Dedicated CPUs significantly increase costs but provide predictable performance. Production workloads typically need dedicated CPUs. ibm_mongo_backup_id \u00a4 IBM Cloud backup CRN (Cloud Resource Name) for restore operations. Required when is_restore=true Environment Variable: IBM_MONGO_BACKUP_ID Default Value: None Purpose : Specifies the IBM Cloud backup resource to restore from. The CRN uniquely identifies a specific backup in IBM Cloud. When to use : - Required only when restoring from an IBM Cloud backup - Obtain CRN from IBM Cloud console or CLI - Leave unset for new deployments Valid values : Valid IBM Cloud CRN for a MongoDB backup (format: crn:v1:... ) Impact : Restores MongoDB to the state captured in the specified backup. All current data will be replaced. Related variables : - is_restore : Must be set to true - restored_mongodb_service_name : Name for the restored service Note : Verify backup CRN before restoring . Restore is destructive and replaces all current data. Test restore procedures in non-production first. is_restore \u00a4 Flag to enable restore from IBM Cloud backup. Optional Environment Variable: IS_RESTORE Default Value: false Purpose : Controls whether to create a new MongoDB instance or restore from an existing backup. Acts as a safety flag to prevent accidental restores. When to use : - Set to true only when intentionally restoring from backup - Leave as false (default) for new deployments - Must be explicitly set to perform restore Valid values : true , false Impact : When true , creates MongoDB instance from backup instead of fresh deployment. Requires ibm_mongo_backup_id and restored_mongodb_service_name . Related variables : - ibm_mongo_backup_id : Required when true - restored_mongodb_service_name : Required when true Note : Always verify backup details before setting to true . Restore operations cannot be undone without another backup. restored_mongodb_service_name \u00a4 Name for the MongoDB service when restoring from backup. Required when is_restore=true Environment Variable: RESTORED_MONGODB_SERVICE_NAME Default Value: None Purpose : Specifies the name for the new MongoDB service created from backup. This allows restoring to a different service name than the original. When to use : - Required only when is_restore=true - Can be same as or different from original service name - Use different name to restore alongside existing instance for testing Valid values : Valid IBM Cloud resource name Impact : The restored MongoDB instance will have this name in IBM Cloud. Choose carefully as it affects resource identification and billing. Related variables : - is_restore : Must be set to true - ibm_mongo_backup_id : Backup to restore from - ibm_mongo_name : Original service name (can be different) Note : Using a different name allows side-by-side comparison of restored and current instances before switching over. Role Variables - AWS DocumentDB \u00a4 aws_access_key_id \u00a4 AWS account access key ID for authentication. Required when mongodb_provider=aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None Purpose : Provides AWS authentication credentials for creating and managing DocumentDB resources. The access key must have sufficient IAM permissions for DocumentDB, VPC, and related services. When to use : - Always required when using AWS DocumentDB provider - Must have IAM permissions for DocumentDB, EC2 (VPC/subnets/security groups) - Should be stored securely (e.g., in Ansible Vault or external secret management) Valid values : Valid AWS access key ID string Impact : This key is used to authenticate all AWS API calls. Insufficient permissions will cause deployment failures. Related variables : - aws_secret_access_key : Must be provided together - aws_region : Access key must have permissions in the target region Note : Never commit AWS credentials to source control . Use secure secret management. Ensure the IAM user/role has appropriate permissions for DocumentDB and VPC operations. aws_secret_access_key \u00a4 AWS account secret access key for authentication. Required when mongodb_provider=aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None Purpose : Provides the secret component of AWS authentication credentials. Works together with aws_access_key_id to authenticate AWS API requests. When to use : - Always required when using AWS DocumentDB provider - Must correspond to the provided aws_access_key_id - Should be stored securely Valid values : Valid AWS secret access key string Impact : Used with access key ID to authenticate AWS API calls. Invalid or mismatched credentials will cause authentication failures. Related variables : - aws_access_key_id : Must be provided together Note : Store securely and never commit to source control . Rotate credentials regularly following AWS security best practices. aws_region \u00a4 AWS region where DocumentDB cluster will be deployed. Required when mongodb_provider=aws Environment Variable: AWS_REGION Default Value: us-east-2 Purpose : Specifies the geographic AWS region for DocumentDB deployment. Region selection affects latency, data residency, availability zones, and pricing. When to use : - Always required when using AWS DocumentDB provider - Choose region closest to your OpenShift cluster for lowest latency - Consider data residency requirements for compliance - Use default ( us-east-2 ) if no specific requirements Valid values : Valid AWS region code (e.g., us-east-1 , us-east-2 , us-west-2 , eu-west-1 , ap-southeast-1 ) Impact : Affects network latency, data residency compliance, available availability zones, and regional pricing. Cannot be changed after creation. Related variables : - vpc_id : VPC must exist in the specified region - aws_access_key_id : Credentials must have permissions in this region Note : Choose region carefully as it cannot be changed. Deploy DocumentDB in the same region as your OpenShift cluster for best performance. vpc_id \u00a4 AWS VPC ID where DocumentDB resources will be created. Required when mongodb_provider=aws Environment Variable: VPC_ID Default Value: None Purpose : Specifies the AWS Virtual Private Cloud where DocumentDB cluster, subnets, and security groups will be created. The VPC provides network isolation and connectivity. When to use : - Always required when using AWS DocumentDB provider - Use the same VPC as your OpenShift cluster for direct connectivity - VPC must exist in the specified aws_region Valid values : Valid AWS VPC ID (format: vpc-xxxxxxxxxxxxxxxxx ) Impact : Determines network connectivity and security boundaries. DocumentDB will only be accessible from resources within this VPC or connected networks. Related variables : - aws_region : VPC must exist in this region - docdb_cidr_az1 , docdb_cidr_az2 , docdb_cidr_az3 : Subnets created within this VPC - docdb_ingress_cidr , docdb_egress_cidr : Should match VPC CIDR ranges Note : Ensure the VPC has sufficient available IP addresses and appropriate routing for DocumentDB connectivity. docdb_cluster_name \u00a4 Name for the AWS DocumentDB cluster. Required when mongodb_provider=aws Environment Variable: DOCDB_CLUSTER_NAME Default Value: None Purpose : Identifies the DocumentDB cluster in AWS. This name is used for resource identification, tagging, and as a prefix for related resources. When to use : - Always required when using AWS DocumentDB provider - Choose a descriptive name that identifies the MAS instance and environment - Name is used as prefix for subnet groups and security groups Valid values : Valid AWS DocumentDB cluster identifier (lowercase, alphanumeric, hyphens, must start with letter) Impact : This name appears in AWS console, CloudWatch metrics, and billing. Related resources (subnet group, security group) are named based on this value. Related variables : - docdb_subnet_group_name : Defaults to docdb-{cluster_name} - docdb_security_group_name : Defaults to docdb-{cluster_name} - docdb_admin_credentials_secret_name : Defaults to {cluster_name}-admin-credentials Note : Choose a meaningful name as it cannot be easily changed. The name must be unique within your AWS account and region. docdb_subnet_group_name \u00a4 Name for the DocumentDB subnet group. Optional Environment Variable: DOCDB_SUBNET_GROUP_NAME Default Value: docdb-{{ docdb_cluster_name }} Purpose : Specifies the name for the DocumentDB subnet group that defines which subnets the cluster can use. The role creates this subnet group automatically. When to use : - Use default naming for standard deployments - Customize only if organizational naming standards require it Valid values : Valid AWS subnet group name Impact : The subnet group associates the DocumentDB cluster with specific subnets across availability zones. Related variables : - docdb_cluster_name : Default name includes this value - docdb_cidr_az1 , docdb_cidr_az2 , docdb_cidr_az3 : Subnets included in this group Note : This is automatically created by the role. Default naming is recommended for consistency. docdb_security_group_name \u00a4 Name for the DocumentDB security group. Optional Environment Variable: DOCDB_SECURITY_GROUP_NAME Default Value: docdb-{{ docdb_cluster_name }} Purpose : Specifies the name for the security group that controls network access to the DocumentDB cluster. The role creates this security group automatically. When to use : - Use default naming for standard deployments - Customize only if organizational naming standards require it Valid values : Valid AWS security group name Impact : The security group defines firewall rules for DocumentDB access based on docdb_ingress_cidr and docdb_egress_cidr . Related variables : - docdb_cluster_name : Default name includes this value - docdb_ingress_cidr : Allowed source CIDR for inbound traffic - docdb_egress_cidr : Allowed destination CIDR for outbound traffic Note : This is automatically created by the role. Default naming is recommended for consistency. docdb_admin_credentials_secret_name \u00a4 Name of the Kubernetes secret containing DocumentDB admin credentials. Optional Environment Variable: DOCDB_ADMIN_CREDENTIALS_SECRET_NAME Default Value: {{ docdb_cluster_name }}-admin-credentials Purpose : Specifies the Kubernetes secret name where DocumentDB administrator credentials are stored. This secret is created automatically by the role. When to use : - Use default naming for standard deployments - Customize only if organizational naming standards require it Valid values : Valid Kubernetes secret name Impact : The secret contains admin username and password for DocumentDB access. MAS and other applications reference this secret for database connectivity. Related variables : - docdb_cluster_name : Default name includes this value - docdb_master_username : Username stored in this secret Note : This secret is created in the MAS core namespace. Default naming is recommended for consistency. docdb_engine_version \u00a4 DocumentDB engine version to deploy. Optional Environment Variable: DOCDB_ENGINE_VERSION Default Value: 5.0.0 Purpose : Specifies the DocumentDB engine version. MAS requires DocumentDB 5.0.0 for MongoDB compatibility. When to use : - Use default ( 5.0.0 ) for MAS deployments (required) - Do not change unless specifically required by MAS version Valid values : 5.0.0 (only version supported by MAS) Impact : Determines MongoDB compatibility and available features. MAS is only certified with DocumentDB 5.0.0. Related variables : None Note : MAS only supports DocumentDB 5.0.0 . Do not change this value unless MAS documentation explicitly supports other versions. docdb_master_username \u00a4 Master username for DocumentDB cluster administration. Optional Environment Variable: DOCDB_MASTER_USERNAME Default Value: docdbadmin Purpose : Specifies the master administrator username for the DocumentDB cluster. This user has full administrative privileges. When to use : - Use default ( docdbadmin ) for standard deployments - Customize if organizational security policies require specific usernames Valid values : Valid DocumentDB username (alphanumeric, must start with letter, 1-63 characters) Impact : This username is used for administrative access and is stored in the Kubernetes secret specified by docdb_admin_credentials_secret_name . Related variables : - docdb_admin_credentials_secret_name : Secret where credentials are stored Note : Choose carefully as the master username cannot be changed after cluster creation. docdb_instance_class \u00a4 AWS instance class for DocumentDB instances. Optional Environment Variable: DOCDB_INSTANCE_CLASS Default Value: db.t3.medium Purpose : Specifies the compute and memory capacity for each DocumentDB instance. Instance class affects performance, availability, and cost. When to use : - Use db.t3.medium (default) for development or small deployments - Use db.r5.large or larger for production workloads - Consider db.r6g instances for better price/performance (ARM-based) Valid values : Valid DocumentDB instance class (e.g., db.t3.medium , db.r5.large , db.r5.xlarge , db.r6g.large ) Impact : Affects CPU, memory, network performance, and cost. Larger instances provide better performance but cost more. Related variables : - docdb_instance_number : Total cost = instance class cost \u00d7 number of instances Note : Production deployments typically need db.r5.large or larger. Review AWS DocumentDB pricing and instance specifications. docdb_instance_number \u00a4 Number of DocumentDB instances in the cluster. Optional Environment Variable: DOCDB_INSTANCE_NUMBER Default Value: 3 Purpose : Determines the number of instances in the DocumentDB cluster. More instances provide higher availability and read scalability. When to use : - Use default ( 3 ) for production deployments with high availability - Use 1 only for development or testing (no high availability) - Use 5 or more for critical workloads requiring higher availability Valid values : Integer from 1 to 16 Impact : - More instances = higher availability and read capacity but higher cost - Instances are distributed across availability zones for fault tolerance - Total cost = instance class cost \u00d7 number of instances Related variables : - docdb_instance_class : Determines per-instance cost and performance - docdb_cidr_az1 , docdb_cidr_az2 , docdb_cidr_az3 : Instances distributed across these AZs Note : Production deployments should use 3 or more instances for high availability. Single instance has no failover capability. docdb_instance_identifier_prefix \u00a4 Prefix for DocumentDB instance identifiers. Required when mongodb_provider=aws Environment Variable: DOCDB_INSTANCE_IDENTIFIER_PREFIX Default Value: None Purpose : Specifies the prefix used to name individual DocumentDB instances. Instance names are formed as {prefix}-{number} . When to use : - Always required when using AWS DocumentDB provider - Use a descriptive prefix that identifies the cluster and environment - Typically matches or relates to docdb_cluster_name Valid values : Valid AWS instance identifier prefix (lowercase, alphanumeric, hyphens) Impact : Instance names appear in AWS console and CloudWatch metrics. Choose a meaningful prefix for easy identification. Related variables : - docdb_cluster_name : Typically related to cluster name Note : Instance identifiers are formed as {prefix}-1 , {prefix}-2 , etc. Choose a clear, descriptive prefix. docdb_ingress_cidr \u00a4 CIDR block allowed to connect to DocumentDB cluster. Required when mongodb_provider=aws Environment Variable: DOCDB_INGRESS_CIDR Default Value: None Purpose : Specifies the IPv4 CIDR range from which incoming connections to DocumentDB are allowed. This is used in security group ingress rules. When to use : - Always required when using AWS DocumentDB provider - Typically set to the CIDR of your OpenShift cluster's VPC - Can be set to specific subnet CIDRs for tighter security Valid values : Valid IPv4 CIDR notation (e.g., 10.0.0.0/16 , 172.31.0.0/16 ) Impact : Only traffic from this CIDR range can connect to DocumentDB. Too restrictive blocks legitimate traffic; too permissive reduces security. Related variables : - vpc_id : Should match VPC CIDR or subnet CIDRs within the VPC - docdb_egress_cidr : Typically set to same value Note : Set to your OpenShift cluster's VPC CIDR for proper connectivity. Verify CIDR ranges before deployment. docdb_egress_cidr \u00a4 CIDR block for outbound connections from DocumentDB cluster. Required when mongodb_provider=aws Environment Variable: DOCDB_EGRESS_CIDR Default Value: None Purpose : Specifies the IPv4 CIDR range to which DocumentDB can send outbound connections. This is used in security group egress rules. When to use : - Always required when using AWS DocumentDB provider - Typically set to the same value as docdb_ingress_cidr - Set to VPC CIDR for standard deployments Valid values : Valid IPv4 CIDR notation (e.g., 10.0.0.0/16 , 172.31.0.0/16 ) Impact : DocumentDB can only send traffic to this CIDR range. Affects ability to respond to client connections. Related variables : - docdb_ingress_cidr : Typically set to same value - vpc_id : Should match VPC CIDR Note : Usually set to the same value as docdb_ingress_cidr . Verify CIDR ranges match your network configuration. docdb_cidr_az1 \u00a4 CIDR block for DocumentDB subnet in availability zone 1. Required when mongodb_provider=aws Environment Variable: DOCDB_CIDR_AZ1 Default Value: None Purpose : Specifies the IPv4 CIDR for the subnet in the first availability zone. If the subnet exists with tag Name: {{ docdb_cluster_name }} , it's used; otherwise, a new subnet is created. When to use : - Always required when using AWS DocumentDB provider - Must be within the VPC CIDR range - Must not overlap with other subnets in the VPC Valid values : Valid IPv4 CIDR notation within VPC range (e.g., 10.0.1.0/24 ) Impact : Defines the IP address range for DocumentDB instances in AZ1. Subnet size affects number of available IP addresses. Related variables : - vpc_id : CIDR must be within this VPC's range - docdb_cidr_az2 , docdb_cidr_az3 : Must not overlap with these subnets Note : Plan subnet sizes carefully. Each DocumentDB instance needs an IP address. Use /24 or larger subnets for flexibility. docdb_cidr_az2 \u00a4 CIDR block for DocumentDB subnet in availability zone 2. Required when mongodb_provider=aws Environment Variable: DOCDB_CIDR_AZ2 Default Value: None Purpose : Specifies the IPv4 CIDR for the subnet in the second availability zone. If the subnet exists with tag Name: {{ docdb_cluster_name }} , it's used; otherwise, a new subnet is created. When to use : - Always required when using AWS DocumentDB provider - Must be within the VPC CIDR range - Must not overlap with other subnets in the VPC Valid values : Valid IPv4 CIDR notation within VPC range (e.g., 10.0.2.0/24 ) Impact : Defines the IP address range for DocumentDB instances in AZ2. Required for multi-AZ high availability. Related variables : - vpc_id : CIDR must be within this VPC's range - docdb_cidr_az1 , docdb_cidr_az3 : Must not overlap with these subnets Note : Use different availability zones for AZ1, AZ2, and AZ3 to ensure high availability across zones. docdb_cidr_az3 \u00a4 CIDR block for DocumentDB subnet in availability zone 3. Required when mongodb_provider=aws Environment Variable: DOCDB_CIDR_AZ3 Default Value: None Purpose : Specifies the IPv4 CIDR for the subnet in the third availability zone. If the subnet exists with tag Name: {{ docdb_cluster_name }} , it's used; otherwise, a new subnet is created. When to use : - Always required when using AWS DocumentDB provider - Must be within the VPC CIDR range - Must not overlap with other subnets in the VPC Valid values : Valid IPv4 CIDR notation within VPC range (e.g., 10.0.3.0/24 ) Impact : Defines the IP address range for DocumentDB instances in AZ3. Provides third availability zone for maximum fault tolerance. Related variables : - vpc_id : CIDR must be within this VPC's range - docdb_cidr_az1 , docdb_cidr_az2 : Must not overlap with these subnets Note : Three availability zones provide best fault tolerance. Ensure subnets are in different AZs for proper distribution. AWS DocumentDB Secret Rotation Variables \u00a4 The following variables are used for rotating DocumentDB credentials. These are typically used with mongodb_action=rotate-secret . docdb_mongo_instance_name \u00a4 DocumentDB instance name for secret rotation. Required when rotating secrets Environment Variable: DOCDB_MONGO_INSTANCE_NAME Default Value: None Purpose : Identifies the specific DocumentDB instance for credential rotation operations. When to use : - Required when performing secret rotation ( mongodb_action=rotate-secret ) - Must match an existing DocumentDB instance name Valid values : Valid DocumentDB instance identifier Impact : Specifies which DocumentDB instance's credentials will be rotated. Related variables : - docdb_cluster_name : Instance belongs to this cluster - docdb_host : Host address of this instance Note : Verify instance name before rotation to avoid affecting wrong instance. docdb_host \u00a4 DocumentDB instance host address for connection. Required when rotating secrets Environment Variable: DOCDB_HOST Default Value: None Purpose : Specifies the host address of a DocumentDB instance for establishing connection during secret rotation. When to use : - Required when performing secret rotation - Use any one host address from the DocumentDB cluster - Obtain from AWS console or DocumentDB cluster endpoint Valid values : Valid DocumentDB instance hostname or endpoint Impact : Used to connect to DocumentDB for credential rotation operations. Related variables : - docdb_port : Port for this host - docdb_mongo_instance_name : Instance identifier Note : Any instance host from the cluster can be used for rotation operations. docdb_port \u00a4 DocumentDB instance port number. Required when rotating secrets Environment Variable: DOCDB_PORT Default Value: None (typically 27017 ) Purpose : Specifies the port number for connecting to the DocumentDB instance during secret rotation. When to use : - Required when performing secret rotation - Typically 27017 (default DocumentDB port) Valid values : Valid port number (typically 27017 ) Impact : Used with docdb_host to establish connection for credential rotation. Related variables : - docdb_host : Host address for this port Note : DocumentDB uses port 27017 by default unless customized during cluster creation. docdb_instance_username \u00a4 Username for which password is being rotated. Required when rotating secrets Environment Variable: DOCDB_INSTANCE_USERNAME Default Value: None Purpose : Specifies the DocumentDB username whose password will be changed during rotation. When to use : - Required when performing secret rotation - Typically the application user or admin user - Must be an existing DocumentDB user Valid values : Valid DocumentDB username Impact : This user's password will be changed. Applications using this username must be updated with the new password. Related variables : - docdb_instance_password_old : Current password for this user - docdb_master_username : Master user for performing rotation Note : Ensure applications can handle password rotation. Consider using connection pooling with reconnection logic. docdb_instance_password_old \u00a4 Current password for the user being rotated. Required when rotating secrets Environment Variable: DOCDB_PASSWORD_OLD Default Value: None Purpose : Provides the current password for authentication before rotation. Used to verify current credentials. When to use : - Required when performing secret rotation - Must be the current valid password Valid values : Current password string Impact : Used to authenticate before changing password. Incorrect password will cause rotation to fail. Related variables : - docdb_instance_username : User for this password Note : Store securely. After rotation, this password will no longer be valid. docdb_master_password \u00a4 DocumentDB master user password for administrative operations. Required when rotating secrets Environment Variable: DOCDB_MASTER_PASSWORD Default Value: None Purpose : Provides master user credentials for performing password rotation operations. Master user has privileges to change other users' passwords. When to use : - Required when performing secret rotation - Must be the current master password Valid values : Valid master password string Impact : Used to authenticate as master user to perform credential rotation. Related variables : - docdb_master_username : Master username for this password Note : Store master credentials securely . Never commit to source control. docdb_master_username \u00a4 DocumentDB master username for administrative operations. Required when rotating secrets Environment Variable: DOCDB_MASTER_USERNAME Default Value: None Purpose : Specifies the master username for performing password rotation operations. Master user has administrative privileges. When to use : - Required when performing secret rotation - Typically docdbadmin or the value set during cluster creation Valid values : Valid DocumentDB master username Impact : Used with docdb_master_password to authenticate for credential rotation. Related variables : - docdb_master_password : Password for this master user Note : This should match the master username set during DocumentDB cluster creation. AWS DocumentDB destroy-data action Variables mas_instance_id \u00a4 The specified MAS instance ID Required Environment Variable: MAS_INSTANCE_ID Default Value: None mongo_username \u00a4 Mongo Username Environment Variable: MONGO_USERNAME Default Value: None mongo_password \u00a4 Mongo password Environment Variable: MONGO_PASSWORD Default Value: None config \u00a4 Mongo Config, please refer to the below example playbook section for details Required Environment Variable: CONFIG Default Value: None certificates \u00a4 Mongo Certificates, please refer to the below example playbook section for details Required Environment Variable: CERTIFICATES Default Value: None Example Playbook \u00a4 Install (CE Operator) \u00a4 - hosts: localhost any_errors_fatal: true vars: mongodb_storage_class: ibmc-block-gold mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.mongodb Backup (CE Operator) \u00a4 - hosts: localhost any_errors_fatal: true vars: mongodb_action: backup mas_instance_id: masinst1 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.mongodb Restore (CE Operator) \u00a4 - hosts: localhost any_errors_fatal: true vars: mongodb_action: restore mas_instance_id: masinst1 masbr_restore_from_version: 20240621021316 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.mongodb Install (IBM Cloud) \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: ibm ibmcloud_apikey: apikey**** ibmcloud_resource_group: mas-test roles: - ibm.mas_devops.mongodb Install (AWS DocumentDB) \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: aws mongodb_action: provision docdb_size: ~/docdb-config.yml docdb_cluster_name: test-db docdb_ingress_cidr: 10.0.0.0/16 docdb_egress_cidr: 10.0.0.0/16 docdb_cidr_az1: 10.0.0.0/26 docdb_cidr_az2: 10.0.0.64/26 docdb_cidr_az3: 10.0.0.128/26 docdb_instance_identifier_prefix: test-db-instance vpc_id: test-vpc-id aws_access_key_id: aws-key aws_secret_access_key: aws-access-key roles: - ibm.mas_devops.mongodb AWS DocumentDb Secret Rotation \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: aws mongodb_action: docdb_secret_rotate docdb_mongo_instance_name: test-db-instance db_host: aws.test1.host7283-***** db_port: 27017 docdb_master_username: admin docdb_master_password: pass*** docdb_instance_password_old: oldpass**** docdb_instance_username: testuser aws_access_key_id: aws-key aws_secret_access_key: aws-access-key roles: - ibm.mas_devops.mongodb AWS DocumentDb destroy-data action \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mongodb_provider: aws mongodb_action: destroy-data mongo_username: pqradmin mongo_password: xyzabc config: configDb: admin authMechanism: DEFAULT retryWrites: false hosts: - host: abc-0.pqr.databases.appdomain.cloud port: 32250 - host: abc-1.pqr.databases.appdomain.cloud port: 32250 - host: abc-2.pqr.databases.appdomain.cloud port: 32250 certificates: - alias: ca crt: | -----BEGIN CERTIFICATE----- MIIDDzCCAfegAwIBAgIJANEH58y2/kzHMA0GCSqGSIb3DQEBCwUAMB4xHDAaBgNV BAMME0lCTSBDbG91ZCBEYXRhYmFzZXMwHhcNMTgwNjI1MTQyOTAwWhcNMjgwNjIy MTQyOTAwWjAeMRwwGgYDVQQDDBNJQk0gQ2xvdWQgRGF0YWJhc2VzMIIBIjANBgkq 1eKI2FLzYKpoKBe5rcnrM7nHgNc/nCdEs5JecHb1dHv1QfPm6pzIxwIDAQABo1Aw TjAdBgNVHQ4EFgQUK3+XZo1wyKs+DEoYXbHruwSpXjgwHwYDVR0jBBgwFoAUK3+X Zo1wyKs+DEoYXbHruwSpXjgwDAYDVR0TBAUwAwEB/zANBgkqhkiG9w0BAQsFAAOC doqqgGIZ2nxCkp5/FXxF/TMb55vteTQwfgBy60jVVkbF7eVOWCv0KaNHPF5hrqbN i+3XjJ7/peF3xMvTMoy35DcT3E2ZeSVjouZs15O90kI3k2daS2OHJABW0vSj4nLz +PQzp/B9cQmOO8dCe049Q3oaUA== -----END CERTIFICATE----- roles: - ibm.mas_devops.mongodb Run Role Playbook \u00a4 export MONGODB_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=masinst1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook ibm.mas_devops.run_role Troubleshooting \u00a4 Important Please be cautious while performing any of the troubleshooting steps outlined below. It is important to understand that the MongoDB Community operator persists data within Persistent Volume Claims. These claims should not be removed inadvertent deletion of the mongoce namespace could result in data loss. MongoDB Replica Set Pods Will Not Start \u00a4 MongoDB 5 has introduced new platform specific requirements. Please consult the Platform Support Notes for detailed information. It is of particular importance to confirm that the AVX instruction set is exposed or available to the MongoDB workloads. This can easily be determined by entering any running pod on the same OpenShift cluster where MongoDB replica set members are failing to start. Once inside of a running pod the following command can be executed to confirm if the AVX instruction set is available: cat /proc/cpuinfo | grep flags | grep avx If avx is not found in the available flags then either the physical processor hosting the OpenShift cluster does not provide the AVX instruction set or the virtual host configuration is not exposing the AVX instruction set. If the latter is suspected the virtual hosting documentation should be referenced for details on how to expose the AVX instruction set. LDAP Authentication \u00a4 If authenticating via LDAP with PLAIN specified for authMechanism then configDb must be set to $external in the MongoCfg. The field configDb in the MongoCfg refers to the authentication database. CA Certificate Renewal \u00a4 Warning If the MongoDB CA Certificate expires the MongoDB replica set will become unusable. Replica set members will not be able to communicate with each other and client applications (i.e. Maximo Application Suite components) will not be to connect. In order to renew the CA Certificate used by the MongoDB replica set the following steps must be taken: Delete the CA Certificate resource Delete the MongoDB server Certificate resource Delete the Secrets resources associated with both the CA Certificate and Server Certificate Delete the Secret resource which contains the MongoDB configuration parameters Delete the ConfigMap resources which contains the CA certificate Delete the Secret resource which contains the sever certificate and private key The following steps illustrate the process required to renew the CA Certificate, sever certificate and reconfigure the MongoDB replica set with the new CA and server certificates. The first step is to stop the Mongo replica set and MongoDb CE Operator pod. oc project mongoce oc delete deployment mongodb-kubernetes-operator Important Make sure the MongoDB Community operator pod has terminated before proceeding. oc delete statefulset mas-mongo-ce Important Make sure all pods in the mongoce namespace have terminated before proceeding Remove expired CA Certificate and Server Certificate resources. Clean up MongoDB Community configuration and then run the mongodb role. oc delete certificate mongo-ca-crt oc delete certificate mongo-server oc delete secret mongo-ca-secret oc delete secret mongo-server-cert oc delete secret mas-mongo-ce-config oc delete configmap mas-mongo-ce-cert-map oc delete secret mas-mongo-ce-server-certificate-key export ROLE_NAME=mongodb ansible-playbook ibm.mas_devops.run_role Once the mongodb role has completed the MongoDb CE Operator pod and Mongo replica set should be configured. After the CA and server Certificates have been renewed you must ensure that that MongoCfg Suite CR is updated with the new CA Certificate. First obtain the CA Certificate from the Secret resource mongo-ca-secret . Then edit the Suite MongoCfg CR in the Maximo Application Suite core namespace. This is done by updating the appropriate certificate under .spec.certificates in the MongoCfg CR: spec: certificates: - alias: ca crt: | -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- If an IBM Suite Licensing Service (SLS) is also connecting to the MongoDB replica set the LicenseService CR must also be updated to reflect the new MongoDB CA. This can be added to the .spec.mongo.certificates section of the LicenseService CR. mongo: certificates: - alias: mongoca crt: | -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- Once the CA certificate has been updated for the MongoCfg and LicenseService CRs several pods in the core and SLS namespaces might need to be restarted to pick up the changes. This would include but is not limited to coreidp, coreapi, api-licensing. License \u00a4 EPL-2.0","title":"mongodb"},{"location":"roles/mongodb/#mongodb","text":"This role currently supports provisioning of mongodb in three different providers: - community - aws (documentdb) - ibm If the selected provider is community then the MongoDB Community Kubernetes Operator will be configured and deployed into the specified namespace. By default a three member MongoDB replica set will be created. The cluster will bind six PVCs, these provide persistence for the data and system logs across the three nodes. Currently there is no support built-in for customizing the cluster beyond this configuration. Tip The role will generate a yaml file containing the definition of a Secret and MongoCfg resource that can be used to configure the deployed instance as the MAS system MongoDb. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/mongocfg-mongoce-system.yaml or used in conjunction with the suite_config role.","title":"mongodb"},{"location":"roles/mongodb/#prerequisites","text":"To run this role with providers as ibm or aws you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role when provider is either ibm or aws . To run the docdb_secret_rotate MONGODB_ACTION when the provider is aws you must have already installed the Mongo Shell . This role will install a GrafanaDashboard used for monitoring the MongoDB instance when the provided is community and you have run the grafana role previously. If you did not run the grafana role then the GrafanaDashboard won't be installed.","title":"Prerequisites"},{"location":"roles/mongodb/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/mongodb/#common-variables","text":"","title":"Common Variables"},{"location":"roles/mongodb/#mas_instance_id","text":"Unique identifier for the MAS instance that will use this MongoDB deployment. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None Purpose : Identifies which MAS instance this MongoDB configuration targets. Used to generate the MongoCfg resource that connects MAS to the MongoDB instance. When to use : - Set when you want to automatically generate MongoCfg for MAS integration - Must be set together with mas_config_dir for MongoCfg generation - Leave unset if manually managing MongoDB configuration for MAS Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : When set with mas_config_dir , generates a MongoCfg YAML file at $MAS_CONFIG_DIR/mongocfg-mongoce-system.yaml . Without this, no MAS configuration file is created. Related variables : Must be set together with mas_config_dir for MongoCfg generation.","title":"mas_instance_id"},{"location":"roles/mongodb/#mas_config_dir","text":"Local directory path where the generated MongoCfg resource file will be saved. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None Purpose : Specifies where to save the MongoCfg YAML file that configures MAS to connect to this MongoDB instance. This file can be applied manually or used with the suite_config role for automated MAS configuration. When to use : - Set when you want to automatically generate MongoCfg for MAS integration - Must be set together with mas_instance_id for MongoCfg generation - Use the same directory across multiple dependency roles (mongodb, db2, sls) to collect all configurations - Leave unset if manually managing MongoDB configuration for MAS Valid values : Any valid local filesystem path (e.g., /home/user/masconfig , ~/masconfig , ./config ) Impact : When set with mas_instance_id , creates file mongocfg-mongoce-system.yaml in this directory. The file contains Secret and MongoCfg resources ready to apply to MAS. Related variables : - Must be set together with mas_instance_id for MongoCfg generation - Used by suite_config role to apply configurations","title":"mas_config_dir"},{"location":"roles/mongodb/#mongodb_provider","text":"Selects which MongoDB deployment option to use for MAS database requirements. Optional Environment Variable: MONGODB_PROVIDER Default Value: community Purpose : Determines the MongoDB infrastructure provider, which affects deployment architecture, management approach, operational requirements, and cost model. Each provider offers different trade-offs between control, convenience, and cost. When to use : - Use community for self-managed deployments on OpenShift with full control - Use ibm for managed IBM Cloud Databases for MongoDB service - Use aws for managed AWS DocumentDB service (MongoDB-compatible) - Consider operational expertise, cloud platform, and management preferences Valid values : - community - MongoDB Community Edition Operator (self-managed on OpenShift) - ibm - IBM Cloud Databases for MongoDB (managed service) - aws - AWS DocumentDB (managed service, MongoDB-compatible) Impact : - community : Requires cluster storage, manual backup management, and operational overhead - ibm : Requires IBM Cloud account, API key, and incurs IBM Cloud service charges - aws : Requires AWS account, VPC configuration, and incurs AWS service charges Related variables : - When community : Requires mongodb_storage_class and related storage/resource variables - When ibm : Requires ibmcloud_apikey , ibm_mongo_region , ibm_mongo_resourcegroup - When aws : Requires aws_access_key_id , aws_secret_access_key , vpc_id , docdb_* variables - Affects which mongodb_action values are supported Note : Provider cannot be changed after initial deployment. Migration between providers requires backup and restore procedures.","title":"mongodb_provider"},{"location":"roles/mongodb/#mongodb_action","text":"Specifies which operation to perform on the MongoDB instance. Optional Environment Variable: MONGODB_ACTION Default Value: install Purpose : Controls what action the role executes against the MongoDB instance. Different providers support different sets of actions based on their capabilities and management model. When to use : - Use install for initial deployment or updates - Use uninstall to remove MongoDB instance (use with caution) - Use backup to create MongoDB backups (community/ibm only) - Use restore to restore from backup (community/ibm only) - Use docdb_secret_rotate to rotate DocumentDB credentials (aws only) - Use destroy-data to delete all data from MongoDB (aws only) - Use create-mongo-service-credentials to generate service credentials (ibm only) Valid values (provider-specific): - community : install , uninstall , backup , restore - aws : install , uninstall , docdb_secret_rotate , destroy-data - ibm : install , uninstall , backup , restore , create-mongo-service-credentials Impact : The action determines what the role will do. Destructive actions like uninstall and destroy-data will permanently delete data. Backup/restore actions require additional variables to be set. Related variables : - mongodb_provider determines which actions are available - Backup actions require masbr_* variables - Restore actions require masbr_restore_from_version - AWS secret rotation requires docdb_* credential variables Note : Always backup data before performing destructive operations. Some actions are irreversible.","title":"mongodb_action"},{"location":"roles/mongodb/#ce-operator-variables","text":"","title":"CE Operator Variables"},{"location":"roles/mongodb/#mongodb_namespace","text":"OpenShift namespace where the MongoDB Community Operator and MongoDB cluster will be deployed. Optional Environment Variable: MONGODB_NAMESPACE Default Value: mongoce Purpose : Defines the Kubernetes namespace for MongoDB resources, providing isolation and organization for the MongoDB deployment within the cluster. When to use : - Use default mongoce for standard deployments - Change only if you need multiple MongoDB instances or have namespace naming requirements - Ensure namespace doesn't conflict with existing deployments Valid values : Any valid Kubernetes namespace name (lowercase alphanumeric and hyphens) Impact : All MongoDB resources (operator, replica set pods, PVCs, secrets, services) will be created in this namespace. Changing this after deployment requires reinstallation. Related variables : Used in MongoCfg generation to reference MongoDB service endpoints.","title":"mongodb_namespace"},{"location":"roles/mongodb/#mongodb_version","text":"Specifies the MongoDB version to deploy. Optional Environment Variable: MONGODB_VERSION Default Value: Automatically defined by the mongo version specified in the latest MAS case bundle available Purpose : Controls which MongoDB version is deployed, ensuring compatibility with MAS requirements and enabling version-specific features. The default aligns with the tested and supported version for your MAS release. When to use : - Leave as default for standard deployments (recommended) - Override only when specific version requirements exist - Use for testing compatibility with newer MongoDB versions - Never use to downgrade an existing MongoDB instance Valid values : 7.0.12 , 7.0.22 , 7.0.23 , 8.0.13 , 8.0.17 (check MAS compatibility matrix for supported versions) Impact : Determines MongoDB feature set, performance characteristics, and compatibility. Changing versions may require data migration or compatibility testing. Related variables : - Version upgrades require corresponding mongodb_v*_upgrade flags - Must be compatible with MAS version requirements Note : Never downgrade MongoDB versions . Always create scheduled backups before version changes. Use mongodb_v5_upgrade , mongodb_v6_upgrade , mongodb_v7_upgrade , or mongodb_v8_upgrade flags when upgrading between major versions.","title":"mongodb_version"},{"location":"roles/mongodb/#mongodb_override_spec","text":"Forces the role to use environment variables instead of preserving existing MongoDB spec settings. Optional Environment Variable: MONGODB_OVERRIDE_SPEC Default Value: false Purpose : Controls whether the role preserves existing MongoDB configuration during upgrades/reinstalls or applies new values from environment variables. This prevents accidental configuration changes during routine operations. When to use : - Leave as false (default) to preserve existing settings during upgrades - Set to true only when intentionally changing MongoDB configuration - Use with caution - requires setting all environment variables to match desired state Valid values : true , false Impact : - When false : Existing CPU, memory, storage, and replica settings are preserved during reinstall/upgrade - When true : All settings are taken from environment variables; unset variables revert to defaults Related variables : When set to true , affects these settings: - mongodb_cpu_limits - mongodb_mem_limits - mongodb_cpu_requests - mongodb_mem_requests - mongodb_storage_class - mongodb_storage_capacity_data - mongodb_storage_capacity_logs - mongodb_replicas Note : Check existing MongoDB installation before enabling . If environment variables don't match current spec, resources may be reset to defaults, potentially causing disruption. Unknown settings are not preserved.","title":"mongodb_override_spec"},{"location":"roles/mongodb/#mongodb_storage_class","text":"Name of the Kubernetes storage class for MongoDB persistent volumes. Required when mongodb_provider=community Environment Variable: MONGODB_STORAGE_CLASS Default Value: None Purpose : Specifies which storage class provides persistent volumes for MongoDB data and logs. The storage class determines performance characteristics, availability, and cost of MongoDB storage. When to use : - Always required for Community Edition deployments - Choose based on performance requirements (SSD vs HDD) - Consider backup and snapshot capabilities of the storage class - Verify storage class exists in your cluster before deployment Valid values : Any valid storage class name in your cluster that supports ReadWriteOnce (RWO) access mode Impact : Affects MongoDB performance, reliability, and cost. Six PVCs will be created (data + logs for each of 3 replicas by default). Storage class cannot be changed after deployment without data migration. Related variables : - mongodb_storage_capacity_data - Size of data PVCs - mongodb_storage_capacity_logs - Size of log PVCs - mongodb_replicas - Number of replica sets (affects total PVC count) Note : Storage class must support ReadWriteOnce (RWO) access mode. Verify with oc get storageclass before deployment.","title":"mongodb_storage_class"},{"location":"roles/mongodb/#mongodb_storage_capacity_data","text":"Size of the persistent volume claim (PVC) for MongoDB data storage on each replica set member. Optional Environment Variable: MONGODB_STORAGE_CAPACITY_DATA Default Value: 20Gi Purpose : Determines disk space allocated for storing MongoDB databases, collections, and indexes on each replica. Proper sizing prevents storage exhaustion and ensures adequate space for data growth. When to use : - Increase from default for production environments with large data volumes - Increase for environments with high data growth rates - Use default (20Gi) for development, testing, or small deployments - Consider backup strategy when sizing (larger volumes take longer to backup) Valid values : Any valid Kubernetes storage size (e.g., 20Gi , 100Gi , 500Gi , 1Ti ) Impact : - Larger values consume more cluster storage resources - Cannot be decreased after deployment (PVC expansion only) - Total storage = this value \u00d7 number of replicas - Affects backup and restore duration Related variables : - mongodb_replicas : Total storage = capacity \u00d7 replicas - mongodb_storage_class : Must support volume expansion if you plan to increase size later - mongodb_storage_capacity_logs : Consider balancing data and log storage Note : PVCs can be expanded but not shrunk. Plan for growth when setting initial size. Monitor storage usage to avoid running out of space.","title":"mongodb_storage_capacity_data"},{"location":"roles/mongodb/#mongodb_storage_capacity_logs","text":"Size of the persistent volume claim (PVC) for MongoDB log storage on each replica set member. Optional Environment Variable: MONGODB_STORAGE_CAPACITY_LOGS Default Value: 20Gi Purpose : Determines disk space allocated for MongoDB operational logs on each replica. Logs are essential for troubleshooting, auditing, and monitoring MongoDB operations. When to use : - Increase for production environments with verbose logging requirements - Increase if log retention policies require longer history - Use default (20Gi) for standard deployments - Consider log rotation and retention policies when sizing Valid values : Any valid Kubernetes storage size (e.g., 10Gi , 20Gi , 50Gi ) Impact : - Larger values consume more cluster storage resources - Cannot be decreased after deployment (PVC expansion only) - Total log storage = this value \u00d7 number of replicas - Insufficient log space can cause MongoDB operational issues Related variables : - mongodb_replicas : Total log storage = capacity \u00d7 replicas - mongodb_storage_class : Must support volume expansion - mongodb_storage_capacity_data : Consider balancing data and log storage Note : Monitor log usage and implement log rotation to prevent filling log volumes. PVCs can be expanded but not shrunk.","title":"mongodb_storage_capacity_logs"},{"location":"roles/mongodb/#mongodb_cpu_limits","text":"Maximum CPU cores allocated to each MongoDB container. Optional Environment Variable: MONGODB_CPU_LIMITS Default Value: 1 Purpose : Sets the upper bound on CPU usage for MongoDB containers, preventing any single MongoDB instance from consuming excessive cluster CPU resources. When to use : - Increase for production workloads with high query volumes - Increase for large datasets requiring more processing power - Use default (1 core) for development or light workloads - Set higher than mongodb_cpu_requests to allow burst capacity Valid values : CPU units as decimal (e.g., 0.5 , 1 , 2 , 4 ) or millicores (e.g., 500m , 1000m , 2000m ) Impact : - Higher limits allow better performance under load but consume more cluster resources - Limits prevent MongoDB from starving other workloads of CPU - Total CPU limit = this value \u00d7 number of replicas - Setting too low can cause performance degradation Related variables : - mongodb_cpu_requests : Should be set lower than limits for burst capacity - mongodb_replicas : Total CPU = limits \u00d7 replicas - mongodb_mem_limits : Balance CPU and memory allocation Note : Ensure cluster has sufficient CPU capacity for all replicas. Monitor actual CPU usage to right-size limits.","title":"mongodb_cpu_limits"},{"location":"roles/mongodb/#mongodb_mem_limits","text":"Maximum memory allocated to each MongoDB container. Optional Environment Variable: MONGODB_MEM_LIMITS Default Value: 1Gi Purpose : Sets the upper bound on memory usage for MongoDB containers. MongoDB uses memory for caching data and indexes, so adequate memory is critical for performance. When to use : - Increase significantly for production workloads (4Gi-8Gi recommended) - Increase for large datasets to improve cache hit rates - Default (1Gi) is only suitable for development/testing - Set higher than mongodb_mem_requests to allow burst capacity Valid values : Memory size (e.g., 1Gi , 2Gi , 4Gi , 8Gi , 16Gi ) Impact : - Higher limits improve performance through better caching - Limits prevent MongoDB from consuming all cluster memory - Total memory limit = this value \u00d7 number of replicas - Setting too low causes frequent cache evictions and poor performance Related variables : - mongodb_mem_requests : Should be set lower than limits - mongodb_replicas : Total memory = limits \u00d7 replicas - mongodb_cpu_limits : Balance CPU and memory allocation Note : MongoDB performance heavily depends on available memory. Production deployments typically need 4Gi-8Gi per replica. Monitor memory usage and adjust accordingly.","title":"mongodb_mem_limits"},{"location":"roles/mongodb/#mongodb_cpu_requests","text":"Guaranteed CPU cores reserved for each MongoDB container. Optional Environment Variable: MONGODB_CPU_REQUESTS Default Value: 500m Purpose : Defines the minimum CPU resources guaranteed to MongoDB containers. Kubernetes scheduler uses this to place pods on nodes with sufficient available CPU. When to use : - Increase for production workloads requiring consistent performance - Set to match expected baseline CPU usage - Use default (500m) for development or light workloads - Set lower than mongodb_cpu_limits to allow burst capacity Valid values : CPU units as decimal (e.g., 0.5 , 1 , 2 ) or millicores (e.g., 500m , 1000m , 2000m ) Impact : - Higher requests guarantee more CPU but may limit pod scheduling if cluster capacity is constrained - Requests ensure MongoDB has minimum CPU even under cluster load - Total CPU request = this value \u00d7 number of replicas - Affects pod Quality of Service (QoS) class Related variables : - mongodb_cpu_limits : Requests should be lower than limits - mongodb_replicas : Total CPU requests = this value \u00d7 replicas - mongodb_mem_requests : Balance CPU and memory requests Note : Set requests based on baseline usage, not peak. The difference between requests and limits provides burst capacity.","title":"mongodb_cpu_requests"},{"location":"roles/mongodb/#mongodb_mem_requests","text":"Guaranteed memory reserved for each MongoDB container. Optional Environment Variable: MONGODB_MEM_REQUESTS Default Value: 1Gi Purpose : Defines the minimum memory resources guaranteed to MongoDB containers. Kubernetes scheduler uses this to place pods on nodes with sufficient available memory. When to use : - Increase for production workloads (2Gi-4Gi recommended) - Set to match expected baseline memory usage - Default (1Gi) is only suitable for development/testing - Set lower than mongodb_mem_limits to allow burst capacity Valid values : Memory size (e.g., 1Gi , 2Gi , 4Gi , 8Gi ) Impact : - Higher requests guarantee more memory but may limit pod scheduling if cluster capacity is constrained - Requests ensure MongoDB has minimum memory even under cluster load - Total memory request = this value \u00d7 number of replicas - Affects pod Quality of Service (QoS) class Related variables : - mongodb_mem_limits : Requests should be lower than limits - mongodb_replicas : Total memory requests = this value \u00d7 replicas - mongodb_cpu_requests : Balance CPU and memory requests Note : MongoDB needs adequate memory for good performance. Production deployments typically need 2Gi-4Gi requests. Set based on baseline usage, not peak.","title":"mongodb_mem_requests"},{"location":"roles/mongodb/#mongodb_replicas","text":"Number of MongoDB replica set members to deploy. Optional Environment Variable: MONGODB_REPLICAS Default Value: 3 Purpose : Determines the size of the MongoDB replica set, which affects high availability, read scalability, and resource consumption. Replica sets provide data redundancy and automatic failover. When to use : - Use default (3) for production deployments with high availability requirements - Set to 1 only for Single Node OpenShift (SNO) clusters or development environments - Use 5 or more for critical production workloads requiring higher availability - Odd numbers (1, 3, 5) are recommended for proper election quorum Valid values : Positive integers, typically 1, 3, or 5 Impact : - More replicas = higher availability but more resource consumption - Each replica requires its own data and log PVCs - Total resources = (CPU + memory + storage) \u00d7 replicas - Affects election behavior and write acknowledgment Related variables : - mongodb_storage_capacity_data : Total data storage = capacity \u00d7 replicas - mongodb_storage_capacity_logs : Total log storage = capacity \u00d7 replicas - mongodb_cpu_limits and mongodb_mem_limits : Total resources = limits \u00d7 replicas Note : Set to 1 for SNO clusters. Production deployments should use 3 or more for high availability. Changing replica count after deployment requires careful planning.","title":"mongodb_replicas"},{"location":"roles/mongodb/#custom_labels","text":"Comma-separated list of key=value labels to apply to MongoDB resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Purpose : Adds Kubernetes labels to MongoDB resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,app=mongodb ) Impact : Labels are applied to MongoDB resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect MongoDB functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management.","title":"custom_labels"},{"location":"roles/mongodb/#mongodb_v5_upgrade","text":"Confirmation flag to upgrade MongoDB from version 4.x to version 5. Optional Environment Variable: MONGODB_V5_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades. Upgrading MongoDB major versions requires careful planning and testing. When to use : - Set to true only when intentionally upgrading from MongoDB 4.2 or 4.4 to version 5 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to a 5.x version, triggers MongoDB upgrade from 4.x to 5.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to a 5.x version for upgrade to proceed - Other upgrade flags: mongodb_v6_upgrade , mongodb_v7_upgrade , mongodb_v8_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 5.0 release notes for breaking changes and new requirements (e.g., AVX instruction set).","title":"mongodb_v5_upgrade"},{"location":"roles/mongodb/#mongodb_v6_upgrade","text":"Confirmation flag to upgrade MongoDB from version 5 to version 6. Optional Environment Variable: MONGODB_V6_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades from version 5 to 6. When to use : - Set to true only when intentionally upgrading from MongoDB 5 to version 6 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to a 6.x version, triggers MongoDB upgrade from 5.x to 6.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to a 6.x version for upgrade to proceed - Other upgrade flags: mongodb_v5_upgrade , mongodb_v7_upgrade , mongodb_v8_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 6.0 release notes for breaking changes.","title":"mongodb_v6_upgrade"},{"location":"roles/mongodb/#mongodb_v7_upgrade","text":"Confirmation flag to upgrade MongoDB from version 6 to version 7. Optional Environment Variable: MONGODB_V7_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades from version 6 to 7. When to use : - Set to true only when intentionally upgrading from MongoDB 6 to version 7 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to a 7.x version, triggers MongoDB upgrade from 6.x to 7.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to a 7.x version for upgrade to proceed - Other upgrade flags: mongodb_v5_upgrade , mongodb_v6_upgrade , mongodb_v8_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 7.0 release notes for breaking changes.","title":"mongodb_v7_upgrade"},{"location":"roles/mongodb/#mongodb_v8_upgrade","text":"Confirmation flag to upgrade MongoDB from version 7 to version 8. Optional Environment Variable: MONGODB_V8_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades from version 7 to 8. When to use : - Set to true only when intentionally upgrading from MongoDB 7 to version 8 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to an 8.x version, triggers MongoDB upgrade from 7.x to 8.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to an 8.x version for upgrade to proceed - Other upgrade flags: mongodb_v5_upgrade , mongodb_v6_upgrade , mongodb_v7_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 8.0 release notes for breaking changes.","title":"mongodb_v8_upgrade"},{"location":"roles/mongodb/#masbr_confirm_cluster","text":"Enables cluster confirmation prompt before executing backup or restore operations. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false Purpose : Provides a safety check to confirm you're connected to the correct cluster before performing backup or restore operations, preventing accidental operations on wrong clusters. When to use : - Set to true in environments with multiple clusters to prevent mistakes - Set to true for production environments as an additional safety measure - Leave as false for automated pipelines where confirmation isn't possible Valid values : true , false Impact : When true , the role will prompt for confirmation of the cluster before proceeding with backup/restore. This adds a manual step but prevents costly mistakes. Related variables : Used with mongodb_action when set to backup or restore .","title":"masbr_confirm_cluster"},{"location":"roles/mongodb/#masbr_copy_timeout_sec","text":"Timeout in seconds for backup/restore file transfer operations. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) Purpose : Sets the maximum time allowed for copying backup files to/from storage. Prevents operations from hanging indefinitely on slow networks or large datasets. When to use : - Increase for very large databases or slow network connections - Decrease for smaller databases to fail faster on issues - Use default (12 hours) for most deployments Valid values : Positive integer representing seconds (e.g., 3600 = 1 hour, 43200 = 12 hours, 86400 = 24 hours) Impact : Operations exceeding this timeout will fail. Setting too low causes failures on legitimate long-running transfers. Setting too high delays detection of stuck operations. Related variables : Used with mongodb_action when set to backup or restore . Note : Consider database size and network speed when setting timeout. Monitor actual transfer times to optimize this value.","title":"masbr_copy_timeout_sec"},{"location":"roles/mongodb/#masbr_job_timezone","text":"Time zone for scheduled backup job execution. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None (uses UTC) Purpose : Specifies the time zone for scheduled backup CronJobs, ensuring backups run at the intended local time rather than UTC. When to use : - Set when scheduling backups to run at specific local times - Use for compliance with backup windows in specific time zones - Leave unset to use UTC (recommended for global deployments) Valid values : Any valid tz database time zone (e.g., America/New_York , Europe/London , Asia/Tokyo ) Impact : Affects when scheduled backups execute. Incorrect time zone can cause backups to run during business hours or miss backup windows. Related variables : - masbr_backup_schedule : Defines the cron schedule - Only applies when masbr_backup_schedule is set Note : When not set, CronJobs use UTC time zone. Consider daylight saving time changes when scheduling backups.","title":"masbr_job_timezone"},{"location":"roles/mongodb/#masbr_storage_local_folder","text":"Local filesystem path where backup files will be stored or retrieved from. Required when mongodb_action is backup or restore Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None Purpose : Specifies the directory for storing MongoDB backup files. This location must have sufficient space and appropriate permissions for backup operations. When to use : - Always required for backup and restore operations - Use a path with adequate storage space for full database backups - Consider using network-attached storage for backup retention - Ensure path is accessible and has proper permissions Valid values : Any valid local filesystem path (e.g., /backup/mongodb , /mnt/nfs/backups , /tmp/masbr ) Impact : Backup files are written to this location. Insufficient space causes backup failures. Path must be accessible during restore operations. Related variables : - masbr_backup_type : Determines if full or incremental backups are stored here - masbr_restore_from_version : Specifies which backup version to restore from this location Note : Ensure adequate disk space (at least 2x database size for full backups). Implement backup retention policies to manage storage usage.","title":"masbr_storage_local_folder"},{"location":"roles/mongodb/#masbr_backup_type","text":"Type of backup to create: full or incremental. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full Purpose : Determines whether to create a complete database backup or an incremental backup containing only changes since the last full backup. Incremental backups save time and storage. When to use : - Use full for initial backups or periodic complete backups - Use incr for frequent backups between full backups to save time and space - Implement a strategy like weekly full + daily incremental backups Valid values : full , incr Impact : - full : Creates complete backup, takes longer, uses more storage - incr : Creates incremental backup, faster, uses less storage, requires base full backup Related variables : - masbr_backup_from_version : Required when incr is used to specify base full backup - mongodb_action : Must be set to backup Note : Incremental backups require a full backup as base. Restore operations may need to apply multiple incremental backups sequentially.","title":"masbr_backup_type"},{"location":"roles/mongodb/#masbr_backup_from_version","text":"Timestamp of the full backup to use as base for incremental backup. Optional Environment Variable: MASBR_BACKUP_FROM_VERSION Default: None (automatically uses latest full backup) Purpose : Specifies which full backup serves as the base for an incremental backup. This links the incremental backup to a specific full backup version. When to use : - Set when creating incremental backups and you want to specify a particular full backup - Leave unset to automatically use the most recent full backup - Only valid when masbr_backup_type=incr Valid values : Timestamp in format YYYYMMDDHHMMSS (e.g., 20240621021316 ) Impact : Incremental backup will contain only changes since the specified full backup. Incorrect version can cause backup chain issues. Related variables : - masbr_backup_type : Must be set to incr - masbr_storage_local_folder : Location where full backup exists Note : If not specified, role automatically finds the latest full backup. Ensure the specified full backup exists in the storage location.","title":"masbr_backup_from_version"},{"location":"roles/mongodb/#masbr_backup_schedule","text":"Cron expression for scheduling automated backups. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None (creates on-demand backup) Purpose : Defines when automated backups should run using standard cron syntax. Enables regular, unattended backup operations. When to use : - Set to schedule regular automated backups (e.g., daily, weekly) - Leave unset for manual, on-demand backups - Consider backup windows and system load when scheduling Valid values : Standard cron expression (e.g., 0 2 * * * for daily at 2 AM, 0 2 * * 0 for weekly on Sunday at 2 AM) Impact : When set, creates a Kubernetes CronJob that automatically runs backups on schedule. Without this, backups only run when role is manually executed. Related variables : - masbr_job_timezone : Specifies time zone for schedule - masbr_backup_type : Determines if scheduled backups are full or incremental Note : Test cron expressions before deploying. Consider backup duration when scheduling to avoid overlapping backup jobs.","title":"masbr_backup_schedule"},{"location":"roles/mongodb/#masbr_restore_from_version","text":"Timestamp of the backup version to restore. Required when mongodb_action=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Purpose : Specifies which backup version to restore from. This allows point-in-time recovery to a specific backup. When to use : - Always required when performing restore operations - Use to restore to a specific point in time - Verify backup version exists before attempting restore Valid values : Timestamp in format YYYYMMDDHHMMSS (e.g., 20240621021316 ) Impact : Restores MongoDB to the state captured in the specified backup. All data after this backup will be lost. This is a destructive operation. Related variables : - mongodb_action : Must be set to restore - masbr_storage_local_folder : Location where backup exists Note : Verify backup version before restoring . List available backups in storage location first. Restore is destructive and cannot be undone without another backup.","title":"masbr_restore_from_version"},{"location":"roles/mongodb/#role-variables-ibm-cloud","text":"","title":"Role Variables - IBM Cloud"},{"location":"roles/mongodb/#ibm_mongo_name","text":"Name for the IBM Cloud Databases for MongoDB instance. Required when mongodb_provider=ibm Environment Variable: IBM_MONGO_NAME Default Value: mongo-${MAS_INSTANCE_ID} Purpose : Identifies the MongoDB database instance in IBM Cloud. This name is used for resource identification, billing, and management within IBM Cloud. When to use : - Always required when using IBM Cloud as the MongoDB provider - Use default naming convention for consistency across MAS instances - Customize only if organizational naming standards require it Valid values : Valid IBM Cloud resource name (alphanumeric, hyphens allowed, must start with letter) Impact : This name appears in IBM Cloud console, billing reports, and resource lists. Changing it after creation requires recreating the database instance. Related variables : - mas_instance_id : Default name includes this value - mongodb_provider : Must be set to ibm Note : Choose a descriptive name that identifies the MAS instance and environment. The name cannot be changed after creation.","title":"ibm_mongo_name"},{"location":"roles/mongodb/#ibm_mongo_admin_password","text":"Administrator password for the IBM Cloud MongoDB instance. Optional Environment Variable: IBM_MONGO_ADMIN_PASSWORD Default Value: Auto-generated 20-character string Purpose : Sets the password for the MongoDB administrator user. If not provided, a secure random password is automatically generated. When to use : - Set explicitly if you need to know the password in advance - Leave unset to use auto-generated secure password (recommended) - Set if integrating with external password management systems Valid values : String meeting IBM Cloud password requirements (minimum length, complexity) Impact : This password is used for administrative access to the MongoDB instance. Auto-generated passwords are stored in Kubernetes secrets. Related variables : - ibm_mongo_admin_credentials_secret_name : Secret where credentials are stored Note : Auto-generated passwords are more secure. If setting manually, ensure password meets security requirements and is stored securely.","title":"ibm_mongo_admin_password"},{"location":"roles/mongodb/#ibm_mongo_admin_credentials_secret_name","text":"Name of the Kubernetes secret containing MongoDB admin credentials. Optional Environment Variable: IBM_MONGO_ADMIN_CREDENTIALS_SECRET_NAME Default Value: <mongo-name>-admin-credentials Purpose : Specifies the Kubernetes secret name where MongoDB administrator credentials are stored. This secret is created automatically by the role. When to use : - Customize if organizational standards require specific secret naming - Use default for standard deployments - Reference this secret name in other automation Valid values : Valid Kubernetes secret name Impact : The secret contains admin username and password for MongoDB access. Other roles and applications reference this secret for database connectivity. Related variables : - ibm_mongo_name : Default secret name includes this value - ibm_mongo_admin_password : Password stored in this secret","title":"ibm_mongo_admin_credentials_secret_name"},{"location":"roles/mongodb/#ibm_mongo_service_credentials_secret_name","text":"Name of the Kubernetes secret containing MongoDB service credentials. Optional Environment Variable: IBM_MONGO_SERVICE_CREDENTIALS_SECRET_NAME Default Value: <mongo-name>-service-credentials Purpose : Specifies the Kubernetes secret name where MongoDB service-level credentials are stored. These credentials are used by MAS applications to connect to MongoDB. When to use : - Customize if organizational standards require specific secret naming - Use default for standard deployments - Reference this secret name when configuring MAS applications Valid values : Valid Kubernetes secret name Impact : The secret contains connection strings and credentials for application-level MongoDB access. MAS applications use this secret to connect to the database. Related variables : - ibm_mongo_name : Default secret name includes this value","title":"ibm_mongo_service_credentials_secret_name"},{"location":"roles/mongodb/#ibm_mongo_resourcegroup","text":"IBM Cloud resource group for MongoDB instance placement. Required when mongodb_provider=ibm Environment Variable: IBM_MONGO_RESOURCEGROUP Default Value: Default Purpose : Specifies which IBM Cloud resource group will contain the MongoDB instance. Resource groups organize IBM Cloud resources for access control and billing. When to use : - Always required when using IBM Cloud provider - Use Default for simple deployments - Specify custom resource group for organizational resource management - Align with IBM Cloud IAM and billing structure Valid values : Name of an existing IBM Cloud resource group in your account Impact : Determines access control, billing allocation, and resource organization. Users need appropriate IAM permissions for the specified resource group. Related variables : - ibmcloud_apikey : API key must have access to the specified resource group Note : Ensure the resource group exists and your API key has permissions to create resources in it.","title":"ibm_mongo_resourcegroup"},{"location":"roles/mongodb/#ibm_mongo_region","text":"IBM Cloud region where MongoDB instance will be deployed. Required when mongodb_provider=ibm Environment Variable: IBM_MONGO_REGION Default Value: us-east Purpose : Specifies the geographic region for MongoDB deployment. Region selection affects latency, data residency, and availability. When to use : - Always required when using IBM Cloud provider - Choose region closest to your OpenShift cluster for lowest latency - Consider data residency requirements for compliance - Use default ( us-east ) if no specific requirements Valid values : Valid IBM Cloud region (e.g., us-east , us-south , eu-gb , eu-de , jp-tok , au-syd ) Impact : Affects network latency between OpenShift and MongoDB, data residency compliance, and regional pricing. Cannot be changed after creation. Related variables : - ibmcloud_apikey : API key must have access to the specified region Note : Choose region carefully as it cannot be changed. Consider deploying MongoDB in the same region as your OpenShift cluster for best performance.","title":"ibm_mongo_region"},{"location":"roles/mongodb/#ibmcloud_apikey","text":"IBM Cloud API key for authentication and resource management. Required when mongodb_provider=ibm Environment Variable: IBMCLOUD_APIKEY Default Value: None Purpose : Provides authentication credentials for creating and managing IBM Cloud resources. The API key must have sufficient permissions to create and manage Databases for MongoDB instances. When to use : - Always required when using IBM Cloud provider - Must have permissions for the target resource group and region - Should be stored securely (e.g., in Ansible Vault or external secret management) Valid values : Valid IBM Cloud API key string Impact : This key is used to authenticate all IBM Cloud API calls. Insufficient permissions will cause deployment failures. Related variables : - ibm_mongo_resourcegroup : API key must have access to this resource group - ibm_mongo_region : API key must have access to this region Note : Never commit API keys to source control . Use secure secret management. Ensure the API key has appropriate IAM permissions for Databases for MongoDB service.","title":"ibmcloud_apikey"},{"location":"roles/mongodb/#ibm_mongo_plan","text":"IBM Cloud service plan for MongoDB instance. Optional Environment Variable: IBM_MONGO_PLAN Default Value: standard Purpose : Specifies the IBM Cloud service plan tier for MongoDB. Different plans offer different performance, availability, and pricing characteristics. When to use : - Use standard (default) for most production deployments - Consider enterprise plan for critical workloads requiring higher SLA - Review IBM Cloud pricing and plan features before selecting Valid values : standard , enterprise (check IBM Cloud documentation for current plan options) Impact : Affects pricing, performance characteristics, SLA, and available features. Plan cannot be changed after creation without recreating the instance. Related variables : - ibm_mongo_memory , ibm_mongo_disk , ibm_mongo_cpu : Resource allocations vary by plan Note : Review IBM Cloud Databases for MongoDB plan documentation for current offerings and pricing.","title":"ibm_mongo_plan"},{"location":"roles/mongodb/#ibm_mongo_service","text":"IBM Cloud service type identifier for MongoDB. Required (Read-only constant) Environment Variable: None (internal constant) Default Value: databases-for-mongodb Purpose : Identifies the IBM Cloud service type. This is a fixed value used internally by the role. When to use : This is set automatically by the role and should not be modified. Valid values : databases-for-mongodb Impact : Used in IBM Cloud API calls to specify the service type. Note : This is a constant value and does not need to be set by users.","title":"ibm_mongo_service"},{"location":"roles/mongodb/#ibm_mongo_service_endpoints","text":"Network endpoint type for MongoDB connectivity. Optional Environment Variable: IBM_MONGO_SERVICE_ENDPOINTS Default Value: public Purpose : Determines whether MongoDB is accessible via public internet or private network only. Private endpoints provide better security and performance for cluster-to-database communication. When to use : - Use public for simple deployments or when OpenShift cluster lacks private network connectivity - Use private for production deployments with private network connectivity (recommended) - Private endpoints require IBM Cloud private network configuration Valid values : public , private Impact : - public : MongoDB accessible over internet (requires firewall rules) - private : MongoDB accessible only via IBM Cloud private network (more secure, lower latency) Related variables : Network configuration must support the chosen endpoint type Note : Private endpoints are recommended for production. Ensure your OpenShift cluster can reach IBM Cloud private network if using private .","title":"ibm_mongo_service_endpoints"},{"location":"roles/mongodb/#ibm_mongo_version","text":"MongoDB version to deploy in IBM Cloud. Optional Environment Variable: IBM_MONGO_VERSION Default Value: 4.2 Purpose : Specifies which MongoDB version to deploy. Version selection affects available features, performance, and compatibility. When to use : - Use default ( 4.2 ) for compatibility with older MAS versions - Specify newer version (e.g., 5.0 , 6.0 ) for new deployments - Check MAS compatibility matrix before selecting version Valid values : MongoDB versions supported by IBM Cloud Databases (e.g., 4.2 , 4.4 , 5.0 , 6.0 ) Impact : Affects available MongoDB features, performance characteristics, and MAS compatibility. Version cannot be easily downgraded. Related variables : - Check MAS compatibility requirements before selecting version Note : Verify version compatibility with your MAS version. Newer MongoDB versions may require MAS updates.","title":"ibm_mongo_version"},{"location":"roles/mongodb/#ibm_mongo_memory","text":"Memory allocation per MongoDB member in MB. Optional Environment Variable: IBM_MONGO_MEMORY Default Value: 3840 (3.75 GB) Purpose : Specifies memory allocation for each MongoDB replica set member. Memory affects caching performance and query execution. When to use : - Use default (3840 MB) for development or small deployments - Increase for production workloads (8192 MB or higher recommended) - Increase for large datasets to improve cache hit rates Valid values : Integer in MB, minimum varies by plan (typically 1024 MB minimum) Impact : Higher memory improves performance through better caching but increases costs. Total cost = memory \u00d7 number of members. Related variables : - ibm_mongo_plan : Available memory ranges vary by plan - ibm_mongo_disk : Balance memory and disk allocation Note : IBM Cloud charges based on allocated memory. Production deployments typically need 8GB+ per member.","title":"ibm_mongo_memory"},{"location":"roles/mongodb/#ibm_mongo_disk","text":"Disk storage allocation per MongoDB member in MB. Optional Environment Variable: IBM_MONGO_DISK Default Value: 30720 (30 GB) Purpose : Specifies disk storage for each MongoDB replica set member. Storage holds databases, indexes, and operational logs. When to use : - Use default (30 GB) for development or small datasets - Increase for production workloads based on data volume - Plan for data growth and backup storage needs Valid values : Integer in MB, minimum varies by plan (typically 5120 MB minimum) Impact : Affects storage capacity and costs. Disk can be expanded but not shrunk. Total storage = disk \u00d7 number of members. Related variables : - ibm_mongo_plan : Available disk ranges vary by plan - ibm_mongo_memory : Balance memory and disk allocation Note : Plan for data growth. Disk can be expanded online but cannot be reduced. Monitor storage usage to avoid running out of space.","title":"ibm_mongo_disk"},{"location":"roles/mongodb/#ibm_mongo_cpu","text":"Dedicated CPU cores per MongoDB member. Optional Environment Variable: IBM_MONGO_CPU Default Value: 0 (shared CPU) Purpose : Specifies dedicated CPU cores for each MongoDB member. Dedicated CPUs provide consistent performance but increase costs. When to use : - Use 0 (default) for shared CPU, suitable for development and light workloads - Set to 3 or higher for production workloads requiring consistent performance - Dedicated CPUs recommended for production environments Valid values : 0 (shared), or integer \u2265 3 for dedicated CPUs Impact : - 0 : Shared CPU, lower cost, variable performance - \u22653 : Dedicated CPUs, higher cost, consistent performance Related variables : - ibm_mongo_plan : CPU options vary by plan - ibm_mongo_memory : Balance CPU and memory allocation Note : Dedicated CPUs significantly increase costs but provide predictable performance. Production workloads typically need dedicated CPUs.","title":"ibm_mongo_cpu"},{"location":"roles/mongodb/#ibm_mongo_backup_id","text":"IBM Cloud backup CRN (Cloud Resource Name) for restore operations. Required when is_restore=true Environment Variable: IBM_MONGO_BACKUP_ID Default Value: None Purpose : Specifies the IBM Cloud backup resource to restore from. The CRN uniquely identifies a specific backup in IBM Cloud. When to use : - Required only when restoring from an IBM Cloud backup - Obtain CRN from IBM Cloud console or CLI - Leave unset for new deployments Valid values : Valid IBM Cloud CRN for a MongoDB backup (format: crn:v1:... ) Impact : Restores MongoDB to the state captured in the specified backup. All current data will be replaced. Related variables : - is_restore : Must be set to true - restored_mongodb_service_name : Name for the restored service Note : Verify backup CRN before restoring . Restore is destructive and replaces all current data. Test restore procedures in non-production first.","title":"ibm_mongo_backup_id"},{"location":"roles/mongodb/#is_restore","text":"Flag to enable restore from IBM Cloud backup. Optional Environment Variable: IS_RESTORE Default Value: false Purpose : Controls whether to create a new MongoDB instance or restore from an existing backup. Acts as a safety flag to prevent accidental restores. When to use : - Set to true only when intentionally restoring from backup - Leave as false (default) for new deployments - Must be explicitly set to perform restore Valid values : true , false Impact : When true , creates MongoDB instance from backup instead of fresh deployment. Requires ibm_mongo_backup_id and restored_mongodb_service_name . Related variables : - ibm_mongo_backup_id : Required when true - restored_mongodb_service_name : Required when true Note : Always verify backup details before setting to true . Restore operations cannot be undone without another backup.","title":"is_restore"},{"location":"roles/mongodb/#restored_mongodb_service_name","text":"Name for the MongoDB service when restoring from backup. Required when is_restore=true Environment Variable: RESTORED_MONGODB_SERVICE_NAME Default Value: None Purpose : Specifies the name for the new MongoDB service created from backup. This allows restoring to a different service name than the original. When to use : - Required only when is_restore=true - Can be same as or different from original service name - Use different name to restore alongside existing instance for testing Valid values : Valid IBM Cloud resource name Impact : The restored MongoDB instance will have this name in IBM Cloud. Choose carefully as it affects resource identification and billing. Related variables : - is_restore : Must be set to true - ibm_mongo_backup_id : Backup to restore from - ibm_mongo_name : Original service name (can be different) Note : Using a different name allows side-by-side comparison of restored and current instances before switching over.","title":"restored_mongodb_service_name"},{"location":"roles/mongodb/#role-variables-aws-documentdb","text":"","title":"Role Variables - AWS DocumentDB"},{"location":"roles/mongodb/#aws_access_key_id","text":"AWS account access key ID for authentication. Required when mongodb_provider=aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None Purpose : Provides AWS authentication credentials for creating and managing DocumentDB resources. The access key must have sufficient IAM permissions for DocumentDB, VPC, and related services. When to use : - Always required when using AWS DocumentDB provider - Must have IAM permissions for DocumentDB, EC2 (VPC/subnets/security groups) - Should be stored securely (e.g., in Ansible Vault or external secret management) Valid values : Valid AWS access key ID string Impact : This key is used to authenticate all AWS API calls. Insufficient permissions will cause deployment failures. Related variables : - aws_secret_access_key : Must be provided together - aws_region : Access key must have permissions in the target region Note : Never commit AWS credentials to source control . Use secure secret management. Ensure the IAM user/role has appropriate permissions for DocumentDB and VPC operations.","title":"aws_access_key_id"},{"location":"roles/mongodb/#aws_secret_access_key","text":"AWS account secret access key for authentication. Required when mongodb_provider=aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None Purpose : Provides the secret component of AWS authentication credentials. Works together with aws_access_key_id to authenticate AWS API requests. When to use : - Always required when using AWS DocumentDB provider - Must correspond to the provided aws_access_key_id - Should be stored securely Valid values : Valid AWS secret access key string Impact : Used with access key ID to authenticate AWS API calls. Invalid or mismatched credentials will cause authentication failures. Related variables : - aws_access_key_id : Must be provided together Note : Store securely and never commit to source control . Rotate credentials regularly following AWS security best practices.","title":"aws_secret_access_key"},{"location":"roles/mongodb/#aws_region","text":"AWS region where DocumentDB cluster will be deployed. Required when mongodb_provider=aws Environment Variable: AWS_REGION Default Value: us-east-2 Purpose : Specifies the geographic AWS region for DocumentDB deployment. Region selection affects latency, data residency, availability zones, and pricing. When to use : - Always required when using AWS DocumentDB provider - Choose region closest to your OpenShift cluster for lowest latency - Consider data residency requirements for compliance - Use default ( us-east-2 ) if no specific requirements Valid values : Valid AWS region code (e.g., us-east-1 , us-east-2 , us-west-2 , eu-west-1 , ap-southeast-1 ) Impact : Affects network latency, data residency compliance, available availability zones, and regional pricing. Cannot be changed after creation. Related variables : - vpc_id : VPC must exist in the specified region - aws_access_key_id : Credentials must have permissions in this region Note : Choose region carefully as it cannot be changed. Deploy DocumentDB in the same region as your OpenShift cluster for best performance.","title":"aws_region"},{"location":"roles/mongodb/#vpc_id","text":"AWS VPC ID where DocumentDB resources will be created. Required when mongodb_provider=aws Environment Variable: VPC_ID Default Value: None Purpose : Specifies the AWS Virtual Private Cloud where DocumentDB cluster, subnets, and security groups will be created. The VPC provides network isolation and connectivity. When to use : - Always required when using AWS DocumentDB provider - Use the same VPC as your OpenShift cluster for direct connectivity - VPC must exist in the specified aws_region Valid values : Valid AWS VPC ID (format: vpc-xxxxxxxxxxxxxxxxx ) Impact : Determines network connectivity and security boundaries. DocumentDB will only be accessible from resources within this VPC or connected networks. Related variables : - aws_region : VPC must exist in this region - docdb_cidr_az1 , docdb_cidr_az2 , docdb_cidr_az3 : Subnets created within this VPC - docdb_ingress_cidr , docdb_egress_cidr : Should match VPC CIDR ranges Note : Ensure the VPC has sufficient available IP addresses and appropriate routing for DocumentDB connectivity.","title":"vpc_id"},{"location":"roles/mongodb/#docdb_cluster_name","text":"Name for the AWS DocumentDB cluster. Required when mongodb_provider=aws Environment Variable: DOCDB_CLUSTER_NAME Default Value: None Purpose : Identifies the DocumentDB cluster in AWS. This name is used for resource identification, tagging, and as a prefix for related resources. When to use : - Always required when using AWS DocumentDB provider - Choose a descriptive name that identifies the MAS instance and environment - Name is used as prefix for subnet groups and security groups Valid values : Valid AWS DocumentDB cluster identifier (lowercase, alphanumeric, hyphens, must start with letter) Impact : This name appears in AWS console, CloudWatch metrics, and billing. Related resources (subnet group, security group) are named based on this value. Related variables : - docdb_subnet_group_name : Defaults to docdb-{cluster_name} - docdb_security_group_name : Defaults to docdb-{cluster_name} - docdb_admin_credentials_secret_name : Defaults to {cluster_name}-admin-credentials Note : Choose a meaningful name as it cannot be easily changed. The name must be unique within your AWS account and region.","title":"docdb_cluster_name"},{"location":"roles/mongodb/#docdb_subnet_group_name","text":"Name for the DocumentDB subnet group. Optional Environment Variable: DOCDB_SUBNET_GROUP_NAME Default Value: docdb-{{ docdb_cluster_name }} Purpose : Specifies the name for the DocumentDB subnet group that defines which subnets the cluster can use. The role creates this subnet group automatically. When to use : - Use default naming for standard deployments - Customize only if organizational naming standards require it Valid values : Valid AWS subnet group name Impact : The subnet group associates the DocumentDB cluster with specific subnets across availability zones. Related variables : - docdb_cluster_name : Default name includes this value - docdb_cidr_az1 , docdb_cidr_az2 , docdb_cidr_az3 : Subnets included in this group Note : This is automatically created by the role. Default naming is recommended for consistency.","title":"docdb_subnet_group_name"},{"location":"roles/mongodb/#docdb_security_group_name","text":"Name for the DocumentDB security group. Optional Environment Variable: DOCDB_SECURITY_GROUP_NAME Default Value: docdb-{{ docdb_cluster_name }} Purpose : Specifies the name for the security group that controls network access to the DocumentDB cluster. The role creates this security group automatically. When to use : - Use default naming for standard deployments - Customize only if organizational naming standards require it Valid values : Valid AWS security group name Impact : The security group defines firewall rules for DocumentDB access based on docdb_ingress_cidr and docdb_egress_cidr . Related variables : - docdb_cluster_name : Default name includes this value - docdb_ingress_cidr : Allowed source CIDR for inbound traffic - docdb_egress_cidr : Allowed destination CIDR for outbound traffic Note : This is automatically created by the role. Default naming is recommended for consistency.","title":"docdb_security_group_name"},{"location":"roles/mongodb/#docdb_admin_credentials_secret_name","text":"Name of the Kubernetes secret containing DocumentDB admin credentials. Optional Environment Variable: DOCDB_ADMIN_CREDENTIALS_SECRET_NAME Default Value: {{ docdb_cluster_name }}-admin-credentials Purpose : Specifies the Kubernetes secret name where DocumentDB administrator credentials are stored. This secret is created automatically by the role. When to use : - Use default naming for standard deployments - Customize only if organizational naming standards require it Valid values : Valid Kubernetes secret name Impact : The secret contains admin username and password for DocumentDB access. MAS and other applications reference this secret for database connectivity. Related variables : - docdb_cluster_name : Default name includes this value - docdb_master_username : Username stored in this secret Note : This secret is created in the MAS core namespace. Default naming is recommended for consistency.","title":"docdb_admin_credentials_secret_name"},{"location":"roles/mongodb/#docdb_engine_version","text":"DocumentDB engine version to deploy. Optional Environment Variable: DOCDB_ENGINE_VERSION Default Value: 5.0.0 Purpose : Specifies the DocumentDB engine version. MAS requires DocumentDB 5.0.0 for MongoDB compatibility. When to use : - Use default ( 5.0.0 ) for MAS deployments (required) - Do not change unless specifically required by MAS version Valid values : 5.0.0 (only version supported by MAS) Impact : Determines MongoDB compatibility and available features. MAS is only certified with DocumentDB 5.0.0. Related variables : None Note : MAS only supports DocumentDB 5.0.0 . Do not change this value unless MAS documentation explicitly supports other versions.","title":"docdb_engine_version"},{"location":"roles/mongodb/#docdb_master_username","text":"Master username for DocumentDB cluster administration. Optional Environment Variable: DOCDB_MASTER_USERNAME Default Value: docdbadmin Purpose : Specifies the master administrator username for the DocumentDB cluster. This user has full administrative privileges. When to use : - Use default ( docdbadmin ) for standard deployments - Customize if organizational security policies require specific usernames Valid values : Valid DocumentDB username (alphanumeric, must start with letter, 1-63 characters) Impact : This username is used for administrative access and is stored in the Kubernetes secret specified by docdb_admin_credentials_secret_name . Related variables : - docdb_admin_credentials_secret_name : Secret where credentials are stored Note : Choose carefully as the master username cannot be changed after cluster creation.","title":"docdb_master_username"},{"location":"roles/mongodb/#docdb_instance_class","text":"AWS instance class for DocumentDB instances. Optional Environment Variable: DOCDB_INSTANCE_CLASS Default Value: db.t3.medium Purpose : Specifies the compute and memory capacity for each DocumentDB instance. Instance class affects performance, availability, and cost. When to use : - Use db.t3.medium (default) for development or small deployments - Use db.r5.large or larger for production workloads - Consider db.r6g instances for better price/performance (ARM-based) Valid values : Valid DocumentDB instance class (e.g., db.t3.medium , db.r5.large , db.r5.xlarge , db.r6g.large ) Impact : Affects CPU, memory, network performance, and cost. Larger instances provide better performance but cost more. Related variables : - docdb_instance_number : Total cost = instance class cost \u00d7 number of instances Note : Production deployments typically need db.r5.large or larger. Review AWS DocumentDB pricing and instance specifications.","title":"docdb_instance_class"},{"location":"roles/mongodb/#docdb_instance_number","text":"Number of DocumentDB instances in the cluster. Optional Environment Variable: DOCDB_INSTANCE_NUMBER Default Value: 3 Purpose : Determines the number of instances in the DocumentDB cluster. More instances provide higher availability and read scalability. When to use : - Use default ( 3 ) for production deployments with high availability - Use 1 only for development or testing (no high availability) - Use 5 or more for critical workloads requiring higher availability Valid values : Integer from 1 to 16 Impact : - More instances = higher availability and read capacity but higher cost - Instances are distributed across availability zones for fault tolerance - Total cost = instance class cost \u00d7 number of instances Related variables : - docdb_instance_class : Determines per-instance cost and performance - docdb_cidr_az1 , docdb_cidr_az2 , docdb_cidr_az3 : Instances distributed across these AZs Note : Production deployments should use 3 or more instances for high availability. Single instance has no failover capability.","title":"docdb_instance_number"},{"location":"roles/mongodb/#docdb_instance_identifier_prefix","text":"Prefix for DocumentDB instance identifiers. Required when mongodb_provider=aws Environment Variable: DOCDB_INSTANCE_IDENTIFIER_PREFIX Default Value: None Purpose : Specifies the prefix used to name individual DocumentDB instances. Instance names are formed as {prefix}-{number} . When to use : - Always required when using AWS DocumentDB provider - Use a descriptive prefix that identifies the cluster and environment - Typically matches or relates to docdb_cluster_name Valid values : Valid AWS instance identifier prefix (lowercase, alphanumeric, hyphens) Impact : Instance names appear in AWS console and CloudWatch metrics. Choose a meaningful prefix for easy identification. Related variables : - docdb_cluster_name : Typically related to cluster name Note : Instance identifiers are formed as {prefix}-1 , {prefix}-2 , etc. Choose a clear, descriptive prefix.","title":"docdb_instance_identifier_prefix"},{"location":"roles/mongodb/#docdb_ingress_cidr","text":"CIDR block allowed to connect to DocumentDB cluster. Required when mongodb_provider=aws Environment Variable: DOCDB_INGRESS_CIDR Default Value: None Purpose : Specifies the IPv4 CIDR range from which incoming connections to DocumentDB are allowed. This is used in security group ingress rules. When to use : - Always required when using AWS DocumentDB provider - Typically set to the CIDR of your OpenShift cluster's VPC - Can be set to specific subnet CIDRs for tighter security Valid values : Valid IPv4 CIDR notation (e.g., 10.0.0.0/16 , 172.31.0.0/16 ) Impact : Only traffic from this CIDR range can connect to DocumentDB. Too restrictive blocks legitimate traffic; too permissive reduces security. Related variables : - vpc_id : Should match VPC CIDR or subnet CIDRs within the VPC - docdb_egress_cidr : Typically set to same value Note : Set to your OpenShift cluster's VPC CIDR for proper connectivity. Verify CIDR ranges before deployment.","title":"docdb_ingress_cidr"},{"location":"roles/mongodb/#docdb_egress_cidr","text":"CIDR block for outbound connections from DocumentDB cluster. Required when mongodb_provider=aws Environment Variable: DOCDB_EGRESS_CIDR Default Value: None Purpose : Specifies the IPv4 CIDR range to which DocumentDB can send outbound connections. This is used in security group egress rules. When to use : - Always required when using AWS DocumentDB provider - Typically set to the same value as docdb_ingress_cidr - Set to VPC CIDR for standard deployments Valid values : Valid IPv4 CIDR notation (e.g., 10.0.0.0/16 , 172.31.0.0/16 ) Impact : DocumentDB can only send traffic to this CIDR range. Affects ability to respond to client connections. Related variables : - docdb_ingress_cidr : Typically set to same value - vpc_id : Should match VPC CIDR Note : Usually set to the same value as docdb_ingress_cidr . Verify CIDR ranges match your network configuration.","title":"docdb_egress_cidr"},{"location":"roles/mongodb/#docdb_cidr_az1","text":"CIDR block for DocumentDB subnet in availability zone 1. Required when mongodb_provider=aws Environment Variable: DOCDB_CIDR_AZ1 Default Value: None Purpose : Specifies the IPv4 CIDR for the subnet in the first availability zone. If the subnet exists with tag Name: {{ docdb_cluster_name }} , it's used; otherwise, a new subnet is created. When to use : - Always required when using AWS DocumentDB provider - Must be within the VPC CIDR range - Must not overlap with other subnets in the VPC Valid values : Valid IPv4 CIDR notation within VPC range (e.g., 10.0.1.0/24 ) Impact : Defines the IP address range for DocumentDB instances in AZ1. Subnet size affects number of available IP addresses. Related variables : - vpc_id : CIDR must be within this VPC's range - docdb_cidr_az2 , docdb_cidr_az3 : Must not overlap with these subnets Note : Plan subnet sizes carefully. Each DocumentDB instance needs an IP address. Use /24 or larger subnets for flexibility.","title":"docdb_cidr_az1"},{"location":"roles/mongodb/#docdb_cidr_az2","text":"CIDR block for DocumentDB subnet in availability zone 2. Required when mongodb_provider=aws Environment Variable: DOCDB_CIDR_AZ2 Default Value: None Purpose : Specifies the IPv4 CIDR for the subnet in the second availability zone. If the subnet exists with tag Name: {{ docdb_cluster_name }} , it's used; otherwise, a new subnet is created. When to use : - Always required when using AWS DocumentDB provider - Must be within the VPC CIDR range - Must not overlap with other subnets in the VPC Valid values : Valid IPv4 CIDR notation within VPC range (e.g., 10.0.2.0/24 ) Impact : Defines the IP address range for DocumentDB instances in AZ2. Required for multi-AZ high availability. Related variables : - vpc_id : CIDR must be within this VPC's range - docdb_cidr_az1 , docdb_cidr_az3 : Must not overlap with these subnets Note : Use different availability zones for AZ1, AZ2, and AZ3 to ensure high availability across zones.","title":"docdb_cidr_az2"},{"location":"roles/mongodb/#docdb_cidr_az3","text":"CIDR block for DocumentDB subnet in availability zone 3. Required when mongodb_provider=aws Environment Variable: DOCDB_CIDR_AZ3 Default Value: None Purpose : Specifies the IPv4 CIDR for the subnet in the third availability zone. If the subnet exists with tag Name: {{ docdb_cluster_name }} , it's used; otherwise, a new subnet is created. When to use : - Always required when using AWS DocumentDB provider - Must be within the VPC CIDR range - Must not overlap with other subnets in the VPC Valid values : Valid IPv4 CIDR notation within VPC range (e.g., 10.0.3.0/24 ) Impact : Defines the IP address range for DocumentDB instances in AZ3. Provides third availability zone for maximum fault tolerance. Related variables : - vpc_id : CIDR must be within this VPC's range - docdb_cidr_az1 , docdb_cidr_az2 : Must not overlap with these subnets Note : Three availability zones provide best fault tolerance. Ensure subnets are in different AZs for proper distribution.","title":"docdb_cidr_az3"},{"location":"roles/mongodb/#aws-documentdb-secret-rotation-variables","text":"The following variables are used for rotating DocumentDB credentials. These are typically used with mongodb_action=rotate-secret .","title":"AWS DocumentDB Secret Rotation Variables"},{"location":"roles/mongodb/#docdb_mongo_instance_name","text":"DocumentDB instance name for secret rotation. Required when rotating secrets Environment Variable: DOCDB_MONGO_INSTANCE_NAME Default Value: None Purpose : Identifies the specific DocumentDB instance for credential rotation operations. When to use : - Required when performing secret rotation ( mongodb_action=rotate-secret ) - Must match an existing DocumentDB instance name Valid values : Valid DocumentDB instance identifier Impact : Specifies which DocumentDB instance's credentials will be rotated. Related variables : - docdb_cluster_name : Instance belongs to this cluster - docdb_host : Host address of this instance Note : Verify instance name before rotation to avoid affecting wrong instance.","title":"docdb_mongo_instance_name"},{"location":"roles/mongodb/#docdb_host","text":"DocumentDB instance host address for connection. Required when rotating secrets Environment Variable: DOCDB_HOST Default Value: None Purpose : Specifies the host address of a DocumentDB instance for establishing connection during secret rotation. When to use : - Required when performing secret rotation - Use any one host address from the DocumentDB cluster - Obtain from AWS console or DocumentDB cluster endpoint Valid values : Valid DocumentDB instance hostname or endpoint Impact : Used to connect to DocumentDB for credential rotation operations. Related variables : - docdb_port : Port for this host - docdb_mongo_instance_name : Instance identifier Note : Any instance host from the cluster can be used for rotation operations.","title":"docdb_host"},{"location":"roles/mongodb/#docdb_port","text":"DocumentDB instance port number. Required when rotating secrets Environment Variable: DOCDB_PORT Default Value: None (typically 27017 ) Purpose : Specifies the port number for connecting to the DocumentDB instance during secret rotation. When to use : - Required when performing secret rotation - Typically 27017 (default DocumentDB port) Valid values : Valid port number (typically 27017 ) Impact : Used with docdb_host to establish connection for credential rotation. Related variables : - docdb_host : Host address for this port Note : DocumentDB uses port 27017 by default unless customized during cluster creation.","title":"docdb_port"},{"location":"roles/mongodb/#docdb_instance_username","text":"Username for which password is being rotated. Required when rotating secrets Environment Variable: DOCDB_INSTANCE_USERNAME Default Value: None Purpose : Specifies the DocumentDB username whose password will be changed during rotation. When to use : - Required when performing secret rotation - Typically the application user or admin user - Must be an existing DocumentDB user Valid values : Valid DocumentDB username Impact : This user's password will be changed. Applications using this username must be updated with the new password. Related variables : - docdb_instance_password_old : Current password for this user - docdb_master_username : Master user for performing rotation Note : Ensure applications can handle password rotation. Consider using connection pooling with reconnection logic.","title":"docdb_instance_username"},{"location":"roles/mongodb/#docdb_instance_password_old","text":"Current password for the user being rotated. Required when rotating secrets Environment Variable: DOCDB_PASSWORD_OLD Default Value: None Purpose : Provides the current password for authentication before rotation. Used to verify current credentials. When to use : - Required when performing secret rotation - Must be the current valid password Valid values : Current password string Impact : Used to authenticate before changing password. Incorrect password will cause rotation to fail. Related variables : - docdb_instance_username : User for this password Note : Store securely. After rotation, this password will no longer be valid.","title":"docdb_instance_password_old"},{"location":"roles/mongodb/#docdb_master_password","text":"DocumentDB master user password for administrative operations. Required when rotating secrets Environment Variable: DOCDB_MASTER_PASSWORD Default Value: None Purpose : Provides master user credentials for performing password rotation operations. Master user has privileges to change other users' passwords. When to use : - Required when performing secret rotation - Must be the current master password Valid values : Valid master password string Impact : Used to authenticate as master user to perform credential rotation. Related variables : - docdb_master_username : Master username for this password Note : Store master credentials securely . Never commit to source control.","title":"docdb_master_password"},{"location":"roles/mongodb/#docdb_master_username_1","text":"DocumentDB master username for administrative operations. Required when rotating secrets Environment Variable: DOCDB_MASTER_USERNAME Default Value: None Purpose : Specifies the master username for performing password rotation operations. Master user has administrative privileges. When to use : - Required when performing secret rotation - Typically docdbadmin or the value set during cluster creation Valid values : Valid DocumentDB master username Impact : Used with docdb_master_password to authenticate for credential rotation. Related variables : - docdb_master_password : Password for this master user Note : This should match the master username set during DocumentDB cluster creation. AWS DocumentDB destroy-data action Variables","title":"docdb_master_username"},{"location":"roles/mongodb/#mas_instance_id_1","text":"The specified MAS instance ID Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/mongodb/#mongo_username","text":"Mongo Username Environment Variable: MONGO_USERNAME Default Value: None","title":"mongo_username"},{"location":"roles/mongodb/#mongo_password","text":"Mongo password Environment Variable: MONGO_PASSWORD Default Value: None","title":"mongo_password"},{"location":"roles/mongodb/#config","text":"Mongo Config, please refer to the below example playbook section for details Required Environment Variable: CONFIG Default Value: None","title":"config"},{"location":"roles/mongodb/#certificates","text":"Mongo Certificates, please refer to the below example playbook section for details Required Environment Variable: CERTIFICATES Default Value: None","title":"certificates"},{"location":"roles/mongodb/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/mongodb/#install-ce-operator","text":"- hosts: localhost any_errors_fatal: true vars: mongodb_storage_class: ibmc-block-gold mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.mongodb","title":"Install (CE Operator)"},{"location":"roles/mongodb/#backup-ce-operator","text":"- hosts: localhost any_errors_fatal: true vars: mongodb_action: backup mas_instance_id: masinst1 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.mongodb","title":"Backup (CE Operator)"},{"location":"roles/mongodb/#restore-ce-operator","text":"- hosts: localhost any_errors_fatal: true vars: mongodb_action: restore mas_instance_id: masinst1 masbr_restore_from_version: 20240621021316 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.mongodb","title":"Restore (CE Operator)"},{"location":"roles/mongodb/#install-ibm-cloud","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: ibm ibmcloud_apikey: apikey**** ibmcloud_resource_group: mas-test roles: - ibm.mas_devops.mongodb","title":"Install (IBM Cloud)"},{"location":"roles/mongodb/#install-aws-documentdb","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: aws mongodb_action: provision docdb_size: ~/docdb-config.yml docdb_cluster_name: test-db docdb_ingress_cidr: 10.0.0.0/16 docdb_egress_cidr: 10.0.0.0/16 docdb_cidr_az1: 10.0.0.0/26 docdb_cidr_az2: 10.0.0.64/26 docdb_cidr_az3: 10.0.0.128/26 docdb_instance_identifier_prefix: test-db-instance vpc_id: test-vpc-id aws_access_key_id: aws-key aws_secret_access_key: aws-access-key roles: - ibm.mas_devops.mongodb","title":"Install (AWS DocumentDB)"},{"location":"roles/mongodb/#aws-documentdb-secret-rotation","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: aws mongodb_action: docdb_secret_rotate docdb_mongo_instance_name: test-db-instance db_host: aws.test1.host7283-***** db_port: 27017 docdb_master_username: admin docdb_master_password: pass*** docdb_instance_password_old: oldpass**** docdb_instance_username: testuser aws_access_key_id: aws-key aws_secret_access_key: aws-access-key roles: - ibm.mas_devops.mongodb","title":"AWS DocumentDb Secret Rotation"},{"location":"roles/mongodb/#aws-documentdb-destroy-data-action","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mongodb_provider: aws mongodb_action: destroy-data mongo_username: pqradmin mongo_password: xyzabc config: configDb: admin authMechanism: DEFAULT retryWrites: false hosts: - host: abc-0.pqr.databases.appdomain.cloud port: 32250 - host: abc-1.pqr.databases.appdomain.cloud port: 32250 - host: abc-2.pqr.databases.appdomain.cloud port: 32250 certificates: - alias: ca crt: | -----BEGIN CERTIFICATE----- MIIDDzCCAfegAwIBAgIJANEH58y2/kzHMA0GCSqGSIb3DQEBCwUAMB4xHDAaBgNV BAMME0lCTSBDbG91ZCBEYXRhYmFzZXMwHhcNMTgwNjI1MTQyOTAwWhcNMjgwNjIy MTQyOTAwWjAeMRwwGgYDVQQDDBNJQk0gQ2xvdWQgRGF0YWJhc2VzMIIBIjANBgkq 1eKI2FLzYKpoKBe5rcnrM7nHgNc/nCdEs5JecHb1dHv1QfPm6pzIxwIDAQABo1Aw TjAdBgNVHQ4EFgQUK3+XZo1wyKs+DEoYXbHruwSpXjgwHwYDVR0jBBgwFoAUK3+X Zo1wyKs+DEoYXbHruwSpXjgwDAYDVR0TBAUwAwEB/zANBgkqhkiG9w0BAQsFAAOC doqqgGIZ2nxCkp5/FXxF/TMb55vteTQwfgBy60jVVkbF7eVOWCv0KaNHPF5hrqbN i+3XjJ7/peF3xMvTMoy35DcT3E2ZeSVjouZs15O90kI3k2daS2OHJABW0vSj4nLz +PQzp/B9cQmOO8dCe049Q3oaUA== -----END CERTIFICATE----- roles: - ibm.mas_devops.mongodb","title":"AWS DocumentDb destroy-data action"},{"location":"roles/mongodb/#run-role-playbook","text":"export MONGODB_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=masinst1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/mongodb/#troubleshooting","text":"Important Please be cautious while performing any of the troubleshooting steps outlined below. It is important to understand that the MongoDB Community operator persists data within Persistent Volume Claims. These claims should not be removed inadvertent deletion of the mongoce namespace could result in data loss.","title":"Troubleshooting"},{"location":"roles/mongodb/#mongodb-replica-set-pods-will-not-start","text":"MongoDB 5 has introduced new platform specific requirements. Please consult the Platform Support Notes for detailed information. It is of particular importance to confirm that the AVX instruction set is exposed or available to the MongoDB workloads. This can easily be determined by entering any running pod on the same OpenShift cluster where MongoDB replica set members are failing to start. Once inside of a running pod the following command can be executed to confirm if the AVX instruction set is available: cat /proc/cpuinfo | grep flags | grep avx If avx is not found in the available flags then either the physical processor hosting the OpenShift cluster does not provide the AVX instruction set or the virtual host configuration is not exposing the AVX instruction set. If the latter is suspected the virtual hosting documentation should be referenced for details on how to expose the AVX instruction set.","title":"MongoDB Replica Set Pods Will Not Start"},{"location":"roles/mongodb/#ldap-authentication","text":"If authenticating via LDAP with PLAIN specified for authMechanism then configDb must be set to $external in the MongoCfg. The field configDb in the MongoCfg refers to the authentication database.","title":"LDAP Authentication"},{"location":"roles/mongodb/#ca-certificate-renewal","text":"Warning If the MongoDB CA Certificate expires the MongoDB replica set will become unusable. Replica set members will not be able to communicate with each other and client applications (i.e. Maximo Application Suite components) will not be to connect. In order to renew the CA Certificate used by the MongoDB replica set the following steps must be taken: Delete the CA Certificate resource Delete the MongoDB server Certificate resource Delete the Secrets resources associated with both the CA Certificate and Server Certificate Delete the Secret resource which contains the MongoDB configuration parameters Delete the ConfigMap resources which contains the CA certificate Delete the Secret resource which contains the sever certificate and private key The following steps illustrate the process required to renew the CA Certificate, sever certificate and reconfigure the MongoDB replica set with the new CA and server certificates. The first step is to stop the Mongo replica set and MongoDb CE Operator pod. oc project mongoce oc delete deployment mongodb-kubernetes-operator Important Make sure the MongoDB Community operator pod has terminated before proceeding. oc delete statefulset mas-mongo-ce Important Make sure all pods in the mongoce namespace have terminated before proceeding Remove expired CA Certificate and Server Certificate resources. Clean up MongoDB Community configuration and then run the mongodb role. oc delete certificate mongo-ca-crt oc delete certificate mongo-server oc delete secret mongo-ca-secret oc delete secret mongo-server-cert oc delete secret mas-mongo-ce-config oc delete configmap mas-mongo-ce-cert-map oc delete secret mas-mongo-ce-server-certificate-key export ROLE_NAME=mongodb ansible-playbook ibm.mas_devops.run_role Once the mongodb role has completed the MongoDb CE Operator pod and Mongo replica set should be configured. After the CA and server Certificates have been renewed you must ensure that that MongoCfg Suite CR is updated with the new CA Certificate. First obtain the CA Certificate from the Secret resource mongo-ca-secret . Then edit the Suite MongoCfg CR in the Maximo Application Suite core namespace. This is done by updating the appropriate certificate under .spec.certificates in the MongoCfg CR: spec: certificates: - alias: ca crt: | -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- If an IBM Suite Licensing Service (SLS) is also connecting to the MongoDB replica set the LicenseService CR must also be updated to reflect the new MongoDB CA. This can be added to the .spec.mongo.certificates section of the LicenseService CR. mongo: certificates: - alias: mongoca crt: | -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- Once the CA certificate has been updated for the MongoCfg and LicenseService CRs several pods in the core and SLS namespaces might need to be restarted to pick up the changes. This would include but is not limited to coreidp, coreapi, api-licensing.","title":"CA Certificate Renewal"},{"location":"roles/mongodb/#license","text":"EPL-2.0","title":"License"},{"location":"roles/nvidia_gpu/","text":"nvidia_gpu \u00a4 This role installs and configures the NVIDIA GPU Operator on OpenShift clusters to enable GPU workloads. The GPU Operator manages the lifecycle of NVIDIA software components required to run GPU-accelerated applications. The role automatically installs the Node Feature Discovery (NFD) Operator as a prerequisite, then deploys the NVIDIA GPU Operator and creates a ClusterPolicy to configure GPU support across the cluster. This is required for applications like Maximo Visual Inspection that need GPU acceleration. Prerequisites \u00a4 OpenShift cluster with GPU-enabled worker nodes Cluster administrator access GPU-capable hardware (NVIDIA GPUs) available in the cluster Role Variables \u00a4 GPU Operator Variables \u00a4 gpu_namespace \u00a4 Namespace for NVIDIA GPU Operator installation. Optional Environment Variable: GPU_NAMESPACE Default: nvidia-gpu-operator Purpose : Specifies the OpenShift namespace where the NVIDIA GPU Operator and its components will be deployed. When to use : Use default unless you have specific namespace requirements or multiple GPU operator instances. Valid values : Valid Kubernetes namespace name (lowercase alphanumeric with hyphens) Impact : All GPU Operator resources (deployments, services, ClusterPolicy) will be created in this namespace. Related variables : nfd_namespace Notes : - Default namespace is recommended for most deployments - Namespace will be created if it doesn't exist - GPU Operator manages cluster-wide GPU resources regardless of namespace gpu_channel \u00a4 NVIDIA GPU Operator subscription channel. Optional Environment Variable: GPU_CHANNEL Default: v24.9 Purpose : Determines which version stream of the NVIDIA GPU Operator will be installed from OperatorHub. When to use : Use default for latest stable version. Specify older channel for compatibility with specific OpenShift versions or when stability is prioritized over features. Valid values : Valid GPU Operator channel versions (e.g., v24.9 , v24.6 , v23.9 ) Impact : Controls which operator version is installed and which features are available. Newer channels may require newer OpenShift versions. Related variables : gpu_driver_version Notes : - Check NVIDIA GPU Operator releases for compatibility - Newer channels provide latest features and driver support - Consider OpenShift version compatibility when selecting channel gpu_driver_version \u00a4 Specific NVIDIA GPU driver version to install. Optional Environment Variable: GPU_DRIVER_VERSION Default: None (uses latest compatible version) Purpose : Pins the NVIDIA GPU driver to a specific version instead of using the latest version provided by the operator channel. When to use : Specify when you need a particular driver version for compatibility, stability, or certification requirements. Leave unset to use the latest compatible driver. Valid values : Valid NVIDIA driver version string (e.g., 535.129.03 , 550.54.15 ) Impact : When set, the specified driver version will be installed. When unset, the operator installs the latest driver compatible with the operator channel. Related variables : gpu_channel , gpu_driver_repository_path Notes : - Latest driver is usually recommended unless specific version is required - Verify driver compatibility with your GPU hardware - Check NVIDIA driver compatibility matrix - Driver version must be available in the specified repository gpu_driver_repository_path \u00a4 Container registry path for NVIDIA driver images. Optional Environment Variable: GPU_DRIVER_REPOSITORY_PATH Default: nvcr.io/nvidia Purpose : Specifies the container registry location where NVIDIA GPU driver container images are stored. When to use : Use default for public NVIDIA registry. Override for air-gapped environments or when using a mirrored registry. Valid values : Valid container registry path (e.g., nvcr.io/nvidia , registry.example.com/nvidia-drivers ) Impact : GPU Operator will pull driver container images from this registry location. Related variables : gpu_driver_version Notes : - Default points to NVIDIA's official registry - For air-gapped deployments, mirror images to internal registry - Ensure registry is accessible from OpenShift cluster - May require image pull secrets for private registries Node Feature Discovery Variables \u00a4 nfd_namespace \u00a4 Namespace for Node Feature Discovery Operator installation. Optional Environment Variable: NFD_NAMESPACE Default: openshift-nfd Purpose : Specifies the OpenShift namespace where the Node Feature Discovery (NFD) Operator will be deployed. NFD is a prerequisite for GPU Operator. When to use : Use default unless you have specific namespace requirements. NFD must be installed before GPU Operator. Valid values : Valid Kubernetes namespace name (lowercase alphanumeric with hyphens) Impact : NFD Operator resources will be created in this namespace. NFD detects GPU hardware and labels nodes accordingly. Related variables : gpu_namespace , nfd_channel Notes : - NFD is automatically installed by this role as a prerequisite - Default openshift-nfd is the standard namespace for NFD - NFD labels nodes with hardware features for GPU Operator to use - Namespace will be created if it doesn't exist nfd_channel \u00a4 Node Feature Discovery Operator subscription channel. Optional Environment Variable: NFD_CHANNEL Default: stable Purpose : Determines which version stream of the Node Feature Discovery Operator will be installed from OperatorHub. When to use : Use default stable for production deployments. Other channels may be available for testing or specific versions. Valid values : Valid NFD Operator channel (typically stable , may include version-specific channels) Impact : Controls which NFD operator version is installed. The stable channel provides production-ready releases. Related variables : nfd_namespace , gpu_channel Notes : - stable channel is recommended for production - NFD is a prerequisite for GPU Operator functionality - Check OperatorHub for available channels in your OpenShift version - NFD version should be compatible with OpenShift version Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: gpu_namespace: nvidia-gpu-operator gpu_channel: v24.9 nfd_namespace: openshift-nfd roles: - ibm.mas_devops.nvidia_gpu Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export GPU_NAMESPACE=nvidia-gpu-operator export GPU_CHANNEL=v24.9 export NFD_NAMESPACE=openshift-nfd ROLE_NAME=nvidia_gpu ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"nvidia_gpu"},{"location":"roles/nvidia_gpu/#nvidia_gpu","text":"This role installs and configures the NVIDIA GPU Operator on OpenShift clusters to enable GPU workloads. The GPU Operator manages the lifecycle of NVIDIA software components required to run GPU-accelerated applications. The role automatically installs the Node Feature Discovery (NFD) Operator as a prerequisite, then deploys the NVIDIA GPU Operator and creates a ClusterPolicy to configure GPU support across the cluster. This is required for applications like Maximo Visual Inspection that need GPU acceleration.","title":"nvidia_gpu"},{"location":"roles/nvidia_gpu/#prerequisites","text":"OpenShift cluster with GPU-enabled worker nodes Cluster administrator access GPU-capable hardware (NVIDIA GPUs) available in the cluster","title":"Prerequisites"},{"location":"roles/nvidia_gpu/#role-variables","text":"","title":"Role Variables"},{"location":"roles/nvidia_gpu/#gpu-operator-variables","text":"","title":"GPU Operator Variables"},{"location":"roles/nvidia_gpu/#gpu_namespace","text":"Namespace for NVIDIA GPU Operator installation. Optional Environment Variable: GPU_NAMESPACE Default: nvidia-gpu-operator Purpose : Specifies the OpenShift namespace where the NVIDIA GPU Operator and its components will be deployed. When to use : Use default unless you have specific namespace requirements or multiple GPU operator instances. Valid values : Valid Kubernetes namespace name (lowercase alphanumeric with hyphens) Impact : All GPU Operator resources (deployments, services, ClusterPolicy) will be created in this namespace. Related variables : nfd_namespace Notes : - Default namespace is recommended for most deployments - Namespace will be created if it doesn't exist - GPU Operator manages cluster-wide GPU resources regardless of namespace","title":"gpu_namespace"},{"location":"roles/nvidia_gpu/#gpu_channel","text":"NVIDIA GPU Operator subscription channel. Optional Environment Variable: GPU_CHANNEL Default: v24.9 Purpose : Determines which version stream of the NVIDIA GPU Operator will be installed from OperatorHub. When to use : Use default for latest stable version. Specify older channel for compatibility with specific OpenShift versions or when stability is prioritized over features. Valid values : Valid GPU Operator channel versions (e.g., v24.9 , v24.6 , v23.9 ) Impact : Controls which operator version is installed and which features are available. Newer channels may require newer OpenShift versions. Related variables : gpu_driver_version Notes : - Check NVIDIA GPU Operator releases for compatibility - Newer channels provide latest features and driver support - Consider OpenShift version compatibility when selecting channel","title":"gpu_channel"},{"location":"roles/nvidia_gpu/#gpu_driver_version","text":"Specific NVIDIA GPU driver version to install. Optional Environment Variable: GPU_DRIVER_VERSION Default: None (uses latest compatible version) Purpose : Pins the NVIDIA GPU driver to a specific version instead of using the latest version provided by the operator channel. When to use : Specify when you need a particular driver version for compatibility, stability, or certification requirements. Leave unset to use the latest compatible driver. Valid values : Valid NVIDIA driver version string (e.g., 535.129.03 , 550.54.15 ) Impact : When set, the specified driver version will be installed. When unset, the operator installs the latest driver compatible with the operator channel. Related variables : gpu_channel , gpu_driver_repository_path Notes : - Latest driver is usually recommended unless specific version is required - Verify driver compatibility with your GPU hardware - Check NVIDIA driver compatibility matrix - Driver version must be available in the specified repository","title":"gpu_driver_version"},{"location":"roles/nvidia_gpu/#gpu_driver_repository_path","text":"Container registry path for NVIDIA driver images. Optional Environment Variable: GPU_DRIVER_REPOSITORY_PATH Default: nvcr.io/nvidia Purpose : Specifies the container registry location where NVIDIA GPU driver container images are stored. When to use : Use default for public NVIDIA registry. Override for air-gapped environments or when using a mirrored registry. Valid values : Valid container registry path (e.g., nvcr.io/nvidia , registry.example.com/nvidia-drivers ) Impact : GPU Operator will pull driver container images from this registry location. Related variables : gpu_driver_version Notes : - Default points to NVIDIA's official registry - For air-gapped deployments, mirror images to internal registry - Ensure registry is accessible from OpenShift cluster - May require image pull secrets for private registries","title":"gpu_driver_repository_path"},{"location":"roles/nvidia_gpu/#node-feature-discovery-variables","text":"","title":"Node Feature Discovery Variables"},{"location":"roles/nvidia_gpu/#nfd_namespace","text":"Namespace for Node Feature Discovery Operator installation. Optional Environment Variable: NFD_NAMESPACE Default: openshift-nfd Purpose : Specifies the OpenShift namespace where the Node Feature Discovery (NFD) Operator will be deployed. NFD is a prerequisite for GPU Operator. When to use : Use default unless you have specific namespace requirements. NFD must be installed before GPU Operator. Valid values : Valid Kubernetes namespace name (lowercase alphanumeric with hyphens) Impact : NFD Operator resources will be created in this namespace. NFD detects GPU hardware and labels nodes accordingly. Related variables : gpu_namespace , nfd_channel Notes : - NFD is automatically installed by this role as a prerequisite - Default openshift-nfd is the standard namespace for NFD - NFD labels nodes with hardware features for GPU Operator to use - Namespace will be created if it doesn't exist","title":"nfd_namespace"},{"location":"roles/nvidia_gpu/#nfd_channel","text":"Node Feature Discovery Operator subscription channel. Optional Environment Variable: NFD_CHANNEL Default: stable Purpose : Determines which version stream of the Node Feature Discovery Operator will be installed from OperatorHub. When to use : Use default stable for production deployments. Other channels may be available for testing or specific versions. Valid values : Valid NFD Operator channel (typically stable , may include version-specific channels) Impact : Controls which NFD operator version is installed. The stable channel provides production-ready releases. Related variables : nfd_namespace , gpu_channel Notes : - stable channel is recommended for production - NFD is a prerequisite for GPU Operator functionality - Check OperatorHub for available channels in your OpenShift version - NFD version should be compatible with OpenShift version","title":"nfd_channel"},{"location":"roles/nvidia_gpu/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: gpu_namespace: nvidia-gpu-operator gpu_channel: v24.9 nfd_namespace: openshift-nfd roles: - ibm.mas_devops.nvidia_gpu","title":"Example Playbook"},{"location":"roles/nvidia_gpu/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export GPU_NAMESPACE=nvidia-gpu-operator export GPU_CHANNEL=v24.9 export NFD_NAMESPACE=openshift-nfd ROLE_NAME=nvidia_gpu ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/nvidia_gpu/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_cluster_monitoring/","text":"ocp_cluster_monitoring \u00a4 Configure OpenShift Container Platform cluster monitoring with persistent storage and user-defined project monitoring. This role enables comprehensive monitoring capabilities essential for MAS deployments, including metrics collection, alerting, and user workload monitoring. Key capabilities: - User-defined project monitoring : Enable monitoring for user applications and MAS workloads - Persistent storage : Configure durable storage for Prometheus metrics and AlertManager data - Version-aware configuration : Automatically applies appropriate templates for OpenShift 4.18+ or earlier versions The role configures both the platform monitoring stack (for OpenShift infrastructure) and user workload monitoring (for MAS applications) in the openshift-monitoring namespace. Role Variables \u00a4 cluster_monitoring_action \u00a4 Action to perform on the cluster monitoring stack. Optional Environment Variable: CLUSTER_MONITORING_ACTION Default: install Purpose : Controls whether to install/configure or uninstall the cluster monitoring stack configuration. When to use : Set to install for normal operations. Use uninstall to remove persistent storage configuration and revert to default ephemeral storage. Valid values : - install - Configure monitoring with persistent storage (default) - uninstall - Remove persistent storage configuration Impact : - install : Applies monitoring configuration with persistent storage - uninstall : Removes custom configuration, monitoring reverts to defaults with ephemeral storage (metrics will be lost on pod restart) Related variables : All other variables only apply when action is install Notes : - Uninstall does not remove the monitoring stack itself, only the persistent storage configuration - Metrics data will be lost when reverting to ephemeral storage - Backup important metrics before uninstalling prometheus_retention_period \u00a4 Retention period for platform Prometheus metrics. Optional Environment Variable: PROMETHEUS_RETENTION_PERIOD Default: 15d Purpose : Defines how long Prometheus retains metrics data before deletion. Longer retention enables historical analysis but requires more storage. When to use : Adjust based on compliance requirements, troubleshooting needs, and storage capacity. Valid values : Duration string with unit suffix (e.g., 7d , 15d , 30d , 90d ). Common values: - 7d - Minimal retention for basic troubleshooting - 15d - Default, suitable for most deployments - 30d - Extended retention for detailed analysis - 90d - Long-term retention for compliance Impact : Directly affects storage consumption. Longer retention requires proportionally more storage space. Related variables : prometheus_storage_size (must be sized appropriately for retention period) Notes : - Only applies when both prometheus_storage_class and prometheus_alertmgr_storage_class are set - Calculate storage needs: ~1-2GB per day for typical MAS deployment - Consider backup strategy for long-term metric retention prometheus_storage_class \u00a4 Storage class for platform Prometheus metrics persistent volume. Required if known storage classes are not available Environment Variable: PROMETHEUS_STORAGE_CLASS Default: Auto-detected ( ibmc-block-gold , ocs-storagecluster-ceph-rbd , or managed-premium ) Purpose : Specifies the storage class for Prometheus metrics data. Must support ReadWriteOnce (RWO) access mode. When to use : Required if the cluster doesn't have one of the auto-detected storage classes. Always specify explicitly for production deployments. Valid values : Valid storage class name that supports RWO access mode. Common values by platform: - IBM Cloud : ibmc-block-gold , ibmc-block-silver - OpenShift Container Storage : ocs-storagecluster-ceph-rbd - Azure : managed-premium , managed-standard - AWS : gp3 , gp2 Impact : Determines performance and availability characteristics of metrics storage. Block storage is recommended for performance. Related variables : prometheus_storage_size , prometheus_retention_period Notes : - Critical : Must support RWO access mode - Block storage preferred over file storage for performance - Verify storage class exists: oc get storageclass - Role auto-detects common storage classes if not specified prometheus_storage_size \u00a4 Size of the persistent volume for platform Prometheus metrics. Optional Environment Variable: PROMETHEUS_STORAGE_SIZE Default: 20Gi Purpose : Defines the capacity of the persistent volume for storing Prometheus metrics data. When to use : Adjust based on retention period, cluster size, and number of monitored workloads. Valid values : Kubernetes resource quantity (e.g., 20Gi , 50Gi , 100Gi , 200Gi ). Sizing guidelines: - 20Gi - Default, suitable for small clusters (< 50 nodes) - 50Gi - Medium clusters (50-100 nodes) or extended retention - 100Gi - Large clusters (100-200 nodes) - 200Gi+ - Very large clusters or long retention periods Impact : Insufficient storage will cause metrics collection to fail. Over-provisioning wastes resources and costs. Related variables : prometheus_retention_period , prometheus_storage_class Notes : - Only applies when both prometheus_storage_class and prometheus_alertmgr_storage_class are set - Calculate: ~1-2GB per day \u00d7 retention days \u00d7 number of nodes / 50 - Monitor usage: oc get pvc -n openshift-monitoring - Can be expanded later if needed (depending on storage class) prometheus_alertmgr_storage_class \u00a4 Storage class for AlertManager persistent volume. Required if known storage classes are not available Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_CLASS Default: Auto-detected ( ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium ) Purpose : Specifies the storage class for AlertManager data. Must support ReadWriteMany (RWX) access mode for high availability. When to use : Required if the cluster doesn't have one of the auto-detected storage classes. Always specify explicitly for production deployments. Valid values : Valid storage class name that supports RWX access mode. Common values by platform: - IBM Cloud : ibmc-file-gold-gid , ibmc-file-silver-gid - OpenShift Container Storage : ocs-storagecluster-cephfs - Azure : azurefiles-premium , azurefiles-standard - AWS : efs-sc (requires EFS CSI driver) Impact : Determines AlertManager high availability and performance. RWX is required for multi-replica AlertManager deployment. Related variables : prometheus_alertmgr_storage_size Notes : - Critical : Must support RWX access mode for HA - File storage required (block storage doesn't support RWX) - Verify storage class exists and supports RWX: oc get storageclass - Role auto-detects common storage classes if not specified prometheus_alertmgr_storage_size \u00a4 Size of the persistent volume for AlertManager. Optional Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_SIZE Default: 20Gi Purpose : Defines the capacity of the persistent volume for storing AlertManager configuration and notification state. When to use : Default is usually sufficient. Increase if managing many alert rules or notification integrations. Valid values : Kubernetes resource quantity (e.g., 20Gi , 50Gi ). Sizing guidelines: - 20Gi - Default, sufficient for most deployments - 50Gi - Large number of alert rules or notification channels Impact : AlertManager storage needs are typically minimal. Default size is adequate for most scenarios. Related variables : prometheus_alertmgr_storage_class Notes : - Only applies when both prometheus_storage_class and prometheus_alertmgr_storage_class are set - AlertManager uses much less storage than Prometheus - Default 20Gi is generous for typical use cases prometheus_userworkload_retention_period \u00a4 Retention period for user workload Prometheus metrics. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_RETENTION_PERIOD Default: 15d Purpose : Defines how long the user workload Prometheus instance retains metrics from user applications and MAS workloads. When to use : Adjust based on application monitoring requirements and storage capacity. May differ from platform metrics retention. Valid values : Duration string with unit suffix (e.g., 7d , 15d , 30d ). Common values: - 7d - Minimal retention - 15d - Default, suitable for most MAS deployments - 30d - Extended retention for detailed application analysis Impact : Affects storage consumption for user workload metrics. MAS applications generate significant metrics data. Related variables : prometheus_userworkload_storage_size Notes : - Applies only to user workload monitoring, not platform monitoring - MAS applications (especially Manage) generate substantial metrics - Consider MAS-specific monitoring requirements when setting retention prometheus_userworkload_storage_class \u00a4 Storage class for user workload Prometheus metrics persistent volume. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_CLASS Default: Value of prometheus_storage_class Purpose : Specifies the storage class for user workload Prometheus metrics. Must support ReadWriteOnce (RWO) access mode. When to use : Typically inherits from prometheus_storage_class . Set explicitly if user workload metrics require different storage characteristics. Valid values : Valid storage class name that supports RWO access mode. Usually same as prometheus_storage_class . Impact : Determines performance and availability of user workload metrics storage. Related variables : prometheus_storage_class , prometheus_userworkload_storage_size Notes : - Defaults to same storage class as platform Prometheus - Must support RWO access mode - Can use different storage class if user workloads have specific requirements prometheus_userworkload_storage_size \u00a4 Size of the persistent volume for user workload Prometheus metrics. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_SIZE Default: 20Gi Purpose : Defines the capacity of the persistent volume for storing user workload metrics from MAS applications. When to use : Adjust based on number of MAS applications, retention period, and metrics volume. Valid values : Kubernetes resource quantity (e.g., 20Gi , 50Gi , 100Gi ). Sizing guidelines: - 20Gi - Default, suitable for 1-2 MAS applications - 50Gi - Multiple MAS applications (3-5) - 100Gi - Full MAS suite with extended retention Impact : MAS applications generate significant metrics. Insufficient storage will cause metrics collection failures. Related variables : prometheus_userworkload_retention_period , prometheus_userworkload_storage_class Notes : - MAS Manage generates substantial metrics data - Size based on: number of MAS apps \u00d7 retention period \u00d7 expected metrics volume - Monitor usage: oc get pvc -n openshift-user-workload-monitoring - Can be expanded later if needed Example Playbook \u00a4 - hosts: localhost vars: prometheus_storage_class: \"ibmc-block-gold\" prometheus_alertmgr_storage_class: \"ibmc-file-gold-gid\" roles: - ibm.mas_devops.ocp_cluster_monitoring License \u00a4 EPL-2.0","title":"ocp_cluster_monitoring"},{"location":"roles/ocp_cluster_monitoring/#ocp_cluster_monitoring","text":"Configure OpenShift Container Platform cluster monitoring with persistent storage and user-defined project monitoring. This role enables comprehensive monitoring capabilities essential for MAS deployments, including metrics collection, alerting, and user workload monitoring. Key capabilities: - User-defined project monitoring : Enable monitoring for user applications and MAS workloads - Persistent storage : Configure durable storage for Prometheus metrics and AlertManager data - Version-aware configuration : Automatically applies appropriate templates for OpenShift 4.18+ or earlier versions The role configures both the platform monitoring stack (for OpenShift infrastructure) and user workload monitoring (for MAS applications) in the openshift-monitoring namespace.","title":"ocp_cluster_monitoring"},{"location":"roles/ocp_cluster_monitoring/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_cluster_monitoring/#cluster_monitoring_action","text":"Action to perform on the cluster monitoring stack. Optional Environment Variable: CLUSTER_MONITORING_ACTION Default: install Purpose : Controls whether to install/configure or uninstall the cluster monitoring stack configuration. When to use : Set to install for normal operations. Use uninstall to remove persistent storage configuration and revert to default ephemeral storage. Valid values : - install - Configure monitoring with persistent storage (default) - uninstall - Remove persistent storage configuration Impact : - install : Applies monitoring configuration with persistent storage - uninstall : Removes custom configuration, monitoring reverts to defaults with ephemeral storage (metrics will be lost on pod restart) Related variables : All other variables only apply when action is install Notes : - Uninstall does not remove the monitoring stack itself, only the persistent storage configuration - Metrics data will be lost when reverting to ephemeral storage - Backup important metrics before uninstalling","title":"cluster_monitoring_action"},{"location":"roles/ocp_cluster_monitoring/#prometheus_retention_period","text":"Retention period for platform Prometheus metrics. Optional Environment Variable: PROMETHEUS_RETENTION_PERIOD Default: 15d Purpose : Defines how long Prometheus retains metrics data before deletion. Longer retention enables historical analysis but requires more storage. When to use : Adjust based on compliance requirements, troubleshooting needs, and storage capacity. Valid values : Duration string with unit suffix (e.g., 7d , 15d , 30d , 90d ). Common values: - 7d - Minimal retention for basic troubleshooting - 15d - Default, suitable for most deployments - 30d - Extended retention for detailed analysis - 90d - Long-term retention for compliance Impact : Directly affects storage consumption. Longer retention requires proportionally more storage space. Related variables : prometheus_storage_size (must be sized appropriately for retention period) Notes : - Only applies when both prometheus_storage_class and prometheus_alertmgr_storage_class are set - Calculate storage needs: ~1-2GB per day for typical MAS deployment - Consider backup strategy for long-term metric retention","title":"prometheus_retention_period"},{"location":"roles/ocp_cluster_monitoring/#prometheus_storage_class","text":"Storage class for platform Prometheus metrics persistent volume. Required if known storage classes are not available Environment Variable: PROMETHEUS_STORAGE_CLASS Default: Auto-detected ( ibmc-block-gold , ocs-storagecluster-ceph-rbd , or managed-premium ) Purpose : Specifies the storage class for Prometheus metrics data. Must support ReadWriteOnce (RWO) access mode. When to use : Required if the cluster doesn't have one of the auto-detected storage classes. Always specify explicitly for production deployments. Valid values : Valid storage class name that supports RWO access mode. Common values by platform: - IBM Cloud : ibmc-block-gold , ibmc-block-silver - OpenShift Container Storage : ocs-storagecluster-ceph-rbd - Azure : managed-premium , managed-standard - AWS : gp3 , gp2 Impact : Determines performance and availability characteristics of metrics storage. Block storage is recommended for performance. Related variables : prometheus_storage_size , prometheus_retention_period Notes : - Critical : Must support RWO access mode - Block storage preferred over file storage for performance - Verify storage class exists: oc get storageclass - Role auto-detects common storage classes if not specified","title":"prometheus_storage_class"},{"location":"roles/ocp_cluster_monitoring/#prometheus_storage_size","text":"Size of the persistent volume for platform Prometheus metrics. Optional Environment Variable: PROMETHEUS_STORAGE_SIZE Default: 20Gi Purpose : Defines the capacity of the persistent volume for storing Prometheus metrics data. When to use : Adjust based on retention period, cluster size, and number of monitored workloads. Valid values : Kubernetes resource quantity (e.g., 20Gi , 50Gi , 100Gi , 200Gi ). Sizing guidelines: - 20Gi - Default, suitable for small clusters (< 50 nodes) - 50Gi - Medium clusters (50-100 nodes) or extended retention - 100Gi - Large clusters (100-200 nodes) - 200Gi+ - Very large clusters or long retention periods Impact : Insufficient storage will cause metrics collection to fail. Over-provisioning wastes resources and costs. Related variables : prometheus_retention_period , prometheus_storage_class Notes : - Only applies when both prometheus_storage_class and prometheus_alertmgr_storage_class are set - Calculate: ~1-2GB per day \u00d7 retention days \u00d7 number of nodes / 50 - Monitor usage: oc get pvc -n openshift-monitoring - Can be expanded later if needed (depending on storage class)","title":"prometheus_storage_size"},{"location":"roles/ocp_cluster_monitoring/#prometheus_alertmgr_storage_class","text":"Storage class for AlertManager persistent volume. Required if known storage classes are not available Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_CLASS Default: Auto-detected ( ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium ) Purpose : Specifies the storage class for AlertManager data. Must support ReadWriteMany (RWX) access mode for high availability. When to use : Required if the cluster doesn't have one of the auto-detected storage classes. Always specify explicitly for production deployments. Valid values : Valid storage class name that supports RWX access mode. Common values by platform: - IBM Cloud : ibmc-file-gold-gid , ibmc-file-silver-gid - OpenShift Container Storage : ocs-storagecluster-cephfs - Azure : azurefiles-premium , azurefiles-standard - AWS : efs-sc (requires EFS CSI driver) Impact : Determines AlertManager high availability and performance. RWX is required for multi-replica AlertManager deployment. Related variables : prometheus_alertmgr_storage_size Notes : - Critical : Must support RWX access mode for HA - File storage required (block storage doesn't support RWX) - Verify storage class exists and supports RWX: oc get storageclass - Role auto-detects common storage classes if not specified","title":"prometheus_alertmgr_storage_class"},{"location":"roles/ocp_cluster_monitoring/#prometheus_alertmgr_storage_size","text":"Size of the persistent volume for AlertManager. Optional Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_SIZE Default: 20Gi Purpose : Defines the capacity of the persistent volume for storing AlertManager configuration and notification state. When to use : Default is usually sufficient. Increase if managing many alert rules or notification integrations. Valid values : Kubernetes resource quantity (e.g., 20Gi , 50Gi ). Sizing guidelines: - 20Gi - Default, sufficient for most deployments - 50Gi - Large number of alert rules or notification channels Impact : AlertManager storage needs are typically minimal. Default size is adequate for most scenarios. Related variables : prometheus_alertmgr_storage_class Notes : - Only applies when both prometheus_storage_class and prometheus_alertmgr_storage_class are set - AlertManager uses much less storage than Prometheus - Default 20Gi is generous for typical use cases","title":"prometheus_alertmgr_storage_size"},{"location":"roles/ocp_cluster_monitoring/#prometheus_userworkload_retention_period","text":"Retention period for user workload Prometheus metrics. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_RETENTION_PERIOD Default: 15d Purpose : Defines how long the user workload Prometheus instance retains metrics from user applications and MAS workloads. When to use : Adjust based on application monitoring requirements and storage capacity. May differ from platform metrics retention. Valid values : Duration string with unit suffix (e.g., 7d , 15d , 30d ). Common values: - 7d - Minimal retention - 15d - Default, suitable for most MAS deployments - 30d - Extended retention for detailed application analysis Impact : Affects storage consumption for user workload metrics. MAS applications generate significant metrics data. Related variables : prometheus_userworkload_storage_size Notes : - Applies only to user workload monitoring, not platform monitoring - MAS applications (especially Manage) generate substantial metrics - Consider MAS-specific monitoring requirements when setting retention","title":"prometheus_userworkload_retention_period"},{"location":"roles/ocp_cluster_monitoring/#prometheus_userworkload_storage_class","text":"Storage class for user workload Prometheus metrics persistent volume. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_CLASS Default: Value of prometheus_storage_class Purpose : Specifies the storage class for user workload Prometheus metrics. Must support ReadWriteOnce (RWO) access mode. When to use : Typically inherits from prometheus_storage_class . Set explicitly if user workload metrics require different storage characteristics. Valid values : Valid storage class name that supports RWO access mode. Usually same as prometheus_storage_class . Impact : Determines performance and availability of user workload metrics storage. Related variables : prometheus_storage_class , prometheus_userworkload_storage_size Notes : - Defaults to same storage class as platform Prometheus - Must support RWO access mode - Can use different storage class if user workloads have specific requirements","title":"prometheus_userworkload_storage_class"},{"location":"roles/ocp_cluster_monitoring/#prometheus_userworkload_storage_size","text":"Size of the persistent volume for user workload Prometheus metrics. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_SIZE Default: 20Gi Purpose : Defines the capacity of the persistent volume for storing user workload metrics from MAS applications. When to use : Adjust based on number of MAS applications, retention period, and metrics volume. Valid values : Kubernetes resource quantity (e.g., 20Gi , 50Gi , 100Gi ). Sizing guidelines: - 20Gi - Default, suitable for 1-2 MAS applications - 50Gi - Multiple MAS applications (3-5) - 100Gi - Full MAS suite with extended retention Impact : MAS applications generate significant metrics. Insufficient storage will cause metrics collection failures. Related variables : prometheus_userworkload_retention_period , prometheus_userworkload_storage_class Notes : - MAS Manage generates substantial metrics data - Size based on: number of MAS apps \u00d7 retention period \u00d7 expected metrics volume - Monitor usage: oc get pvc -n openshift-user-workload-monitoring - Can be expanded later if needed","title":"prometheus_userworkload_storage_size"},{"location":"roles/ocp_cluster_monitoring/#example-playbook","text":"- hosts: localhost vars: prometheus_storage_class: \"ibmc-block-gold\" prometheus_alertmgr_storage_class: \"ibmc-file-gold-gid\" roles: - ibm.mas_devops.ocp_cluster_monitoring","title":"Example Playbook"},{"location":"roles/ocp_cluster_monitoring/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_config/","text":"ocp_config \u00a4 Configure OpenShift Container Platform cluster-level settings for optimal MAS deployment. This role provides essential tuning for ingress timeouts, TLS cipher compatibility with IBM Java Semeru FIPS mode, and OperatorHub catalog source management. Key capabilities: - Ingress timeout tuning : Prevent request failures for long-running operations - FIPS-compatible TLS ciphers : Enable IBM Java Semeru runtime in FIPS mode - Catalog source management : Disable default Red Hat catalogs for air-gapped environments - Path-based routing : Configure namespace ownership for multi-namespace route support This role configures: - Tune the IngressController to avoid request failures due to timeout for long running requests - Configure the IngressController namespace ownership for path-based routing support - Update APIServer and IngressController to set a custom tlsSecurityProfile to accommodate ciphers supported by IBM Java Semeru runtime. This is required for allowing the Java applications using Semeru runtime to run in FIPS mode. The following ciphers will be enabled: - TLS_AES_128_GCM_SHA256 - TLS_AES_256_GCM_SHA384 - TLS_CHACHA20_POLY1305_SHA256 - ECDHE-ECDSA-AES128-GCM-SHA256 - ECDHE-RSA-AES128-GCM-SHA256 - ECDHE-ECDSA-AES256-GCM-SHA384 - ECDHE-RSA-AES256-GCM-SHA384 - ECDHE-ECDSA-CHACHA20-POLY1305 - ECDHE-RSA-CHACHA20-POLY1305 - DHE-RSA-AES128-GCM-SHA256 - DHE-RSA-AES256-GCM-SHA384 - ECDHE-RSA-AES128-SHA256 - ECDHE-RSA-AES128-SHA - ECDHE-RSA-AES256-SHA - Disable the default Red Hat CatalogSources : - certified-operators - community-operators - redhat-operators Role Variables - API Server \u00a4 Role Variables - General \u00a4 ocp_update_ciphers_for_semeru \u00a4 Enable custom TLS cipher configuration for IBM Java Semeru FIPS mode compatibility. Optional Environment Variable: OCP_UPDATE_CIPHERS_FOR_SEMERU Default: false Purpose : Configures the API Server and Ingress Controller with a custom TLS security profile that includes ciphers compatible with IBM Java Semeru runtime in FIPS mode. When to use : Required when running MAS applications (particularly Manage) in FIPS mode with IBM Java Semeru runtime. Valid values : - true - Apply custom cipher configuration - false - Use default OpenShift cipher configuration Impact : When enabled, updates the tlsSecurityProfile on both APIServer and IngressController resources to include the following ciphers: - TLS 1.3: TLS_AES_128_GCM_SHA256 , TLS_AES_256_GCM_SHA384 , TLS_CHACHA20_POLY1305_SHA256 - TLS 1.2: ECDHE-ECDSA-AES128-GCM-SHA256 , ECDHE-RSA-AES128-GCM-SHA256 , ECDHE-ECDSA-AES256-GCM-SHA384 , ECDHE-RSA-AES256-GCM-SHA384 , ECDHE-ECDSA-CHACHA20-POLY1305 , ECDHE-RSA-CHACHA20-POLY1305 , DHE-RSA-AES128-GCM-SHA256 , DHE-RSA-AES256-GCM-SHA384 , ECDHE-RSA-AES128-SHA256 , ECDHE-RSA-AES128-SHA , ECDHE-RSA-AES256-SHA Related variables : None Notes : - Required for FIPS-compliant MAS deployments using Semeru runtime - Changes affect cluster-wide TLS configuration - May require cluster restart to fully apply - Verify compatibility with other applications in the cluster Role Variables - Ingress Controller \u00a4 ocp_ingress_controller_name \u00a4 The name of the Ingress Controller to configure. Optional Environment Variable: OCP_INGRESS_CONTROLLER_NAME Default Value: default Purpose : Specifies which Ingress Controller instance to configure. This applies to both timeout and namespace ownership settings. When to use : Use the default value unless you have multiple Ingress Controllers in your cluster and need to configure a specific one. Valid values : - default - The default cluster Ingress Controller (most common) - Any custom Ingress Controller name in your cluster Impact : All timeout and namespace ownership configurations will be applied to this specific Ingress Controller. Related variables : - ocp_ingress_update_timeouts - Timeout settings apply to this controller - ocp_ingress_namespace_ownership - Namespace policy applies to this controller Notes : - Most OpenShift clusters use only the default Ingress Controller - Custom Ingress Controllers are rare and typically used for advanced routing scenarios - Verify the controller name exists before configuring ocp_ingress_update_timeouts \u00a4 Enable custom timeout configuration for the OpenShift Ingress Controller. Optional Environment Variable: OCP_INGRESS_UPDATE_TIMEOUTS Default: false Purpose : Controls whether to apply custom client and server timeout values to the Ingress Controller. When to use : Enable when experiencing timeout issues with long-running MAS operations such as large file uploads, report generation, or data imports. Valid values : - true - Apply custom timeout values - false - Use default OpenShift timeout values (30s) Impact : When enabled, updates the IngressController with the values specified in ocp_ingress_client_timeout and ocp_ingress_server_timeout . Related variables : ocp_ingress_client_timeout , ocp_ingress_server_timeout Notes : - Default 30s timeout may be insufficient for MAS Manage operations - Recommended to enable for production MAS deployments - Changes apply to all routes in the cluster ocp_ingress_client_timeout \u00a4 Client-side timeout duration for ingress connections. Optional Environment Variable: OCP_INGRESS_CLIENT_TIMEOUT Default: 30s Purpose : Specifies how long a connection is held open while waiting for a client to send data or complete a request. When to use : Increase when clients need more time to upload large files or send data to MAS applications. Valid values : Duration string with unit suffix (e.g., 30s , 5m , 1h ). Common values: - 30s - Default, suitable for most operations - 5m - Recommended for MAS Manage file uploads - 10m - For very large file operations Impact : Connections from clients will be kept alive for this duration while waiting for data. Too short may cause upload failures; too long may consume resources. Related variables : ocp_ingress_update_timeouts (must be true ), ocp_ingress_server_timeout Notes : - Only applies when ocp_ingress_update_timeouts is true - Consider network latency and file sizes when setting - Affects all routes in the cluster - Balance between user experience and resource consumption ocp_ingress_server_timeout \u00a4 Server-side timeout duration for ingress connections. Optional Environment Variable: OCP_INGRESS_SERVER_TIMEOUT Default: 30s Purpose : Specifies how long a connection is held open while waiting for a server (backend application) to respond. When to use : Increase when MAS applications perform long-running operations like report generation, data processing, or complex queries. Valid values : Duration string with unit suffix (e.g., 30s , 5m , 1h ). Common values: - 30s - Default, suitable for most operations - 5m - Recommended for MAS Manage report generation - 10m - For complex data processing operations - 30m - For very long-running batch operations Impact : Backend connections will be kept alive for this duration while waiting for responses. Too short may cause operation failures; too long may mask application issues. Related variables : ocp_ingress_update_timeouts (must be true ), ocp_ingress_client_timeout Notes : - Only applies when ocp_ingress_update_timeouts is true - Critical for MAS Manage report generation and data imports - Affects all routes in the cluster - Monitor application logs to determine appropriate values ocp_ingress_namespace_ownership \u00a4 Specifies the namespace ownership policy for the Ingress Controller. Set to InterNamespaceAllowed to enable path-based routing support, which allows routes to claim the same hostname across different namespaces. Optional Environment Variable: OCP_INGRESS_NAMESPACE_OWNERSHIP Default Value: Not set (empty string) Note When both timeout settings and namespace ownership are configured, they will be applied in a single atomic operation to the IngressController resource. Role Variables - OperatorHub \u00a4 ocp_operatorhub_disable_redhat_sources \u00a4 Disable default Red Hat OperatorHub catalog sources. Optional Environment Variable: OCP_OPERATORHUB_DISABLE_REDHAT_SOURCES Default: false Purpose : Disables the default Red Hat catalog sources ( certified-operators , community-operators , redhat-operators ) in OperatorHub. When to use : Required for air-gapped/disconnected environments where external catalog sources are not accessible. Also useful to enforce use of mirrored catalogs only. Valid values : - true - Disable default Red Hat catalog sources - false - Leave catalog sources unchanged (default) Impact : When true , disables the three default Red Hat catalog sources. Operators must be installed from custom or mirrored catalogs. Related variables : None Notes : - Important : Setting to false does NOT re-enable disabled sources - Required for disconnected/air-gapped MAS installations - Ensure custom catalogs are configured before disabling defaults - Affects all operator installations in the cluster - Cannot install operators from Red Hat catalogs after disabling Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: ocp_update_ciphers_for_semeru: True ocp_ingress_update_timeouts: True ocp_ingress_client_timeout: 30s ocp_ingress_server_timeout: 30s ocp_ingress_namespace_ownership: InterNamespaceAllowed ocp_ingress_controller_name: default ocp_operatorhub_disable_redhat_sources: True roles: - ibm.mas_devops.ocp_config License \u00a4 EPL-2.0","title":"ocp_config"},{"location":"roles/ocp_config/#ocp_config","text":"Configure OpenShift Container Platform cluster-level settings for optimal MAS deployment. This role provides essential tuning for ingress timeouts, TLS cipher compatibility with IBM Java Semeru FIPS mode, and OperatorHub catalog source management. Key capabilities: - Ingress timeout tuning : Prevent request failures for long-running operations - FIPS-compatible TLS ciphers : Enable IBM Java Semeru runtime in FIPS mode - Catalog source management : Disable default Red Hat catalogs for air-gapped environments - Path-based routing : Configure namespace ownership for multi-namespace route support This role configures: - Tune the IngressController to avoid request failures due to timeout for long running requests - Configure the IngressController namespace ownership for path-based routing support - Update APIServer and IngressController to set a custom tlsSecurityProfile to accommodate ciphers supported by IBM Java Semeru runtime. This is required for allowing the Java applications using Semeru runtime to run in FIPS mode. The following ciphers will be enabled: - TLS_AES_128_GCM_SHA256 - TLS_AES_256_GCM_SHA384 - TLS_CHACHA20_POLY1305_SHA256 - ECDHE-ECDSA-AES128-GCM-SHA256 - ECDHE-RSA-AES128-GCM-SHA256 - ECDHE-ECDSA-AES256-GCM-SHA384 - ECDHE-RSA-AES256-GCM-SHA384 - ECDHE-ECDSA-CHACHA20-POLY1305 - ECDHE-RSA-CHACHA20-POLY1305 - DHE-RSA-AES128-GCM-SHA256 - DHE-RSA-AES256-GCM-SHA384 - ECDHE-RSA-AES128-SHA256 - ECDHE-RSA-AES128-SHA - ECDHE-RSA-AES256-SHA - Disable the default Red Hat CatalogSources : - certified-operators - community-operators - redhat-operators","title":"ocp_config"},{"location":"roles/ocp_config/#role-variables-api-server","text":"","title":"Role Variables - API Server"},{"location":"roles/ocp_config/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/ocp_config/#ocp_update_ciphers_for_semeru","text":"Enable custom TLS cipher configuration for IBM Java Semeru FIPS mode compatibility. Optional Environment Variable: OCP_UPDATE_CIPHERS_FOR_SEMERU Default: false Purpose : Configures the API Server and Ingress Controller with a custom TLS security profile that includes ciphers compatible with IBM Java Semeru runtime in FIPS mode. When to use : Required when running MAS applications (particularly Manage) in FIPS mode with IBM Java Semeru runtime. Valid values : - true - Apply custom cipher configuration - false - Use default OpenShift cipher configuration Impact : When enabled, updates the tlsSecurityProfile on both APIServer and IngressController resources to include the following ciphers: - TLS 1.3: TLS_AES_128_GCM_SHA256 , TLS_AES_256_GCM_SHA384 , TLS_CHACHA20_POLY1305_SHA256 - TLS 1.2: ECDHE-ECDSA-AES128-GCM-SHA256 , ECDHE-RSA-AES128-GCM-SHA256 , ECDHE-ECDSA-AES256-GCM-SHA384 , ECDHE-RSA-AES256-GCM-SHA384 , ECDHE-ECDSA-CHACHA20-POLY1305 , ECDHE-RSA-CHACHA20-POLY1305 , DHE-RSA-AES128-GCM-SHA256 , DHE-RSA-AES256-GCM-SHA384 , ECDHE-RSA-AES128-SHA256 , ECDHE-RSA-AES128-SHA , ECDHE-RSA-AES256-SHA Related variables : None Notes : - Required for FIPS-compliant MAS deployments using Semeru runtime - Changes affect cluster-wide TLS configuration - May require cluster restart to fully apply - Verify compatibility with other applications in the cluster","title":"ocp_update_ciphers_for_semeru"},{"location":"roles/ocp_config/#role-variables-ingress-controller","text":"","title":"Role Variables - Ingress Controller"},{"location":"roles/ocp_config/#ocp_ingress_controller_name","text":"The name of the Ingress Controller to configure. Optional Environment Variable: OCP_INGRESS_CONTROLLER_NAME Default Value: default Purpose : Specifies which Ingress Controller instance to configure. This applies to both timeout and namespace ownership settings. When to use : Use the default value unless you have multiple Ingress Controllers in your cluster and need to configure a specific one. Valid values : - default - The default cluster Ingress Controller (most common) - Any custom Ingress Controller name in your cluster Impact : All timeout and namespace ownership configurations will be applied to this specific Ingress Controller. Related variables : - ocp_ingress_update_timeouts - Timeout settings apply to this controller - ocp_ingress_namespace_ownership - Namespace policy applies to this controller Notes : - Most OpenShift clusters use only the default Ingress Controller - Custom Ingress Controllers are rare and typically used for advanced routing scenarios - Verify the controller name exists before configuring","title":"ocp_ingress_controller_name"},{"location":"roles/ocp_config/#ocp_ingress_update_timeouts","text":"Enable custom timeout configuration for the OpenShift Ingress Controller. Optional Environment Variable: OCP_INGRESS_UPDATE_TIMEOUTS Default: false Purpose : Controls whether to apply custom client and server timeout values to the Ingress Controller. When to use : Enable when experiencing timeout issues with long-running MAS operations such as large file uploads, report generation, or data imports. Valid values : - true - Apply custom timeout values - false - Use default OpenShift timeout values (30s) Impact : When enabled, updates the IngressController with the values specified in ocp_ingress_client_timeout and ocp_ingress_server_timeout . Related variables : ocp_ingress_client_timeout , ocp_ingress_server_timeout Notes : - Default 30s timeout may be insufficient for MAS Manage operations - Recommended to enable for production MAS deployments - Changes apply to all routes in the cluster","title":"ocp_ingress_update_timeouts"},{"location":"roles/ocp_config/#ocp_ingress_client_timeout","text":"Client-side timeout duration for ingress connections. Optional Environment Variable: OCP_INGRESS_CLIENT_TIMEOUT Default: 30s Purpose : Specifies how long a connection is held open while waiting for a client to send data or complete a request. When to use : Increase when clients need more time to upload large files or send data to MAS applications. Valid values : Duration string with unit suffix (e.g., 30s , 5m , 1h ). Common values: - 30s - Default, suitable for most operations - 5m - Recommended for MAS Manage file uploads - 10m - For very large file operations Impact : Connections from clients will be kept alive for this duration while waiting for data. Too short may cause upload failures; too long may consume resources. Related variables : ocp_ingress_update_timeouts (must be true ), ocp_ingress_server_timeout Notes : - Only applies when ocp_ingress_update_timeouts is true - Consider network latency and file sizes when setting - Affects all routes in the cluster - Balance between user experience and resource consumption","title":"ocp_ingress_client_timeout"},{"location":"roles/ocp_config/#ocp_ingress_server_timeout","text":"Server-side timeout duration for ingress connections. Optional Environment Variable: OCP_INGRESS_SERVER_TIMEOUT Default: 30s Purpose : Specifies how long a connection is held open while waiting for a server (backend application) to respond. When to use : Increase when MAS applications perform long-running operations like report generation, data processing, or complex queries. Valid values : Duration string with unit suffix (e.g., 30s , 5m , 1h ). Common values: - 30s - Default, suitable for most operations - 5m - Recommended for MAS Manage report generation - 10m - For complex data processing operations - 30m - For very long-running batch operations Impact : Backend connections will be kept alive for this duration while waiting for responses. Too short may cause operation failures; too long may mask application issues. Related variables : ocp_ingress_update_timeouts (must be true ), ocp_ingress_client_timeout Notes : - Only applies when ocp_ingress_update_timeouts is true - Critical for MAS Manage report generation and data imports - Affects all routes in the cluster - Monitor application logs to determine appropriate values","title":"ocp_ingress_server_timeout"},{"location":"roles/ocp_config/#ocp_ingress_namespace_ownership","text":"Specifies the namespace ownership policy for the Ingress Controller. Set to InterNamespaceAllowed to enable path-based routing support, which allows routes to claim the same hostname across different namespaces. Optional Environment Variable: OCP_INGRESS_NAMESPACE_OWNERSHIP Default Value: Not set (empty string) Note When both timeout settings and namespace ownership are configured, they will be applied in a single atomic operation to the IngressController resource.","title":"ocp_ingress_namespace_ownership"},{"location":"roles/ocp_config/#role-variables-operatorhub","text":"","title":"Role Variables - OperatorHub"},{"location":"roles/ocp_config/#ocp_operatorhub_disable_redhat_sources","text":"Disable default Red Hat OperatorHub catalog sources. Optional Environment Variable: OCP_OPERATORHUB_DISABLE_REDHAT_SOURCES Default: false Purpose : Disables the default Red Hat catalog sources ( certified-operators , community-operators , redhat-operators ) in OperatorHub. When to use : Required for air-gapped/disconnected environments where external catalog sources are not accessible. Also useful to enforce use of mirrored catalogs only. Valid values : - true - Disable default Red Hat catalog sources - false - Leave catalog sources unchanged (default) Impact : When true , disables the three default Red Hat catalog sources. Operators must be installed from custom or mirrored catalogs. Related variables : None Notes : - Important : Setting to false does NOT re-enable disabled sources - Required for disconnected/air-gapped MAS installations - Ensure custom catalogs are configured before disabling defaults - Affects all operator installations in the cluster - Cannot install operators from Red Hat catalogs after disabling","title":"ocp_operatorhub_disable_redhat_sources"},{"location":"roles/ocp_config/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: ocp_update_ciphers_for_semeru: True ocp_ingress_update_timeouts: True ocp_ingress_client_timeout: 30s ocp_ingress_server_timeout: 30s ocp_ingress_namespace_ownership: InterNamespaceAllowed ocp_ingress_controller_name: default ocp_operatorhub_disable_redhat_sources: True roles: - ibm.mas_devops.ocp_config","title":"Example Playbook"},{"location":"roles/ocp_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_deprovision/","text":"ocp_deprovision \u00a4 Deprovision OCP cluster in Fyre, IBM Cloud, & ROSA. Role Variables - General \u00a4 cluster_type \u00a4 Infrastructure provider type for cluster deprovisioning. Required Environment Variable: CLUSTER_TYPE Default: None Purpose : Specifies which infrastructure provider was used to provision the cluster. Determines the deprovisioning method and required credentials. When to use : - Always required for cluster deprovisioning - Must match the provider used to create the cluster - Each type requires different provider-specific variables Valid values : fyre , roks , rosa , ipi - fyre : IBM DevIT Fyre clusters (internal development) - roks : IBM Cloud Red Hat OpenShift Kubernetes Service - rosa : AWS Red Hat OpenShift Service on AWS - ipi : Installer-Provisioned Infrastructure (AWS, GCP) Impact : Determines deprovisioning workflow and which provider-specific variables are required. Using wrong type will cause deprovisioning to fail. Related variables : - cluster_name : Name of the cluster to deprovision - Provider-specific credentials (ibmcloud_apikey, rosa_token, fyre_apikey, etc.) Note : IMPORTANT - Must match the provider used to create the cluster. Verify cluster type before deprovisioning to avoid errors. cluster_name \u00a4 Name of the cluster to deprovision. Required Environment Variable: CLUSTER_NAME Default: None Purpose : Identifies which cluster to deprovision. Used to locate and delete the cluster in the provider's system. When to use : - Always required for cluster deprovisioning - Must exactly match the cluster name in the provider - Used to target the specific cluster for deletion Valid values : String matching the cluster name in the provider's system Impact : Determines which cluster is deleted. CRITICAL - Incorrect name may cause wrong cluster to be deleted or deprovisioning to fail. Related variables : - cluster_type : Provider where cluster exists Note : WARNING - Deprovisioning permanently deletes the cluster and all its resources. Verify the cluster name carefully before proceeding. This operation cannot be undone. Role Variables - ROKS \u00a4 ibmcloud_apikey \u00a4 IBM Cloud API key for authentication. Required (when cluster_type=roks ) Environment Variable: IBMCLOUD_APIKEY Default: None Purpose : Authenticates with IBM Cloud to deprovision ROKS clusters. Used by the ibmcloud CLI for cluster deletion operations. When to use : - Always required for ROKS cluster deprovisioning - Must have permissions to delete clusters in the resource group - Same API key used for cluster provisioning can be used Valid values : IBM Cloud API key string (typically 40+ characters) Impact : Without a valid API key with delete permissions, cluster deprovisioning will fail. Related variables : - cluster_name : ROKS cluster to deprovision Note : The API key must have cluster deletion permissions in the resource group. Deprovisioning permanently deletes the cluster. Role Variables - ROSA \u00a4 rosa_token \u00a4 Red Hat OpenShift Service on AWS (ROSA) authentication token. Required (when cluster_type=rosa ) Environment Variable: ROSA_TOKEN Default: None Purpose : Authenticates with the ROSA service to deprovision OpenShift clusters on AWS. Required for cluster deletion operations. When to use : - Always required for ROSA cluster deprovisioning - Obtain from OpenShift Cluster Manager - Token must be valid and not expired Valid values : ROSA API token string from Red Hat OpenShift Cluster Manager Impact : Without a valid token, ROSA cluster deprovisioning will fail. Related variables : - cluster_name : ROSA cluster to deprovision Note : Tokens expire periodically. Obtain a fresh token before deprovisioning. Deprovisioning permanently deletes the cluster and all AWS resources. Role Variables - FYRE \u00a4 fyre_username \u00a4 Fyre username for authentication. Required (when cluster_type=fyre ) Environment Variable: FYRE_USERNAME Default: None Purpose : Authenticates with the IBM DevIT Fyre API to deprovision OpenShift clusters on Fyre infrastructure. When to use : - Always required for Fyre cluster deprovisioning - Must be a valid Fyre account username - Used for internal IBM development and testing Valid values : Valid Fyre username (IBM intranet ID) Impact : Without valid credentials, Fyre cluster deprovisioning will fail. Related variables : - fyre_apikey : API key paired with this username - cluster_name : Fyre cluster to deprovision Note : Fyre is an internal IBM development platform. Access requires IBM credentials and appropriate permissions. fyre_apikey \u00a4 Fyre API key for authentication. Required (when cluster_type=fyre ) Environment Variable: FYRE_APIKEY Default: None Purpose : Authenticates with the Fyre API for cluster deprovisioning operations. Paired with Fyre username for authentication. When to use : - Always required for Fyre cluster deprovisioning - Obtain from Fyre portal - Keep secure and rotate regularly Valid values : Valid Fyre API key string Impact : Without a valid API key, Fyre cluster deprovisioning will fail. Related variables : - fyre_username : Username paired with this API key - cluster_name : Fyre cluster to deprovision Note : Keep API keys secure. Obtain from the Fyre portal. Keys may expire and need renewal. fyre_site \u00a4 Fyre datacenter site location. Optional Environment Variable: FYRE_SITE Default: svl Purpose : Specifies which Fyre datacenter site the cluster was provisioned in. Used to locate the cluster for deprovisioning. When to use : - Use default ( svl - San Jose/Silicon Valley) if cluster was provisioned there - Set to the site where cluster was originally provisioned - Must match the site used during provisioning Valid values : Valid Fyre site code (e.g., svl , rtp , raleigh ) Impact : Determines where to look for the cluster. Incorrect site will cause deprovisioning to fail. Related variables : - cluster_name : Cluster to deprovision at this site Note : Must match the site where the cluster was originally provisioned. SVL is the default and most commonly used site. Role Variables - IPI \u00a4 The following variables are only used when cluster_type = ipi . ipi_dir \u00a4 Working directory for IPI deprovisioning. Optional Environment Variable: IPI_DIR Default: ~/openshift-install Purpose : Specifies the directory containing the openshift-install executable and cluster metadata required for deprovisioning. When to use : - Use default if cluster was provisioned with default directory - Set to match the directory used during cluster provisioning - Directory must contain cluster metadata for deprovisioning Valid values : Absolute filesystem path Impact : Deprovisioning requires cluster metadata from this directory. Incorrect path will cause deprovisioning to fail. Related variables : - cluster_name : Cluster metadata stored in subdirectory Note : Must be the same directory used during cluster provisioning. The directory contains cluster metadata required for proper cleanup of all resources. ipi_platform \u00a4 Cloud platform for IPI cluster deprovisioning. Required (when cluster_type=ipi ) Environment Variable: IPI_PLATFORM Default: None Purpose : Specifies which cloud platform the IPI cluster was deployed on. Determines which provider-specific credentials are required for deprovisioning. When to use : - Always required for IPI cluster deprovisioning - Must match the platform used during provisioning - Each platform requires different credentials Valid values : aws , gcp Impact : Determines which cloud provider is used for deprovisioning and which credentials are required. Related variables : - aws_access_key_id , aws_secret_access_key : Required when aws - gcp_service_account_file : Required when gcp Note : Must match the platform used during cluster provisioning. AWS and GCP are supported. Role Variables - AWS \u00a4 The following variables are only used when cluster_type = ipi and ipi_platform = aws . aws_access_key_id \u00a4 AWS access key ID for authentication. Required (when cluster_type=ipi and ipi_platform=aws ) Environment Variable: AWS_ACCESS_KEY_ID Default: None Purpose : Authenticates with AWS to deprovision IPI cluster infrastructure. Must have permissions to delete VPCs, instances, load balancers, and other AWS resources. When to use : - Always required for AWS IPI cluster deprovisioning - Must be associated with IAM user or role with deletion permissions - Can be same credentials used for provisioning Valid values : AWS access key ID string (typically 20 characters, starts with AKIA ) Impact : Without valid credentials with appropriate permissions, cluster deprovisioning will fail or be incomplete. Related variables : - aws_secret_access_key : Secret key paired with this access key ID Note : The IAM user/role must have permissions to delete all cluster resources (VPC, EC2, ELB, Route53, IAM, etc.). Incomplete permissions may leave orphaned resources. aws_secret_access_key \u00a4 AWS secret access key for authentication. Required (when cluster_type=ipi and ipi_platform=aws ) Environment Variable: AWS_SECRET_ACCESS_KEY Default: None Purpose : Authenticates with AWS to deprovision IPI cluster infrastructure. Paired with AWS access key ID for authentication. When to use : - Always required for AWS IPI cluster deprovisioning - Must correspond to the provided access key ID - Keep secure and rotate regularly Valid values : AWS secret access key string (typically 40 characters) Impact : Without valid credentials, cluster deprovisioning will fail. Related variables : - aws_access_key_id : Access key ID paired with this secret key Note : Keep secret keys secure. Ensure credentials have permissions to delete all cluster resources to avoid orphaned resources. Role Variables - GCP \u00a4 The following variables are only used when cluster_type = ipi and ipi_platform = gcp . gcp_service_account_file \u00a4 Path to GCP service account credentials file. Required (when cluster_type=ipi and ipi_platform=gcp ) Environment Variable: GOOGLE_APPLICATION_CREDENTIALS Default: None Purpose : Authenticates with Google Cloud Platform to deprovision IPI cluster infrastructure. Service account must have permissions to delete instances and networking resources. When to use : - Always required for GCP IPI cluster deprovisioning - Must be a valid service account JSON key file - Service account must have cluster deletion permissions Valid values : Absolute path to GCP service account JSON key file Impact : Without valid credentials with appropriate permissions, cluster deprovisioning will fail or be incomplete. Related variables : - cluster_name : Cluster to deprovision Note : The service account must have permissions to delete all cluster resources (Compute, Networking, IAM, etc.). Can be same service account used for provisioning. Incomplete permissions may leave orphaned resources. Example Playbook \u00a4 - hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_deprovision License \u00a4 EPL-2.0","title":"ocp_deprovision"},{"location":"roles/ocp_deprovision/#ocp_deprovision","text":"Deprovision OCP cluster in Fyre, IBM Cloud, & ROSA.","title":"ocp_deprovision"},{"location":"roles/ocp_deprovision/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/ocp_deprovision/#cluster_type","text":"Infrastructure provider type for cluster deprovisioning. Required Environment Variable: CLUSTER_TYPE Default: None Purpose : Specifies which infrastructure provider was used to provision the cluster. Determines the deprovisioning method and required credentials. When to use : - Always required for cluster deprovisioning - Must match the provider used to create the cluster - Each type requires different provider-specific variables Valid values : fyre , roks , rosa , ipi - fyre : IBM DevIT Fyre clusters (internal development) - roks : IBM Cloud Red Hat OpenShift Kubernetes Service - rosa : AWS Red Hat OpenShift Service on AWS - ipi : Installer-Provisioned Infrastructure (AWS, GCP) Impact : Determines deprovisioning workflow and which provider-specific variables are required. Using wrong type will cause deprovisioning to fail. Related variables : - cluster_name : Name of the cluster to deprovision - Provider-specific credentials (ibmcloud_apikey, rosa_token, fyre_apikey, etc.) Note : IMPORTANT - Must match the provider used to create the cluster. Verify cluster type before deprovisioning to avoid errors.","title":"cluster_type"},{"location":"roles/ocp_deprovision/#cluster_name","text":"Name of the cluster to deprovision. Required Environment Variable: CLUSTER_NAME Default: None Purpose : Identifies which cluster to deprovision. Used to locate and delete the cluster in the provider's system. When to use : - Always required for cluster deprovisioning - Must exactly match the cluster name in the provider - Used to target the specific cluster for deletion Valid values : String matching the cluster name in the provider's system Impact : Determines which cluster is deleted. CRITICAL - Incorrect name may cause wrong cluster to be deleted or deprovisioning to fail. Related variables : - cluster_type : Provider where cluster exists Note : WARNING - Deprovisioning permanently deletes the cluster and all its resources. Verify the cluster name carefully before proceeding. This operation cannot be undone.","title":"cluster_name"},{"location":"roles/ocp_deprovision/#role-variables-roks","text":"","title":"Role Variables - ROKS"},{"location":"roles/ocp_deprovision/#ibmcloud_apikey","text":"IBM Cloud API key for authentication. Required (when cluster_type=roks ) Environment Variable: IBMCLOUD_APIKEY Default: None Purpose : Authenticates with IBM Cloud to deprovision ROKS clusters. Used by the ibmcloud CLI for cluster deletion operations. When to use : - Always required for ROKS cluster deprovisioning - Must have permissions to delete clusters in the resource group - Same API key used for cluster provisioning can be used Valid values : IBM Cloud API key string (typically 40+ characters) Impact : Without a valid API key with delete permissions, cluster deprovisioning will fail. Related variables : - cluster_name : ROKS cluster to deprovision Note : The API key must have cluster deletion permissions in the resource group. Deprovisioning permanently deletes the cluster.","title":"ibmcloud_apikey"},{"location":"roles/ocp_deprovision/#role-variables-rosa","text":"","title":"Role Variables - ROSA"},{"location":"roles/ocp_deprovision/#rosa_token","text":"Red Hat OpenShift Service on AWS (ROSA) authentication token. Required (when cluster_type=rosa ) Environment Variable: ROSA_TOKEN Default: None Purpose : Authenticates with the ROSA service to deprovision OpenShift clusters on AWS. Required for cluster deletion operations. When to use : - Always required for ROSA cluster deprovisioning - Obtain from OpenShift Cluster Manager - Token must be valid and not expired Valid values : ROSA API token string from Red Hat OpenShift Cluster Manager Impact : Without a valid token, ROSA cluster deprovisioning will fail. Related variables : - cluster_name : ROSA cluster to deprovision Note : Tokens expire periodically. Obtain a fresh token before deprovisioning. Deprovisioning permanently deletes the cluster and all AWS resources.","title":"rosa_token"},{"location":"roles/ocp_deprovision/#role-variables-fyre","text":"","title":"Role Variables - FYRE"},{"location":"roles/ocp_deprovision/#fyre_username","text":"Fyre username for authentication. Required (when cluster_type=fyre ) Environment Variable: FYRE_USERNAME Default: None Purpose : Authenticates with the IBM DevIT Fyre API to deprovision OpenShift clusters on Fyre infrastructure. When to use : - Always required for Fyre cluster deprovisioning - Must be a valid Fyre account username - Used for internal IBM development and testing Valid values : Valid Fyre username (IBM intranet ID) Impact : Without valid credentials, Fyre cluster deprovisioning will fail. Related variables : - fyre_apikey : API key paired with this username - cluster_name : Fyre cluster to deprovision Note : Fyre is an internal IBM development platform. Access requires IBM credentials and appropriate permissions.","title":"fyre_username"},{"location":"roles/ocp_deprovision/#fyre_apikey","text":"Fyre API key for authentication. Required (when cluster_type=fyre ) Environment Variable: FYRE_APIKEY Default: None Purpose : Authenticates with the Fyre API for cluster deprovisioning operations. Paired with Fyre username for authentication. When to use : - Always required for Fyre cluster deprovisioning - Obtain from Fyre portal - Keep secure and rotate regularly Valid values : Valid Fyre API key string Impact : Without a valid API key, Fyre cluster deprovisioning will fail. Related variables : - fyre_username : Username paired with this API key - cluster_name : Fyre cluster to deprovision Note : Keep API keys secure. Obtain from the Fyre portal. Keys may expire and need renewal.","title":"fyre_apikey"},{"location":"roles/ocp_deprovision/#fyre_site","text":"Fyre datacenter site location. Optional Environment Variable: FYRE_SITE Default: svl Purpose : Specifies which Fyre datacenter site the cluster was provisioned in. Used to locate the cluster for deprovisioning. When to use : - Use default ( svl - San Jose/Silicon Valley) if cluster was provisioned there - Set to the site where cluster was originally provisioned - Must match the site used during provisioning Valid values : Valid Fyre site code (e.g., svl , rtp , raleigh ) Impact : Determines where to look for the cluster. Incorrect site will cause deprovisioning to fail. Related variables : - cluster_name : Cluster to deprovision at this site Note : Must match the site where the cluster was originally provisioned. SVL is the default and most commonly used site.","title":"fyre_site"},{"location":"roles/ocp_deprovision/#role-variables-ipi","text":"The following variables are only used when cluster_type = ipi .","title":"Role Variables - IPI"},{"location":"roles/ocp_deprovision/#ipi_dir","text":"Working directory for IPI deprovisioning. Optional Environment Variable: IPI_DIR Default: ~/openshift-install Purpose : Specifies the directory containing the openshift-install executable and cluster metadata required for deprovisioning. When to use : - Use default if cluster was provisioned with default directory - Set to match the directory used during cluster provisioning - Directory must contain cluster metadata for deprovisioning Valid values : Absolute filesystem path Impact : Deprovisioning requires cluster metadata from this directory. Incorrect path will cause deprovisioning to fail. Related variables : - cluster_name : Cluster metadata stored in subdirectory Note : Must be the same directory used during cluster provisioning. The directory contains cluster metadata required for proper cleanup of all resources.","title":"ipi_dir"},{"location":"roles/ocp_deprovision/#ipi_platform","text":"Cloud platform for IPI cluster deprovisioning. Required (when cluster_type=ipi ) Environment Variable: IPI_PLATFORM Default: None Purpose : Specifies which cloud platform the IPI cluster was deployed on. Determines which provider-specific credentials are required for deprovisioning. When to use : - Always required for IPI cluster deprovisioning - Must match the platform used during provisioning - Each platform requires different credentials Valid values : aws , gcp Impact : Determines which cloud provider is used for deprovisioning and which credentials are required. Related variables : - aws_access_key_id , aws_secret_access_key : Required when aws - gcp_service_account_file : Required when gcp Note : Must match the platform used during cluster provisioning. AWS and GCP are supported.","title":"ipi_platform"},{"location":"roles/ocp_deprovision/#role-variables-aws","text":"The following variables are only used when cluster_type = ipi and ipi_platform = aws .","title":"Role Variables - AWS"},{"location":"roles/ocp_deprovision/#aws_access_key_id","text":"AWS access key ID for authentication. Required (when cluster_type=ipi and ipi_platform=aws ) Environment Variable: AWS_ACCESS_KEY_ID Default: None Purpose : Authenticates with AWS to deprovision IPI cluster infrastructure. Must have permissions to delete VPCs, instances, load balancers, and other AWS resources. When to use : - Always required for AWS IPI cluster deprovisioning - Must be associated with IAM user or role with deletion permissions - Can be same credentials used for provisioning Valid values : AWS access key ID string (typically 20 characters, starts with AKIA ) Impact : Without valid credentials with appropriate permissions, cluster deprovisioning will fail or be incomplete. Related variables : - aws_secret_access_key : Secret key paired with this access key ID Note : The IAM user/role must have permissions to delete all cluster resources (VPC, EC2, ELB, Route53, IAM, etc.). Incomplete permissions may leave orphaned resources.","title":"aws_access_key_id"},{"location":"roles/ocp_deprovision/#aws_secret_access_key","text":"AWS secret access key for authentication. Required (when cluster_type=ipi and ipi_platform=aws ) Environment Variable: AWS_SECRET_ACCESS_KEY Default: None Purpose : Authenticates with AWS to deprovision IPI cluster infrastructure. Paired with AWS access key ID for authentication. When to use : - Always required for AWS IPI cluster deprovisioning - Must correspond to the provided access key ID - Keep secure and rotate regularly Valid values : AWS secret access key string (typically 40 characters) Impact : Without valid credentials, cluster deprovisioning will fail. Related variables : - aws_access_key_id : Access key ID paired with this secret key Note : Keep secret keys secure. Ensure credentials have permissions to delete all cluster resources to avoid orphaned resources.","title":"aws_secret_access_key"},{"location":"roles/ocp_deprovision/#role-variables-gcp","text":"The following variables are only used when cluster_type = ipi and ipi_platform = gcp .","title":"Role Variables - GCP"},{"location":"roles/ocp_deprovision/#gcp_service_account_file","text":"Path to GCP service account credentials file. Required (when cluster_type=ipi and ipi_platform=gcp ) Environment Variable: GOOGLE_APPLICATION_CREDENTIALS Default: None Purpose : Authenticates with Google Cloud Platform to deprovision IPI cluster infrastructure. Service account must have permissions to delete instances and networking resources. When to use : - Always required for GCP IPI cluster deprovisioning - Must be a valid service account JSON key file - Service account must have cluster deletion permissions Valid values : Absolute path to GCP service account JSON key file Impact : Without valid credentials with appropriate permissions, cluster deprovisioning will fail or be incomplete. Related variables : - cluster_name : Cluster to deprovision Note : The service account must have permissions to delete all cluster resources (Compute, Networking, IAM, etc.). Can be same service account used for provisioning. Incomplete permissions may leave orphaned resources.","title":"gcp_service_account_file"},{"location":"roles/ocp_deprovision/#example-playbook","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_deprovision","title":"Example Playbook"},{"location":"roles/ocp_deprovision/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_efs/","text":"ocp_efs \u00a4 Provision and configure AWS Elastic File System (EFS) storage for Red Hat OpenShift Service on AWS (ROSA) clusters. This role automates the complete EFS setup including security group configuration, EFS instance creation, access points, mount targets, and StorageClass creation. EFS provides ReadWriteMany (RWX) persistent storage essential for MAS applications that require shared file access across multiple pods, such as Manage attachments and customer files. The role performs the following operations: 1. Configures security group inbound rules for NFS access 2. Creates EFS file system in the cluster's VPC 3. Establishes mount targets in all availability zones 4. Creates access points for isolated storage 5. Deploys EFS CSI driver and StorageClass Role Variables \u00a4 aws_access_key_id \u00a4 AWS access key ID for authentication. Required Environment Variable: AWS_ACCESS_KEY_ID Default: None Purpose : Provides AWS credentials for creating and configuring EFS resources via AWS CLI. When to use : Always required. Must have permissions to create EFS, modify security groups, and manage VPC resources. Valid values : Valid AWS access key ID (20-character alphanumeric string). Impact : Used to authenticate all AWS API calls. Insufficient permissions will cause provisioning failures. Related variables : aws_secret_access_key , aws_region Notes : - Requires IAM permissions for EFS, EC2, and VPC operations - Consider using IAM roles instead of access keys for better security - Store credentials securely, never commit to version control aws_secret_access_key \u00a4 AWS secret access key for authentication. Required Environment Variable: AWS_SECRET_ACCESS_KEY Default: None Purpose : Provides the secret key component of AWS credentials for API authentication. When to use : Always required. Must correspond to the aws_access_key_id . Valid values : Valid AWS secret access key (40-character alphanumeric string). Impact : Used with access key ID to authenticate AWS API calls. Incorrect key will cause authentication failures. Related variables : aws_access_key_id , aws_region Notes : - Keep secret key secure and never expose in logs or version control - Rotate credentials regularly per security best practices - Consider using AWS STS temporary credentials for enhanced security aws_region \u00a4 AWS region where the EFS instance will be provisioned. Optional Environment Variable: AWS_DEFAULT_REGION Default: eu-west-2 Purpose : Specifies the AWS region for EFS provisioning. Must match the ROSA cluster's region. When to use : Always set to match your ROSA cluster's region. Default is suitable for EU deployments. Valid values : Valid AWS region code (e.g., us-east-1 , eu-west-2 , ap-southeast-1 ). Common values: - us-east-1 - US East (N. Virginia) - us-west-2 - US West (Oregon) - eu-west-1 - Europe (Ireland) - eu-west-2 - Europe (London) - ap-southeast-1 - Asia Pacific (Singapore) Impact : EFS must be in the same region as the ROSA cluster. Cross-region access is not supported. Related variables : cluster_name Notes : - Critical : Must match ROSA cluster region exactly - Verify cluster region: rosa describe cluster -c <cluster-name> - EFS pricing varies by region cluster_name \u00a4 Name of the ROSA cluster to attach EFS storage. Required Environment Variable: CLUSTER_NAME Default: None Purpose : Identifies the target ROSA cluster for EFS integration. Used to locate the cluster's VPC and configure security groups. When to use : Always required. Must be the exact name of an existing ROSA cluster. Valid values : Valid ROSA cluster name as shown in rosa list clusters . Impact : The role uses this name to find the cluster's VPC ID and configure EFS mount targets in the correct network. Related variables : aws_region , efs_unique_id Notes : - Cluster must exist before running this role - Name is case-sensitive - Verify cluster name: rosa list clusters - Used to tag AWS resources for cost tracking efs_unique_id \u00a4 Unique identifier for the EFS instance and StorageClass. Optional Environment Variable: EFS_UNIQUE_ID Default: Value of cluster_name Purpose : Provides a unique identifier used in EFS resource naming and StorageClass creation. Enables multiple EFS instances per cluster. When to use : Set when you need multiple EFS instances in the same cluster, or want a more descriptive name than the cluster name. Valid values : Alphanumeric string, typically lowercase (e.g., mas-prod , manage-storage , shared-files ). Impact : - StorageClass will be named efs-<efs_unique_id> - EFS file system will be tagged with this identifier - Creation tokens will include this identifier Related variables : cluster_name , creation_token_prefix , create_storage_class Notes : - Defaults to cluster name if not specified - Use descriptive names for multiple EFS instances (e.g., mas-attachments , mas-logs ) - Must be unique within the cluster if creating multiple EFS instances creation_token_prefix \u00a4 Prefix for AWS resource creation tokens. Optional Environment Variable: CREATION_TOKEN_PREFIX Default: mas_devops. Purpose : Provides a prefix for creation tokens used to ensure idempotency of AWS resource creation. When to use : Customize to identify resources created by this automation or to avoid conflicts with other tools. Valid values : String prefix, typically ending with a dot or dash (e.g., mas_devops. , myorg- , prod_ ). Impact : Creation tokens are built by concatenating this prefix with efs_unique_id . Used for idempotent resource creation. Related variables : efs_unique_id Notes : - Default mas_devops. identifies resources created by this collection - Helps track resource provenance in AWS - Change if you need to distinguish from other automation tools create_storage_class \u00a4 Enable automatic StorageClass creation for the EFS instance. Optional Environment Variable: CREATE_STORAGE_CLASS Default: true Purpose : Controls whether a Kubernetes StorageClass is automatically created for the EFS instance. When to use : Set to true for normal operations. Set to false if you want to manually create the StorageClass or use a custom configuration. Valid values : - true - Automatically create StorageClass named efs-<efs_unique_id> (default) - false - Skip StorageClass creation Impact : When true , creates a StorageClass that applications can use to provision PVCs backed by EFS. When false , you must manually create the StorageClass. Related variables : efs_unique_id Notes : - Default true is recommended for most deployments - StorageClass name format: efs-<efs_unique_id> - StorageClass supports dynamic provisioning with RWX access mode - Set to false only if you need custom StorageClass parameters Example Playbook \u00a4 - hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.ocp_efs License \u00a4 EPL-2.0","title":"ocp_efs"},{"location":"roles/ocp_efs/#ocp_efs","text":"Provision and configure AWS Elastic File System (EFS) storage for Red Hat OpenShift Service on AWS (ROSA) clusters. This role automates the complete EFS setup including security group configuration, EFS instance creation, access points, mount targets, and StorageClass creation. EFS provides ReadWriteMany (RWX) persistent storage essential for MAS applications that require shared file access across multiple pods, such as Manage attachments and customer files. The role performs the following operations: 1. Configures security group inbound rules for NFS access 2. Creates EFS file system in the cluster's VPC 3. Establishes mount targets in all availability zones 4. Creates access points for isolated storage 5. Deploys EFS CSI driver and StorageClass","title":"ocp_efs"},{"location":"roles/ocp_efs/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_efs/#aws_access_key_id","text":"AWS access key ID for authentication. Required Environment Variable: AWS_ACCESS_KEY_ID Default: None Purpose : Provides AWS credentials for creating and configuring EFS resources via AWS CLI. When to use : Always required. Must have permissions to create EFS, modify security groups, and manage VPC resources. Valid values : Valid AWS access key ID (20-character alphanumeric string). Impact : Used to authenticate all AWS API calls. Insufficient permissions will cause provisioning failures. Related variables : aws_secret_access_key , aws_region Notes : - Requires IAM permissions for EFS, EC2, and VPC operations - Consider using IAM roles instead of access keys for better security - Store credentials securely, never commit to version control","title":"aws_access_key_id"},{"location":"roles/ocp_efs/#aws_secret_access_key","text":"AWS secret access key for authentication. Required Environment Variable: AWS_SECRET_ACCESS_KEY Default: None Purpose : Provides the secret key component of AWS credentials for API authentication. When to use : Always required. Must correspond to the aws_access_key_id . Valid values : Valid AWS secret access key (40-character alphanumeric string). Impact : Used with access key ID to authenticate AWS API calls. Incorrect key will cause authentication failures. Related variables : aws_access_key_id , aws_region Notes : - Keep secret key secure and never expose in logs or version control - Rotate credentials regularly per security best practices - Consider using AWS STS temporary credentials for enhanced security","title":"aws_secret_access_key"},{"location":"roles/ocp_efs/#aws_region","text":"AWS region where the EFS instance will be provisioned. Optional Environment Variable: AWS_DEFAULT_REGION Default: eu-west-2 Purpose : Specifies the AWS region for EFS provisioning. Must match the ROSA cluster's region. When to use : Always set to match your ROSA cluster's region. Default is suitable for EU deployments. Valid values : Valid AWS region code (e.g., us-east-1 , eu-west-2 , ap-southeast-1 ). Common values: - us-east-1 - US East (N. Virginia) - us-west-2 - US West (Oregon) - eu-west-1 - Europe (Ireland) - eu-west-2 - Europe (London) - ap-southeast-1 - Asia Pacific (Singapore) Impact : EFS must be in the same region as the ROSA cluster. Cross-region access is not supported. Related variables : cluster_name Notes : - Critical : Must match ROSA cluster region exactly - Verify cluster region: rosa describe cluster -c <cluster-name> - EFS pricing varies by region","title":"aws_region"},{"location":"roles/ocp_efs/#cluster_name","text":"Name of the ROSA cluster to attach EFS storage. Required Environment Variable: CLUSTER_NAME Default: None Purpose : Identifies the target ROSA cluster for EFS integration. Used to locate the cluster's VPC and configure security groups. When to use : Always required. Must be the exact name of an existing ROSA cluster. Valid values : Valid ROSA cluster name as shown in rosa list clusters . Impact : The role uses this name to find the cluster's VPC ID and configure EFS mount targets in the correct network. Related variables : aws_region , efs_unique_id Notes : - Cluster must exist before running this role - Name is case-sensitive - Verify cluster name: rosa list clusters - Used to tag AWS resources for cost tracking","title":"cluster_name"},{"location":"roles/ocp_efs/#efs_unique_id","text":"Unique identifier for the EFS instance and StorageClass. Optional Environment Variable: EFS_UNIQUE_ID Default: Value of cluster_name Purpose : Provides a unique identifier used in EFS resource naming and StorageClass creation. Enables multiple EFS instances per cluster. When to use : Set when you need multiple EFS instances in the same cluster, or want a more descriptive name than the cluster name. Valid values : Alphanumeric string, typically lowercase (e.g., mas-prod , manage-storage , shared-files ). Impact : - StorageClass will be named efs-<efs_unique_id> - EFS file system will be tagged with this identifier - Creation tokens will include this identifier Related variables : cluster_name , creation_token_prefix , create_storage_class Notes : - Defaults to cluster name if not specified - Use descriptive names for multiple EFS instances (e.g., mas-attachments , mas-logs ) - Must be unique within the cluster if creating multiple EFS instances","title":"efs_unique_id"},{"location":"roles/ocp_efs/#creation_token_prefix","text":"Prefix for AWS resource creation tokens. Optional Environment Variable: CREATION_TOKEN_PREFIX Default: mas_devops. Purpose : Provides a prefix for creation tokens used to ensure idempotency of AWS resource creation. When to use : Customize to identify resources created by this automation or to avoid conflicts with other tools. Valid values : String prefix, typically ending with a dot or dash (e.g., mas_devops. , myorg- , prod_ ). Impact : Creation tokens are built by concatenating this prefix with efs_unique_id . Used for idempotent resource creation. Related variables : efs_unique_id Notes : - Default mas_devops. identifies resources created by this collection - Helps track resource provenance in AWS - Change if you need to distinguish from other automation tools","title":"creation_token_prefix"},{"location":"roles/ocp_efs/#create_storage_class","text":"Enable automatic StorageClass creation for the EFS instance. Optional Environment Variable: CREATE_STORAGE_CLASS Default: true Purpose : Controls whether a Kubernetes StorageClass is automatically created for the EFS instance. When to use : Set to true for normal operations. Set to false if you want to manually create the StorageClass or use a custom configuration. Valid values : - true - Automatically create StorageClass named efs-<efs_unique_id> (default) - false - Skip StorageClass creation Impact : When true , creates a StorageClass that applications can use to provision PVCs backed by EFS. When false , you must manually create the StorageClass. Related variables : efs_unique_id Notes : - Default true is recommended for most deployments - StorageClass name format: efs-<efs_unique_id> - StorageClass supports dynamic provisioning with RWX access mode - Set to false only if you need custom StorageClass parameters","title":"create_storage_class"},{"location":"roles/ocp_efs/#example-playbook","text":"- hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.ocp_efs","title":"Example Playbook"},{"location":"roles/ocp_efs/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_github_oauth/","text":"ocp_github_oauth \u00a4 Configure OpenShift cluster authentication using GitHub OAuth. This role enables users to log into OpenShift using their GitHub credentials, with support for both public GitHub and GitHub Enterprise. It also manages user groups and cluster role bindings based on GitHub organization membership. Prerequisites : - GitHub OAuth application must be configured in your GitHub organization before using this role - OAuth application must use ibmgithub as the OAuth ID - Requires GitHub organization admin permissions The role creates OAuth configuration, manages user groups, and assigns cluster roles based on GitHub organization membership. Role Variables \u00a4 oauth.github_client_secret_value \u00a4 GitHub OAuth application client secret. Required Environment Variable: None (passed as variable) Default: None Purpose : Provides the client secret from the GitHub OAuth application configuration for secure authentication. When to use : Always required. Obtained from GitHub OAuth app settings. Valid values : Valid GitHub OAuth client secret string (generated by GitHub). Impact : Used to authenticate the OpenShift cluster with GitHub. Incorrect secret will prevent GitHub authentication. Related variables : oauth.github_client_id_value , oauth.github_hostname Notes : - Obtain from GitHub OAuth app settings in your organization - Keep secret secure, never commit to version control - Stored as Kubernetes secret in the cluster - Regenerate if compromised oauth.github_client_id_value \u00a4 GitHub OAuth application client ID. Required Environment Variable: None (passed as variable) Default: None Purpose : Provides the client ID from the GitHub OAuth application configuration. When to use : Always required. Obtained from GitHub OAuth app settings. Valid values : Valid GitHub OAuth client ID string (generated by GitHub). Impact : Identifies the OAuth application to GitHub during authentication flow. Related variables : oauth.github_client_secret_value , oauth.github_hostname Notes : - Obtain from GitHub OAuth app settings in your organization - Public value, but should still be managed securely - Must match the OAuth app configured with ibmgithub as the OAuth ID oauth.github_hostname \u00a4 GitHub server hostname for authentication. Optional Environment Variable: None (passed as variable) Default: github.com (public GitHub) Purpose : Specifies the GitHub server to use for authentication, supporting both public GitHub and GitHub Enterprise. When to use : Set to your GitHub Enterprise hostname if using enterprise GitHub. Leave default for public GitHub. Valid values : - github.com - Public GitHub (default) - github.ibm.com - IBM GitHub Enterprise - Custom GitHub Enterprise hostname (e.g., github.example.com ) Impact : Determines which GitHub server handles authentication requests. Related variables : oauth.github_client_id_value , oauth.github_client_secret_value Notes : - Default github.com is for public GitHub - Use your enterprise hostname for GitHub Enterprise - Hostname must be accessible from the OpenShift cluster - Verify SSL certificates are properly configured oauth.organizations \u00a4 List of GitHub organizations for authentication. Required Environment Variable: None (passed as variable) Default: None Purpose : Defines which GitHub organizations are allowed to authenticate to the OpenShift cluster. When to use : Always required. Specify one or more GitHub organizations whose members can access the cluster. Valid values : List of valid GitHub organization names (e.g., ['ibm', 'redhat'] , ['myorg'] ). Impact : Only users who are members of the specified organizations can authenticate to the cluster. Related variables : oauth.groups Notes : - Users must be members of at least one listed organization - Organization membership is verified during each login - Multiple organizations can be specified for broader access - Organization names are case-sensitive oauth.groups \u00a4 List of OpenShift groups to create with GitHub user mappings. Optional Environment Variable: None (passed as variable) Default: None Purpose : Defines OpenShift groups to be created, their GitHub user members, and associated cluster role bindings. When to use : Use to map GitHub users to OpenShift groups and assign cluster permissions. Valid values : List of group objects, each containing: - name : OpenShift group name - users : List of GitHub usernames to add to the group - groups_cluster_rolebindings : List of cluster roles to bind to the group Impact : Creates OpenShift groups and assigns cluster roles, controlling user permissions in the cluster. Related variables : oauth.organizations Notes : - Groups enable role-based access control (RBAC) - Users must be members of specified organizations - Common cluster roles: cluster-admin , admin , edit , view - Groups are created if they don't exist oauth.groups.name \u00a4 Name of the OpenShift group to create. Required (when defining groups) Environment Variable: None (passed as variable) Default: None Purpose : Specifies the name of the OpenShift group for organizing users and assigning permissions. When to use : Required for each group definition in the oauth.groups list. Valid values : Valid Kubernetes group name (alphanumeric with hyphens, e.g., mas-admins , developers , viewers ). Impact : Group name is used in role bindings and for user organization. Related variables : oauth.groups.users , oauth.groups.groups_cluster_rolebindings Notes : - Use descriptive names that reflect the group's purpose - Common patterns: <project>-admins , <team>-developers - Group names must be unique within the cluster oauth.groups.users \u00a4 List of GitHub usernames to add to the group. Required (when defining groups) Environment Variable: None (passed as variable) Default: None Purpose : Specifies which GitHub users should be members of the OpenShift group. When to use : Required for each group definition. List all GitHub usernames that should have the group's permissions. Valid values : List of valid GitHub usernames (e.g., ['user1', 'user2', 'user3'] ). Impact : Listed users will be added to the group and inherit all associated cluster role bindings. Related variables : oauth.groups.name , oauth.organizations Notes : - Users must be members of the specified GitHub organizations - GitHub usernames are case-sensitive - Users are added to groups upon first login - Update list to add or remove user access oauth.groups.groups_cluster_rolebindings \u00a4 List of cluster roles to bind to the group. Required (when defining groups) Environment Variable: None (passed as variable) Default: None Purpose : Defines which cluster-level permissions the group members will have. When to use : Required for each group definition. Specify the cluster roles that match the group's intended access level. Valid values : List of valid OpenShift cluster role names. Common values: - cluster-admin - Full cluster administration - admin - Full access to project resources - edit - Modify project resources - view - Read-only access to project resources - Custom cluster roles Impact : Group members will have all permissions defined by the specified cluster roles. Related variables : oauth.groups.name , oauth.groups.users Notes : - Warning : cluster-admin grants full cluster control - Use least privilege principle when assigning roles - Multiple roles can be assigned to a single group - Verify roles exist: oc get clusterroles Example Playbook \u00a4 - hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.ocp_github_oauth License \u00a4 EPL-2.0","title":"ocp_github_oauth"},{"location":"roles/ocp_github_oauth/#ocp_github_oauth","text":"Configure OpenShift cluster authentication using GitHub OAuth. This role enables users to log into OpenShift using their GitHub credentials, with support for both public GitHub and GitHub Enterprise. It also manages user groups and cluster role bindings based on GitHub organization membership. Prerequisites : - GitHub OAuth application must be configured in your GitHub organization before using this role - OAuth application must use ibmgithub as the OAuth ID - Requires GitHub organization admin permissions The role creates OAuth configuration, manages user groups, and assigns cluster roles based on GitHub organization membership.","title":"ocp_github_oauth"},{"location":"roles/ocp_github_oauth/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_github_oauth/#oauthgithub_client_secret_value","text":"GitHub OAuth application client secret. Required Environment Variable: None (passed as variable) Default: None Purpose : Provides the client secret from the GitHub OAuth application configuration for secure authentication. When to use : Always required. Obtained from GitHub OAuth app settings. Valid values : Valid GitHub OAuth client secret string (generated by GitHub). Impact : Used to authenticate the OpenShift cluster with GitHub. Incorrect secret will prevent GitHub authentication. Related variables : oauth.github_client_id_value , oauth.github_hostname Notes : - Obtain from GitHub OAuth app settings in your organization - Keep secret secure, never commit to version control - Stored as Kubernetes secret in the cluster - Regenerate if compromised","title":"oauth.github_client_secret_value"},{"location":"roles/ocp_github_oauth/#oauthgithub_client_id_value","text":"GitHub OAuth application client ID. Required Environment Variable: None (passed as variable) Default: None Purpose : Provides the client ID from the GitHub OAuth application configuration. When to use : Always required. Obtained from GitHub OAuth app settings. Valid values : Valid GitHub OAuth client ID string (generated by GitHub). Impact : Identifies the OAuth application to GitHub during authentication flow. Related variables : oauth.github_client_secret_value , oauth.github_hostname Notes : - Obtain from GitHub OAuth app settings in your organization - Public value, but should still be managed securely - Must match the OAuth app configured with ibmgithub as the OAuth ID","title":"oauth.github_client_id_value"},{"location":"roles/ocp_github_oauth/#oauthgithub_hostname","text":"GitHub server hostname for authentication. Optional Environment Variable: None (passed as variable) Default: github.com (public GitHub) Purpose : Specifies the GitHub server to use for authentication, supporting both public GitHub and GitHub Enterprise. When to use : Set to your GitHub Enterprise hostname if using enterprise GitHub. Leave default for public GitHub. Valid values : - github.com - Public GitHub (default) - github.ibm.com - IBM GitHub Enterprise - Custom GitHub Enterprise hostname (e.g., github.example.com ) Impact : Determines which GitHub server handles authentication requests. Related variables : oauth.github_client_id_value , oauth.github_client_secret_value Notes : - Default github.com is for public GitHub - Use your enterprise hostname for GitHub Enterprise - Hostname must be accessible from the OpenShift cluster - Verify SSL certificates are properly configured","title":"oauth.github_hostname"},{"location":"roles/ocp_github_oauth/#oauthorganizations","text":"List of GitHub organizations for authentication. Required Environment Variable: None (passed as variable) Default: None Purpose : Defines which GitHub organizations are allowed to authenticate to the OpenShift cluster. When to use : Always required. Specify one or more GitHub organizations whose members can access the cluster. Valid values : List of valid GitHub organization names (e.g., ['ibm', 'redhat'] , ['myorg'] ). Impact : Only users who are members of the specified organizations can authenticate to the cluster. Related variables : oauth.groups Notes : - Users must be members of at least one listed organization - Organization membership is verified during each login - Multiple organizations can be specified for broader access - Organization names are case-sensitive","title":"oauth.organizations"},{"location":"roles/ocp_github_oauth/#oauthgroups","text":"List of OpenShift groups to create with GitHub user mappings. Optional Environment Variable: None (passed as variable) Default: None Purpose : Defines OpenShift groups to be created, their GitHub user members, and associated cluster role bindings. When to use : Use to map GitHub users to OpenShift groups and assign cluster permissions. Valid values : List of group objects, each containing: - name : OpenShift group name - users : List of GitHub usernames to add to the group - groups_cluster_rolebindings : List of cluster roles to bind to the group Impact : Creates OpenShift groups and assigns cluster roles, controlling user permissions in the cluster. Related variables : oauth.organizations Notes : - Groups enable role-based access control (RBAC) - Users must be members of specified organizations - Common cluster roles: cluster-admin , admin , edit , view - Groups are created if they don't exist","title":"oauth.groups"},{"location":"roles/ocp_github_oauth/#oauthgroupsname","text":"Name of the OpenShift group to create. Required (when defining groups) Environment Variable: None (passed as variable) Default: None Purpose : Specifies the name of the OpenShift group for organizing users and assigning permissions. When to use : Required for each group definition in the oauth.groups list. Valid values : Valid Kubernetes group name (alphanumeric with hyphens, e.g., mas-admins , developers , viewers ). Impact : Group name is used in role bindings and for user organization. Related variables : oauth.groups.users , oauth.groups.groups_cluster_rolebindings Notes : - Use descriptive names that reflect the group's purpose - Common patterns: <project>-admins , <team>-developers - Group names must be unique within the cluster","title":"oauth.groups.name"},{"location":"roles/ocp_github_oauth/#oauthgroupsusers","text":"List of GitHub usernames to add to the group. Required (when defining groups) Environment Variable: None (passed as variable) Default: None Purpose : Specifies which GitHub users should be members of the OpenShift group. When to use : Required for each group definition. List all GitHub usernames that should have the group's permissions. Valid values : List of valid GitHub usernames (e.g., ['user1', 'user2', 'user3'] ). Impact : Listed users will be added to the group and inherit all associated cluster role bindings. Related variables : oauth.groups.name , oauth.organizations Notes : - Users must be members of the specified GitHub organizations - GitHub usernames are case-sensitive - Users are added to groups upon first login - Update list to add or remove user access","title":"oauth.groups.users"},{"location":"roles/ocp_github_oauth/#oauthgroupsgroups_cluster_rolebindings","text":"List of cluster roles to bind to the group. Required (when defining groups) Environment Variable: None (passed as variable) Default: None Purpose : Defines which cluster-level permissions the group members will have. When to use : Required for each group definition. Specify the cluster roles that match the group's intended access level. Valid values : List of valid OpenShift cluster role names. Common values: - cluster-admin - Full cluster administration - admin - Full access to project resources - edit - Modify project resources - view - Read-only access to project resources - Custom cluster roles Impact : Group members will have all permissions defined by the specified cluster roles. Related variables : oauth.groups.name , oauth.groups.users Notes : - Warning : cluster-admin grants full cluster control - Use least privilege principle when assigning roles - Multiple roles can be assigned to a single group - Verify roles exist: oc get clusterroles","title":"oauth.groups.groups_cluster_rolebindings"},{"location":"roles/ocp_github_oauth/#example-playbook","text":"- hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.ocp_github_oauth","title":"Example Playbook"},{"location":"roles/ocp_github_oauth/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_idms/","text":"ocp_idms \u00a4 Installs an ImageDigestMirrorSet (IDMS)for IBM Maximo Application Suite's Maximo Operator Catalog. Also install IDMS suitable for the Red Hat Operator Catalogs created by mirror_ocp . If there are legacy ImageContentSourcePolicies installed by previous versions of this role, they will be deleted. If PRODUCT_FAMILY is aiservice then it will install an ImageTagMirrorSet for OpenDataHub Warning This role doesn't work on IBMCloud ROKS. IBM Cloud RedHat OpenShift Service does not implement support for ImageDigestMirrorSet . If you want to use image mirroring you must manually configure each worker node individually using the IBM Cloud command line tool. IBM Maximo Operator Catalog Content All content used in the MAS install is sourced from three registries: icr.io , cp.icr.io , & quay.io : icr.io/cpopen All IBM operators icr.io/ibm-truststore-mgr IBM truststore manager worker image icr.io/ibm-sls IBM SLS content icr.io/db2u IBM Db2 Universal operator content cp.icr.io/cp All IBM entitled container images quay.io/opencloudio IBM common services quay.io/mongodb MongoDb Community Edition Operator & associated container images quay.io/amlen Eclipse Amlen - Message Broker for IoT/Mobile/Web quay.io/ibmmas Non-product IBM Maximo Application Suite images (e.g. MAS CLI) Red Hat Operator Catalog Content All content from the subset of the Red Hat operator catalogs supported by mirror_ocp is sourced from eight registries: icr.io , docker.io , quay.io , gcr.io , ghcr.io , nvcr.io , registry.connect.redhat.com , and registry.redhat.io : icr.io/cpopen docker.io/grafana quay.io/community-operator-pipeline-prod quay.io/operator-pipeline-prod quay.io/openshift-community-operators quay.io/strimzi quay.io/rh-marketplace gcr.io/kubebuilder ghcr.io/grafana ghcr.io/open-telemetry nvcr.io/nvidia registry.connect.redhat.com/crunchydata registry.connect.redhat.com/nvidia registry.connect.redhat.com/turbonomic registry.connect.redhat.com/rh-marketplace registry.redhat.io/openshift4 registry.redhat.io/source-to-image registry.redhat.io/odf4 registry.redhat.io/cert-manager registry.redhat.io/rhceph registry.redhat.io/amq-streams registry.redhat.io/ubi8 registry.redhat.io/openshift-pipelines registry.redhat.io/openshift-serverless-1 registry.redhat.io/lvms4 Note A content source policy for this content is only configured when setup_redhat_catalogs is set to True . If you are managing the Red Hat Operator Catalogs yourself the content therein may well be different depending how you have configured mirroring. Role Variables - General \u00a4 product_family \u00a4 Product family for ImageDigestMirrorSet configuration. Optional Environment Variable: PRODUCT_FAMILY Default: mas Purpose : Specifies which product family's ImageDigestMirrorSet (IDMS) to create. Different product families have different image source registries. When to use : - Use default ( mas ) for IBM Maximo Application Suite deployments - Set to aiservice for AI Service deployments (creates ImageTagMirrorSet for OpenDataHub) - Determines which image registries are configured in the IDMS Valid values : mas , aiservice Impact : - mas : Creates IDMS for MAS-related registries (icr.io, cp.icr.io, quay.io) - aiservice : Creates ImageTagMirrorSet for OpenDataHub registries Related variables : - registry_prefix : Prefix for IBM content in target registry Note : MAS and AI Service have different image source registries. The IDMS/ITMS configuration varies based on the product family. setup_redhat_release \u00a4 Enable ImageDigestMirrorSet for Red Hat release content. Optional Environment Variable: SETUP_REDHAT_RELEASE Default: false Purpose : Controls whether to create an ImageDigestMirrorSet for mirrored Red Hat OpenShift platform release content. Required when using mirrored OpenShift platform images. When to use : - Set to true when you've mirrored OpenShift platform images with mirror_ocp role - Leave as false (default) if not using mirrored platform images - Only needed for air-gapped OpenShift installations or upgrades Valid values : true , false Impact : - true : Creates IDMS named ibm-mas-redhat-release for platform images - false : No IDMS created for platform images Related variables : - registry_prefix_redhat : Prefix for Red Hat content in target registry - setup_redhat_catalogs : Related but separate (for operator catalogs) Note : This creates an additional IDMS policy. Only enable if you've mirrored OpenShift platform images using the mirror_ocp role with mirror_redhat_platform=true . setup_redhat_catalogs \u00a4 Enable CatalogSources and ImageDigestMirrorSet for Red Hat operator catalogs. Optional Environment Variable: SETUP_REDHAT_CATALOGS Default: false Purpose : Controls whether to create CatalogSources and ImageDigestMirrorSet for mirrored Red Hat operator catalog content. Required when using mirrored Red Hat operators. When to use : - Set to true when you've mirrored Red Hat operator catalogs with mirror_ocp role - Leave as false (default) if not using mirrored operator catalogs - Required for air-gapped deployments using Red Hat operators Valid values : true , false Impact : - true : Creates CatalogSources and IDMS named ibm-mas-redhat-catalogs for operator images - false : No CatalogSources or IDMS created for operator catalogs Related variables : - registry_prefix_redhat : Prefix for Red Hat content in target registry - redhat_catalogs_prefix : Optional prefix for CatalogSource names - setup_redhat_release : Related but separate (for platform images) Note : This creates CatalogSources for certified-operator-index, community-operator-index, and redhat-operator-index. Only enable if you've mirrored operator catalogs using the mirror_ocp role with mirror_redhat_operators=true . Role Variables - Target Registry \u00a4 registry_private_host \u00a4 Private hostname for the mirror registry. Required Environment Variable: REGISTRY_PRIVATE_HOST Default: None Purpose : Specifies the private/internal hostname of the mirror registry accessible from within the OpenShift cluster. Used in ImageDigestMirrorSet to redirect image pulls. When to use : - Always required for IDMS configuration - Must be the hostname accessible from cluster nodes - Typically an internal/private hostname or IP address Valid values : Valid hostname or IP address accessible from cluster (e.g., registry.internal.example.com , 10.0.0.100 ) Impact : Cluster nodes will pull images from this registry. Incorrect hostname will cause image pull failures. Related variables : - registry_private_port : Port for this registry - registry_private_ca_file : CA certificate for this registry Note : This must be the hostname accessible from within the cluster, not necessarily the public hostname. For disconnected environments, this is typically an internal registry. registry_private_port \u00a4 Private port for the mirror registry. Optional Environment Variable: REGISTRY_PRIVATE_PORT Default: None Purpose : Specifies the private/internal port of the mirror registry accessible from within the OpenShift cluster. When to use : - Set if registry uses a non-standard port - Leave unset if registry uses standard HTTPS port (443) - Must match the port accessible from cluster nodes Valid values : Valid port number (e.g., 443 , 5000 , 32500 ) Impact : Cluster nodes will pull images from this port. Incorrect port will cause image pull failures. Related variables : - registry_private_host : Hostname for this registry Note : If unset, the registry URL will not include a port (assumes standard HTTPS port 443). registry_private_ca_file \u00a4 Path to registry CA certificate file. Required Environment Variable: REGISTRY_PRIVATE_CA_FILE Default: None Purpose : Specifies the path to the CA certificate file for the mirror registry. Required for cluster nodes to trust the registry's TLS certificate. When to use : - Always required for IDMS configuration - Must be the CA certificate that signed the registry's TLS certificate - Required even if registry uses self-signed certificates Valid values : Absolute path to CA certificate file (e.g., ~/registry-ca.crt , /tmp/registry-ca.pem ) Impact : CA certificate is added to cluster nodes' trust store. Without it, nodes cannot pull images from the registry. Related variables : - registry_private_host : Registry using this CA certificate Note : The CA certificate is added to all cluster nodes via MachineConfig. This causes nodes to reboot. Ensure the certificate is valid and matches the registry's TLS certificate. registry_username \u00a4 Username for mirror registry authentication. Required Environment Variable: REGISTRY_USERNAME Default: None Purpose : Provides the username for authenticating to the mirror registry. Used to create pull secrets for cluster nodes. When to use : - Always required for IDMS configuration - Must have pull permissions from the mirror registry - Credentials are stored in cluster pull secret Valid values : Valid username for the mirror registry Impact : Used to authenticate image pulls from the mirror registry. Without valid credentials, image pulls will fail. Related variables : - registry_password : Password paired with this username Note : Credentials are added to the cluster's global pull secret. Keep credentials secure. registry_password \u00a4 Password for mirror registry authentication. Required Environment Variable: REGISTRY_PASSWORD Default: None Purpose : Provides the password for authenticating to the mirror registry. Used to create pull secrets for cluster nodes. When to use : - Always required for IDMS configuration - Must correspond to the provided username - Credentials are stored in cluster pull secret Valid values : Valid password for the mirror registry Impact : Used to authenticate image pulls from the mirror registry. Without valid credentials, image pulls will fail. Related variables : - registry_username : Username paired with this password Note : Credentials are added to the cluster's global pull secret. Keep passwords secure. Never commit to version control. registry_prefix \u00a4 Path prefix for IBM content in mirror registry. Optional Environment Variable: REGISTRY_PREFIX Default: None Purpose : Specifies an optional path prefix for IBM Maximo Operator Catalog images in the mirror registry. Helps organize registry content. When to use : - Leave unset if images are at registry root - Set to match the prefix used when mirroring with mirror_images role - Recommended: Use catalog datestamp (e.g., mas-241107 , mas-241205 ) Valid values : Valid registry path (e.g., mas-241107 , mas-241205 , ibm-mas ) Impact : IDMS will redirect image pulls to {host}:{port}/{prefix}/{reponame} . Must match the actual image locations in the registry. Related variables : - registry_private_host : Registry containing these images - registry_prefix_redhat : Separate prefix for Red Hat content Note : Must match the prefix used when mirroring images. Using datestamp prefixes helps organize multiple mirror versions in the same registry. registry_prefix_redhat \u00a4 Path prefix for Red Hat content in mirror registry. Optional (Required when setup_redhat_release=true or setup_redhat_catalogs=true ) Environment Variable: REGISTRY_PREFIX_REDHAT Default: None Purpose : Specifies an optional path prefix for Red Hat Release and Operator Catalog images in the mirror registry. Helps organize registry content. When to use : - Required when setup_redhat_release=true or setup_redhat_catalogs=true - Set to match the prefix used when mirroring with mirror_ocp role - Recommended: Use OpenShift release (e.g., ocp-412 , ocp-414 ) Valid values : Valid registry path (e.g., ocp-412 , ocp-414 , ocp-419 ) Impact : IDMS will redirect Red Hat image pulls to {host}:{port}/{prefix}/{reponame} . Must match the actual image locations in the registry. Related variables : - setup_redhat_release : Whether to create IDMS for platform images - setup_redhat_catalogs : Whether to create IDMS for operator catalogs - registry_prefix : Separate prefix for IBM content Note : Must match the prefix used when mirroring Red Hat content with mirror_ocp role. Using OpenShift release prefixes helps organize multiple OCP versions in the same registry. redhat_catalogs_prefix \u00a4 Prefix for Red Hat CatalogSource names. Optional Environment Variable: REDHAT_CATALOGS_PREFIX Default: None Purpose : Specifies an optional prefix for the CatalogSource names created for Red Hat operator catalogs. Helps avoid naming conflicts. When to use : - Leave unset for default CatalogSource names - Set to add a prefix to CatalogSource names (e.g., ibm-mas ) - Only applies when setup_redhat_catalogs=true Valid values : Valid Kubernetes resource name prefix (e.g., ibm-mas , mas , custom ) Impact : - When set to ibm-mas : Creates ibm-mas-certified-operator-index , ibm-mas-community-operator-index , ibm-mas-redhat-operator-index - When unset: Creates certified-operator-index , community-operator-index , redhat-operator-index Related variables : - setup_redhat_catalogs : Must be true for this to apply Note : Use a prefix if you need to distinguish these CatalogSources from others in the cluster or to avoid naming conflicts. machine_config_multiupdate \u00a4 Enable parallel node updates during MachineConfig application. Optional Environment Variable: MACHINE_CONFIG_MULTIUPDATE Default: false Purpose : Controls whether multiple worker nodes can be updated in parallel when applying MachineConfig changes (for CA certificate installation). Speeds up initial setup but requires careful consideration. When to use : - Leave as false (default) for production environments - Set to true only during initial cluster setup with lightly loaded nodes - Only recommended when nodes can be safely drained in parallel Valid values : true , false Impact : - true : Multiple worker nodes updated in parallel (faster but riskier) - false : Worker nodes updated one at a time (slower but safer) Related variables : - registry_private_ca_file : CA certificate that triggers MachineConfig updates Note : WARNING - Only enable during initial setup when nodes are lightly loaded. In production, leave as false to ensure workload availability during node updates. MachineConfig changes cause node reboots. Example Playbook \u00a4 - hosts: localhost vars: registry_private_host: myocp-5f1320191125833da1cac8216c06779e-0000.us-south.containers.appdomain.cloud registry_private_port: 32500 registry_private_ca_file: ~/registry-ca.crt registry_username: admin registry_password: 8934jk77s862! # Not a real password, don't worry security folks setup_redhat_catalogs: true roles: - ibm.mas_devops.ocp_contentsourcepolicy License \u00a4 EPL-2.0","title":"ocp_idms"},{"location":"roles/ocp_idms/#ocp_idms","text":"Installs an ImageDigestMirrorSet (IDMS)for IBM Maximo Application Suite's Maximo Operator Catalog. Also install IDMS suitable for the Red Hat Operator Catalogs created by mirror_ocp . If there are legacy ImageContentSourcePolicies installed by previous versions of this role, they will be deleted. If PRODUCT_FAMILY is aiservice then it will install an ImageTagMirrorSet for OpenDataHub Warning This role doesn't work on IBMCloud ROKS. IBM Cloud RedHat OpenShift Service does not implement support for ImageDigestMirrorSet . If you want to use image mirroring you must manually configure each worker node individually using the IBM Cloud command line tool. IBM Maximo Operator Catalog Content All content used in the MAS install is sourced from three registries: icr.io , cp.icr.io , & quay.io : icr.io/cpopen All IBM operators icr.io/ibm-truststore-mgr IBM truststore manager worker image icr.io/ibm-sls IBM SLS content icr.io/db2u IBM Db2 Universal operator content cp.icr.io/cp All IBM entitled container images quay.io/opencloudio IBM common services quay.io/mongodb MongoDb Community Edition Operator & associated container images quay.io/amlen Eclipse Amlen - Message Broker for IoT/Mobile/Web quay.io/ibmmas Non-product IBM Maximo Application Suite images (e.g. MAS CLI) Red Hat Operator Catalog Content All content from the subset of the Red Hat operator catalogs supported by mirror_ocp is sourced from eight registries: icr.io , docker.io , quay.io , gcr.io , ghcr.io , nvcr.io , registry.connect.redhat.com , and registry.redhat.io : icr.io/cpopen docker.io/grafana quay.io/community-operator-pipeline-prod quay.io/operator-pipeline-prod quay.io/openshift-community-operators quay.io/strimzi quay.io/rh-marketplace gcr.io/kubebuilder ghcr.io/grafana ghcr.io/open-telemetry nvcr.io/nvidia registry.connect.redhat.com/crunchydata registry.connect.redhat.com/nvidia registry.connect.redhat.com/turbonomic registry.connect.redhat.com/rh-marketplace registry.redhat.io/openshift4 registry.redhat.io/source-to-image registry.redhat.io/odf4 registry.redhat.io/cert-manager registry.redhat.io/rhceph registry.redhat.io/amq-streams registry.redhat.io/ubi8 registry.redhat.io/openshift-pipelines registry.redhat.io/openshift-serverless-1 registry.redhat.io/lvms4 Note A content source policy for this content is only configured when setup_redhat_catalogs is set to True . If you are managing the Red Hat Operator Catalogs yourself the content therein may well be different depending how you have configured mirroring.","title":"ocp_idms"},{"location":"roles/ocp_idms/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/ocp_idms/#product_family","text":"Product family for ImageDigestMirrorSet configuration. Optional Environment Variable: PRODUCT_FAMILY Default: mas Purpose : Specifies which product family's ImageDigestMirrorSet (IDMS) to create. Different product families have different image source registries. When to use : - Use default ( mas ) for IBM Maximo Application Suite deployments - Set to aiservice for AI Service deployments (creates ImageTagMirrorSet for OpenDataHub) - Determines which image registries are configured in the IDMS Valid values : mas , aiservice Impact : - mas : Creates IDMS for MAS-related registries (icr.io, cp.icr.io, quay.io) - aiservice : Creates ImageTagMirrorSet for OpenDataHub registries Related variables : - registry_prefix : Prefix for IBM content in target registry Note : MAS and AI Service have different image source registries. The IDMS/ITMS configuration varies based on the product family.","title":"product_family"},{"location":"roles/ocp_idms/#setup_redhat_release","text":"Enable ImageDigestMirrorSet for Red Hat release content. Optional Environment Variable: SETUP_REDHAT_RELEASE Default: false Purpose : Controls whether to create an ImageDigestMirrorSet for mirrored Red Hat OpenShift platform release content. Required when using mirrored OpenShift platform images. When to use : - Set to true when you've mirrored OpenShift platform images with mirror_ocp role - Leave as false (default) if not using mirrored platform images - Only needed for air-gapped OpenShift installations or upgrades Valid values : true , false Impact : - true : Creates IDMS named ibm-mas-redhat-release for platform images - false : No IDMS created for platform images Related variables : - registry_prefix_redhat : Prefix for Red Hat content in target registry - setup_redhat_catalogs : Related but separate (for operator catalogs) Note : This creates an additional IDMS policy. Only enable if you've mirrored OpenShift platform images using the mirror_ocp role with mirror_redhat_platform=true .","title":"setup_redhat_release"},{"location":"roles/ocp_idms/#setup_redhat_catalogs","text":"Enable CatalogSources and ImageDigestMirrorSet for Red Hat operator catalogs. Optional Environment Variable: SETUP_REDHAT_CATALOGS Default: false Purpose : Controls whether to create CatalogSources and ImageDigestMirrorSet for mirrored Red Hat operator catalog content. Required when using mirrored Red Hat operators. When to use : - Set to true when you've mirrored Red Hat operator catalogs with mirror_ocp role - Leave as false (default) if not using mirrored operator catalogs - Required for air-gapped deployments using Red Hat operators Valid values : true , false Impact : - true : Creates CatalogSources and IDMS named ibm-mas-redhat-catalogs for operator images - false : No CatalogSources or IDMS created for operator catalogs Related variables : - registry_prefix_redhat : Prefix for Red Hat content in target registry - redhat_catalogs_prefix : Optional prefix for CatalogSource names - setup_redhat_release : Related but separate (for platform images) Note : This creates CatalogSources for certified-operator-index, community-operator-index, and redhat-operator-index. Only enable if you've mirrored operator catalogs using the mirror_ocp role with mirror_redhat_operators=true .","title":"setup_redhat_catalogs"},{"location":"roles/ocp_idms/#role-variables-target-registry","text":"","title":"Role Variables - Target Registry"},{"location":"roles/ocp_idms/#registry_private_host","text":"Private hostname for the mirror registry. Required Environment Variable: REGISTRY_PRIVATE_HOST Default: None Purpose : Specifies the private/internal hostname of the mirror registry accessible from within the OpenShift cluster. Used in ImageDigestMirrorSet to redirect image pulls. When to use : - Always required for IDMS configuration - Must be the hostname accessible from cluster nodes - Typically an internal/private hostname or IP address Valid values : Valid hostname or IP address accessible from cluster (e.g., registry.internal.example.com , 10.0.0.100 ) Impact : Cluster nodes will pull images from this registry. Incorrect hostname will cause image pull failures. Related variables : - registry_private_port : Port for this registry - registry_private_ca_file : CA certificate for this registry Note : This must be the hostname accessible from within the cluster, not necessarily the public hostname. For disconnected environments, this is typically an internal registry.","title":"registry_private_host"},{"location":"roles/ocp_idms/#registry_private_port","text":"Private port for the mirror registry. Optional Environment Variable: REGISTRY_PRIVATE_PORT Default: None Purpose : Specifies the private/internal port of the mirror registry accessible from within the OpenShift cluster. When to use : - Set if registry uses a non-standard port - Leave unset if registry uses standard HTTPS port (443) - Must match the port accessible from cluster nodes Valid values : Valid port number (e.g., 443 , 5000 , 32500 ) Impact : Cluster nodes will pull images from this port. Incorrect port will cause image pull failures. Related variables : - registry_private_host : Hostname for this registry Note : If unset, the registry URL will not include a port (assumes standard HTTPS port 443).","title":"registry_private_port"},{"location":"roles/ocp_idms/#registry_private_ca_file","text":"Path to registry CA certificate file. Required Environment Variable: REGISTRY_PRIVATE_CA_FILE Default: None Purpose : Specifies the path to the CA certificate file for the mirror registry. Required for cluster nodes to trust the registry's TLS certificate. When to use : - Always required for IDMS configuration - Must be the CA certificate that signed the registry's TLS certificate - Required even if registry uses self-signed certificates Valid values : Absolute path to CA certificate file (e.g., ~/registry-ca.crt , /tmp/registry-ca.pem ) Impact : CA certificate is added to cluster nodes' trust store. Without it, nodes cannot pull images from the registry. Related variables : - registry_private_host : Registry using this CA certificate Note : The CA certificate is added to all cluster nodes via MachineConfig. This causes nodes to reboot. Ensure the certificate is valid and matches the registry's TLS certificate.","title":"registry_private_ca_file"},{"location":"roles/ocp_idms/#registry_username","text":"Username for mirror registry authentication. Required Environment Variable: REGISTRY_USERNAME Default: None Purpose : Provides the username for authenticating to the mirror registry. Used to create pull secrets for cluster nodes. When to use : - Always required for IDMS configuration - Must have pull permissions from the mirror registry - Credentials are stored in cluster pull secret Valid values : Valid username for the mirror registry Impact : Used to authenticate image pulls from the mirror registry. Without valid credentials, image pulls will fail. Related variables : - registry_password : Password paired with this username Note : Credentials are added to the cluster's global pull secret. Keep credentials secure.","title":"registry_username"},{"location":"roles/ocp_idms/#registry_password","text":"Password for mirror registry authentication. Required Environment Variable: REGISTRY_PASSWORD Default: None Purpose : Provides the password for authenticating to the mirror registry. Used to create pull secrets for cluster nodes. When to use : - Always required for IDMS configuration - Must correspond to the provided username - Credentials are stored in cluster pull secret Valid values : Valid password for the mirror registry Impact : Used to authenticate image pulls from the mirror registry. Without valid credentials, image pulls will fail. Related variables : - registry_username : Username paired with this password Note : Credentials are added to the cluster's global pull secret. Keep passwords secure. Never commit to version control.","title":"registry_password"},{"location":"roles/ocp_idms/#registry_prefix","text":"Path prefix for IBM content in mirror registry. Optional Environment Variable: REGISTRY_PREFIX Default: None Purpose : Specifies an optional path prefix for IBM Maximo Operator Catalog images in the mirror registry. Helps organize registry content. When to use : - Leave unset if images are at registry root - Set to match the prefix used when mirroring with mirror_images role - Recommended: Use catalog datestamp (e.g., mas-241107 , mas-241205 ) Valid values : Valid registry path (e.g., mas-241107 , mas-241205 , ibm-mas ) Impact : IDMS will redirect image pulls to {host}:{port}/{prefix}/{reponame} . Must match the actual image locations in the registry. Related variables : - registry_private_host : Registry containing these images - registry_prefix_redhat : Separate prefix for Red Hat content Note : Must match the prefix used when mirroring images. Using datestamp prefixes helps organize multiple mirror versions in the same registry.","title":"registry_prefix"},{"location":"roles/ocp_idms/#registry_prefix_redhat","text":"Path prefix for Red Hat content in mirror registry. Optional (Required when setup_redhat_release=true or setup_redhat_catalogs=true ) Environment Variable: REGISTRY_PREFIX_REDHAT Default: None Purpose : Specifies an optional path prefix for Red Hat Release and Operator Catalog images in the mirror registry. Helps organize registry content. When to use : - Required when setup_redhat_release=true or setup_redhat_catalogs=true - Set to match the prefix used when mirroring with mirror_ocp role - Recommended: Use OpenShift release (e.g., ocp-412 , ocp-414 ) Valid values : Valid registry path (e.g., ocp-412 , ocp-414 , ocp-419 ) Impact : IDMS will redirect Red Hat image pulls to {host}:{port}/{prefix}/{reponame} . Must match the actual image locations in the registry. Related variables : - setup_redhat_release : Whether to create IDMS for platform images - setup_redhat_catalogs : Whether to create IDMS for operator catalogs - registry_prefix : Separate prefix for IBM content Note : Must match the prefix used when mirroring Red Hat content with mirror_ocp role. Using OpenShift release prefixes helps organize multiple OCP versions in the same registry.","title":"registry_prefix_redhat"},{"location":"roles/ocp_idms/#redhat_catalogs_prefix","text":"Prefix for Red Hat CatalogSource names. Optional Environment Variable: REDHAT_CATALOGS_PREFIX Default: None Purpose : Specifies an optional prefix for the CatalogSource names created for Red Hat operator catalogs. Helps avoid naming conflicts. When to use : - Leave unset for default CatalogSource names - Set to add a prefix to CatalogSource names (e.g., ibm-mas ) - Only applies when setup_redhat_catalogs=true Valid values : Valid Kubernetes resource name prefix (e.g., ibm-mas , mas , custom ) Impact : - When set to ibm-mas : Creates ibm-mas-certified-operator-index , ibm-mas-community-operator-index , ibm-mas-redhat-operator-index - When unset: Creates certified-operator-index , community-operator-index , redhat-operator-index Related variables : - setup_redhat_catalogs : Must be true for this to apply Note : Use a prefix if you need to distinguish these CatalogSources from others in the cluster or to avoid naming conflicts.","title":"redhat_catalogs_prefix"},{"location":"roles/ocp_idms/#machine_config_multiupdate","text":"Enable parallel node updates during MachineConfig application. Optional Environment Variable: MACHINE_CONFIG_MULTIUPDATE Default: false Purpose : Controls whether multiple worker nodes can be updated in parallel when applying MachineConfig changes (for CA certificate installation). Speeds up initial setup but requires careful consideration. When to use : - Leave as false (default) for production environments - Set to true only during initial cluster setup with lightly loaded nodes - Only recommended when nodes can be safely drained in parallel Valid values : true , false Impact : - true : Multiple worker nodes updated in parallel (faster but riskier) - false : Worker nodes updated one at a time (slower but safer) Related variables : - registry_private_ca_file : CA certificate that triggers MachineConfig updates Note : WARNING - Only enable during initial setup when nodes are lightly loaded. In production, leave as false to ensure workload availability during node updates. MachineConfig changes cause node reboots.","title":"machine_config_multiupdate"},{"location":"roles/ocp_idms/#example-playbook","text":"- hosts: localhost vars: registry_private_host: myocp-5f1320191125833da1cac8216c06779e-0000.us-south.containers.appdomain.cloud registry_private_port: 32500 registry_private_ca_file: ~/registry-ca.crt registry_username: admin registry_password: 8934jk77s862! # Not a real password, don't worry security folks setup_redhat_catalogs: true roles: - ibm.mas_devops.ocp_contentsourcepolicy","title":"Example Playbook"},{"location":"roles/ocp_idms/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_login/","text":"ocp_login \u00a4 This role provides support to login to a cluster using the oc CLI by looking up cluster information from the infrastructure provider's APIs, it also supports setting ocp_server and ocp_token directly to support login to any Kubernetes cluster. Role Variables - General \u00a4 cluster_name \u00a4 Cluster name for credential lookup. Required (unless ocp_server and ocp_token are set) Environment Variable: CLUSTER_NAME Default: None Purpose : Identifies the cluster to login to by name. Used to automatically lookup login credentials from the infrastructure provider's API. When to use : - Use with cluster_type for automatic credential lookup - Alternative to manually providing ocp_server and ocp_token - Recommended for managed clusters (ROKS, ROSA, Fyre) Valid values : String matching your cluster name in the provider's system Impact : Combined with cluster_type , automatically retrieves cluster server URL and login token from provider API. Related variables : - cluster_type : Required with this variable (roks, fyre, rosa) - ocp_server / ocp_token : Alternative manual login method Note : Requires provider-specific credentials (e.g., ibmcloud_apikey for ROKS, rosa_token for ROSA, fyre_apikey for Fyre). cluster_type \u00a4 Infrastructure provider type for cluster. Required (unless ocp_server and ocp_token are set) Environment Variable: CLUSTER_TYPE Default: None Purpose : Specifies the infrastructure provider type to determine which API to use for credential lookup. When to use : - Use with cluster_name for automatic credential lookup - Required for managed cluster login - Each type requires specific provider credentials Valid values : roks , fyre , rosa - roks : IBM Cloud Red Hat OpenShift Kubernetes Service - fyre : IBM DevIT Fyre clusters - rosa : AWS Red Hat OpenShift Service on AWS Impact : Determines which provider API is called to retrieve cluster credentials. Each type requires different authentication variables. Related variables : - cluster_name : Cluster to lookup - ibmcloud_apikey : Required for roks - fyre_username / fyre_apikey : Required for fyre - rosa_token : Required for rosa Note : Alternative to using ocp_server and ocp_token for direct login. ocp_server \u00a4 OpenShift server URL for direct login. Required (unless cluster_name and cluster_type are set) Environment Variable: OCP_SERVER Default: None Purpose : Specifies the OpenShift API server URL for direct cluster login without provider API lookup. When to use : - Use with ocp_token for direct login - When cluster is not managed by supported providers - For custom or on-premises clusters - Alternative to cluster_name / cluster_type approach Valid values : Full OpenShift API server URL (e.g., https://api.cluster.example.com:6443 ) Impact : Used directly for oc login command. Bypasses provider API credential lookup. Related variables : - ocp_token : Required with this variable - cluster_name / cluster_type : Alternative automatic lookup method Note : Both ocp_server and ocp_token must be provided together for direct login. This method works with any Kubernetes cluster. ocp_token \u00a4 Authentication token for direct login. Required (unless cluster_name and cluster_type are set) Environment Variable: OCP_TOKEN Default: None Purpose : Provides the authentication token for direct cluster login without provider API lookup. When to use : - Use with ocp_server for direct login - When cluster is not managed by supported providers - For service account tokens or manually obtained tokens - Alternative to cluster_name / cluster_type approach Valid values : Valid OpenShift authentication token string Impact : Used directly for oc login --token command. Bypasses provider API credential lookup. Related variables : - ocp_server : Required with this variable - cluster_name / cluster_type : Alternative automatic lookup method Note : SECURITY - Both ocp_server and ocp_token must be provided together. Token should be kept secure and not committed to version control. This method works with any Kubernetes cluster. Role Variables - IBMCloud ROKS \u00a4 ibmcloud_apikey \u00a4 IBM Cloud API key for ROKS authentication. Required (when cluster_type=roks ) Environment Variable: IBMCLOUD_APIKEY Default: None Purpose : Provides IBM Cloud API key for authenticating with IBM Cloud to retrieve ROKS cluster credentials. When to use : - Required when cluster_type=roks - Used to authenticate with IBM Cloud API - Enables automatic cluster credential lookup Valid values : Valid IBM Cloud API key string Impact : Used to authenticate with IBM Cloud and retrieve cluster server URL and login token for ROKS clusters. Related variables : - cluster_type : Must be roks - cluster_name : ROKS cluster name to lookup - ibmcloud_endpoint : Optional API endpoint override Note : SECURITY - API key should be kept secure and not committed to version control. Obtain from IBM Cloud IAM. ibmcloud_endpoint \u00a4 IBM Cloud API endpoint URL. Optional Environment Variable: IBMCLOUD_ENDPOINT Default: https://cloud.ibm.com Purpose : Overrides the default IBM Cloud API endpoint for ROKS cluster credential lookup. When to use : - Use default for public IBM Cloud - Override for private or regional endpoints - Only applies when cluster_type=roks Valid values : Valid IBM Cloud API endpoint URL Impact : Determines which IBM Cloud API endpoint is used for authentication and cluster lookup. Related variables : - cluster_type : Must be roks - ibmcloud_apikey : Required for authentication Note : The default public endpoint works for most deployments. Override only for specific regional or private cloud requirements. Role Variables - IBM DevIT Fyre \u00a4 fyre_username \u00a4 IBM DevIT Fyre username. Required (when cluster_type=fyre ) Environment Variable: FYRE_USERNAME Default: None Purpose : Provides Fyre username for authenticating with IBM DevIT Fyre to retrieve cluster credentials. When to use : - Required when cluster_type=fyre - Used with fyre_apikey for Fyre authentication - Enables automatic cluster credential lookup Valid values : Valid Fyre username string Impact : Used to authenticate with Fyre API and retrieve cluster server URL and login token. Related variables : - cluster_type : Must be fyre - fyre_apikey : Required for authentication - cluster_name : Fyre cluster name to lookup - fyre_site : Fyre site location Note : Fyre is IBM's internal development and test infrastructure. Requires IBM internal credentials. fyre_apikey \u00a4 IBM DevIT Fyre API key. Required (when cluster_type=fyre ) Environment Variable: FYRE_APIKEY Default: None Purpose : Provides Fyre API key for authenticating with IBM DevIT Fyre to retrieve cluster credentials. When to use : - Required when cluster_type=fyre - Used with fyre_username for Fyre authentication - Enables automatic cluster credential lookup Valid values : Valid Fyre API key string Impact : Used to authenticate with Fyre API and retrieve cluster server URL and login token. Related variables : - cluster_type : Must be fyre - fyre_username : Required for authentication - cluster_name : Fyre cluster name to lookup - fyre_site : Fyre site location Note : SECURITY - API key should be kept secure. Fyre is IBM's internal development and test infrastructure. fyre_site \u00a4 Fyre site location for cluster. Optional Environment Variable: FYRE_SITE Default: svl Purpose : Specifies which Fyre site the cluster was provisioned in for proper API routing. When to use : - Use default ( svl ) for most Fyre clusters - Override for clusters in other sites (e.g., rtp ) - Only applies when cluster_type=fyre Valid values : Fyre site codes (e.g., svl , rtp ) Impact : Determines which Fyre site API endpoint is used for cluster lookup. Related variables : - cluster_type : Must be fyre - enable_ipv6 : Required for RTP site Note : SVL (San Jose Valley) is the default site. RTP (Research Triangle Park) requires IPv6 enablement. enable_ipv6 \u00a4 Enable IPv6 for Fyre RTP site. Optional Environment Variable: ENABLE_IPV6 Default: false Purpose : Enables IPv6 networking for Fyre clusters at the RTP (Research Triangle Park) site. When to use : - Set to true only for Fyre RTP site clusters - Leave as false for all other sites (including SVL) - Only applies when cluster_type=fyre Valid values : true , false Impact : Configures network settings for IPv6 connectivity to RTP site clusters. Related variables : - cluster_type : Must be fyre - fyre_site : Should be rtp when this is true Note : Only required for Fyre RTP site. SVL and other sites use IPv4. Role Variables - AWS ROSA \u00a4 rosa_token \u00a4 AWS ROSA authentication token. Required (when cluster_type=rosa ) Environment Variable: ROSA_TOKEN Default: None Purpose : Provides ROSA (Red Hat OpenShift Service on AWS) authentication token for retrieving cluster credentials. When to use : - Required when cluster_type=rosa - Used to authenticate with ROSA API - Enables automatic cluster credential lookup Valid values : Valid ROSA authentication token string Impact : Used to authenticate with ROSA API and retrieve cluster server URL and login credentials. Related variables : - cluster_type : Must be rosa - cluster_name : ROSA cluster name to lookup - rosa_cluster_admin_password : Required for cluster-admin login Note : SECURITY - Token should be kept secure and not committed to version control. Obtain from Red Hat Hybrid Cloud Console. rosa_cluster_admin_password \u00a4 ROSA cluster-admin account password. Required (when cluster_type=rosa ) Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default: None Purpose : Provides the password for the cluster-admin account to login to ROSA clusters. When to use : - Required when cluster_type=rosa - Password created during cluster provisioning - Used for cluster-admin level access Valid values : Valid cluster-admin password string Impact : Used to authenticate as cluster-admin user on ROSA clusters. Related variables : - cluster_type : Must be rosa - rosa_token : Required for ROSA API authentication - cluster_name : ROSA cluster name to login to Note : SECURITY - Password should be kept secure and not committed to version control. This is the cluster-admin account password set during ROSA cluster provisioning. Example Playbooks Direct Login \u00a4 - hosts: localhost vars: ocp_server: xxxxx ocp_token: xxxxx roles: - ibm.mas_devops.ocp_login IBMCloud ROKS \u00a4 - hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx ibmcloud_resourcegroup: mygroup roles: - ibm.mas_devops.ocp_login AWS ROSA \u00a4 - hosts: localhost vars: cluster_name: mycluster cluster_type: rosa rosa_token: xxxxx rosa_cluster_admin_password: xxxxx roles: - ibm.mas_devops.ocp_login IBM DevIT Fyre \u00a4 - hosts: localhost vars: cluster_name: mycluster cluster_type: fyre fyre_username: xxxxx fyre_password: xxxxx roles: - ibm.mas_devops.ocp_login Example Playbook \u00a4 - hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.ocp_login License \u00a4 EPL-2.0","title":"ocp_login"},{"location":"roles/ocp_login/#ocp_login","text":"This role provides support to login to a cluster using the oc CLI by looking up cluster information from the infrastructure provider's APIs, it also supports setting ocp_server and ocp_token directly to support login to any Kubernetes cluster.","title":"ocp_login"},{"location":"roles/ocp_login/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/ocp_login/#cluster_name","text":"Cluster name for credential lookup. Required (unless ocp_server and ocp_token are set) Environment Variable: CLUSTER_NAME Default: None Purpose : Identifies the cluster to login to by name. Used to automatically lookup login credentials from the infrastructure provider's API. When to use : - Use with cluster_type for automatic credential lookup - Alternative to manually providing ocp_server and ocp_token - Recommended for managed clusters (ROKS, ROSA, Fyre) Valid values : String matching your cluster name in the provider's system Impact : Combined with cluster_type , automatically retrieves cluster server URL and login token from provider API. Related variables : - cluster_type : Required with this variable (roks, fyre, rosa) - ocp_server / ocp_token : Alternative manual login method Note : Requires provider-specific credentials (e.g., ibmcloud_apikey for ROKS, rosa_token for ROSA, fyre_apikey for Fyre).","title":"cluster_name"},{"location":"roles/ocp_login/#cluster_type","text":"Infrastructure provider type for cluster. Required (unless ocp_server and ocp_token are set) Environment Variable: CLUSTER_TYPE Default: None Purpose : Specifies the infrastructure provider type to determine which API to use for credential lookup. When to use : - Use with cluster_name for automatic credential lookup - Required for managed cluster login - Each type requires specific provider credentials Valid values : roks , fyre , rosa - roks : IBM Cloud Red Hat OpenShift Kubernetes Service - fyre : IBM DevIT Fyre clusters - rosa : AWS Red Hat OpenShift Service on AWS Impact : Determines which provider API is called to retrieve cluster credentials. Each type requires different authentication variables. Related variables : - cluster_name : Cluster to lookup - ibmcloud_apikey : Required for roks - fyre_username / fyre_apikey : Required for fyre - rosa_token : Required for rosa Note : Alternative to using ocp_server and ocp_token for direct login.","title":"cluster_type"},{"location":"roles/ocp_login/#ocp_server","text":"OpenShift server URL for direct login. Required (unless cluster_name and cluster_type are set) Environment Variable: OCP_SERVER Default: None Purpose : Specifies the OpenShift API server URL for direct cluster login without provider API lookup. When to use : - Use with ocp_token for direct login - When cluster is not managed by supported providers - For custom or on-premises clusters - Alternative to cluster_name / cluster_type approach Valid values : Full OpenShift API server URL (e.g., https://api.cluster.example.com:6443 ) Impact : Used directly for oc login command. Bypasses provider API credential lookup. Related variables : - ocp_token : Required with this variable - cluster_name / cluster_type : Alternative automatic lookup method Note : Both ocp_server and ocp_token must be provided together for direct login. This method works with any Kubernetes cluster.","title":"ocp_server"},{"location":"roles/ocp_login/#ocp_token","text":"Authentication token for direct login. Required (unless cluster_name and cluster_type are set) Environment Variable: OCP_TOKEN Default: None Purpose : Provides the authentication token for direct cluster login without provider API lookup. When to use : - Use with ocp_server for direct login - When cluster is not managed by supported providers - For service account tokens or manually obtained tokens - Alternative to cluster_name / cluster_type approach Valid values : Valid OpenShift authentication token string Impact : Used directly for oc login --token command. Bypasses provider API credential lookup. Related variables : - ocp_server : Required with this variable - cluster_name / cluster_type : Alternative automatic lookup method Note : SECURITY - Both ocp_server and ocp_token must be provided together. Token should be kept secure and not committed to version control. This method works with any Kubernetes cluster.","title":"ocp_token"},{"location":"roles/ocp_login/#role-variables-ibmcloud-roks","text":"","title":"Role Variables - IBMCloud ROKS"},{"location":"roles/ocp_login/#ibmcloud_apikey","text":"IBM Cloud API key for ROKS authentication. Required (when cluster_type=roks ) Environment Variable: IBMCLOUD_APIKEY Default: None Purpose : Provides IBM Cloud API key for authenticating with IBM Cloud to retrieve ROKS cluster credentials. When to use : - Required when cluster_type=roks - Used to authenticate with IBM Cloud API - Enables automatic cluster credential lookup Valid values : Valid IBM Cloud API key string Impact : Used to authenticate with IBM Cloud and retrieve cluster server URL and login token for ROKS clusters. Related variables : - cluster_type : Must be roks - cluster_name : ROKS cluster name to lookup - ibmcloud_endpoint : Optional API endpoint override Note : SECURITY - API key should be kept secure and not committed to version control. Obtain from IBM Cloud IAM.","title":"ibmcloud_apikey"},{"location":"roles/ocp_login/#ibmcloud_endpoint","text":"IBM Cloud API endpoint URL. Optional Environment Variable: IBMCLOUD_ENDPOINT Default: https://cloud.ibm.com Purpose : Overrides the default IBM Cloud API endpoint for ROKS cluster credential lookup. When to use : - Use default for public IBM Cloud - Override for private or regional endpoints - Only applies when cluster_type=roks Valid values : Valid IBM Cloud API endpoint URL Impact : Determines which IBM Cloud API endpoint is used for authentication and cluster lookup. Related variables : - cluster_type : Must be roks - ibmcloud_apikey : Required for authentication Note : The default public endpoint works for most deployments. Override only for specific regional or private cloud requirements.","title":"ibmcloud_endpoint"},{"location":"roles/ocp_login/#role-variables-ibm-devit-fyre","text":"","title":"Role Variables - IBM DevIT Fyre"},{"location":"roles/ocp_login/#fyre_username","text":"IBM DevIT Fyre username. Required (when cluster_type=fyre ) Environment Variable: FYRE_USERNAME Default: None Purpose : Provides Fyre username for authenticating with IBM DevIT Fyre to retrieve cluster credentials. When to use : - Required when cluster_type=fyre - Used with fyre_apikey for Fyre authentication - Enables automatic cluster credential lookup Valid values : Valid Fyre username string Impact : Used to authenticate with Fyre API and retrieve cluster server URL and login token. Related variables : - cluster_type : Must be fyre - fyre_apikey : Required for authentication - cluster_name : Fyre cluster name to lookup - fyre_site : Fyre site location Note : Fyre is IBM's internal development and test infrastructure. Requires IBM internal credentials.","title":"fyre_username"},{"location":"roles/ocp_login/#fyre_apikey","text":"IBM DevIT Fyre API key. Required (when cluster_type=fyre ) Environment Variable: FYRE_APIKEY Default: None Purpose : Provides Fyre API key for authenticating with IBM DevIT Fyre to retrieve cluster credentials. When to use : - Required when cluster_type=fyre - Used with fyre_username for Fyre authentication - Enables automatic cluster credential lookup Valid values : Valid Fyre API key string Impact : Used to authenticate with Fyre API and retrieve cluster server URL and login token. Related variables : - cluster_type : Must be fyre - fyre_username : Required for authentication - cluster_name : Fyre cluster name to lookup - fyre_site : Fyre site location Note : SECURITY - API key should be kept secure. Fyre is IBM's internal development and test infrastructure.","title":"fyre_apikey"},{"location":"roles/ocp_login/#fyre_site","text":"Fyre site location for cluster. Optional Environment Variable: FYRE_SITE Default: svl Purpose : Specifies which Fyre site the cluster was provisioned in for proper API routing. When to use : - Use default ( svl ) for most Fyre clusters - Override for clusters in other sites (e.g., rtp ) - Only applies when cluster_type=fyre Valid values : Fyre site codes (e.g., svl , rtp ) Impact : Determines which Fyre site API endpoint is used for cluster lookup. Related variables : - cluster_type : Must be fyre - enable_ipv6 : Required for RTP site Note : SVL (San Jose Valley) is the default site. RTP (Research Triangle Park) requires IPv6 enablement.","title":"fyre_site"},{"location":"roles/ocp_login/#enable_ipv6","text":"Enable IPv6 for Fyre RTP site. Optional Environment Variable: ENABLE_IPV6 Default: false Purpose : Enables IPv6 networking for Fyre clusters at the RTP (Research Triangle Park) site. When to use : - Set to true only for Fyre RTP site clusters - Leave as false for all other sites (including SVL) - Only applies when cluster_type=fyre Valid values : true , false Impact : Configures network settings for IPv6 connectivity to RTP site clusters. Related variables : - cluster_type : Must be fyre - fyre_site : Should be rtp when this is true Note : Only required for Fyre RTP site. SVL and other sites use IPv4.","title":"enable_ipv6"},{"location":"roles/ocp_login/#role-variables-aws-rosa","text":"","title":"Role Variables - AWS ROSA"},{"location":"roles/ocp_login/#rosa_token","text":"AWS ROSA authentication token. Required (when cluster_type=rosa ) Environment Variable: ROSA_TOKEN Default: None Purpose : Provides ROSA (Red Hat OpenShift Service on AWS) authentication token for retrieving cluster credentials. When to use : - Required when cluster_type=rosa - Used to authenticate with ROSA API - Enables automatic cluster credential lookup Valid values : Valid ROSA authentication token string Impact : Used to authenticate with ROSA API and retrieve cluster server URL and login credentials. Related variables : - cluster_type : Must be rosa - cluster_name : ROSA cluster name to lookup - rosa_cluster_admin_password : Required for cluster-admin login Note : SECURITY - Token should be kept secure and not committed to version control. Obtain from Red Hat Hybrid Cloud Console.","title":"rosa_token"},{"location":"roles/ocp_login/#rosa_cluster_admin_password","text":"ROSA cluster-admin account password. Required (when cluster_type=rosa ) Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default: None Purpose : Provides the password for the cluster-admin account to login to ROSA clusters. When to use : - Required when cluster_type=rosa - Password created during cluster provisioning - Used for cluster-admin level access Valid values : Valid cluster-admin password string Impact : Used to authenticate as cluster-admin user on ROSA clusters. Related variables : - cluster_type : Must be rosa - rosa_token : Required for ROSA API authentication - cluster_name : ROSA cluster name to login to Note : SECURITY - Password should be kept secure and not committed to version control. This is the cluster-admin account password set during ROSA cluster provisioning. Example Playbooks","title":"rosa_cluster_admin_password"},{"location":"roles/ocp_login/#direct-login","text":"- hosts: localhost vars: ocp_server: xxxxx ocp_token: xxxxx roles: - ibm.mas_devops.ocp_login","title":"Direct Login"},{"location":"roles/ocp_login/#ibmcloud-roks","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx ibmcloud_resourcegroup: mygroup roles: - ibm.mas_devops.ocp_login","title":"IBMCloud ROKS"},{"location":"roles/ocp_login/#aws-rosa","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: rosa rosa_token: xxxxx rosa_cluster_admin_password: xxxxx roles: - ibm.mas_devops.ocp_login","title":"AWS ROSA"},{"location":"roles/ocp_login/#ibm-devit-fyre","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: fyre fyre_username: xxxxx fyre_password: xxxxx roles: - ibm.mas_devops.ocp_login","title":"IBM DevIT Fyre"},{"location":"roles/ocp_login/#example-playbook","text":"- hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.ocp_login","title":"Example Playbook"},{"location":"roles/ocp_login/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_node_config/","text":"ocp_node_config \u00a4 Configure individual OpenShift nodes with custom labels and taints to control workload placement. This role enables creation of dedicated nodes for specific workloads (e.g., Db2, MongoDB, compute-intensive applications) by applying Kubernetes node selectors and taints. Use labels to direct workloads to specific nodes and taints to prevent unwanted workloads from running on dedicated nodes. This is essential for performance isolation, resource management, and meeting specific hardware requirements. Verification : Use the following command to verify labels and taints: oc get pods --all-namespaces -o wide --field-selector spec.nodeName=<node-name> Role Variables - Node Selection \u00a4 Important : Specify either ocp_node_name OR ocp_node_index to identify the target node. If both are specified, ocp_node_index takes priority. If neither is specified, the role will fail. Role Variables - General \u00a4 ocp_node_name \u00a4 Node name to configure with labels and/or taints. Optional (but one of ocp_node_name or ocp_node_index is required) Environment Variable: OCP_NODE_NAME Default: None Purpose : Identifies a specific node by its name for label and taint configuration. When to use : Use when you know the exact node name. Preferred method for production environments where node names are predictable. Valid values : Valid OpenShift node name (e.g., worker-0.example.com , 10.172.168.89 ). Impact : The specified node will be configured with the provided labels and taints. Incorrect node name will cause the role to fail. Related variables : ocp_node_index (alternative selection method) Notes : - Get node names with: oc get nodes - Node names may be hostnames, IP addresses, or cloud provider identifiers - If both ocp_node_name and ocp_node_index are set, index takes priority ocp_node_index \u00a4 Zero-based index of the node in the cluster's node list. Optional (but one of ocp_node_name or ocp_node_index is required) Environment Variable: OCP_NODE_INDEX Default: None Purpose : Identifies a node by its position in the sorted list of cluster nodes. When to use : Useful in automation scenarios where node names are not known in advance, or when configuring nodes sequentially. Valid values : Non-negative integer starting from 0 (e.g., 0 for first node, 1 for second node). Impact : The node at the specified index position will be configured. Index out of range will cause the role to fail. Related variables : ocp_node_name (alternative selection method) Notes : - Index 0 refers to the first node in alphabetically sorted list - Node order may change if nodes are added/removed - Takes priority over ocp_node_name if both are set - Less reliable than node name for production use Role Variables - Node Labels \u00a4 ocp_node_label_keys \u00a4 Comma-separated list of label keys to add to the selected node. Optional Environment Variable: OCP_NODE_LABEL_KEYS Default: None Purpose : Defines the label keys that will be applied to the node. Labels are used by pod node selectors to direct workloads to specific nodes. When to use : Use to categorize nodes by workload type, hardware capabilities, or any custom classification scheme. Valid values : Comma-separated list of valid Kubernetes label keys (e.g., workload , storage-type,gpu-enabled ). Label keys must: - Start and end with alphanumeric characters - Contain only alphanumeric, dash, underscore, or dot characters - Be 63 characters or less Impact : Labels enable pod scheduling based on node selectors. Pods with matching node selectors will prefer or require these nodes. Related variables : ocp_node_label_values (must have same number of values as keys) Notes : - Number of keys must match number of values - Common label keys: workload , node-role , storage-type , hardware-type - Labels are additive; existing labels are not removed - Use with pod nodeSelector or nodeAffinity for workload placement ocp_node_label_values \u00a4 Comma-separated list of label values corresponding to the label keys. Optional Environment Variable: OCP_NODE_LABEL_VALUES Default: None Purpose : Provides the values for each label key specified in ocp_node_label_keys . When to use : Required when ocp_node_label_keys is set. Must provide exactly one value for each key. Valid values : Comma-separated list of valid Kubernetes label values (e.g., db2 , ssd,true ). Label values must: - Start and end with alphanumeric characters - Contain only alphanumeric, dash, underscore, or dot characters - Be 63 characters or less Impact : Combined with keys, creates complete labels (key=value pairs) on the node. Related variables : ocp_node_label_keys (must have same number of keys as values) Notes : - Number of values must exactly match number of keys - Order matters: first value pairs with first key, etc. - Common label values: db2 , mongodb , compute , storage - Example: keys= workload,storage values= db2,ssd creates labels workload=db2 and storage=ssd Role Variables - Node Taints \u00a4 ocp_node_taint_keys \u00a4 Comma-separated list of taint keys to add to the selected node. Optional Environment Variable: OCP_NODE_TAINT_KEYS Default: None Purpose : Defines the taint keys that will prevent pods without matching tolerations from being scheduled on the node. When to use : Use to dedicate nodes to specific workloads by preventing other pods from running on them. Valid values : Comma-separated list of valid Kubernetes taint keys (e.g., dedicatedDb2Node , gpu-workload,high-memory ). Impact : Pods without matching tolerations will not be scheduled on this node. Existing pods may be evicted depending on taint effect. Related variables : ocp_node_taint_values , ocp_node_taint_effects (must have same number of entries) Notes : - Number of keys must match number of values and effects - Taints are more restrictive than labels - Common taint keys: dedicatedDb2Node , dedicatedMongoNode , gpu-required - Pods must have matching tolerations to run on tainted nodes ocp_node_taint_values \u00a4 Comma-separated list of taint values corresponding to the taint keys. Optional Environment Variable: OCP_NODE_TAINT_VALUES Default: None Purpose : Provides the values for each taint key, typically identifying which workload or instance the node is dedicated to. When to use : Required when ocp_node_taint_keys is set. Must provide exactly one value for each key. Valid values : Comma-separated list of taint values (e.g., masinst1 , prod-db,gpu-cluster ). Impact : Combined with keys and effects, creates complete taints that control pod scheduling. Related variables : ocp_node_taint_keys , ocp_node_taint_effects (must have same number of entries) Notes : - Number of values must exactly match number of keys and effects - Values often identify specific instances or workload identifiers - Example: key= dedicatedDb2Node value= masinst1 effect= NoExecute dedicates node to masinst1's Db2 ocp_node_taint_effects \u00a4 Comma-separated list of taint effects defining how the taint impacts pod scheduling. Optional Environment Variable: OCP_NODE_TAINT_EFFECTS Default: None Purpose : Specifies the enforcement level for each taint, controlling both new pod scheduling and existing pod eviction. When to use : Required when ocp_node_taint_keys is set. Choose effect based on desired behavior. Valid values : Comma-separated list of taint effects: - NoSchedule - New pods without tolerations will not be scheduled; existing pods remain - PreferNoSchedule - Scheduler tries to avoid placing pods without tolerations; not guaranteed - NoExecute - New pods without tolerations will not be scheduled; existing pods without tolerations are evicted Impact : - NoSchedule : Prevents new workloads but preserves existing ones - PreferNoSchedule : Soft constraint, may be overridden by scheduler - NoExecute : Strongest enforcement, evicts non-tolerating pods immediately Related variables : ocp_node_taint_keys , ocp_node_taint_values (must have same number of entries) Notes : - Number of effects must exactly match number of keys and values - NoExecute is most disruptive but ensures dedicated nodes - NoSchedule is common for gradual migration to dedicated nodes - PreferNoSchedule is least restrictive, useful for soft preferences - Example: Use NoExecute for Db2 dedicated nodes to ensure no other workloads run Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # Turn this worker node into a dedicated Db2 worker node ocp_node_name: \"10.172.168.89\" # Add the label that will be applied to all dedicated Db2 nodes # this will be used to direct Db2 workloads to these nodes ocp_node_label_keys: workload ocp_node_label_values: db2 # Set a taint preventing anything other than Db2 workloads for masinst1 running # on this node specific Db2 node ocp_node_taint_keys: dedicatedDb2Node ocp_node_taint_values: masinst1 ocp_node_taint_effects: NoExecute roles: - ibm.mas_devops.ocp_node_config License \u00a4 EPL-2.0","title":"ocp_node_config"},{"location":"roles/ocp_node_config/#ocp_node_config","text":"Configure individual OpenShift nodes with custom labels and taints to control workload placement. This role enables creation of dedicated nodes for specific workloads (e.g., Db2, MongoDB, compute-intensive applications) by applying Kubernetes node selectors and taints. Use labels to direct workloads to specific nodes and taints to prevent unwanted workloads from running on dedicated nodes. This is essential for performance isolation, resource management, and meeting specific hardware requirements. Verification : Use the following command to verify labels and taints: oc get pods --all-namespaces -o wide --field-selector spec.nodeName=<node-name>","title":"ocp_node_config"},{"location":"roles/ocp_node_config/#role-variables-node-selection","text":"Important : Specify either ocp_node_name OR ocp_node_index to identify the target node. If both are specified, ocp_node_index takes priority. If neither is specified, the role will fail.","title":"Role Variables - Node Selection"},{"location":"roles/ocp_node_config/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/ocp_node_config/#ocp_node_name","text":"Node name to configure with labels and/or taints. Optional (but one of ocp_node_name or ocp_node_index is required) Environment Variable: OCP_NODE_NAME Default: None Purpose : Identifies a specific node by its name for label and taint configuration. When to use : Use when you know the exact node name. Preferred method for production environments where node names are predictable. Valid values : Valid OpenShift node name (e.g., worker-0.example.com , 10.172.168.89 ). Impact : The specified node will be configured with the provided labels and taints. Incorrect node name will cause the role to fail. Related variables : ocp_node_index (alternative selection method) Notes : - Get node names with: oc get nodes - Node names may be hostnames, IP addresses, or cloud provider identifiers - If both ocp_node_name and ocp_node_index are set, index takes priority","title":"ocp_node_name"},{"location":"roles/ocp_node_config/#ocp_node_index","text":"Zero-based index of the node in the cluster's node list. Optional (but one of ocp_node_name or ocp_node_index is required) Environment Variable: OCP_NODE_INDEX Default: None Purpose : Identifies a node by its position in the sorted list of cluster nodes. When to use : Useful in automation scenarios where node names are not known in advance, or when configuring nodes sequentially. Valid values : Non-negative integer starting from 0 (e.g., 0 for first node, 1 for second node). Impact : The node at the specified index position will be configured. Index out of range will cause the role to fail. Related variables : ocp_node_name (alternative selection method) Notes : - Index 0 refers to the first node in alphabetically sorted list - Node order may change if nodes are added/removed - Takes priority over ocp_node_name if both are set - Less reliable than node name for production use","title":"ocp_node_index"},{"location":"roles/ocp_node_config/#role-variables-node-labels","text":"","title":"Role Variables - Node Labels"},{"location":"roles/ocp_node_config/#ocp_node_label_keys","text":"Comma-separated list of label keys to add to the selected node. Optional Environment Variable: OCP_NODE_LABEL_KEYS Default: None Purpose : Defines the label keys that will be applied to the node. Labels are used by pod node selectors to direct workloads to specific nodes. When to use : Use to categorize nodes by workload type, hardware capabilities, or any custom classification scheme. Valid values : Comma-separated list of valid Kubernetes label keys (e.g., workload , storage-type,gpu-enabled ). Label keys must: - Start and end with alphanumeric characters - Contain only alphanumeric, dash, underscore, or dot characters - Be 63 characters or less Impact : Labels enable pod scheduling based on node selectors. Pods with matching node selectors will prefer or require these nodes. Related variables : ocp_node_label_values (must have same number of values as keys) Notes : - Number of keys must match number of values - Common label keys: workload , node-role , storage-type , hardware-type - Labels are additive; existing labels are not removed - Use with pod nodeSelector or nodeAffinity for workload placement","title":"ocp_node_label_keys"},{"location":"roles/ocp_node_config/#ocp_node_label_values","text":"Comma-separated list of label values corresponding to the label keys. Optional Environment Variable: OCP_NODE_LABEL_VALUES Default: None Purpose : Provides the values for each label key specified in ocp_node_label_keys . When to use : Required when ocp_node_label_keys is set. Must provide exactly one value for each key. Valid values : Comma-separated list of valid Kubernetes label values (e.g., db2 , ssd,true ). Label values must: - Start and end with alphanumeric characters - Contain only alphanumeric, dash, underscore, or dot characters - Be 63 characters or less Impact : Combined with keys, creates complete labels (key=value pairs) on the node. Related variables : ocp_node_label_keys (must have same number of keys as values) Notes : - Number of values must exactly match number of keys - Order matters: first value pairs with first key, etc. - Common label values: db2 , mongodb , compute , storage - Example: keys= workload,storage values= db2,ssd creates labels workload=db2 and storage=ssd","title":"ocp_node_label_values"},{"location":"roles/ocp_node_config/#role-variables-node-taints","text":"","title":"Role Variables - Node Taints"},{"location":"roles/ocp_node_config/#ocp_node_taint_keys","text":"Comma-separated list of taint keys to add to the selected node. Optional Environment Variable: OCP_NODE_TAINT_KEYS Default: None Purpose : Defines the taint keys that will prevent pods without matching tolerations from being scheduled on the node. When to use : Use to dedicate nodes to specific workloads by preventing other pods from running on them. Valid values : Comma-separated list of valid Kubernetes taint keys (e.g., dedicatedDb2Node , gpu-workload,high-memory ). Impact : Pods without matching tolerations will not be scheduled on this node. Existing pods may be evicted depending on taint effect. Related variables : ocp_node_taint_values , ocp_node_taint_effects (must have same number of entries) Notes : - Number of keys must match number of values and effects - Taints are more restrictive than labels - Common taint keys: dedicatedDb2Node , dedicatedMongoNode , gpu-required - Pods must have matching tolerations to run on tainted nodes","title":"ocp_node_taint_keys"},{"location":"roles/ocp_node_config/#ocp_node_taint_values","text":"Comma-separated list of taint values corresponding to the taint keys. Optional Environment Variable: OCP_NODE_TAINT_VALUES Default: None Purpose : Provides the values for each taint key, typically identifying which workload or instance the node is dedicated to. When to use : Required when ocp_node_taint_keys is set. Must provide exactly one value for each key. Valid values : Comma-separated list of taint values (e.g., masinst1 , prod-db,gpu-cluster ). Impact : Combined with keys and effects, creates complete taints that control pod scheduling. Related variables : ocp_node_taint_keys , ocp_node_taint_effects (must have same number of entries) Notes : - Number of values must exactly match number of keys and effects - Values often identify specific instances or workload identifiers - Example: key= dedicatedDb2Node value= masinst1 effect= NoExecute dedicates node to masinst1's Db2","title":"ocp_node_taint_values"},{"location":"roles/ocp_node_config/#ocp_node_taint_effects","text":"Comma-separated list of taint effects defining how the taint impacts pod scheduling. Optional Environment Variable: OCP_NODE_TAINT_EFFECTS Default: None Purpose : Specifies the enforcement level for each taint, controlling both new pod scheduling and existing pod eviction. When to use : Required when ocp_node_taint_keys is set. Choose effect based on desired behavior. Valid values : Comma-separated list of taint effects: - NoSchedule - New pods without tolerations will not be scheduled; existing pods remain - PreferNoSchedule - Scheduler tries to avoid placing pods without tolerations; not guaranteed - NoExecute - New pods without tolerations will not be scheduled; existing pods without tolerations are evicted Impact : - NoSchedule : Prevents new workloads but preserves existing ones - PreferNoSchedule : Soft constraint, may be overridden by scheduler - NoExecute : Strongest enforcement, evicts non-tolerating pods immediately Related variables : ocp_node_taint_keys , ocp_node_taint_values (must have same number of entries) Notes : - Number of effects must exactly match number of keys and values - NoExecute is most disruptive but ensures dedicated nodes - NoSchedule is common for gradual migration to dedicated nodes - PreferNoSchedule is least restrictive, useful for soft preferences - Example: Use NoExecute for Db2 dedicated nodes to ensure no other workloads run","title":"ocp_node_taint_effects"},{"location":"roles/ocp_node_config/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Turn this worker node into a dedicated Db2 worker node ocp_node_name: \"10.172.168.89\" # Add the label that will be applied to all dedicated Db2 nodes # this will be used to direct Db2 workloads to these nodes ocp_node_label_keys: workload ocp_node_label_values: db2 # Set a taint preventing anything other than Db2 workloads for masinst1 running # on this node specific Db2 node ocp_node_taint_keys: dedicatedDb2Node ocp_node_taint_values: masinst1 ocp_node_taint_effects: NoExecute roles: - ibm.mas_devops.ocp_node_config","title":"Example Playbook"},{"location":"roles/ocp_node_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_provision/","text":"ocp_provision \u00a4 Provision OCP cluster on IBM Cloud ROKS, ROSA, or DevIT Fyre. Fyre clusters will be automatically reconfigured to enable NFS storage. By default this is made available via the nfs-client storage class and supports both ReadWriteOnce and ReadWriteMany access modes. The image-registry-storage PVC used by the OpenShift image registry component will also be reconfigured to use this storage class. Role Variables - General \u00a4 cluster_type \u00a4 Infrastructure provider type for cluster provisioning. Required Environment Variable: CLUSTER_TYPE Default: None Purpose : Specifies which infrastructure provider to use for provisioning the OpenShift cluster. Determines provisioning method and required variables. When to use : - Always required for cluster provisioning - Each type requires different provider-specific variables - Determines available features (e.g., GPU support for ROKS) Valid values : fyre , roks , rosa , ipi - fyre : IBM DevIT Fyre clusters (internal development) - roks : IBM Cloud Red Hat OpenShift Kubernetes Service - rosa : AWS Red Hat OpenShift Service on AWS - ipi : Installer-Provisioned Infrastructure (bare metal/on-premises) Impact : Determines provisioning workflow and which provider-specific variables are required. Each type has different capabilities and configuration options. Related variables : - cluster_name : Name for the new cluster - ocp_version : OpenShift version to install - Provider-specific variables (ibmcloud_apikey, rosa_token, fyre_apikey, etc.) Note : Fyre clusters automatically configure NFS storage. ROKS requires version format like 4.19_openshift . cluster_name \u00a4 Name for the new cluster. Required Environment Variable: CLUSTER_NAME Default: None Purpose : Specifies the name for the OpenShift cluster to be provisioned. Used as the cluster identifier in the provider's system. When to use : - Always required for cluster provisioning - Must be unique within the provider's account/region - Used for cluster identification and resource naming Valid values : String following provider naming conventions (typically lowercase alphanumeric with hyphens) Impact : Determines the cluster name in the provider's system. Used for DNS, resource naming, and cluster identification. Related variables : - cluster_type : Provider where cluster will be created - ocp_version : OpenShift version for the cluster Note : Name must follow provider-specific naming rules. Some providers have length limits or character restrictions. ocp_version \u00a4 OpenShift version to install. Required Environment Variable: OCP_VERSION Default: None Purpose : Specifies which version of OpenShift Container Platform to install on the provisioned cluster. When to use : - Always required for cluster provisioning - Use specific version for production (e.g., 4.19.14 ) - Use default for latest MAS-supported version - Use rotate for testing (version changes by day of week) Valid values : - Specific version: 4.19 , 4.19.14 - Alias: default (newest MAS-supported version) - Alias: rotate (predetermined version by day, for testing) - ROKS format : Must append _openshift (e.g., 4.19_openshift , 4.19.14_openshift ) Impact : Determines OpenShift version installed. Version must be compatible with MAS and available from the provider. Related variables : - cluster_type : ROKS requires _openshift suffix Note : IMPORTANT - For ROKS ( cluster_type=roks ), version MUST include _openshift suffix. The default alias selects the newest MAS-supported version. The rotate alias is for testing only. ocp_storage_provider \u00a4 Storage provider configuration for Fyre clusters. Optional Environment Variable: OCP_STORAGE_PROVIDER Default: None Purpose : Configures NFS storage for Fyre clusters, creating an nfs-client storage class and reconfiguring the image registry. When to use : - Set to nfs for Fyre clusters to enable NFS storage - Only applies when cluster_type=fyre - Leave unset for other cluster types (ROKS, ROSA, IPI) Valid values : nfs (for Fyre clusters only) Impact : - nfs : Creates nfs-client storage class connected to infrastructure node, reconfigures image registry PVC to use NFS - Unset: No storage configuration changes Related variables : - cluster_type : Must be fyre for this to have effect Note : Only functional for Fyre clusters. When enabled, the existing image registry PVC is deleted and recreated with NFS storage. NFS storage class supports both ReadWriteOnce (RWO) and ReadWriteMany (RWX) access modes. Role Variables - GPU Node Support \u00a4 ocp_provision_gpu \u00a4 Enable GPU worker nodes during provisioning. Optional Environment Variable: OCP_PROVISION_GPU Default: false Purpose : Controls whether GPU-enabled worker nodes are provisioned with the cluster. Required for GPU-intensive applications like MAS Visual Inspection (MVI). When to use : - Set to true for MAS Visual Inspection deployments - Set to true for other GPU-intensive workloads - Leave as false (default) for standard deployments - Currently only supported for ROKS clusters Valid values : true , false Impact : - true : Provisions GPU worker pool with specified number of GPU nodes - false : No GPU nodes provisioned (standard cluster) Related variables : - gpu_workerpool_name : Name of GPU worker pool - gpu_workers : Number of GPU nodes to provision - cluster_type : Must be roks for GPU support Note : GPU support is currently only available for ROKS clusters. GPU nodes use mg4c.32x384.2xp100-GPU flavor with P100 GPUs. gpu_workerpool_name \u00a4 Name for GPU worker pool. Optional Environment Variable: GPU_WORKERPOOL_NAME Default: gpu Purpose : Specifies the name for the GPU worker pool to be created or modified in the cluster. When to use : - Use default ( gpu ) for new GPU deployments - Set to existing pool name to modify rather than create new - Only applies when ocp_provision_gpu=true Valid values : String following worker pool naming conventions Impact : Determines GPU worker pool name. Using an existing name modifies that pool; using a new name creates a new pool. Related variables : - ocp_provision_gpu : Must be true for this to apply - gpu_workers : Number of nodes in this pool - cluster_type : Must be roks Note : If a GPU worker pool already exists with this name, it will be modified rather than creating a duplicate. Use the existing name to avoid multiple GPU pools. gpu_workers \u00a4 Number of GPU worker nodes to provision in the cluster. Optional Environment Variable: GPU_WORKERS Default: 1 Purpose : Specifies how many GPU-enabled worker nodes to provision in the GPU worker pool. Each node uses mg4c.32x384.2xp100-GPU flavor with P100 GPUs. When to use : - Use default (1) for minimal GPU deployments or testing - Increase for production MAS Visual Inspection deployments - Scale based on GPU workload requirements - Only applies when ocp_provision_gpu=true Valid values : Positive integer (e.g., 1 , 2 , 3 ) Impact : Determines the number of GPU nodes provisioned. More nodes provide more GPU capacity but increase costs. Related variables : - ocp_provision_gpu : Must be true for this to apply - gpu_workerpool_name : Name of the pool containing these nodes - cluster_type : Must be roks for GPU support Note : GPU nodes use expensive hardware (P100 GPUs). Only provision what you need. Currently only supported on ROKS clusters. Role Variables - ROKS \u00a4 The following variables are only used when cluster_type = roks . ibmcloud_apikey \u00a4 IBM Cloud API key for authentication. Required (when cluster_type=roks ) Environment Variable: IBMCLOUD_APIKEY Default: None Purpose : Authenticates with IBM Cloud to provision and manage ROKS clusters. Used by the ibmcloud CLI for all cluster operations. When to use : - Always required for ROKS cluster provisioning - Must have permissions to create clusters in the target resource group - Obtain from IBM Cloud IAM (Identity and Access Management) Valid values : IBM Cloud API key string (typically 40+ characters) Impact : Without a valid API key, cluster provisioning will fail. The key must have appropriate IAM permissions for cluster creation. Related variables : - ibmcloud_resourcegroup : Resource group where cluster will be created - ibmcloud_endpoint : IBM Cloud API endpoint to authenticate against Note : Keep API keys secure. Use environment variables or secure vaults rather than hardcoding in playbooks. The key needs cluster creation permissions in the specified resource group. ibmcloud_endpoint \u00a4 IBM Cloud API endpoint URL. Optional Environment Variable: IBMCLOUD_ENDPOINT Default: https://cloud.ibm.com Purpose : Specifies the IBM Cloud API endpoint for authentication and cluster operations. Allows targeting different IBM Cloud environments. When to use : - Use default for standard IBM Cloud (public cloud) - Override for IBM Cloud dedicated or private environments - Change for testing against staging environments Valid values : Valid IBM Cloud API endpoint URL Impact : Determines which IBM Cloud environment is targeted for cluster provisioning. Related variables : - ibmcloud_apikey : API key used with this endpoint Note : The default endpoint works for standard IBM Cloud deployments. Only change if you're using a dedicated or private IBM Cloud environment. ibmcloud_resourcegroup \u00a4 IBM Cloud resource group for the cluster. Optional Environment Variable: IBMCLOUD_RESOURCEGROUP Default: Default Purpose : Specifies which IBM Cloud resource group the ROKS cluster will be created in. Resource groups organize and manage access to IBM Cloud resources. When to use : - Use default ( Default ) for simple deployments - Specify a different resource group for organizational separation - Ensure the API key has access to the specified resource group Valid values : Name of an existing IBM Cloud resource group Impact : Cluster is created in the specified resource group. The API key must have permissions in this resource group. Related variables : - ibmcloud_apikey : Must have permissions in this resource group Note : The resource group must exist before provisioning. The API key must have appropriate IAM permissions in the target resource group. roks_zone \u00a4 IBM Cloud availability zone for cluster deployment. Optional Environment Variable: ROKS_ZONE Default: dal10 Purpose : Specifies the IBM Cloud availability zone where the ROKS cluster will be provisioned. Determines the physical location of cluster resources. When to use : - Use default ( dal10 ) for Dallas datacenter - Change based on geographic requirements or latency needs - Consider data residency and compliance requirements Valid values : Valid IBM Cloud zone identifier (e.g., dal10 , lon02 , fra02 , tok02 ) Impact : Determines cluster location, which affects latency, data residency, and available services. Related variables : - roks_flavor : Worker node flavors available vary by zone Note : Not all zones support all worker node flavors. Verify flavor availability in your target zone. Zone selection affects network latency to your users and services. roks_flavor \u00a4 Worker node machine type for ROKS cluster. Optional Environment Variable: ROKS_FLAVOR Default: b3c.16x64.300gb Purpose : Specifies the machine type (flavor) for worker nodes in the ROKS cluster. Determines CPU, memory, and local storage for each worker node. When to use : - Use default ( b3c.16x64.300gb ) for standard MAS deployments (16 vCPU, 64GB RAM, 300GB storage) - Increase for larger workloads or more applications - Decrease for development/testing environments Valid values : Valid IBM Cloud worker node flavor (e.g., b3c.4x16 , b3c.16x64.300gb , b3c.32x128 ) Impact : Determines worker node capacity. Affects cluster performance and cost. Larger flavors cost more but provide more resources. Related variables : - roks_workers : Number of nodes with this flavor - roks_zone : Not all flavors available in all zones Note : The default flavor (16 vCPU, 64GB RAM) is suitable for most MAS deployments. Verify flavor availability in your target zone. Consider total cluster capacity (flavor \u00d7 worker count). roks_workers \u00a4 Number of worker nodes in the ROKS cluster. Optional Environment Variable: ROKS_WORKERS Default: 3 Purpose : Specifies how many worker nodes to provision in the ROKS cluster. Determines cluster capacity and high availability. When to use : - Use default (3) for standard high-availability deployments - Increase for larger workloads or more applications - Minimum 3 recommended for production (high availability) Valid values : Positive integer, minimum 1 (3+ recommended for production) Impact : Determines total cluster capacity (workers \u00d7 flavor resources). More workers provide more capacity and better high availability but increase costs. Related variables : - roks_flavor : Machine type for each worker - ocp_provision_gpu : Additional GPU workers can be added separately Note : For production, use at least 3 workers for high availability. Total cluster capacity = worker count \u00d7 flavor resources. Consider workload requirements when sizing. roks_flags \u00a4 Additional flags for ROKS cluster creation. Optional Environment Variable: ROKS_FLAGS Default: None Purpose : Allows passing additional command-line flags to the ibmcloud ks cluster create command for advanced cluster configuration. When to use : - Leave unset for standard deployments - Use for advanced configurations not covered by other variables - Consult IBM Cloud documentation for available flags Valid values : Valid ibmcloud CLI flags (e.g., --disable-public-service-endpoint , --pod-subnet , --service-subnet ) Impact : Passes additional configuration options to cluster creation. Incorrect flags may cause provisioning to fail. Related variables : - All other ROKS variables: Flags supplement standard configuration Note : Use with caution. Incorrect flags can cause provisioning failures. Consult IBM Cloud Kubernetes Service documentation for available options. Role Variables - ROSA \u00a4 The following variables are only used when cluster_type = rosa . rosa_token \u00a4 Red Hat OpenShift Service on AWS (ROSA) authentication token. Required (when cluster_type=rosa ) Environment Variable: ROSA_TOKEN Default: None Purpose : Authenticates with the ROSA service to provision and manage OpenShift clusters on AWS. Required for all ROSA cluster operations. When to use : - Always required for ROSA cluster provisioning - Obtain from OpenShift Cluster Manager - Token must be valid and not expired Valid values : ROSA API token string from Red Hat OpenShift Cluster Manager Impact : Without a valid token, ROSA cluster provisioning will fail. Token must have permissions to create clusters. Related variables : - cluster_name : Name for the ROSA cluster - rosa_compute_nodes : Number of worker nodes to provision Note : Tokens expire periodically. Obtain a fresh token from the OpenShift Cluster Manager before provisioning. Keep tokens secure. rosa_cluster_admin_password \u00a4 Password for the cluster-admin user account. Optional Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default: None (auto-generated) Purpose : Sets the password for the cluster-admin user account on the ROSA cluster. Used to log into the cluster after provisioning. When to use : - Set to specify a known password for cluster access - Leave unset to auto-generate a secure password - Auto-generated password is saved to config directory Valid values : String meeting OpenShift password requirements (typically 8+ characters) Impact : - When set: Uses specified password for cluster-admin account - When unset: Auto-generates a secure password and saves it to config Related variables : - rosa_config_dir : Location where auto-generated password is saved Note : If not set, the auto-generated password is saved in the rosa config file. Keep passwords secure. Consider using auto-generation for better security. rosa_compute_nodes \u00a4 Number of compute (worker) nodes in the ROSA cluster. Optional Environment Variable: ROSA_COMPUTE_NODES Default: 3 Purpose : Specifies how many worker nodes to provision in the ROSA cluster. Determines cluster capacity and high availability. When to use : - Use default (3) for standard high-availability deployments - Increase for larger workloads or more applications - Minimum 2 required, 3+ recommended for production Valid values : Positive integer, minimum 2 (3+ recommended for production) Impact : Determines total cluster capacity. More nodes provide more capacity and better high availability but increase AWS costs. Related variables : - rosa_compute_machine_type : Machine type for each worker node Note : For production, use at least 3 workers for high availability. Total cluster capacity = worker count \u00d7 machine type resources. ROSA clusters run on AWS infrastructure. rosa_compute_machine_type \u00a4 AWS machine type for ROSA worker nodes. Optional Environment Variable: ROSA_COMPUTE_MACHINE_TYPE Default: m5.4xlarge Purpose : Specifies the AWS EC2 instance type for worker nodes in the ROSA cluster. Determines CPU, memory, and network capacity for each worker. When to use : - Use default ( m5.4xlarge : 16 vCPU, 64GB RAM) for standard MAS deployments - Increase for larger workloads (e.g., m5.8xlarge , m5.12xlarge ) - Decrease for development/testing (e.g., m5.2xlarge ) Valid values : Valid AWS EC2 instance type (e.g., m5.2xlarge , m5.4xlarge , m5.8xlarge , m5.12xlarge ) Impact : Determines worker node capacity. Affects cluster performance and AWS costs. Larger instance types cost more but provide more resources. Related variables : - rosa_compute_nodes : Number of nodes with this instance type Note : The default m5.4xlarge (16 vCPU, 64GB RAM) is suitable for most MAS deployments. Consider total cluster capacity (instance type \u00d7 node count). Verify instance type availability in your AWS region. rosa_config_dir \u00a4 Directory for storing ROSA cluster configuration files. Optional Environment Variable: ROSA_CONFIG_DIR Default: None Purpose : Specifies the directory where ROSA cluster configuration files are saved, including the rosa-{{cluster_name}}-details.yaml file containing API endpoint and cluster-admin credentials. When to use : - Set to save cluster details to a specific location - Leave unset to skip saving configuration files - Useful for automation and cluster access management Valid values : Absolute filesystem path (e.g., /tmp/rosa-config , ~/rosa-clusters ) Impact : - When set: Cluster details (API endpoint, credentials) are saved to this directory - When unset: Configuration files are not saved (must retrieve details manually) Related variables : - rosa_cluster_admin_password : Auto-generated password saved here if not specified - cluster_name : Used in config filename Note : The config file contains sensitive information (cluster-admin password). Ensure the directory has appropriate permissions. File format: rosa-{{cluster_name}}-details.yaml . Role Variables - FYRE \u00a4 The following variables are only used when cluster_type = fyre . fyre_username \u00a4 Fyre username for authentication. Required (when cluster_type=fyre ) Environment Variable: FYRE_USERNAME Default: None Purpose : Authenticates with the IBM DevIT Fyre API to provision and manage OpenShift clusters on Fyre infrastructure. When to use : - Always required for Fyre cluster provisioning - Must be a valid Fyre account username - Used for internal IBM development and testing Valid values : Valid Fyre username (IBM intranet ID) Impact : Without valid credentials, Fyre cluster provisioning will fail. Related variables : - fyre_apikey : API key paired with this username Note : Fyre is an internal IBM development platform. Access requires IBM credentials and appropriate permissions. fyre_apikey \u00a4 Fyre API key for authentication. Required (when cluster_type=fyre ) Environment Variable: FYRE_APIKEY Default: None Purpose : Authenticates with the Fyre API for cluster provisioning operations. Paired with Fyre username for authentication. When to use : - Always required for Fyre cluster provisioning - Obtain from Fyre portal - Keep secure and rotate regularly Valid values : Valid Fyre API key string Impact : Without a valid API key, Fyre cluster provisioning will fail. Related variables : - fyre_username : Username paired with this API key Note : Keep API keys secure. Obtain from the Fyre portal. Keys may expire and need renewal. fyre_quota_type \u00a4 Fyre quota type for cluster provisioning. Required (when cluster_type=fyre ) Environment Variable: FYRE_QUOTA_TYPE Default: quick_burn Purpose : Specifies which type of Fyre quota to use for cluster provisioning. Determines billing method and available configuration options. When to use : - Use quick_burn (default) for pre-defined cluster sizes (faster, simpler) - Use product_group for custom cluster configurations (more control) - Choice affects which other variables are required Valid values : quick_burn , product_group Impact : - quick_burn : Uses pre-defined cluster sizes, requires fyre_cluster_size - product_group : Allows custom sizing, requires fyre_worker_count , fyre_worker_cpu , fyre_worker_memory Related variables : - fyre_cluster_size : Required when quick_burn - fyre_worker_count , fyre_worker_cpu , fyre_worker_memory : Required when product_group - fyre_product_id : Required for both types Note : Quick burn is simpler but less flexible. Product group allows custom sizing but requires more configuration. fyre_product_id \u00a4 Product ID for Fyre accounting. Required (when cluster_type=fyre ) Environment Variable: FYRE_PRODUCT_ID Default: None Purpose : Associates the Fyre cluster with a product ID for internal IBM accounting and cost tracking purposes. When to use : - Always required for Fyre cluster provisioning - Obtain from your IBM product team or manager - Used for internal cost allocation Valid values : Valid IBM product ID Impact : Cluster costs are charged to this product ID. Incorrect ID may cause provisioning to fail or incorrect billing. Related variables : - fyre_quota_type : Product ID required for both quota types Note : Contact your IBM product team or manager to obtain the correct product ID for your project. fyre_site \u00a4 Fyre datacenter site location. Optional Environment Variable: FYRE_SITE Default: svl Purpose : Specifies which Fyre datacenter site to provision the cluster in. Determines physical location and network connectivity. When to use : - Use default ( svl - San Jose/Silicon Valley) for most cases - Change based on geographic requirements or network proximity - Consider latency to your development location Valid values : Valid Fyre site code (e.g., svl , rtp , raleigh ) Impact : Determines cluster location, which affects network latency and available resources. Related variables : - enable_ipv6 : IPv6 only available at RTP site Note : Not all sites support all features. SVL is the default and most commonly used site. fyre_cluster_description \u00a4 Description for the Fyre cluster. Optional Environment Variable: FYRE_CLUSTER_DESCRIPTION Default: None Purpose : Provides a human-readable description for the Fyre cluster. Helps identify cluster purpose in Fyre portal. When to use : - Set to document cluster purpose (e.g., \"MAS 9.0 testing\", \"Development cluster\") - Leave unset for unnamed clusters - Useful for tracking and managing multiple clusters Valid values : Any descriptive string Impact : Description appears in Fyre portal. No functional impact on cluster. Related variables : - cluster_name : Cluster identifier Note : Good descriptions help manage multiple clusters. Include purpose, owner, or project information. ocp_fips_enabled \u00a4 Enable FIPS mode for the cluster. Optional Environment Variable: OCP_FIPS_ENABLED Default: false Purpose : Controls whether the OpenShift cluster is provisioned with FIPS (Federal Information Processing Standards) 140-2 cryptographic mode enabled. When to use : - Set to true for compliance with FIPS 140-2 requirements - Set to true for government or regulated environments - Leave as false (default) for standard deployments Valid values : true , false Impact : - true : Cluster uses FIPS-validated cryptographic modules (required for some compliance) - false : Standard cryptography (better performance) Related variables : - cluster_type : FIPS support varies by cluster type Note : FIPS mode may impact performance. Only enable if required for compliance. Cannot be changed after cluster creation. fyre_cluster_size \u00a4 Pre-defined Fyre cluster size. Required (when cluster_type=fyre and fyre_quota_type=quick_burn ) Environment Variable: FYRE_CLUSTER_SIZE Default: medium Purpose : Specifies which pre-defined cluster size to use when provisioning with quick_burn quota. Determines worker node count and resources. When to use : - Only applies when fyre_quota_type=quick_burn - Use medium (default) for standard development/testing - Use small for minimal testing - Use large for more intensive workloads Valid values : Fyre pre-defined sizes (e.g., small , medium , large ) Impact : Determines cluster capacity based on Fyre's pre-defined configurations. Cannot customize individual resources. Related variables : - fyre_quota_type : Must be quick_burn for this to apply Note : Quick burn sizes are pre-defined by Fyre. For custom sizing, use fyre_quota_type=product_group instead. fyre_worker_count \u00a4 Number of worker nodes for custom Fyre clusters. Required (when cluster_type=fyre and fyre_quota_type=product_group ) Environment Variable: FYRE_WORKER_COUNT Default: 2 Purpose : Specifies the number of worker nodes to provision when using product_group quota with custom sizing. When to use : - Only applies when fyre_quota_type=product_group - Use 2+ for development/testing - Use 3+ for high availability testing Valid values : Positive integer (typically 2-10) Impact : Determines cluster capacity. More workers provide more resources but consume more quota. Related variables : - fyre_quota_type : Must be product_group for this to apply - fyre_worker_cpu , fyre_worker_memory : Resources per worker Note : Total cluster capacity = worker count \u00d7 (CPU + memory per worker). Consider quota limits. fyre_worker_cpu \u00a4 CPU cores per worker node for custom Fyre clusters. Required (when cluster_type=fyre and fyre_quota_type=product_group ) Environment Variable: FYRE_WORKER_CPU Default: 8 Purpose : Specifies the number of CPU cores to assign to each worker node when using product_group quota. When to use : - Only applies when fyre_quota_type=product_group - Use default (8) for standard workloads - Increase for CPU-intensive testing Valid values : Positive integer, maximum 16 (Fyre limit) Impact : Determines CPU capacity per worker. More CPUs provide better performance but consume more quota. Related variables : - fyre_quota_type : Must be product_group for this to apply - fyre_worker_count : Number of workers with this CPU allocation - fyre_worker_memory : Memory paired with CPU Note : Maximum 16 CPUs per worker (Fyre limitation). Total cluster CPUs = worker count \u00d7 CPU per worker. fyre_worker_memory \u00a4 Memory (GB) per worker node for custom Fyre clusters. Required (when cluster_type=fyre and fyre_quota_type=product_group ) Environment Variable: FYRE_WORKER_MEMORY Default: 32 Purpose : Specifies the amount of memory (in GB) to assign to each worker node when using product_group quota. When to use : - Only applies when fyre_quota_type=product_group - Use default (32GB) for standard workloads - Increase for memory-intensive testing Valid values : Positive integer, maximum 64 (Fyre limit) Impact : Determines memory capacity per worker. More memory supports larger workloads but consumes more quota. Related variables : - fyre_quota_type : Must be product_group for this to apply - fyre_worker_count : Number of workers with this memory allocation - fyre_worker_cpu : CPU paired with memory Note : Maximum 64GB per worker (Fyre limitation). Total cluster memory = worker count \u00d7 memory per worker. fyre_worker_additional_disks \u00a4 Additional disk sizes for Fyre worker nodes. Optional Environment Variable: FYRE_WORKER_ADDITIONAL_DISKS Default: None Purpose : Specifies additional disks to attach to each worker node. Useful for testing storage configurations or providing extra capacity. When to use : - Leave unset for standard deployments (no additional disks) - Set to add extra storage for testing or specific workloads - Use comma-separated list for multiple disks Valid values : Comma-separated list of disk sizes in GB (e.g., 400 , 400,400 , 200,300,400 ) Impact : Each specified disk is attached to every worker node. Increases storage capacity but consumes more quota. Related variables : - fyre_worker_count : Additional disks added to each worker Note : Example: 400,400 adds two 400GB disks to each worker. Total additional storage = disk sizes \u00d7 worker count. fyre_nfs_image_registry_size \u00a4 NFS storage size for OpenShift image registry. Optional Environment Variable: FYRE_NFS_IMAGE_REGISTRY_SIZE Default: 100Gi Purpose : Specifies the size of NFS storage allocated for the OpenShift image registry when NFS storage is configured on Fyre clusters. When to use : - Use default (100Gi) for most deployments - Increase for clusters with many container images - Only applies when ocp_storage_provider=nfs Valid values : Kubernetes storage size (e.g., 50Gi , 100Gi , 200Gi ) Impact : Determines image registry storage capacity. Size cannot exceed available storage on Fyre infrastructure node. Related variables : - ocp_storage_provider : Must be nfs for this to apply Note : Size is limited by available storage on the Fyre infrastructure node. Image registry stores all container images used in the cluster. enable_ipv6 \u00a4 Enable IPv6 networking for Fyre cluster. Optional Environment Variable: ENABLE_IPV6 Default: false Purpose : Enables IPv6 networking for the Fyre cluster. Only supported at the RTP (Raleigh) Fyre site. When to use : - Set to true for IPv6 testing - Only works at RTP site - Leave as false (default) for standard IPv4 networking Valid values : true , false Impact : - true : Cluster uses IPv6 networking (RTP site only) - false : Standard IPv4 networking Related variables : - fyre_site : Must be rtp for IPv6 support Note : IMPORTANT - IPv6 is only supported at the RTP (Raleigh) Fyre site. Will fail at other sites. Role Variables - IPI \u00a4 These variables are only used when cluster_type = ipi . Note IPI stands for Installer Provisioned Infrastructure . OpenShift offers two possible deployment methods: IPI and UPI (User Provisioned Infrastructure). The difference is the degree of automation and customization. IPI will not only deploy OpenShift but also all infrastructure components and configurations. ipi_platform \u00a4 Cloud platform for IPI cluster deployment. Optional Environment Variable: IPI_PLATFORM Default: aws Purpose : Specifies which cloud platform to use for Installer-Provisioned Infrastructure (IPI) deployment. Determines infrastructure provider and configuration requirements. When to use : - Use default ( aws ) for AWS deployments - Set to gcp for Google Cloud Platform deployments - Other platforms supported by openshift-install may work but are untested Valid values : aws , gcp (other openshift-install platforms may work) Impact : Determines which cloud provider is used and which provider-specific variables are required. Related variables : - aws_access_key_id , aws_secret_access_key : Required when aws - gcp_service_account_file , ipi_gcp_projectid : Required when gcp Note : AWS and GCP are tested and supported. Other platforms supported by openshift-install may work but have not been specifically tested. ipi_region \u00a4 Cloud platform region for cluster deployment. Optional Environment Variable: IPI_REGION Default: us-east-1 Purpose : Specifies the cloud platform region where the IPI cluster will be deployed. Determines physical location and available services. When to use : - Use default ( us-east-1 ) for AWS US East region - Change based on geographic requirements or latency needs - Ensure region supports required instance types Valid values : Valid region for the selected platform (e.g., us-east-1 , us-west-2 , eu-west-1 for AWS) Impact : Determines cluster location, which affects latency, data residency, and service availability. Related variables : - ipi_platform : Region must be valid for this platform Note : Not all regions support all instance types. Verify instance type availability in your target region. ipi_base_domain \u00a4 Base DNS domain for the cluster. Required (when cluster_type=ipi ) Environment Variable: IPI_BASE_DOMAIN Default: None Purpose : Specifies the base DNS domain for the OpenShift cluster. Used to construct cluster URLs and DNS records. When to use : - Always required for IPI cluster provisioning - Must be a domain you control - DNS must be properly configured for the domain Valid values : Valid DNS domain name (e.g., example.com , ocp.mycompany.com ) Impact : Cluster URLs will be subdomains of this base domain (e.g., api.clustername.example.com ). DNS must be configured to route to the cluster. Related variables : - cluster_name : Combined with base domain for cluster URLs Note : You must have control over this domain and ability to configure DNS records. The openshift-install process will create DNS records in this domain. ipi_pull_secret_file \u00a4 Path to Red Hat OpenShift pull secret file. Required (when cluster_type=ipi ) Environment Variable: IPI_PULL_SECRET_FILE Default: None Purpose : Specifies the location of the file containing your Red Hat OpenShift pull secret. Required to pull OpenShift container images during installation. When to use : - Always required for IPI cluster provisioning - Obtain from Red Hat Hybrid Cloud Console - Must be a valid, non-expired pull secret Valid values : Absolute path to pull secret JSON file Impact : Without a valid pull secret, cluster installation will fail when attempting to pull container images. Related variables : - None Note : Download your pull secret from the Red Hat Hybrid Cloud Console. Keep the file secure as it contains authentication credentials. ipi_dir \u00a4 Working directory for IPI installation. Optional Environment Variable: IPI_DIR Default: ~/openshift-install Purpose : Specifies the working directory for the IPI installation process. Contains the openshift-install executable, configuration files, and generated logs. When to use : - Use default for standard installations - Change to specify a different working location - Useful for organizing multiple cluster installations Valid values : Absolute filesystem path Impact : All installation files, configs, and logs are stored in this directory. Directory must be writable. Related variables : - cluster_name : Used in subdirectory structure Note : The directory will contain sensitive information (kubeconfig, credentials). Ensure appropriate permissions. Preserve this directory for cluster management and troubleshooting. sshKey \u00a4 Public SSH key for cluster node access. Optional Environment Variable: SSH_PUB_KEY Default: None Purpose : Specifies the public SSH key to be installed on OpenShift cluster nodes. Enables SSH access to nodes via bastion host. When to use : - Set to enable SSH access to cluster nodes for troubleshooting - Leave unset if SSH access is not needed - Useful for debugging and advanced cluster management Valid values : Valid SSH public key string (e.g., ssh-rsa AAAAB3... ) Impact : - When set: SSH key is installed on all cluster nodes - When unset: No SSH access to nodes (standard for managed clusters) Related variables : - None Note : SSH access to nodes is typically not needed for normal operations. Only set if you need direct node access for troubleshooting. ipi_controlplane_type \u00a4 Machine type for control plane nodes. Optional Environment Variable: IPI_CONTROLPLANE_TYPE Default: m5.4xlarge Purpose : Specifies the machine type for control plane (master) nodes in the IPI cluster. Determines CPU, memory, and network capacity for control plane. When to use : - Use default ( m5.4xlarge : 16 vCPU, 64GB RAM) for standard deployments - Increase for very large clusters (many nodes or workloads) - Decrease for small test clusters (not recommended for production) Valid values : Valid machine type for the selected platform (e.g., m5.2xlarge , m5.4xlarge , m5.8xlarge for AWS) Impact : Determines control plane capacity. Affects cluster management performance and costs. Undersized control plane can impact cluster stability. Related variables : - ipi_controlplane_replicas : Number of nodes with this type - ipi_platform : Machine type must be valid for this platform Note : Control plane nodes run cluster management services. Don't undersize for production. The default m5.4xlarge is suitable for most deployments. ipi_controlplane_replicas \u00a4 Number of control plane (master) nodes. Optional Environment Variable: IPI_CONTROLPLANE_REPLICAS Default: 3 Purpose : Specifies the number of control plane (master) nodes to provision. Determines cluster management high availability. When to use : - Use default (3) for production (high availability) - Must be odd number (1, 3, 5) for etcd quorum - Never use 1 for production (no high availability) Valid values : Odd positive integer (1, 3, 5, 7), 3 recommended for production Impact : Determines control plane high availability. 3 nodes provide HA with one node failure tolerance. Related variables : - ipi_controlplane_type : Machine type for each control plane node Note : IMPORTANT - Must be an odd number for etcd quorum. Use 3 for production (HA). Using 1 provides no high availability. ipi_compute_type \u00a4 Machine type for compute (worker) nodes. Optional Environment Variable: IPI_COMPUTE_TYPE Default: m5.4xlarge Purpose : Specifies the machine type for compute (worker) nodes in the IPI cluster. Determines CPU, memory, and network capacity for workloads. When to use : - Use default ( m5.4xlarge : 16 vCPU, 64GB RAM) for standard MAS deployments - Increase for larger workloads (e.g., m5.8xlarge , m5.12xlarge ) - Decrease for development/testing (e.g., m5.2xlarge ) Valid values : Valid machine type for the selected platform (e.g., m5.2xlarge , m5.4xlarge , m5.8xlarge for AWS) Impact : Determines worker node capacity. Affects workload performance and costs. Larger types cost more but provide more resources. Related variables : - ipi_compute_replicas : Number of nodes with this type - ipi_platform : Machine type must be valid for this platform Note : The default m5.4xlarge (16 vCPU, 64GB RAM) is suitable for most MAS deployments. Consider total cluster capacity (type \u00d7 replicas). ipi_compute_replicas \u00a4 Number of compute (worker) nodes. Optional Environment Variable: IPI_COMPUTE_REPLICAS Default: 3 Purpose : Specifies the number of compute (worker) nodes to provision. Determines cluster workload capacity and high availability. When to use : - Use default (3) for standard high-availability deployments - Increase for larger workloads or more applications - Minimum 2 required, 3+ recommended for production Valid values : Positive integer, minimum 2 (3+ recommended for production) Impact : Determines total cluster capacity. More nodes provide more capacity and better high availability but increase costs. Related variables : - ipi_compute_type : Machine type for each worker node Note : For production, use at least 3 workers for high availability. Total cluster capacity = worker count \u00d7 machine type resources. ipi_rootvolume_size \u00a4 Root volume size for cluster nodes. Optional Environment Variable: IPI_ROOTVOLUME_SIZE Default: Platform default (typically 120GB) Purpose : Specifies the size of the root volume (in GiB) for cluster nodes. Determines available disk space for OS, container images, and ephemeral storage. When to use : - Leave unset to use platform default (typically sufficient) - Increase for clusters with many container images - Increase for workloads with high ephemeral storage needs Valid values : Positive integer (GiB), e.g., 120 , 200 , 500 Impact : Larger volumes provide more disk space but increase costs. Insufficient space can cause node issues. Related variables : - None Note : Platform defaults are typically sufficient. Only increase if you have specific requirements for more disk space. Role Variables - AWS \u00a4 The following variables are only used when cluster_type = ipi and ipi_platform = aws . aws_access_key_id \u00a4 AWS access key ID for authentication. Required (when cluster_type=ipi and ipi_platform=aws ) Environment Variable: AWS_ACCESS_KEY_ID Default: None Purpose : Authenticates with AWS to provision IPI cluster infrastructure. Must have permissions to create VPCs, instances, load balancers, and other AWS resources. When to use : - Always required for AWS IPI cluster provisioning - Must be associated with IAM user or role with cluster creation permissions - Obtain from AWS IAM Valid values : AWS access key ID string (typically 20 characters, starts with AKIA ) Impact : Without valid credentials with appropriate permissions, cluster provisioning will fail. Related variables : - aws_secret_access_key : Secret key paired with this access key ID Note : The IAM user/role must have extensive permissions (VPC, EC2, ELB, Route53, IAM, etc.). Consider using a dedicated IAM user for cluster provisioning. aws_secret_access_key \u00a4 AWS secret access key for authentication. Required (when cluster_type=ipi and ipi_platform=aws ) Environment Variable: AWS_SECRET_ACCESS_KEY Default: None Purpose : Authenticates with AWS to provision IPI cluster infrastructure. Paired with AWS access key ID for authentication. When to use : - Always required for AWS IPI cluster provisioning - Must correspond to the provided access key ID - Keep secure and rotate regularly Valid values : AWS secret access key string (typically 40 characters) Impact : Without valid credentials, cluster provisioning will fail. Related variables : - aws_access_key_id : Access key ID paired with this secret key Note : Keep secret keys secure. Never commit to version control. Use environment variables or secure vaults. Rotate keys regularly. Role Variables - GCP \u00a4 The following variables are only used when cluster_type = ipi and ipi_platform = gcp . gcp_service_account_file \u00a4 Path to GCP service account credentials file. Required (when cluster_type=ipi and ipi_platform=gcp ) Environment Variable: GOOGLE_APPLICATION_CREDENTIALS Default: None Purpose : Authenticates with Google Cloud Platform to provision IPI cluster infrastructure. Service account must have permissions to create instances and networking resources. When to use : - Always required for GCP IPI cluster provisioning - Must be a valid service account JSON key file - Service account must have cluster creation permissions Valid values : Absolute path to GCP service account JSON key file Impact : Without valid credentials with appropriate permissions, cluster provisioning will fail. Related variables : - ipi_gcp_projectid : GCP project where cluster will be created Note : The service account must have extensive permissions (Compute, Networking, IAM, etc.). Download the JSON key file from GCP IAM. Keep the file secure. ipi_gcp_projectid \u00a4 GCP project ID for cluster deployment. Required (when cluster_type=ipi and ipi_platform=gcp ) Environment Variable: GOOGLE_PROJECTID Default: None Purpose : Specifies the GCP project where the IPI cluster will be deployed. All cluster resources are created in this project. When to use : - Always required for GCP IPI cluster provisioning - Must be a valid, existing GCP project - Service account must have permissions in this project Valid values : Valid GCP project ID string Impact : Cluster resources are created in this project. Costs are billed to this project. Related variables : - gcp_service_account_file : Service account must have permissions in this project Note : The project must exist before provisioning. Ensure the service account has appropriate permissions in the project. All cluster costs are billed to this project. Example Playbook \u00a4 - hosts: localhost vars: cluster_type: roks cluster_name: mycluster ocp_version: 4.10 ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_provision License \u00a4 EPL-2.0","title":"ocp_provision"},{"location":"roles/ocp_provision/#ocp_provision","text":"Provision OCP cluster on IBM Cloud ROKS, ROSA, or DevIT Fyre. Fyre clusters will be automatically reconfigured to enable NFS storage. By default this is made available via the nfs-client storage class and supports both ReadWriteOnce and ReadWriteMany access modes. The image-registry-storage PVC used by the OpenShift image registry component will also be reconfigured to use this storage class.","title":"ocp_provision"},{"location":"roles/ocp_provision/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/ocp_provision/#cluster_type","text":"Infrastructure provider type for cluster provisioning. Required Environment Variable: CLUSTER_TYPE Default: None Purpose : Specifies which infrastructure provider to use for provisioning the OpenShift cluster. Determines provisioning method and required variables. When to use : - Always required for cluster provisioning - Each type requires different provider-specific variables - Determines available features (e.g., GPU support for ROKS) Valid values : fyre , roks , rosa , ipi - fyre : IBM DevIT Fyre clusters (internal development) - roks : IBM Cloud Red Hat OpenShift Kubernetes Service - rosa : AWS Red Hat OpenShift Service on AWS - ipi : Installer-Provisioned Infrastructure (bare metal/on-premises) Impact : Determines provisioning workflow and which provider-specific variables are required. Each type has different capabilities and configuration options. Related variables : - cluster_name : Name for the new cluster - ocp_version : OpenShift version to install - Provider-specific variables (ibmcloud_apikey, rosa_token, fyre_apikey, etc.) Note : Fyre clusters automatically configure NFS storage. ROKS requires version format like 4.19_openshift .","title":"cluster_type"},{"location":"roles/ocp_provision/#cluster_name","text":"Name for the new cluster. Required Environment Variable: CLUSTER_NAME Default: None Purpose : Specifies the name for the OpenShift cluster to be provisioned. Used as the cluster identifier in the provider's system. When to use : - Always required for cluster provisioning - Must be unique within the provider's account/region - Used for cluster identification and resource naming Valid values : String following provider naming conventions (typically lowercase alphanumeric with hyphens) Impact : Determines the cluster name in the provider's system. Used for DNS, resource naming, and cluster identification. Related variables : - cluster_type : Provider where cluster will be created - ocp_version : OpenShift version for the cluster Note : Name must follow provider-specific naming rules. Some providers have length limits or character restrictions.","title":"cluster_name"},{"location":"roles/ocp_provision/#ocp_version","text":"OpenShift version to install. Required Environment Variable: OCP_VERSION Default: None Purpose : Specifies which version of OpenShift Container Platform to install on the provisioned cluster. When to use : - Always required for cluster provisioning - Use specific version for production (e.g., 4.19.14 ) - Use default for latest MAS-supported version - Use rotate for testing (version changes by day of week) Valid values : - Specific version: 4.19 , 4.19.14 - Alias: default (newest MAS-supported version) - Alias: rotate (predetermined version by day, for testing) - ROKS format : Must append _openshift (e.g., 4.19_openshift , 4.19.14_openshift ) Impact : Determines OpenShift version installed. Version must be compatible with MAS and available from the provider. Related variables : - cluster_type : ROKS requires _openshift suffix Note : IMPORTANT - For ROKS ( cluster_type=roks ), version MUST include _openshift suffix. The default alias selects the newest MAS-supported version. The rotate alias is for testing only.","title":"ocp_version"},{"location":"roles/ocp_provision/#ocp_storage_provider","text":"Storage provider configuration for Fyre clusters. Optional Environment Variable: OCP_STORAGE_PROVIDER Default: None Purpose : Configures NFS storage for Fyre clusters, creating an nfs-client storage class and reconfiguring the image registry. When to use : - Set to nfs for Fyre clusters to enable NFS storage - Only applies when cluster_type=fyre - Leave unset for other cluster types (ROKS, ROSA, IPI) Valid values : nfs (for Fyre clusters only) Impact : - nfs : Creates nfs-client storage class connected to infrastructure node, reconfigures image registry PVC to use NFS - Unset: No storage configuration changes Related variables : - cluster_type : Must be fyre for this to have effect Note : Only functional for Fyre clusters. When enabled, the existing image registry PVC is deleted and recreated with NFS storage. NFS storage class supports both ReadWriteOnce (RWO) and ReadWriteMany (RWX) access modes.","title":"ocp_storage_provider"},{"location":"roles/ocp_provision/#role-variables-gpu-node-support","text":"","title":"Role Variables - GPU Node Support"},{"location":"roles/ocp_provision/#ocp_provision_gpu","text":"Enable GPU worker nodes during provisioning. Optional Environment Variable: OCP_PROVISION_GPU Default: false Purpose : Controls whether GPU-enabled worker nodes are provisioned with the cluster. Required for GPU-intensive applications like MAS Visual Inspection (MVI). When to use : - Set to true for MAS Visual Inspection deployments - Set to true for other GPU-intensive workloads - Leave as false (default) for standard deployments - Currently only supported for ROKS clusters Valid values : true , false Impact : - true : Provisions GPU worker pool with specified number of GPU nodes - false : No GPU nodes provisioned (standard cluster) Related variables : - gpu_workerpool_name : Name of GPU worker pool - gpu_workers : Number of GPU nodes to provision - cluster_type : Must be roks for GPU support Note : GPU support is currently only available for ROKS clusters. GPU nodes use mg4c.32x384.2xp100-GPU flavor with P100 GPUs.","title":"ocp_provision_gpu"},{"location":"roles/ocp_provision/#gpu_workerpool_name","text":"Name for GPU worker pool. Optional Environment Variable: GPU_WORKERPOOL_NAME Default: gpu Purpose : Specifies the name for the GPU worker pool to be created or modified in the cluster. When to use : - Use default ( gpu ) for new GPU deployments - Set to existing pool name to modify rather than create new - Only applies when ocp_provision_gpu=true Valid values : String following worker pool naming conventions Impact : Determines GPU worker pool name. Using an existing name modifies that pool; using a new name creates a new pool. Related variables : - ocp_provision_gpu : Must be true for this to apply - gpu_workers : Number of nodes in this pool - cluster_type : Must be roks Note : If a GPU worker pool already exists with this name, it will be modified rather than creating a duplicate. Use the existing name to avoid multiple GPU pools.","title":"gpu_workerpool_name"},{"location":"roles/ocp_provision/#gpu_workers","text":"Number of GPU worker nodes to provision in the cluster. Optional Environment Variable: GPU_WORKERS Default: 1 Purpose : Specifies how many GPU-enabled worker nodes to provision in the GPU worker pool. Each node uses mg4c.32x384.2xp100-GPU flavor with P100 GPUs. When to use : - Use default (1) for minimal GPU deployments or testing - Increase for production MAS Visual Inspection deployments - Scale based on GPU workload requirements - Only applies when ocp_provision_gpu=true Valid values : Positive integer (e.g., 1 , 2 , 3 ) Impact : Determines the number of GPU nodes provisioned. More nodes provide more GPU capacity but increase costs. Related variables : - ocp_provision_gpu : Must be true for this to apply - gpu_workerpool_name : Name of the pool containing these nodes - cluster_type : Must be roks for GPU support Note : GPU nodes use expensive hardware (P100 GPUs). Only provision what you need. Currently only supported on ROKS clusters.","title":"gpu_workers"},{"location":"roles/ocp_provision/#role-variables-roks","text":"The following variables are only used when cluster_type = roks .","title":"Role Variables - ROKS"},{"location":"roles/ocp_provision/#ibmcloud_apikey","text":"IBM Cloud API key for authentication. Required (when cluster_type=roks ) Environment Variable: IBMCLOUD_APIKEY Default: None Purpose : Authenticates with IBM Cloud to provision and manage ROKS clusters. Used by the ibmcloud CLI for all cluster operations. When to use : - Always required for ROKS cluster provisioning - Must have permissions to create clusters in the target resource group - Obtain from IBM Cloud IAM (Identity and Access Management) Valid values : IBM Cloud API key string (typically 40+ characters) Impact : Without a valid API key, cluster provisioning will fail. The key must have appropriate IAM permissions for cluster creation. Related variables : - ibmcloud_resourcegroup : Resource group where cluster will be created - ibmcloud_endpoint : IBM Cloud API endpoint to authenticate against Note : Keep API keys secure. Use environment variables or secure vaults rather than hardcoding in playbooks. The key needs cluster creation permissions in the specified resource group.","title":"ibmcloud_apikey"},{"location":"roles/ocp_provision/#ibmcloud_endpoint","text":"IBM Cloud API endpoint URL. Optional Environment Variable: IBMCLOUD_ENDPOINT Default: https://cloud.ibm.com Purpose : Specifies the IBM Cloud API endpoint for authentication and cluster operations. Allows targeting different IBM Cloud environments. When to use : - Use default for standard IBM Cloud (public cloud) - Override for IBM Cloud dedicated or private environments - Change for testing against staging environments Valid values : Valid IBM Cloud API endpoint URL Impact : Determines which IBM Cloud environment is targeted for cluster provisioning. Related variables : - ibmcloud_apikey : API key used with this endpoint Note : The default endpoint works for standard IBM Cloud deployments. Only change if you're using a dedicated or private IBM Cloud environment.","title":"ibmcloud_endpoint"},{"location":"roles/ocp_provision/#ibmcloud_resourcegroup","text":"IBM Cloud resource group for the cluster. Optional Environment Variable: IBMCLOUD_RESOURCEGROUP Default: Default Purpose : Specifies which IBM Cloud resource group the ROKS cluster will be created in. Resource groups organize and manage access to IBM Cloud resources. When to use : - Use default ( Default ) for simple deployments - Specify a different resource group for organizational separation - Ensure the API key has access to the specified resource group Valid values : Name of an existing IBM Cloud resource group Impact : Cluster is created in the specified resource group. The API key must have permissions in this resource group. Related variables : - ibmcloud_apikey : Must have permissions in this resource group Note : The resource group must exist before provisioning. The API key must have appropriate IAM permissions in the target resource group.","title":"ibmcloud_resourcegroup"},{"location":"roles/ocp_provision/#roks_zone","text":"IBM Cloud availability zone for cluster deployment. Optional Environment Variable: ROKS_ZONE Default: dal10 Purpose : Specifies the IBM Cloud availability zone where the ROKS cluster will be provisioned. Determines the physical location of cluster resources. When to use : - Use default ( dal10 ) for Dallas datacenter - Change based on geographic requirements or latency needs - Consider data residency and compliance requirements Valid values : Valid IBM Cloud zone identifier (e.g., dal10 , lon02 , fra02 , tok02 ) Impact : Determines cluster location, which affects latency, data residency, and available services. Related variables : - roks_flavor : Worker node flavors available vary by zone Note : Not all zones support all worker node flavors. Verify flavor availability in your target zone. Zone selection affects network latency to your users and services.","title":"roks_zone"},{"location":"roles/ocp_provision/#roks_flavor","text":"Worker node machine type for ROKS cluster. Optional Environment Variable: ROKS_FLAVOR Default: b3c.16x64.300gb Purpose : Specifies the machine type (flavor) for worker nodes in the ROKS cluster. Determines CPU, memory, and local storage for each worker node. When to use : - Use default ( b3c.16x64.300gb ) for standard MAS deployments (16 vCPU, 64GB RAM, 300GB storage) - Increase for larger workloads or more applications - Decrease for development/testing environments Valid values : Valid IBM Cloud worker node flavor (e.g., b3c.4x16 , b3c.16x64.300gb , b3c.32x128 ) Impact : Determines worker node capacity. Affects cluster performance and cost. Larger flavors cost more but provide more resources. Related variables : - roks_workers : Number of nodes with this flavor - roks_zone : Not all flavors available in all zones Note : The default flavor (16 vCPU, 64GB RAM) is suitable for most MAS deployments. Verify flavor availability in your target zone. Consider total cluster capacity (flavor \u00d7 worker count).","title":"roks_flavor"},{"location":"roles/ocp_provision/#roks_workers","text":"Number of worker nodes in the ROKS cluster. Optional Environment Variable: ROKS_WORKERS Default: 3 Purpose : Specifies how many worker nodes to provision in the ROKS cluster. Determines cluster capacity and high availability. When to use : - Use default (3) for standard high-availability deployments - Increase for larger workloads or more applications - Minimum 3 recommended for production (high availability) Valid values : Positive integer, minimum 1 (3+ recommended for production) Impact : Determines total cluster capacity (workers \u00d7 flavor resources). More workers provide more capacity and better high availability but increase costs. Related variables : - roks_flavor : Machine type for each worker - ocp_provision_gpu : Additional GPU workers can be added separately Note : For production, use at least 3 workers for high availability. Total cluster capacity = worker count \u00d7 flavor resources. Consider workload requirements when sizing.","title":"roks_workers"},{"location":"roles/ocp_provision/#roks_flags","text":"Additional flags for ROKS cluster creation. Optional Environment Variable: ROKS_FLAGS Default: None Purpose : Allows passing additional command-line flags to the ibmcloud ks cluster create command for advanced cluster configuration. When to use : - Leave unset for standard deployments - Use for advanced configurations not covered by other variables - Consult IBM Cloud documentation for available flags Valid values : Valid ibmcloud CLI flags (e.g., --disable-public-service-endpoint , --pod-subnet , --service-subnet ) Impact : Passes additional configuration options to cluster creation. Incorrect flags may cause provisioning to fail. Related variables : - All other ROKS variables: Flags supplement standard configuration Note : Use with caution. Incorrect flags can cause provisioning failures. Consult IBM Cloud Kubernetes Service documentation for available options.","title":"roks_flags"},{"location":"roles/ocp_provision/#role-variables-rosa","text":"The following variables are only used when cluster_type = rosa .","title":"Role Variables - ROSA"},{"location":"roles/ocp_provision/#rosa_token","text":"Red Hat OpenShift Service on AWS (ROSA) authentication token. Required (when cluster_type=rosa ) Environment Variable: ROSA_TOKEN Default: None Purpose : Authenticates with the ROSA service to provision and manage OpenShift clusters on AWS. Required for all ROSA cluster operations. When to use : - Always required for ROSA cluster provisioning - Obtain from OpenShift Cluster Manager - Token must be valid and not expired Valid values : ROSA API token string from Red Hat OpenShift Cluster Manager Impact : Without a valid token, ROSA cluster provisioning will fail. Token must have permissions to create clusters. Related variables : - cluster_name : Name for the ROSA cluster - rosa_compute_nodes : Number of worker nodes to provision Note : Tokens expire periodically. Obtain a fresh token from the OpenShift Cluster Manager before provisioning. Keep tokens secure.","title":"rosa_token"},{"location":"roles/ocp_provision/#rosa_cluster_admin_password","text":"Password for the cluster-admin user account. Optional Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default: None (auto-generated) Purpose : Sets the password for the cluster-admin user account on the ROSA cluster. Used to log into the cluster after provisioning. When to use : - Set to specify a known password for cluster access - Leave unset to auto-generate a secure password - Auto-generated password is saved to config directory Valid values : String meeting OpenShift password requirements (typically 8+ characters) Impact : - When set: Uses specified password for cluster-admin account - When unset: Auto-generates a secure password and saves it to config Related variables : - rosa_config_dir : Location where auto-generated password is saved Note : If not set, the auto-generated password is saved in the rosa config file. Keep passwords secure. Consider using auto-generation for better security.","title":"rosa_cluster_admin_password"},{"location":"roles/ocp_provision/#rosa_compute_nodes","text":"Number of compute (worker) nodes in the ROSA cluster. Optional Environment Variable: ROSA_COMPUTE_NODES Default: 3 Purpose : Specifies how many worker nodes to provision in the ROSA cluster. Determines cluster capacity and high availability. When to use : - Use default (3) for standard high-availability deployments - Increase for larger workloads or more applications - Minimum 2 required, 3+ recommended for production Valid values : Positive integer, minimum 2 (3+ recommended for production) Impact : Determines total cluster capacity. More nodes provide more capacity and better high availability but increase AWS costs. Related variables : - rosa_compute_machine_type : Machine type for each worker node Note : For production, use at least 3 workers for high availability. Total cluster capacity = worker count \u00d7 machine type resources. ROSA clusters run on AWS infrastructure.","title":"rosa_compute_nodes"},{"location":"roles/ocp_provision/#rosa_compute_machine_type","text":"AWS machine type for ROSA worker nodes. Optional Environment Variable: ROSA_COMPUTE_MACHINE_TYPE Default: m5.4xlarge Purpose : Specifies the AWS EC2 instance type for worker nodes in the ROSA cluster. Determines CPU, memory, and network capacity for each worker. When to use : - Use default ( m5.4xlarge : 16 vCPU, 64GB RAM) for standard MAS deployments - Increase for larger workloads (e.g., m5.8xlarge , m5.12xlarge ) - Decrease for development/testing (e.g., m5.2xlarge ) Valid values : Valid AWS EC2 instance type (e.g., m5.2xlarge , m5.4xlarge , m5.8xlarge , m5.12xlarge ) Impact : Determines worker node capacity. Affects cluster performance and AWS costs. Larger instance types cost more but provide more resources. Related variables : - rosa_compute_nodes : Number of nodes with this instance type Note : The default m5.4xlarge (16 vCPU, 64GB RAM) is suitable for most MAS deployments. Consider total cluster capacity (instance type \u00d7 node count). Verify instance type availability in your AWS region.","title":"rosa_compute_machine_type"},{"location":"roles/ocp_provision/#rosa_config_dir","text":"Directory for storing ROSA cluster configuration files. Optional Environment Variable: ROSA_CONFIG_DIR Default: None Purpose : Specifies the directory where ROSA cluster configuration files are saved, including the rosa-{{cluster_name}}-details.yaml file containing API endpoint and cluster-admin credentials. When to use : - Set to save cluster details to a specific location - Leave unset to skip saving configuration files - Useful for automation and cluster access management Valid values : Absolute filesystem path (e.g., /tmp/rosa-config , ~/rosa-clusters ) Impact : - When set: Cluster details (API endpoint, credentials) are saved to this directory - When unset: Configuration files are not saved (must retrieve details manually) Related variables : - rosa_cluster_admin_password : Auto-generated password saved here if not specified - cluster_name : Used in config filename Note : The config file contains sensitive information (cluster-admin password). Ensure the directory has appropriate permissions. File format: rosa-{{cluster_name}}-details.yaml .","title":"rosa_config_dir"},{"location":"roles/ocp_provision/#role-variables-fyre","text":"The following variables are only used when cluster_type = fyre .","title":"Role Variables - FYRE"},{"location":"roles/ocp_provision/#fyre_username","text":"Fyre username for authentication. Required (when cluster_type=fyre ) Environment Variable: FYRE_USERNAME Default: None Purpose : Authenticates with the IBM DevIT Fyre API to provision and manage OpenShift clusters on Fyre infrastructure. When to use : - Always required for Fyre cluster provisioning - Must be a valid Fyre account username - Used for internal IBM development and testing Valid values : Valid Fyre username (IBM intranet ID) Impact : Without valid credentials, Fyre cluster provisioning will fail. Related variables : - fyre_apikey : API key paired with this username Note : Fyre is an internal IBM development platform. Access requires IBM credentials and appropriate permissions.","title":"fyre_username"},{"location":"roles/ocp_provision/#fyre_apikey","text":"Fyre API key for authentication. Required (when cluster_type=fyre ) Environment Variable: FYRE_APIKEY Default: None Purpose : Authenticates with the Fyre API for cluster provisioning operations. Paired with Fyre username for authentication. When to use : - Always required for Fyre cluster provisioning - Obtain from Fyre portal - Keep secure and rotate regularly Valid values : Valid Fyre API key string Impact : Without a valid API key, Fyre cluster provisioning will fail. Related variables : - fyre_username : Username paired with this API key Note : Keep API keys secure. Obtain from the Fyre portal. Keys may expire and need renewal.","title":"fyre_apikey"},{"location":"roles/ocp_provision/#fyre_quota_type","text":"Fyre quota type for cluster provisioning. Required (when cluster_type=fyre ) Environment Variable: FYRE_QUOTA_TYPE Default: quick_burn Purpose : Specifies which type of Fyre quota to use for cluster provisioning. Determines billing method and available configuration options. When to use : - Use quick_burn (default) for pre-defined cluster sizes (faster, simpler) - Use product_group for custom cluster configurations (more control) - Choice affects which other variables are required Valid values : quick_burn , product_group Impact : - quick_burn : Uses pre-defined cluster sizes, requires fyre_cluster_size - product_group : Allows custom sizing, requires fyre_worker_count , fyre_worker_cpu , fyre_worker_memory Related variables : - fyre_cluster_size : Required when quick_burn - fyre_worker_count , fyre_worker_cpu , fyre_worker_memory : Required when product_group - fyre_product_id : Required for both types Note : Quick burn is simpler but less flexible. Product group allows custom sizing but requires more configuration.","title":"fyre_quota_type"},{"location":"roles/ocp_provision/#fyre_product_id","text":"Product ID for Fyre accounting. Required (when cluster_type=fyre ) Environment Variable: FYRE_PRODUCT_ID Default: None Purpose : Associates the Fyre cluster with a product ID for internal IBM accounting and cost tracking purposes. When to use : - Always required for Fyre cluster provisioning - Obtain from your IBM product team or manager - Used for internal cost allocation Valid values : Valid IBM product ID Impact : Cluster costs are charged to this product ID. Incorrect ID may cause provisioning to fail or incorrect billing. Related variables : - fyre_quota_type : Product ID required for both quota types Note : Contact your IBM product team or manager to obtain the correct product ID for your project.","title":"fyre_product_id"},{"location":"roles/ocp_provision/#fyre_site","text":"Fyre datacenter site location. Optional Environment Variable: FYRE_SITE Default: svl Purpose : Specifies which Fyre datacenter site to provision the cluster in. Determines physical location and network connectivity. When to use : - Use default ( svl - San Jose/Silicon Valley) for most cases - Change based on geographic requirements or network proximity - Consider latency to your development location Valid values : Valid Fyre site code (e.g., svl , rtp , raleigh ) Impact : Determines cluster location, which affects network latency and available resources. Related variables : - enable_ipv6 : IPv6 only available at RTP site Note : Not all sites support all features. SVL is the default and most commonly used site.","title":"fyre_site"},{"location":"roles/ocp_provision/#fyre_cluster_description","text":"Description for the Fyre cluster. Optional Environment Variable: FYRE_CLUSTER_DESCRIPTION Default: None Purpose : Provides a human-readable description for the Fyre cluster. Helps identify cluster purpose in Fyre portal. When to use : - Set to document cluster purpose (e.g., \"MAS 9.0 testing\", \"Development cluster\") - Leave unset for unnamed clusters - Useful for tracking and managing multiple clusters Valid values : Any descriptive string Impact : Description appears in Fyre portal. No functional impact on cluster. Related variables : - cluster_name : Cluster identifier Note : Good descriptions help manage multiple clusters. Include purpose, owner, or project information.","title":"fyre_cluster_description"},{"location":"roles/ocp_provision/#ocp_fips_enabled","text":"Enable FIPS mode for the cluster. Optional Environment Variable: OCP_FIPS_ENABLED Default: false Purpose : Controls whether the OpenShift cluster is provisioned with FIPS (Federal Information Processing Standards) 140-2 cryptographic mode enabled. When to use : - Set to true for compliance with FIPS 140-2 requirements - Set to true for government or regulated environments - Leave as false (default) for standard deployments Valid values : true , false Impact : - true : Cluster uses FIPS-validated cryptographic modules (required for some compliance) - false : Standard cryptography (better performance) Related variables : - cluster_type : FIPS support varies by cluster type Note : FIPS mode may impact performance. Only enable if required for compliance. Cannot be changed after cluster creation.","title":"ocp_fips_enabled"},{"location":"roles/ocp_provision/#fyre_cluster_size","text":"Pre-defined Fyre cluster size. Required (when cluster_type=fyre and fyre_quota_type=quick_burn ) Environment Variable: FYRE_CLUSTER_SIZE Default: medium Purpose : Specifies which pre-defined cluster size to use when provisioning with quick_burn quota. Determines worker node count and resources. When to use : - Only applies when fyre_quota_type=quick_burn - Use medium (default) for standard development/testing - Use small for minimal testing - Use large for more intensive workloads Valid values : Fyre pre-defined sizes (e.g., small , medium , large ) Impact : Determines cluster capacity based on Fyre's pre-defined configurations. Cannot customize individual resources. Related variables : - fyre_quota_type : Must be quick_burn for this to apply Note : Quick burn sizes are pre-defined by Fyre. For custom sizing, use fyre_quota_type=product_group instead.","title":"fyre_cluster_size"},{"location":"roles/ocp_provision/#fyre_worker_count","text":"Number of worker nodes for custom Fyre clusters. Required (when cluster_type=fyre and fyre_quota_type=product_group ) Environment Variable: FYRE_WORKER_COUNT Default: 2 Purpose : Specifies the number of worker nodes to provision when using product_group quota with custom sizing. When to use : - Only applies when fyre_quota_type=product_group - Use 2+ for development/testing - Use 3+ for high availability testing Valid values : Positive integer (typically 2-10) Impact : Determines cluster capacity. More workers provide more resources but consume more quota. Related variables : - fyre_quota_type : Must be product_group for this to apply - fyre_worker_cpu , fyre_worker_memory : Resources per worker Note : Total cluster capacity = worker count \u00d7 (CPU + memory per worker). Consider quota limits.","title":"fyre_worker_count"},{"location":"roles/ocp_provision/#fyre_worker_cpu","text":"CPU cores per worker node for custom Fyre clusters. Required (when cluster_type=fyre and fyre_quota_type=product_group ) Environment Variable: FYRE_WORKER_CPU Default: 8 Purpose : Specifies the number of CPU cores to assign to each worker node when using product_group quota. When to use : - Only applies when fyre_quota_type=product_group - Use default (8) for standard workloads - Increase for CPU-intensive testing Valid values : Positive integer, maximum 16 (Fyre limit) Impact : Determines CPU capacity per worker. More CPUs provide better performance but consume more quota. Related variables : - fyre_quota_type : Must be product_group for this to apply - fyre_worker_count : Number of workers with this CPU allocation - fyre_worker_memory : Memory paired with CPU Note : Maximum 16 CPUs per worker (Fyre limitation). Total cluster CPUs = worker count \u00d7 CPU per worker.","title":"fyre_worker_cpu"},{"location":"roles/ocp_provision/#fyre_worker_memory","text":"Memory (GB) per worker node for custom Fyre clusters. Required (when cluster_type=fyre and fyre_quota_type=product_group ) Environment Variable: FYRE_WORKER_MEMORY Default: 32 Purpose : Specifies the amount of memory (in GB) to assign to each worker node when using product_group quota. When to use : - Only applies when fyre_quota_type=product_group - Use default (32GB) for standard workloads - Increase for memory-intensive testing Valid values : Positive integer, maximum 64 (Fyre limit) Impact : Determines memory capacity per worker. More memory supports larger workloads but consumes more quota. Related variables : - fyre_quota_type : Must be product_group for this to apply - fyre_worker_count : Number of workers with this memory allocation - fyre_worker_cpu : CPU paired with memory Note : Maximum 64GB per worker (Fyre limitation). Total cluster memory = worker count \u00d7 memory per worker.","title":"fyre_worker_memory"},{"location":"roles/ocp_provision/#fyre_worker_additional_disks","text":"Additional disk sizes for Fyre worker nodes. Optional Environment Variable: FYRE_WORKER_ADDITIONAL_DISKS Default: None Purpose : Specifies additional disks to attach to each worker node. Useful for testing storage configurations or providing extra capacity. When to use : - Leave unset for standard deployments (no additional disks) - Set to add extra storage for testing or specific workloads - Use comma-separated list for multiple disks Valid values : Comma-separated list of disk sizes in GB (e.g., 400 , 400,400 , 200,300,400 ) Impact : Each specified disk is attached to every worker node. Increases storage capacity but consumes more quota. Related variables : - fyre_worker_count : Additional disks added to each worker Note : Example: 400,400 adds two 400GB disks to each worker. Total additional storage = disk sizes \u00d7 worker count.","title":"fyre_worker_additional_disks"},{"location":"roles/ocp_provision/#fyre_nfs_image_registry_size","text":"NFS storage size for OpenShift image registry. Optional Environment Variable: FYRE_NFS_IMAGE_REGISTRY_SIZE Default: 100Gi Purpose : Specifies the size of NFS storage allocated for the OpenShift image registry when NFS storage is configured on Fyre clusters. When to use : - Use default (100Gi) for most deployments - Increase for clusters with many container images - Only applies when ocp_storage_provider=nfs Valid values : Kubernetes storage size (e.g., 50Gi , 100Gi , 200Gi ) Impact : Determines image registry storage capacity. Size cannot exceed available storage on Fyre infrastructure node. Related variables : - ocp_storage_provider : Must be nfs for this to apply Note : Size is limited by available storage on the Fyre infrastructure node. Image registry stores all container images used in the cluster.","title":"fyre_nfs_image_registry_size"},{"location":"roles/ocp_provision/#enable_ipv6","text":"Enable IPv6 networking for Fyre cluster. Optional Environment Variable: ENABLE_IPV6 Default: false Purpose : Enables IPv6 networking for the Fyre cluster. Only supported at the RTP (Raleigh) Fyre site. When to use : - Set to true for IPv6 testing - Only works at RTP site - Leave as false (default) for standard IPv4 networking Valid values : true , false Impact : - true : Cluster uses IPv6 networking (RTP site only) - false : Standard IPv4 networking Related variables : - fyre_site : Must be rtp for IPv6 support Note : IMPORTANT - IPv6 is only supported at the RTP (Raleigh) Fyre site. Will fail at other sites.","title":"enable_ipv6"},{"location":"roles/ocp_provision/#role-variables-ipi","text":"These variables are only used when cluster_type = ipi . Note IPI stands for Installer Provisioned Infrastructure . OpenShift offers two possible deployment methods: IPI and UPI (User Provisioned Infrastructure). The difference is the degree of automation and customization. IPI will not only deploy OpenShift but also all infrastructure components and configurations.","title":"Role Variables - IPI"},{"location":"roles/ocp_provision/#ipi_platform","text":"Cloud platform for IPI cluster deployment. Optional Environment Variable: IPI_PLATFORM Default: aws Purpose : Specifies which cloud platform to use for Installer-Provisioned Infrastructure (IPI) deployment. Determines infrastructure provider and configuration requirements. When to use : - Use default ( aws ) for AWS deployments - Set to gcp for Google Cloud Platform deployments - Other platforms supported by openshift-install may work but are untested Valid values : aws , gcp (other openshift-install platforms may work) Impact : Determines which cloud provider is used and which provider-specific variables are required. Related variables : - aws_access_key_id , aws_secret_access_key : Required when aws - gcp_service_account_file , ipi_gcp_projectid : Required when gcp Note : AWS and GCP are tested and supported. Other platforms supported by openshift-install may work but have not been specifically tested.","title":"ipi_platform"},{"location":"roles/ocp_provision/#ipi_region","text":"Cloud platform region for cluster deployment. Optional Environment Variable: IPI_REGION Default: us-east-1 Purpose : Specifies the cloud platform region where the IPI cluster will be deployed. Determines physical location and available services. When to use : - Use default ( us-east-1 ) for AWS US East region - Change based on geographic requirements or latency needs - Ensure region supports required instance types Valid values : Valid region for the selected platform (e.g., us-east-1 , us-west-2 , eu-west-1 for AWS) Impact : Determines cluster location, which affects latency, data residency, and service availability. Related variables : - ipi_platform : Region must be valid for this platform Note : Not all regions support all instance types. Verify instance type availability in your target region.","title":"ipi_region"},{"location":"roles/ocp_provision/#ipi_base_domain","text":"Base DNS domain for the cluster. Required (when cluster_type=ipi ) Environment Variable: IPI_BASE_DOMAIN Default: None Purpose : Specifies the base DNS domain for the OpenShift cluster. Used to construct cluster URLs and DNS records. When to use : - Always required for IPI cluster provisioning - Must be a domain you control - DNS must be properly configured for the domain Valid values : Valid DNS domain name (e.g., example.com , ocp.mycompany.com ) Impact : Cluster URLs will be subdomains of this base domain (e.g., api.clustername.example.com ). DNS must be configured to route to the cluster. Related variables : - cluster_name : Combined with base domain for cluster URLs Note : You must have control over this domain and ability to configure DNS records. The openshift-install process will create DNS records in this domain.","title":"ipi_base_domain"},{"location":"roles/ocp_provision/#ipi_pull_secret_file","text":"Path to Red Hat OpenShift pull secret file. Required (when cluster_type=ipi ) Environment Variable: IPI_PULL_SECRET_FILE Default: None Purpose : Specifies the location of the file containing your Red Hat OpenShift pull secret. Required to pull OpenShift container images during installation. When to use : - Always required for IPI cluster provisioning - Obtain from Red Hat Hybrid Cloud Console - Must be a valid, non-expired pull secret Valid values : Absolute path to pull secret JSON file Impact : Without a valid pull secret, cluster installation will fail when attempting to pull container images. Related variables : - None Note : Download your pull secret from the Red Hat Hybrid Cloud Console. Keep the file secure as it contains authentication credentials.","title":"ipi_pull_secret_file"},{"location":"roles/ocp_provision/#ipi_dir","text":"Working directory for IPI installation. Optional Environment Variable: IPI_DIR Default: ~/openshift-install Purpose : Specifies the working directory for the IPI installation process. Contains the openshift-install executable, configuration files, and generated logs. When to use : - Use default for standard installations - Change to specify a different working location - Useful for organizing multiple cluster installations Valid values : Absolute filesystem path Impact : All installation files, configs, and logs are stored in this directory. Directory must be writable. Related variables : - cluster_name : Used in subdirectory structure Note : The directory will contain sensitive information (kubeconfig, credentials). Ensure appropriate permissions. Preserve this directory for cluster management and troubleshooting.","title":"ipi_dir"},{"location":"roles/ocp_provision/#sshkey","text":"Public SSH key for cluster node access. Optional Environment Variable: SSH_PUB_KEY Default: None Purpose : Specifies the public SSH key to be installed on OpenShift cluster nodes. Enables SSH access to nodes via bastion host. When to use : - Set to enable SSH access to cluster nodes for troubleshooting - Leave unset if SSH access is not needed - Useful for debugging and advanced cluster management Valid values : Valid SSH public key string (e.g., ssh-rsa AAAAB3... ) Impact : - When set: SSH key is installed on all cluster nodes - When unset: No SSH access to nodes (standard for managed clusters) Related variables : - None Note : SSH access to nodes is typically not needed for normal operations. Only set if you need direct node access for troubleshooting.","title":"sshKey"},{"location":"roles/ocp_provision/#ipi_controlplane_type","text":"Machine type for control plane nodes. Optional Environment Variable: IPI_CONTROLPLANE_TYPE Default: m5.4xlarge Purpose : Specifies the machine type for control plane (master) nodes in the IPI cluster. Determines CPU, memory, and network capacity for control plane. When to use : - Use default ( m5.4xlarge : 16 vCPU, 64GB RAM) for standard deployments - Increase for very large clusters (many nodes or workloads) - Decrease for small test clusters (not recommended for production) Valid values : Valid machine type for the selected platform (e.g., m5.2xlarge , m5.4xlarge , m5.8xlarge for AWS) Impact : Determines control plane capacity. Affects cluster management performance and costs. Undersized control plane can impact cluster stability. Related variables : - ipi_controlplane_replicas : Number of nodes with this type - ipi_platform : Machine type must be valid for this platform Note : Control plane nodes run cluster management services. Don't undersize for production. The default m5.4xlarge is suitable for most deployments.","title":"ipi_controlplane_type"},{"location":"roles/ocp_provision/#ipi_controlplane_replicas","text":"Number of control plane (master) nodes. Optional Environment Variable: IPI_CONTROLPLANE_REPLICAS Default: 3 Purpose : Specifies the number of control plane (master) nodes to provision. Determines cluster management high availability. When to use : - Use default (3) for production (high availability) - Must be odd number (1, 3, 5) for etcd quorum - Never use 1 for production (no high availability) Valid values : Odd positive integer (1, 3, 5, 7), 3 recommended for production Impact : Determines control plane high availability. 3 nodes provide HA with one node failure tolerance. Related variables : - ipi_controlplane_type : Machine type for each control plane node Note : IMPORTANT - Must be an odd number for etcd quorum. Use 3 for production (HA). Using 1 provides no high availability.","title":"ipi_controlplane_replicas"},{"location":"roles/ocp_provision/#ipi_compute_type","text":"Machine type for compute (worker) nodes. Optional Environment Variable: IPI_COMPUTE_TYPE Default: m5.4xlarge Purpose : Specifies the machine type for compute (worker) nodes in the IPI cluster. Determines CPU, memory, and network capacity for workloads. When to use : - Use default ( m5.4xlarge : 16 vCPU, 64GB RAM) for standard MAS deployments - Increase for larger workloads (e.g., m5.8xlarge , m5.12xlarge ) - Decrease for development/testing (e.g., m5.2xlarge ) Valid values : Valid machine type for the selected platform (e.g., m5.2xlarge , m5.4xlarge , m5.8xlarge for AWS) Impact : Determines worker node capacity. Affects workload performance and costs. Larger types cost more but provide more resources. Related variables : - ipi_compute_replicas : Number of nodes with this type - ipi_platform : Machine type must be valid for this platform Note : The default m5.4xlarge (16 vCPU, 64GB RAM) is suitable for most MAS deployments. Consider total cluster capacity (type \u00d7 replicas).","title":"ipi_compute_type"},{"location":"roles/ocp_provision/#ipi_compute_replicas","text":"Number of compute (worker) nodes. Optional Environment Variable: IPI_COMPUTE_REPLICAS Default: 3 Purpose : Specifies the number of compute (worker) nodes to provision. Determines cluster workload capacity and high availability. When to use : - Use default (3) for standard high-availability deployments - Increase for larger workloads or more applications - Minimum 2 required, 3+ recommended for production Valid values : Positive integer, minimum 2 (3+ recommended for production) Impact : Determines total cluster capacity. More nodes provide more capacity and better high availability but increase costs. Related variables : - ipi_compute_type : Machine type for each worker node Note : For production, use at least 3 workers for high availability. Total cluster capacity = worker count \u00d7 machine type resources.","title":"ipi_compute_replicas"},{"location":"roles/ocp_provision/#ipi_rootvolume_size","text":"Root volume size for cluster nodes. Optional Environment Variable: IPI_ROOTVOLUME_SIZE Default: Platform default (typically 120GB) Purpose : Specifies the size of the root volume (in GiB) for cluster nodes. Determines available disk space for OS, container images, and ephemeral storage. When to use : - Leave unset to use platform default (typically sufficient) - Increase for clusters with many container images - Increase for workloads with high ephemeral storage needs Valid values : Positive integer (GiB), e.g., 120 , 200 , 500 Impact : Larger volumes provide more disk space but increase costs. Insufficient space can cause node issues. Related variables : - None Note : Platform defaults are typically sufficient. Only increase if you have specific requirements for more disk space.","title":"ipi_rootvolume_size"},{"location":"roles/ocp_provision/#role-variables-aws","text":"The following variables are only used when cluster_type = ipi and ipi_platform = aws .","title":"Role Variables - AWS"},{"location":"roles/ocp_provision/#aws_access_key_id","text":"AWS access key ID for authentication. Required (when cluster_type=ipi and ipi_platform=aws ) Environment Variable: AWS_ACCESS_KEY_ID Default: None Purpose : Authenticates with AWS to provision IPI cluster infrastructure. Must have permissions to create VPCs, instances, load balancers, and other AWS resources. When to use : - Always required for AWS IPI cluster provisioning - Must be associated with IAM user or role with cluster creation permissions - Obtain from AWS IAM Valid values : AWS access key ID string (typically 20 characters, starts with AKIA ) Impact : Without valid credentials with appropriate permissions, cluster provisioning will fail. Related variables : - aws_secret_access_key : Secret key paired with this access key ID Note : The IAM user/role must have extensive permissions (VPC, EC2, ELB, Route53, IAM, etc.). Consider using a dedicated IAM user for cluster provisioning.","title":"aws_access_key_id"},{"location":"roles/ocp_provision/#aws_secret_access_key","text":"AWS secret access key for authentication. Required (when cluster_type=ipi and ipi_platform=aws ) Environment Variable: AWS_SECRET_ACCESS_KEY Default: None Purpose : Authenticates with AWS to provision IPI cluster infrastructure. Paired with AWS access key ID for authentication. When to use : - Always required for AWS IPI cluster provisioning - Must correspond to the provided access key ID - Keep secure and rotate regularly Valid values : AWS secret access key string (typically 40 characters) Impact : Without valid credentials, cluster provisioning will fail. Related variables : - aws_access_key_id : Access key ID paired with this secret key Note : Keep secret keys secure. Never commit to version control. Use environment variables or secure vaults. Rotate keys regularly.","title":"aws_secret_access_key"},{"location":"roles/ocp_provision/#role-variables-gcp","text":"The following variables are only used when cluster_type = ipi and ipi_platform = gcp .","title":"Role Variables - GCP"},{"location":"roles/ocp_provision/#gcp_service_account_file","text":"Path to GCP service account credentials file. Required (when cluster_type=ipi and ipi_platform=gcp ) Environment Variable: GOOGLE_APPLICATION_CREDENTIALS Default: None Purpose : Authenticates with Google Cloud Platform to provision IPI cluster infrastructure. Service account must have permissions to create instances and networking resources. When to use : - Always required for GCP IPI cluster provisioning - Must be a valid service account JSON key file - Service account must have cluster creation permissions Valid values : Absolute path to GCP service account JSON key file Impact : Without valid credentials with appropriate permissions, cluster provisioning will fail. Related variables : - ipi_gcp_projectid : GCP project where cluster will be created Note : The service account must have extensive permissions (Compute, Networking, IAM, etc.). Download the JSON key file from GCP IAM. Keep the file secure.","title":"gcp_service_account_file"},{"location":"roles/ocp_provision/#ipi_gcp_projectid","text":"GCP project ID for cluster deployment. Required (when cluster_type=ipi and ipi_platform=gcp ) Environment Variable: GOOGLE_PROJECTID Default: None Purpose : Specifies the GCP project where the IPI cluster will be deployed. All cluster resources are created in this project. When to use : - Always required for GCP IPI cluster provisioning - Must be a valid, existing GCP project - Service account must have permissions in this project Valid values : Valid GCP project ID string Impact : Cluster resources are created in this project. Costs are billed to this project. Related variables : - gcp_service_account_file : Service account must have permissions in this project Note : The project must exist before provisioning. Ensure the service account has appropriate permissions in the project. All cluster costs are billed to this project.","title":"ipi_gcp_projectid"},{"location":"roles/ocp_provision/#example-playbook","text":"- hosts: localhost vars: cluster_type: roks cluster_name: mycluster ocp_version: 4.10 ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_provision","title":"Example Playbook"},{"location":"roles/ocp_provision/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_roks_upgrade_registry_storage/","text":"ocp_roks_upgrade_registry_storage \u00a4 Upgrade the storage capacity of the OpenShift image registry for IBM Cloud Red Hat OpenShift Kubernetes Service (ROKS) clusters. This role uses IBM Cloud APIs to expand the persistent volume backing the internal image registry from the default 100GB to 400GB. Purpose : The default 100GB registry storage is insufficient for comprehensive Cloud Pak for Data installations or environments with many container images. This role prevents registry storage exhaustion that would block image pulls and deployments. Important : This operation is specific to ROKS clusters on IBM Cloud and requires cluster downtime for the registry during the resize operation. Role Variables \u00a4 ibmcloud_apikey \u00a4 IBM Cloud API key for authentication and volume management. Required Environment Variable: IBMCLOUD_APIKEY Default: None Purpose : Provides authentication credentials for IBM Cloud APIs to modify the storage volume associated with the OpenShift image registry. When to use : Always required. Must have permissions to manage storage volumes in the IBM Cloud account. Valid values : Valid IBM Cloud API key string (typically 40+ characters). Impact : Used to authenticate with IBM Cloud and perform volume resize operations. Insufficient permissions will cause the operation to fail. Related variables : image_registry_size Notes : - Requires IAM permissions for Block Storage and Kubernetes Service - API key must have access to the cluster's resource group - Store securely, never commit to version control - Create API key in IBM Cloud console: Manage > Access (IAM) > API keys - Recommended: Use service ID API key rather than user API key image_registry_size \u00a4 Target size for the image registry storage volume in GB. Optional Environment Variable: None (hardcoded in defaults) Default: 400 Purpose : Specifies the target capacity in gigabytes for the image registry persistent volume. When to use : Default 400GB is suitable for most Cloud Pak for Data installations. Adjust if you need more or less storage. Valid values : Integer representing GB capacity. Common values: - 200 - Minimal CP4D installation - 400 - Default, suitable for full CP4D with multiple services - 600 - Large deployments with many custom images - 1000 - Very large environments with extensive image catalogs Impact : Determines the final size of the registry volume. Larger sizes cost more but provide more capacity for container images. Related variables : ibmcloud_apikey Notes : - Default 400GB is 4x the original 100GB capacity - Volume can only be expanded, not shrunk - Expansion requires registry downtime (typically 5-15 minutes) - Consider image retention policies to manage storage usage - Monitor registry storage: oc get pvc -n openshift-image-registry - IBM Cloud charges based on provisioned storage capacity Example Playbook \u00a4 - hosts: localhost roles: - ibm.mas_devops.ocp_roks_tuning License \u00a4 EPL-2.0","title":"ocp_roks_upgrade_registry_storage"},{"location":"roles/ocp_roks_upgrade_registry_storage/#ocp_roks_upgrade_registry_storage","text":"Upgrade the storage capacity of the OpenShift image registry for IBM Cloud Red Hat OpenShift Kubernetes Service (ROKS) clusters. This role uses IBM Cloud APIs to expand the persistent volume backing the internal image registry from the default 100GB to 400GB. Purpose : The default 100GB registry storage is insufficient for comprehensive Cloud Pak for Data installations or environments with many container images. This role prevents registry storage exhaustion that would block image pulls and deployments. Important : This operation is specific to ROKS clusters on IBM Cloud and requires cluster downtime for the registry during the resize operation.","title":"ocp_roks_upgrade_registry_storage"},{"location":"roles/ocp_roks_upgrade_registry_storage/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_roks_upgrade_registry_storage/#ibmcloud_apikey","text":"IBM Cloud API key for authentication and volume management. Required Environment Variable: IBMCLOUD_APIKEY Default: None Purpose : Provides authentication credentials for IBM Cloud APIs to modify the storage volume associated with the OpenShift image registry. When to use : Always required. Must have permissions to manage storage volumes in the IBM Cloud account. Valid values : Valid IBM Cloud API key string (typically 40+ characters). Impact : Used to authenticate with IBM Cloud and perform volume resize operations. Insufficient permissions will cause the operation to fail. Related variables : image_registry_size Notes : - Requires IAM permissions for Block Storage and Kubernetes Service - API key must have access to the cluster's resource group - Store securely, never commit to version control - Create API key in IBM Cloud console: Manage > Access (IAM) > API keys - Recommended: Use service ID API key rather than user API key","title":"ibmcloud_apikey"},{"location":"roles/ocp_roks_upgrade_registry_storage/#image_registry_size","text":"Target size for the image registry storage volume in GB. Optional Environment Variable: None (hardcoded in defaults) Default: 400 Purpose : Specifies the target capacity in gigabytes for the image registry persistent volume. When to use : Default 400GB is suitable for most Cloud Pak for Data installations. Adjust if you need more or less storage. Valid values : Integer representing GB capacity. Common values: - 200 - Minimal CP4D installation - 400 - Default, suitable for full CP4D with multiple services - 600 - Large deployments with many custom images - 1000 - Very large environments with extensive image catalogs Impact : Determines the final size of the registry volume. Larger sizes cost more but provide more capacity for container images. Related variables : ibmcloud_apikey Notes : - Default 400GB is 4x the original 100GB capacity - Volume can only be expanded, not shrunk - Expansion requires registry downtime (typically 5-15 minutes) - Consider image retention policies to manage storage usage - Monitor registry storage: oc get pvc -n openshift-image-registry - IBM Cloud charges based on provisioned storage capacity","title":"image_registry_size"},{"location":"roles/ocp_roks_upgrade_registry_storage/#example-playbook","text":"- hosts: localhost roles: - ibm.mas_devops.ocp_roks_tuning","title":"Example Playbook"},{"location":"roles/ocp_roks_upgrade_registry_storage/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_simulate_disconnected_network/","text":"ocp_simulate_disconnected_network \u00a4 Simulate an air-gapped/disconnected network environment for testing purposes by modifying DNS resolution on OpenShift cluster nodes. This role adds bogus entries to the /etc/hosts file on all nodes (workers and masters) to break DNS resolution for external container registries, forcing the cluster to use mirrored registries. Purpose : Enable testing of disconnected/air-gapped MAS installations without physically isolating the cluster from the network. The cluster can still access other network resources, but cannot reach the specified container registries. Important Notes : - Primarily designed and tested for Fyre clusters - May require modifications for other cluster providers - Uses MachineConfig to modify node host files - Changes persist across node reboots Verification : Check node hosts file: oc debug node/<node-name> sh-4.4# cat /host/etc/hosts Role Variables \u00a4 airgap_network_exclusions \u00a4 Space-separated list of container registry hostnames to block. Optional Environment Variable: None (hardcoded in defaults) Default: quay.io registry.redhat.io registry.connect.redhat.com gcr.io nvcr.io icr.io cp.icr.io docker-na-public.artifactory.swg-devops.com docker-na-proxy-svl.artifactory.swg-devops.com docker-na-proxy-rtp.artifactory.swg-devops.com Purpose : Defines which container registry hostnames will have DNS resolution blocked to simulate disconnected network access. When to use : Default list covers common registries used by MAS and OpenShift. Customize to add or remove registries based on your testing requirements. Valid values : Space-separated list of valid hostnames. Default registries: - quay.io - Red Hat Quay registry - registry.redhat.io - Red Hat Container Catalog - registry.connect.redhat.com - Red Hat Partner registry - gcr.io - Google Container Registry - nvcr.io - NVIDIA Container Registry - icr.io - IBM Cloud Container Registry (public) - cp.icr.io - IBM Cloud Container Registry (entitled) - docker-na-public.artifactory.swg-devops.com - IBM internal Artifactory - docker-na-proxy-svl.artifactory.swg-devops.com - IBM internal Artifactory proxy - docker-na-proxy-rtp.artifactory.swg-devops.com - IBM internal Artifactory proxy Impact : Listed registries will be unreachable from cluster nodes. Image pulls from these registries will fail unless mirrored registries are configured. Related variables : registry_private_ca_file , machine_config_multiupdate Notes : - Blocks DNS resolution by adding bogus IP addresses to /etc/hosts - Does not affect internal OpenShift image registry - Ensure mirrored registries are configured before blocking external registries - Add custom registries if testing with additional image sources registry_private_ca_file \u00a4 Local file path to the private registry CA certificate. Optional Environment Variable: REGISTRY_PRIVATE_CA_FILE Default: None Purpose : Provides the CA certificate for the private/mirrored registry that will be used in the disconnected environment. When to use : Required when using a private registry with self-signed or custom CA certificates in the simulated air-gap environment. Valid values : Absolute or relative file path to a valid PEM-encoded CA certificate file (e.g., /tmp/registry-ca.pem , ./certs/mirror-registry-ca.crt ). Impact : The CA certificate is added to the cluster's trusted certificate bundle, enabling nodes to trust the private registry. Related variables : registry_private_ca_crt , airgap_network_exclusions Notes : - Required for private registries with custom CAs - File must be accessible from the Ansible controller - Certificate must be in PEM format - Not needed if using a registry with publicly trusted certificates registry_private_ca_crt \u00a4 Content of the private registry CA certificate. Optional (derived from registry_private_ca_file ) Environment Variable: None (loaded from file) Default: None Purpose : Contains the actual CA certificate content loaded from registry_private_ca_file . Used internally by the role. When to use : Automatically populated when registry_private_ca_file is set. Do not set manually. Valid values : PEM-encoded CA certificate content. Impact : Certificate content is embedded in MachineConfig and distributed to all cluster nodes. Related variables : registry_private_ca_file Notes : - Automatically loaded from the file specified in registry_private_ca_file - Do not set this variable directly - Used internally by the role to configure node trust machine_config_multiupdate \u00a4 Enable multiple MachineConfig updates in a single operation. Optional Environment Variable: MACHINE_CONFIG_MULTIUPDATE Default: false Purpose : Controls whether multiple MachineConfig changes are applied together or separately, affecting node reboot behavior. When to use : Set to true when applying multiple configuration changes to minimize node reboots. Set to false for safer, incremental updates. Valid values : - true - Apply multiple MachineConfig changes together - false - Apply MachineConfig changes separately (default) Impact : - true : Reduces number of node reboots but increases risk if configuration is incorrect - false : More node reboots but safer rollback if issues occur Related variables : None Notes : - MachineConfig changes trigger node reboots - false is safer for production-like testing - true can save time in development environments - Node reboots are required to apply host file changes - Monitor MachineConfigPool status: oc get mcp Example Playbook \u00a4 - hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.ocp_simulate_disconnected_network License \u00a4 EPL-2.0","title":"ocp_simulate_disconnected_network"},{"location":"roles/ocp_simulate_disconnected_network/#ocp_simulate_disconnected_network","text":"Simulate an air-gapped/disconnected network environment for testing purposes by modifying DNS resolution on OpenShift cluster nodes. This role adds bogus entries to the /etc/hosts file on all nodes (workers and masters) to break DNS resolution for external container registries, forcing the cluster to use mirrored registries. Purpose : Enable testing of disconnected/air-gapped MAS installations without physically isolating the cluster from the network. The cluster can still access other network resources, but cannot reach the specified container registries. Important Notes : - Primarily designed and tested for Fyre clusters - May require modifications for other cluster providers - Uses MachineConfig to modify node host files - Changes persist across node reboots Verification : Check node hosts file: oc debug node/<node-name> sh-4.4# cat /host/etc/hosts","title":"ocp_simulate_disconnected_network"},{"location":"roles/ocp_simulate_disconnected_network/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_simulate_disconnected_network/#airgap_network_exclusions","text":"Space-separated list of container registry hostnames to block. Optional Environment Variable: None (hardcoded in defaults) Default: quay.io registry.redhat.io registry.connect.redhat.com gcr.io nvcr.io icr.io cp.icr.io docker-na-public.artifactory.swg-devops.com docker-na-proxy-svl.artifactory.swg-devops.com docker-na-proxy-rtp.artifactory.swg-devops.com Purpose : Defines which container registry hostnames will have DNS resolution blocked to simulate disconnected network access. When to use : Default list covers common registries used by MAS and OpenShift. Customize to add or remove registries based on your testing requirements. Valid values : Space-separated list of valid hostnames. Default registries: - quay.io - Red Hat Quay registry - registry.redhat.io - Red Hat Container Catalog - registry.connect.redhat.com - Red Hat Partner registry - gcr.io - Google Container Registry - nvcr.io - NVIDIA Container Registry - icr.io - IBM Cloud Container Registry (public) - cp.icr.io - IBM Cloud Container Registry (entitled) - docker-na-public.artifactory.swg-devops.com - IBM internal Artifactory - docker-na-proxy-svl.artifactory.swg-devops.com - IBM internal Artifactory proxy - docker-na-proxy-rtp.artifactory.swg-devops.com - IBM internal Artifactory proxy Impact : Listed registries will be unreachable from cluster nodes. Image pulls from these registries will fail unless mirrored registries are configured. Related variables : registry_private_ca_file , machine_config_multiupdate Notes : - Blocks DNS resolution by adding bogus IP addresses to /etc/hosts - Does not affect internal OpenShift image registry - Ensure mirrored registries are configured before blocking external registries - Add custom registries if testing with additional image sources","title":"airgap_network_exclusions"},{"location":"roles/ocp_simulate_disconnected_network/#registry_private_ca_file","text":"Local file path to the private registry CA certificate. Optional Environment Variable: REGISTRY_PRIVATE_CA_FILE Default: None Purpose : Provides the CA certificate for the private/mirrored registry that will be used in the disconnected environment. When to use : Required when using a private registry with self-signed or custom CA certificates in the simulated air-gap environment. Valid values : Absolute or relative file path to a valid PEM-encoded CA certificate file (e.g., /tmp/registry-ca.pem , ./certs/mirror-registry-ca.crt ). Impact : The CA certificate is added to the cluster's trusted certificate bundle, enabling nodes to trust the private registry. Related variables : registry_private_ca_crt , airgap_network_exclusions Notes : - Required for private registries with custom CAs - File must be accessible from the Ansible controller - Certificate must be in PEM format - Not needed if using a registry with publicly trusted certificates","title":"registry_private_ca_file"},{"location":"roles/ocp_simulate_disconnected_network/#registry_private_ca_crt","text":"Content of the private registry CA certificate. Optional (derived from registry_private_ca_file ) Environment Variable: None (loaded from file) Default: None Purpose : Contains the actual CA certificate content loaded from registry_private_ca_file . Used internally by the role. When to use : Automatically populated when registry_private_ca_file is set. Do not set manually. Valid values : PEM-encoded CA certificate content. Impact : Certificate content is embedded in MachineConfig and distributed to all cluster nodes. Related variables : registry_private_ca_file Notes : - Automatically loaded from the file specified in registry_private_ca_file - Do not set this variable directly - Used internally by the role to configure node trust","title":"registry_private_ca_crt"},{"location":"roles/ocp_simulate_disconnected_network/#machine_config_multiupdate","text":"Enable multiple MachineConfig updates in a single operation. Optional Environment Variable: MACHINE_CONFIG_MULTIUPDATE Default: false Purpose : Controls whether multiple MachineConfig changes are applied together or separately, affecting node reboot behavior. When to use : Set to true when applying multiple configuration changes to minimize node reboots. Set to false for safer, incremental updates. Valid values : - true - Apply multiple MachineConfig changes together - false - Apply MachineConfig changes separately (default) Impact : - true : Reduces number of node reboots but increases risk if configuration is incorrect - false : More node reboots but safer rollback if issues occur Related variables : None Notes : - MachineConfig changes trigger node reboots - false is safer for production-like testing - true can save time in development environments - Node reboots are required to apply host file changes - Monitor MachineConfigPool status: oc get mcp","title":"machine_config_multiupdate"},{"location":"roles/ocp_simulate_disconnected_network/#example-playbook","text":"- hosts: localhost vars: # Add required variables here roles: - ibm.mas_devops.ocp_simulate_disconnected_network","title":"Example Playbook"},{"location":"roles/ocp_simulate_disconnected_network/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_upgrade/","text":"ocp_upgrade \u00a4 This role supports the upgrade of the Openshift Cluster version for master and worker nodes in IBM Cloud provider. Role Variables \u00a4 cluster_type \u00a4 Cluster type for upgrade operation. Required Environment Variable: CLUSTER_TYPE Default: None Purpose : Specifies the type of OpenShift cluster to upgrade. Currently only IBM Cloud ROKS clusters are supported by this role. When to use : - Always required for cluster upgrade operations - Must be set to roks (only supported type) - Role will fail if any other cluster type is specified Valid values : roks (IBM Cloud Red Hat OpenShift Kubernetes Service) Impact : Determines the upgrade method and CLI commands used. Only ROKS clusters can be upgraded with this role. Related variables : - cluster_name : Name of the ROKS cluster to upgrade - ocp_version_upgrade : Target version for the upgrade Note : IMPORTANT - Only IBM Cloud ROKS clusters are supported. The role will fail if cluster_type is not roks . For other cluster types, use provider-specific upgrade procedures. cluster_name \u00a4 Name of the cluster to upgrade. Required Environment Variable: CLUSTER_NAME Default: None Purpose : Identifies which IBM Cloud ROKS cluster to upgrade. Used to target the specific cluster for the upgrade operation. When to use : - Always required for cluster upgrade operations - Must match the exact cluster name in IBM Cloud - Used by IBM Cloud CLI to locate the cluster Valid values : String matching an existing ROKS cluster name in your IBM Cloud account Impact : Determines which cluster is upgraded. Incorrect name will cause the upgrade to fail. Related variables : - cluster_type : Must be roks for this role - ocp_version_upgrade : Target version for this cluster Note : The cluster name must exactly match the name in IBM Cloud. Verify the cluster name before running the upgrade to avoid targeting the wrong cluster. ocp_version_upgrade \u00a4 Target OpenShift version for upgrade. Required Environment Variable: OCP_VERSION_UPGRADE Default: None Purpose : Specifies the target OpenShift Container Platform version to upgrade the cluster to. Determines which version will be installed. When to use : - Always required for cluster upgrade operations - Must be a valid version available for ROKS clusters - Should be a supported upgrade path from current version Valid values : ROKS version format with _openshift suffix (e.g., 4.10_openshift , 4.11_openshift , 4.12_openshift ) Impact : Cluster is upgraded to this version. Upgrade may take significant time and cause temporary service disruption. Version must be a valid upgrade path from current version. Related variables : - cluster_name : Cluster to upgrade to this version - cluster_type : Must be roks Note : IMPORTANT - Version must include _openshift suffix for ROKS clusters (e.g., 4.10_openshift ). Verify the version is a supported upgrade path from your current version. Upgrades cannot be rolled back. Plan for maintenance window as upgrade causes temporary disruption. Check IBM Cloud documentation for available versions and supported upgrade paths. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: cluster_name: my-ocp-cluster cluster_type: roks ocp_version_upgrade: 4.10_openshift roles: - ibm.mas_devops.ocp_upgrade License \u00a4 EPL-2.0","title":"ocp_upgrade"},{"location":"roles/ocp_upgrade/#ocp_upgrade","text":"This role supports the upgrade of the Openshift Cluster version for master and worker nodes in IBM Cloud provider.","title":"ocp_upgrade"},{"location":"roles/ocp_upgrade/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_upgrade/#cluster_type","text":"Cluster type for upgrade operation. Required Environment Variable: CLUSTER_TYPE Default: None Purpose : Specifies the type of OpenShift cluster to upgrade. Currently only IBM Cloud ROKS clusters are supported by this role. When to use : - Always required for cluster upgrade operations - Must be set to roks (only supported type) - Role will fail if any other cluster type is specified Valid values : roks (IBM Cloud Red Hat OpenShift Kubernetes Service) Impact : Determines the upgrade method and CLI commands used. Only ROKS clusters can be upgraded with this role. Related variables : - cluster_name : Name of the ROKS cluster to upgrade - ocp_version_upgrade : Target version for the upgrade Note : IMPORTANT - Only IBM Cloud ROKS clusters are supported. The role will fail if cluster_type is not roks . For other cluster types, use provider-specific upgrade procedures.","title":"cluster_type"},{"location":"roles/ocp_upgrade/#cluster_name","text":"Name of the cluster to upgrade. Required Environment Variable: CLUSTER_NAME Default: None Purpose : Identifies which IBM Cloud ROKS cluster to upgrade. Used to target the specific cluster for the upgrade operation. When to use : - Always required for cluster upgrade operations - Must match the exact cluster name in IBM Cloud - Used by IBM Cloud CLI to locate the cluster Valid values : String matching an existing ROKS cluster name in your IBM Cloud account Impact : Determines which cluster is upgraded. Incorrect name will cause the upgrade to fail. Related variables : - cluster_type : Must be roks for this role - ocp_version_upgrade : Target version for this cluster Note : The cluster name must exactly match the name in IBM Cloud. Verify the cluster name before running the upgrade to avoid targeting the wrong cluster.","title":"cluster_name"},{"location":"roles/ocp_upgrade/#ocp_version_upgrade","text":"Target OpenShift version for upgrade. Required Environment Variable: OCP_VERSION_UPGRADE Default: None Purpose : Specifies the target OpenShift Container Platform version to upgrade the cluster to. Determines which version will be installed. When to use : - Always required for cluster upgrade operations - Must be a valid version available for ROKS clusters - Should be a supported upgrade path from current version Valid values : ROKS version format with _openshift suffix (e.g., 4.10_openshift , 4.11_openshift , 4.12_openshift ) Impact : Cluster is upgraded to this version. Upgrade may take significant time and cause temporary service disruption. Version must be a valid upgrade path from current version. Related variables : - cluster_name : Cluster to upgrade to this version - cluster_type : Must be roks Note : IMPORTANT - Version must include _openshift suffix for ROKS clusters (e.g., 4.10_openshift ). Verify the version is a supported upgrade path from your current version. Upgrades cannot be rolled back. Plan for maintenance window as upgrade causes temporary disruption. Check IBM Cloud documentation for available versions and supported upgrade paths.","title":"ocp_version_upgrade"},{"location":"roles/ocp_upgrade/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: cluster_name: my-ocp-cluster cluster_type: roks ocp_version_upgrade: 4.10_openshift roles: - ibm.mas_devops.ocp_upgrade","title":"Example Playbook"},{"location":"roles/ocp_upgrade/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_verify/","text":"ocp_verify \u00a4 This role will verify that the target OCP cluster is ready to be setup for MAS. For example, in IBMCloud ROKS we have seen delays of over an hour before the Red Hat Operator catalog is ready to use. This will cause attempts to install anything from that CatalogSource to fail as the timeouts built into the roles in this collection are designed to catch problems with an install, rather than a half-provisioned cluster that is not properly ready to use yet. Role Variables \u00a4 verify_cluster \u00a4 Enable cluster health verification. Optional Environment Variable: VERIFY_CLUSTER Default: true Purpose : Verifies that the OCP cluster is healthy and ready to use by checking the ClusterVersion resource Ready condition. When to use : - Leave as true (default) for comprehensive cluster verification - Set to false only to skip cluster health checks - Recommended to keep enabled for production deployments Valid values : true , false Impact : - true : Verifies cluster Ready condition (fails if not ready within 1 hour) - false : Skips cluster health verification Related variables : - Other verify_* variables control additional verification checks Note : This check ensures the cluster is fully provisioned and ready. In some environments (e.g., IBMCloud ROKS), clusters may take time to become fully ready. The 1-hour timeout accommodates typical provisioning delays. verify_catalogsources \u00a4 Enable catalog source health verification. Optional Environment Variable: VERIFY_CATALOGSOURCES Default: true Purpose : Verifies that all installed OCP catalog sources are healthy and ready to provide operators. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip catalog source checks - Critical for ensuring operator installation will succeed Valid values : true , false Impact : - true : Verifies all CatalogSources report lastObservedState as READY (fails if not ready within 30 minutes) - false : Skips catalog source verification Related variables : - verify_cluster : Cluster-level health check - verify_subscriptions : Operator subscription verification Note : Catalog sources must be ready before operators can be installed. In some environments (e.g., IBMCloud ROKS), catalog sources may take time to sync. The 30-minute timeout accommodates typical delays. This check prevents operator installation failures due to unavailable catalogs. verify_subscriptions \u00a4 Enable operator subscription verification. Optional Environment Variable: VERIFY_SUBSCRIPTIONS Default: true Purpose : Verifies that all operator subscriptions are up to date and at their latest known version. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip subscription checks - Important for ensuring operators are properly installed Valid values : true , false Impact : - true : Verifies all Subscriptions report state as AtLatestKnown (fails if not ready within 5 hours) - false : Skips subscription verification Related variables : - verify_catalogsources : Catalog source health (prerequisite) - verify_workloads : Workload deployment verification Note : Subscriptions must be at latest known version before operators are fully functional. The 5-hour timeout accommodates operator installation and upgrade processes. This check ensures operators are properly installed and ready to manage resources. verify_workloads \u00a4 Enable workload deployment verification. Optional Environment Variable: VERIFY_WORKLOADS Default: true Purpose : Verifies that all deployments and statefulsets are fully rolled out with all replicas updated and available. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip workload checks - Critical for ensuring cluster workloads are healthy Valid values : true , false Impact : - true : Verifies all Deployments and StatefulSets have updatedReplicas and availableReplicas equal to replicas (fails if not ready within 10 hours) - false : Skips workload verification Related variables : - verify_subscriptions : Operator subscription verification (prerequisite) - verify_cluster : Cluster-level health check Note : Workloads must be fully deployed before the cluster is ready for MAS installation. The 10-hour timeout accommodates large-scale deployments and rolling updates. This check ensures all pods are running and ready. verify_ingress \u00a4 Enable ingress TLS certificate verification. Optional Environment Variable: VERIFY_INGRESS Default: true Purpose : Verifies that the cluster ingress TLS certificate can be obtained. This certificate is required by multiple roles in the collection. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip ingress certificate checks - Required for roles that need cluster ingress certificate Valid values : true , false Impact : - true : Verifies ingress TLS certificate can be retrieved - false : Skips ingress certificate verification Related variables : - cluster_name : Cluster name for certificate lookup - ocp_ingress_tls_secret_name : Secret name containing certificate Note : Many roles in this collection require the cluster ingress certificate. This check ensures the certificate is accessible before proceeding with MAS installation. cluster_name \u00a4 Cluster name for ingress certificate lookup. Optional (only used when verify_ingress=true ) Environment Variable: CLUSTER_NAME Default: None Purpose : Specifies the cluster name used to determine the default router certificate name in certain cluster configurations. When to use : - Only required when verify_ingress=true - Set when cluster setup requires cluster name for certificate lookup - Leave unset if certificate can be found without cluster name Valid values : String matching your cluster name Impact : Used to construct the ingress TLS secret name in cluster configurations where the secret name includes the cluster name. Related variables : - verify_ingress : Must be true for this variable to be used - ocp_ingress_tls_secret_name : Alternative way to specify certificate secret Note : Only needed in specific cluster configurations. Most clusters use the default secret name and don't require this variable. ocp_ingress_tls_secret_name \u00a4 Ingress TLS secret name. Optional (only used when verify_ingress=true ) Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default: router-certs-default Purpose : Specifies the name of the Kubernetes secret containing the cluster's default router certificate. When to use : - Only applies when verify_ingress=true - Use default ( router-certs-default ) for most clusters - Override only if your cluster uses a different secret name Valid values : String matching the secret name in openshift-ingress namespace Impact : Determines which secret is checked for the ingress TLS certificate. Incorrect name will cause verification to fail. Related variables : - verify_ingress : Must be true for this variable to be used - cluster_name : Alternative way to determine secret name Note : The default router-certs-default works for most OCP clusters. Only override if your cluster uses a custom ingress certificate secret name. Example Playbook \u00a4 - hosts: localhost vars: verify_cluster: True verify_catalogsources: True verify_subscriptions: True verify_workloads: True verify_ingress: True roles: - ibm.mas_devops.ocp_verify License \u00a4 EPL-2.0","title":"ocp_verify"},{"location":"roles/ocp_verify/#ocp_verify","text":"This role will verify that the target OCP cluster is ready to be setup for MAS. For example, in IBMCloud ROKS we have seen delays of over an hour before the Red Hat Operator catalog is ready to use. This will cause attempts to install anything from that CatalogSource to fail as the timeouts built into the roles in this collection are designed to catch problems with an install, rather than a half-provisioned cluster that is not properly ready to use yet.","title":"ocp_verify"},{"location":"roles/ocp_verify/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_verify/#verify_cluster","text":"Enable cluster health verification. Optional Environment Variable: VERIFY_CLUSTER Default: true Purpose : Verifies that the OCP cluster is healthy and ready to use by checking the ClusterVersion resource Ready condition. When to use : - Leave as true (default) for comprehensive cluster verification - Set to false only to skip cluster health checks - Recommended to keep enabled for production deployments Valid values : true , false Impact : - true : Verifies cluster Ready condition (fails if not ready within 1 hour) - false : Skips cluster health verification Related variables : - Other verify_* variables control additional verification checks Note : This check ensures the cluster is fully provisioned and ready. In some environments (e.g., IBMCloud ROKS), clusters may take time to become fully ready. The 1-hour timeout accommodates typical provisioning delays.","title":"verify_cluster"},{"location":"roles/ocp_verify/#verify_catalogsources","text":"Enable catalog source health verification. Optional Environment Variable: VERIFY_CATALOGSOURCES Default: true Purpose : Verifies that all installed OCP catalog sources are healthy and ready to provide operators. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip catalog source checks - Critical for ensuring operator installation will succeed Valid values : true , false Impact : - true : Verifies all CatalogSources report lastObservedState as READY (fails if not ready within 30 minutes) - false : Skips catalog source verification Related variables : - verify_cluster : Cluster-level health check - verify_subscriptions : Operator subscription verification Note : Catalog sources must be ready before operators can be installed. In some environments (e.g., IBMCloud ROKS), catalog sources may take time to sync. The 30-minute timeout accommodates typical delays. This check prevents operator installation failures due to unavailable catalogs.","title":"verify_catalogsources"},{"location":"roles/ocp_verify/#verify_subscriptions","text":"Enable operator subscription verification. Optional Environment Variable: VERIFY_SUBSCRIPTIONS Default: true Purpose : Verifies that all operator subscriptions are up to date and at their latest known version. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip subscription checks - Important for ensuring operators are properly installed Valid values : true , false Impact : - true : Verifies all Subscriptions report state as AtLatestKnown (fails if not ready within 5 hours) - false : Skips subscription verification Related variables : - verify_catalogsources : Catalog source health (prerequisite) - verify_workloads : Workload deployment verification Note : Subscriptions must be at latest known version before operators are fully functional. The 5-hour timeout accommodates operator installation and upgrade processes. This check ensures operators are properly installed and ready to manage resources.","title":"verify_subscriptions"},{"location":"roles/ocp_verify/#verify_workloads","text":"Enable workload deployment verification. Optional Environment Variable: VERIFY_WORKLOADS Default: true Purpose : Verifies that all deployments and statefulsets are fully rolled out with all replicas updated and available. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip workload checks - Critical for ensuring cluster workloads are healthy Valid values : true , false Impact : - true : Verifies all Deployments and StatefulSets have updatedReplicas and availableReplicas equal to replicas (fails if not ready within 10 hours) - false : Skips workload verification Related variables : - verify_subscriptions : Operator subscription verification (prerequisite) - verify_cluster : Cluster-level health check Note : Workloads must be fully deployed before the cluster is ready for MAS installation. The 10-hour timeout accommodates large-scale deployments and rolling updates. This check ensures all pods are running and ready.","title":"verify_workloads"},{"location":"roles/ocp_verify/#verify_ingress","text":"Enable ingress TLS certificate verification. Optional Environment Variable: VERIFY_INGRESS Default: true Purpose : Verifies that the cluster ingress TLS certificate can be obtained. This certificate is required by multiple roles in the collection. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip ingress certificate checks - Required for roles that need cluster ingress certificate Valid values : true , false Impact : - true : Verifies ingress TLS certificate can be retrieved - false : Skips ingress certificate verification Related variables : - cluster_name : Cluster name for certificate lookup - ocp_ingress_tls_secret_name : Secret name containing certificate Note : Many roles in this collection require the cluster ingress certificate. This check ensures the certificate is accessible before proceeding with MAS installation.","title":"verify_ingress"},{"location":"roles/ocp_verify/#cluster_name","text":"Cluster name for ingress certificate lookup. Optional (only used when verify_ingress=true ) Environment Variable: CLUSTER_NAME Default: None Purpose : Specifies the cluster name used to determine the default router certificate name in certain cluster configurations. When to use : - Only required when verify_ingress=true - Set when cluster setup requires cluster name for certificate lookup - Leave unset if certificate can be found without cluster name Valid values : String matching your cluster name Impact : Used to construct the ingress TLS secret name in cluster configurations where the secret name includes the cluster name. Related variables : - verify_ingress : Must be true for this variable to be used - ocp_ingress_tls_secret_name : Alternative way to specify certificate secret Note : Only needed in specific cluster configurations. Most clusters use the default secret name and don't require this variable.","title":"cluster_name"},{"location":"roles/ocp_verify/#ocp_ingress_tls_secret_name","text":"Ingress TLS secret name. Optional (only used when verify_ingress=true ) Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default: router-certs-default Purpose : Specifies the name of the Kubernetes secret containing the cluster's default router certificate. When to use : - Only applies when verify_ingress=true - Use default ( router-certs-default ) for most clusters - Override only if your cluster uses a different secret name Valid values : String matching the secret name in openshift-ingress namespace Impact : Determines which secret is checked for the ingress TLS certificate. Incorrect name will cause verification to fail. Related variables : - verify_ingress : Must be true for this variable to be used - cluster_name : Alternative way to determine secret name Note : The default router-certs-default works for most OCP clusters. Only override if your cluster uses a custom ingress certificate secret name.","title":"ocp_ingress_tls_secret_name"},{"location":"roles/ocp_verify/#example-playbook","text":"- hosts: localhost vars: verify_cluster: True verify_catalogsources: True verify_subscriptions: True verify_workloads: True verify_ingress: True roles: - ibm.mas_devops.ocp_verify","title":"Example Playbook"},{"location":"roles/ocp_verify/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocs/","text":"ocs \u00a4 Install and configure OpenShift Data Foundation (ODF) Operator, formerly known as OpenShift Container Storage (OCS), along with the Local Storage Operator (LSO) for persistent storage in OpenShift clusters. This role provides automated installation and upgrade of ODF/OCS with local storage backend. The role automatically detects the OpenShift version and installs the appropriate operator version. Version Detection \u00a4 OCP 4.10 and earlier : Installs OCS operator OCP 4.11 and later : Installs ODF operator Platform Limitations \u00a4 IBM Cloud ROKS Limitation Starting from OCP 4.8, IBM/Red Hat no longer support OCS/ODF installation via OperatorHub on IBM Cloud ROKS clusters. ROKS clusters are automatically provisioned with their own storage plugin. If you attempt to install ODF on ROKS via OperatorHub, you will encounter this error: \"Failed to apply object: admission webhook 'validate.managed.openshift.io' denied the request: Installing OpenShift Data Foundation on IBM Cloud by using OperatorHub is not supported. You can install OpenShift Data Foundation by using the IBM Cloud add-on.\" For ROKS deployments, use the IBM Cloud add-on instead. Features \u00a4 Automatic Version Detection : Installs OCS or ODF based on OpenShift version Local Storage Integration : Configures Local Storage Operator for block storage Storage Cluster Setup : Creates and configures ODF/OCS storage cluster Upgrade Support : Can upgrade existing ODF/OCS installations Role Variables \u00a4 lso_device_path \u00a4 Local disk device path for block storage. Required Environment Variable: LSO_DEVICE_PATH Default: /dev/vdb Purpose : Specifies the local disk device path to be used by the Local Storage Operator for block storage provisioning. When to use : Always required when installing ODF/OCS. Must point to an available, unformatted local disk on worker nodes. Valid values : Valid Linux block device path (e.g., /dev/vdb , /dev/sdc , /dev/nvme0n1 ) Impact : This disk will be used for ODF/OCS storage cluster. All data on the disk will be erased during setup. Related variables : ocs_action Notes : - Critical : Disk must be unformatted and not in use - All data on the specified disk will be permanently erased - Disk must be available on all worker nodes designated for storage - Verify disk path: lsblk on worker nodes - Common paths: /dev/vdb (virtual machines), /dev/sdc (physical servers) - For NVMe drives, use format /dev/nvme0n1 , /dev/nvme1n1 , etc. - Ensure disk has sufficient capacity for your storage requirements ocs_action \u00a4 Action to perform on ODF/OCS installation. Optional Environment Variable: OCS_ACTION Default: install Purpose : Controls whether to perform a fresh installation or upgrade of ODF/OCS operators and storage cluster. When to use : - Use install (default) for new ODF/OCS deployments - Use upgrade to update existing ODF/OCS installation to match current OpenShift version Valid values : - install - Fresh installation of Local Storage Operator, ODF/OCS operator, and storage cluster - upgrade - Upgrade existing operators and storage cluster based on OpenShift version Impact : - install : Creates new LSO and ODF/OCS operators, configures local storage, creates storage cluster - upgrade : Updates operator subscriptions and storage cluster configuration to match OpenShift version Related variables : lso_device_path Notes : - Default install is for new deployments - Use upgrade after OpenShift cluster upgrades to keep storage in sync - Upgrade process updates operators to versions compatible with current OCP version - Always backup data before performing upgrades - Verify cluster health before and after upgrade operations Example Playbook \u00a4 Fresh Installation \u00a4 - hosts: localhost any_errors_fatal: true vars: lso_device_path: /dev/vdb ocs_action: install roles: - ibm.mas_devops.ocs Upgrade Existing Installation \u00a4 - hosts: localhost any_errors_fatal: true vars: lso_device_path: /dev/vdb ocs_action: upgrade roles: - ibm.mas_devops.ocs Custom Disk Path \u00a4 - hosts: localhost any_errors_fatal: true vars: lso_device_path: /dev/nvme0n1 ocs_action: install roles: - ibm.mas_devops.ocs License \u00a4 EPL-2.0","title":"ocs"},{"location":"roles/ocs/#ocs","text":"Install and configure OpenShift Data Foundation (ODF) Operator, formerly known as OpenShift Container Storage (OCS), along with the Local Storage Operator (LSO) for persistent storage in OpenShift clusters. This role provides automated installation and upgrade of ODF/OCS with local storage backend. The role automatically detects the OpenShift version and installs the appropriate operator version.","title":"ocs"},{"location":"roles/ocs/#version-detection","text":"OCP 4.10 and earlier : Installs OCS operator OCP 4.11 and later : Installs ODF operator","title":"Version Detection"},{"location":"roles/ocs/#platform-limitations","text":"IBM Cloud ROKS Limitation Starting from OCP 4.8, IBM/Red Hat no longer support OCS/ODF installation via OperatorHub on IBM Cloud ROKS clusters. ROKS clusters are automatically provisioned with their own storage plugin. If you attempt to install ODF on ROKS via OperatorHub, you will encounter this error: \"Failed to apply object: admission webhook 'validate.managed.openshift.io' denied the request: Installing OpenShift Data Foundation on IBM Cloud by using OperatorHub is not supported. You can install OpenShift Data Foundation by using the IBM Cloud add-on.\" For ROKS deployments, use the IBM Cloud add-on instead.","title":"Platform Limitations"},{"location":"roles/ocs/#features","text":"Automatic Version Detection : Installs OCS or ODF based on OpenShift version Local Storage Integration : Configures Local Storage Operator for block storage Storage Cluster Setup : Creates and configures ODF/OCS storage cluster Upgrade Support : Can upgrade existing ODF/OCS installations","title":"Features"},{"location":"roles/ocs/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocs/#lso_device_path","text":"Local disk device path for block storage. Required Environment Variable: LSO_DEVICE_PATH Default: /dev/vdb Purpose : Specifies the local disk device path to be used by the Local Storage Operator for block storage provisioning. When to use : Always required when installing ODF/OCS. Must point to an available, unformatted local disk on worker nodes. Valid values : Valid Linux block device path (e.g., /dev/vdb , /dev/sdc , /dev/nvme0n1 ) Impact : This disk will be used for ODF/OCS storage cluster. All data on the disk will be erased during setup. Related variables : ocs_action Notes : - Critical : Disk must be unformatted and not in use - All data on the specified disk will be permanently erased - Disk must be available on all worker nodes designated for storage - Verify disk path: lsblk on worker nodes - Common paths: /dev/vdb (virtual machines), /dev/sdc (physical servers) - For NVMe drives, use format /dev/nvme0n1 , /dev/nvme1n1 , etc. - Ensure disk has sufficient capacity for your storage requirements","title":"lso_device_path"},{"location":"roles/ocs/#ocs_action","text":"Action to perform on ODF/OCS installation. Optional Environment Variable: OCS_ACTION Default: install Purpose : Controls whether to perform a fresh installation or upgrade of ODF/OCS operators and storage cluster. When to use : - Use install (default) for new ODF/OCS deployments - Use upgrade to update existing ODF/OCS installation to match current OpenShift version Valid values : - install - Fresh installation of Local Storage Operator, ODF/OCS operator, and storage cluster - upgrade - Upgrade existing operators and storage cluster based on OpenShift version Impact : - install : Creates new LSO and ODF/OCS operators, configures local storage, creates storage cluster - upgrade : Updates operator subscriptions and storage cluster configuration to match OpenShift version Related variables : lso_device_path Notes : - Default install is for new deployments - Use upgrade after OpenShift cluster upgrades to keep storage in sync - Upgrade process updates operators to versions compatible with current OCP version - Always backup data before performing upgrades - Verify cluster health before and after upgrade operations","title":"ocs_action"},{"location":"roles/ocs/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/ocs/#fresh-installation","text":"- hosts: localhost any_errors_fatal: true vars: lso_device_path: /dev/vdb ocs_action: install roles: - ibm.mas_devops.ocs","title":"Fresh Installation"},{"location":"roles/ocs/#upgrade-existing-installation","text":"- hosts: localhost any_errors_fatal: true vars: lso_device_path: /dev/vdb ocs_action: upgrade roles: - ibm.mas_devops.ocs","title":"Upgrade Existing Installation"},{"location":"roles/ocs/#custom-disk-path","text":"- hosts: localhost any_errors_fatal: true vars: lso_device_path: /dev/nvme0n1 ocs_action: install roles: - ibm.mas_devops.ocs","title":"Custom Disk Path"},{"location":"roles/ocs/#license","text":"EPL-2.0","title":"License"},{"location":"roles/registry/","text":"registry \u00a4 Create a Docker Registry running on RedHat OpenShift cluster. The registry will be backed by persistant storage, and accessible via either a clusterIP or loadbalancer service. This role can also be used to delete a docker registry on a cluster for a clean start. See usage below for more information. Usage \u00a4 If you set up the registry with a loadbalancer service you will be able to push to the registry via the cluster's hostname, but before you can use the registry you will need to install the registry's CA certificate and restart the Docker daemon so that your client trusts the new registry: CACERT=$(oc -n airgap-registry get secret airgap-registry-certificate -o jsonpath='{.data.ca\\.crt}' | base64 -d) DOMAIN=$(oc get ingress.config cluster -o jsonpath='{.spec.domain}') sudo mkdir -p /etc/docker/certs.d/$DOMAIN\\:32500/ sudo echo \"$CACERT\" > /etc/docker/certs.d/$DOMAIN\\:32500/ca.crt sudo service docker restart You can now use the registry as normal: DOMAIN=$(oc get ingress.config cluster -o jsonpath='{.spec.domain}') docker pull registry.access.redhat.com/ubi8/ubi-minimal docker tag registry.access.redhat.com/ubi8/ubi-minimal $DOMAIN:32500/ubi8/ubi-minimal docker push $DOMAIN:32500/ubi8/ubi-minimal If you set up the registry with a clusterip service you will only be able to push to the registry after using port forwarding: oc -n airgap-registry port-forward deployment/airgap-registry 9000:5000 docker pull registry.access.redhat.com/ubi8/ubi-minimal docker tag registry.access.redhat.com/ubi8/ubi-minimal localhost:9000/ubi8/ubi-minimal docker push localhost:9000/ubi8/ubi-minimal However, you will still need to set up Docker trust for the \"local\" registry: CACERT=$(oc -n airgap-registry get secret airgap-registry-certificate -o jsonpath='{.data.ca\\.crt}' | base64 -d) sudo mkdir -p /etc/docker/certs.d/$DOMAIN\\:32500/ sudo mkdir /etc/docker/certs.d/localhost\\:9000 sudo echo \"$CACERT\" > /etc/docker/certs.d/localhost\\:9000/ca.crt sudo service docker restart Usage for tear-down action This role can also be used to permanently delete a mirror registry from a given cluster by setting the registry_action to tear-down and specifying the corresponding registry_namespace , if not using the default value. Note that the tear-down action deletes the registry completely including the PVC storage and the registry namespace. To start up the registry again, the role needs to be run again with the registry_action on default or setup . Images previously stored in the registry before the tear-down will no longer be available and will need to be mirrored again once the registry setup has completed. Take precaution when using this function and expect that images can no longer be accessed from the registry that has been torn down. Note: Recreating the registry will also create a new ca cert for the new registry. An appropriate time to use this tear-down function is when the registry has too many images that are not being used or when there has been a shift to support newer versions but images of older versions are clogging the registry. The tear-down function frees the disk space and allows for a new registry to be setup. Role Variables \u00a4 registry_action \u00a4 Action to perform with the registry deployment. Optional Environment Variable: REGISTRY_ACTION Default: setup Purpose : Controls whether to set up a new registry or tear down an existing one. When to use : Set to tear-down when you need to completely remove a registry to free disk space or start fresh. Use default setup for normal installation. Valid values : - setup - Install and configure the registry - tear-down - Permanently delete the registry, namespace, and all stored images Impact : The tear-down action is destructive and irreversible. All images stored in the registry will be lost, and the PVC storage will be deleted. A new CA certificate will be generated if the registry is recreated. Related variables : registry_namespace Notes : - Use tear-down when the registry contains too many unused images or when migrating to newer versions - After tear-down, you must re-mirror all required images - Docker clients will need to trust the new CA certificate after recreation registry_namespace \u00a4 Namespace where the Docker registry will be deployed. Optional Environment Variable: REGISTRY_NAMESPACE Default: airgap-registry Purpose : Isolates the registry resources in a dedicated namespace for organization and access control. When to use : Override the default if you have namespace naming conventions or need multiple registries in the same cluster. Valid values : Any valid Kubernetes namespace name (lowercase alphanumeric and hyphens) Impact : All registry resources (deployment, service, PVC, secrets) will be created in this namespace. The namespace will be deleted during tear-down operations. Related variables : registry_action Notes : Ensure the namespace name doesn't conflict with existing namespaces in your cluster. registry_storage_class \u00a4 Storage class for the registry's persistent volume. Required (except on IBM Cloud ROKS where it defaults to ibmc-block-gold ) Environment Variable: REGISTRY_STORAGE_CLASS Default: None (IBM Cloud ROKS: ibmc-block-gold ) Purpose : Provides persistent storage for container images stored in the registry. When to use : Must be specified for all non-ROKS clusters. On ROKS, the default is usually appropriate unless you have specific performance requirements. Valid values : Any storage class name available in your cluster that supports ReadWriteOnce (RWO) access mode. Common examples: - IBM Cloud ROKS: ibmc-block-gold , ibmc-block-silver , ibmc-block-bronze - AWS: gp2 , gp3 , io1 - Azure: managed-premium , managed-standard - On-premises: Depends on your storage provider Impact : Determines the performance characteristics and availability of the registry storage. The storage class must support RWO access mode. Related variables : registry_storage_capacity Notes : - Verify the storage class exists before deployment: oc get storageclass - The storage class cannot be changed after initial deployment without recreating the registry registry_storage_capacity \u00a4 Size of the persistent volume claim for registry storage. Optional Environment Variable: REGISTRY_STORAGE_CAPACITY Default: 100Gi Purpose : Allocates disk space for storing container images in the registry. When to use : Increase from the default based on the number and size of images you plan to store. Consider your mirroring requirements and image retention policies. Valid values : Any valid Kubernetes storage size (e.g., 100Gi , 500Gi , 1Ti ). Must be supported by your storage class. Impact : Determines how many images can be stored before the registry runs out of space. Insufficient capacity will cause push operations to fail. Related variables : registry_storage_class Notes : - Plan for growth - container images can be large (1-5GB each) - Monitor usage regularly: oc get pvc -n <registry_namespace> - Expanding PVC size after deployment depends on your storage class capabilities - For airgap environments, calculate total size of all images to be mirrored plus 20% buffer registry_service_type \u00a4 Service type for exposing the registry. Optional Environment Variable: REGISTRY_SERVICE_TYPE Default: loadbalancer Purpose : Controls how the registry is exposed for access from Docker clients. When to use : - Use loadbalancer (default) when you need to push images from outside the cluster - Use clusterip when you only need internal cluster access or will use port-forwarding Valid values : - loadbalancer - Exposes registry externally via cluster domain on port 32500 - clusterip - Internal cluster access only (requires port-forwarding for external access) Impact : - loadbalancer : Enables direct external access via <cluster-domain>:32500 , but requires port 32500 to be available - clusterip : More secure (no external exposure) but requires oc port-forward for each push operation Related variables : None Notes : - LIMITATION : The loadbalancer port (32500) cannot be customized. If port 32500 is already in use by another service, you must use clusterip mode - With loadbalancer, you must install the registry's CA certificate on your Docker client (see Usage section) - With clusterip, you still need to configure Docker trust for localhost:9000 (see Usage section) Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: registry_storage_class: ibmc-block-gold registry_storage_capacity: 500Gb registry_service_type: loadbalancer roles: - ibm.mas_devops.registry License \u00a4 EPL-2.0","title":"registry"},{"location":"roles/registry/#registry","text":"Create a Docker Registry running on RedHat OpenShift cluster. The registry will be backed by persistant storage, and accessible via either a clusterIP or loadbalancer service. This role can also be used to delete a docker registry on a cluster for a clean start. See usage below for more information.","title":"registry"},{"location":"roles/registry/#usage","text":"If you set up the registry with a loadbalancer service you will be able to push to the registry via the cluster's hostname, but before you can use the registry you will need to install the registry's CA certificate and restart the Docker daemon so that your client trusts the new registry: CACERT=$(oc -n airgap-registry get secret airgap-registry-certificate -o jsonpath='{.data.ca\\.crt}' | base64 -d) DOMAIN=$(oc get ingress.config cluster -o jsonpath='{.spec.domain}') sudo mkdir -p /etc/docker/certs.d/$DOMAIN\\:32500/ sudo echo \"$CACERT\" > /etc/docker/certs.d/$DOMAIN\\:32500/ca.crt sudo service docker restart You can now use the registry as normal: DOMAIN=$(oc get ingress.config cluster -o jsonpath='{.spec.domain}') docker pull registry.access.redhat.com/ubi8/ubi-minimal docker tag registry.access.redhat.com/ubi8/ubi-minimal $DOMAIN:32500/ubi8/ubi-minimal docker push $DOMAIN:32500/ubi8/ubi-minimal If you set up the registry with a clusterip service you will only be able to push to the registry after using port forwarding: oc -n airgap-registry port-forward deployment/airgap-registry 9000:5000 docker pull registry.access.redhat.com/ubi8/ubi-minimal docker tag registry.access.redhat.com/ubi8/ubi-minimal localhost:9000/ubi8/ubi-minimal docker push localhost:9000/ubi8/ubi-minimal However, you will still need to set up Docker trust for the \"local\" registry: CACERT=$(oc -n airgap-registry get secret airgap-registry-certificate -o jsonpath='{.data.ca\\.crt}' | base64 -d) sudo mkdir -p /etc/docker/certs.d/$DOMAIN\\:32500/ sudo mkdir /etc/docker/certs.d/localhost\\:9000 sudo echo \"$CACERT\" > /etc/docker/certs.d/localhost\\:9000/ca.crt sudo service docker restart Usage for tear-down action This role can also be used to permanently delete a mirror registry from a given cluster by setting the registry_action to tear-down and specifying the corresponding registry_namespace , if not using the default value. Note that the tear-down action deletes the registry completely including the PVC storage and the registry namespace. To start up the registry again, the role needs to be run again with the registry_action on default or setup . Images previously stored in the registry before the tear-down will no longer be available and will need to be mirrored again once the registry setup has completed. Take precaution when using this function and expect that images can no longer be accessed from the registry that has been torn down. Note: Recreating the registry will also create a new ca cert for the new registry. An appropriate time to use this tear-down function is when the registry has too many images that are not being used or when there has been a shift to support newer versions but images of older versions are clogging the registry. The tear-down function frees the disk space and allows for a new registry to be setup.","title":"Usage"},{"location":"roles/registry/#role-variables","text":"","title":"Role Variables"},{"location":"roles/registry/#registry_action","text":"Action to perform with the registry deployment. Optional Environment Variable: REGISTRY_ACTION Default: setup Purpose : Controls whether to set up a new registry or tear down an existing one. When to use : Set to tear-down when you need to completely remove a registry to free disk space or start fresh. Use default setup for normal installation. Valid values : - setup - Install and configure the registry - tear-down - Permanently delete the registry, namespace, and all stored images Impact : The tear-down action is destructive and irreversible. All images stored in the registry will be lost, and the PVC storage will be deleted. A new CA certificate will be generated if the registry is recreated. Related variables : registry_namespace Notes : - Use tear-down when the registry contains too many unused images or when migrating to newer versions - After tear-down, you must re-mirror all required images - Docker clients will need to trust the new CA certificate after recreation","title":"registry_action"},{"location":"roles/registry/#registry_namespace","text":"Namespace where the Docker registry will be deployed. Optional Environment Variable: REGISTRY_NAMESPACE Default: airgap-registry Purpose : Isolates the registry resources in a dedicated namespace for organization and access control. When to use : Override the default if you have namespace naming conventions or need multiple registries in the same cluster. Valid values : Any valid Kubernetes namespace name (lowercase alphanumeric and hyphens) Impact : All registry resources (deployment, service, PVC, secrets) will be created in this namespace. The namespace will be deleted during tear-down operations. Related variables : registry_action Notes : Ensure the namespace name doesn't conflict with existing namespaces in your cluster.","title":"registry_namespace"},{"location":"roles/registry/#registry_storage_class","text":"Storage class for the registry's persistent volume. Required (except on IBM Cloud ROKS where it defaults to ibmc-block-gold ) Environment Variable: REGISTRY_STORAGE_CLASS Default: None (IBM Cloud ROKS: ibmc-block-gold ) Purpose : Provides persistent storage for container images stored in the registry. When to use : Must be specified for all non-ROKS clusters. On ROKS, the default is usually appropriate unless you have specific performance requirements. Valid values : Any storage class name available in your cluster that supports ReadWriteOnce (RWO) access mode. Common examples: - IBM Cloud ROKS: ibmc-block-gold , ibmc-block-silver , ibmc-block-bronze - AWS: gp2 , gp3 , io1 - Azure: managed-premium , managed-standard - On-premises: Depends on your storage provider Impact : Determines the performance characteristics and availability of the registry storage. The storage class must support RWO access mode. Related variables : registry_storage_capacity Notes : - Verify the storage class exists before deployment: oc get storageclass - The storage class cannot be changed after initial deployment without recreating the registry","title":"registry_storage_class"},{"location":"roles/registry/#registry_storage_capacity","text":"Size of the persistent volume claim for registry storage. Optional Environment Variable: REGISTRY_STORAGE_CAPACITY Default: 100Gi Purpose : Allocates disk space for storing container images in the registry. When to use : Increase from the default based on the number and size of images you plan to store. Consider your mirroring requirements and image retention policies. Valid values : Any valid Kubernetes storage size (e.g., 100Gi , 500Gi , 1Ti ). Must be supported by your storage class. Impact : Determines how many images can be stored before the registry runs out of space. Insufficient capacity will cause push operations to fail. Related variables : registry_storage_class Notes : - Plan for growth - container images can be large (1-5GB each) - Monitor usage regularly: oc get pvc -n <registry_namespace> - Expanding PVC size after deployment depends on your storage class capabilities - For airgap environments, calculate total size of all images to be mirrored plus 20% buffer","title":"registry_storage_capacity"},{"location":"roles/registry/#registry_service_type","text":"Service type for exposing the registry. Optional Environment Variable: REGISTRY_SERVICE_TYPE Default: loadbalancer Purpose : Controls how the registry is exposed for access from Docker clients. When to use : - Use loadbalancer (default) when you need to push images from outside the cluster - Use clusterip when you only need internal cluster access or will use port-forwarding Valid values : - loadbalancer - Exposes registry externally via cluster domain on port 32500 - clusterip - Internal cluster access only (requires port-forwarding for external access) Impact : - loadbalancer : Enables direct external access via <cluster-domain>:32500 , but requires port 32500 to be available - clusterip : More secure (no external exposure) but requires oc port-forward for each push operation Related variables : None Notes : - LIMITATION : The loadbalancer port (32500) cannot be customized. If port 32500 is already in use by another service, you must use clusterip mode - With loadbalancer, you must install the registry's CA certificate on your Docker client (see Usage section) - With clusterip, you still need to configure Docker trust for localhost:9000 (see Usage section)","title":"registry_service_type"},{"location":"roles/registry/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: registry_storage_class: ibmc-block-gold registry_storage_capacity: 500Gb registry_service_type: loadbalancer roles: - ibm.mas_devops.registry","title":"Example Playbook"},{"location":"roles/registry/#license","text":"EPL-2.0","title":"License"},{"location":"roles/sls/","text":"sls \u00a4 Install IBM Suite License Service and generate a configuration that can be directly applied to IBM Maximo Application Suite. The role assumes that you have already installed the Certificate Manager in the target cluster. This action is performed by the cert_manager role if you want to use this collection to install the cert-manager operator. Role Variables - General \u00a4 General Variables \u00a4 sls_action \u00a4 Specifies which operation to perform on the Suite License Service (SLS) instance. Optional Environment Variable: SLS_ACTION Default: install Purpose : Controls what action the role executes against the SLS instance. This allows the same role to handle installation, configuration generation, and removal of SLS. When to use : - Use install for initial SLS deployment or updates - Use gencfg to generate SLS configuration for MAS without installing SLS (when using existing SLS) - Use uninstall to remove SLS instance (use with caution) Valid values : install , gencfg , uninstall Impact : - install : Deploys or updates SLS operator and instance - gencfg : Only generates SLSCfg resource for MAS integration - uninstall : Removes SLS instance and operator (destructive operation) Related variables : When using gencfg , requires sls_url to be set to point to existing SLS instance. Note : Always backup license data before using uninstall . The gencfg action is useful when SLS is shared across multiple MAS instances. Role Variables - Installation \u00a4 If sls_url is set then the role will skip the installation of an SLS instance and simply generate the SLSCfg resource for the SLS instance defined. artifactory_username \u00a4 Username for authenticating to IBM Artifactory to access development builds of SLS. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None Purpose : Provides authentication credentials to pull development/pre-release SLS operator images from IBM's Artifactory registry. Required only when installing development builds for testing or early access. When to use : - Required when sls_catalog_source is set to ibm-sls-operators (development catalog) - Not needed for production installations using ibm-operator-catalog - Use your IBM w3Id username for development builds Valid values : Valid IBM Artifactory username (typically your w3Id) Impact : Without valid credentials, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_token : Must be set together with this username - sls_catalog_source : Determines if Artifactory credentials are needed Note : Only required for development/pre-release builds. Production installations use ibm_entitlement_key instead (for SLS 3.7.0 and earlier). artifactory_token \u00a4 API token for authenticating to IBM Artifactory to access development builds of SLS. Optional Environment Variable: ARTIFACTORY_TOKEN Default: None Purpose : Provides the API token/password credential to authenticate with IBM's Artifactory registry when pulling development/pre-release SLS operator images. When to use : - Required when sls_catalog_source is set to ibm-sls-operators (development catalog) - Not needed for production installations using ibm-operator-catalog - Use your IBM Artifactory API key for development builds Valid values : Valid IBM Artifactory API token Impact : Without a valid token, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_username : Must be set together with this token - sls_catalog_source : Determines if Artifactory credentials are needed Note : Only required for development/pre-release builds. Keep this token secure and do not commit to source control. sls_catalog_source \u00a4 Specifies the OpenShift operator catalog source containing the SLS operator subscription. Optional Environment Variable: SLS_CATALOG_SOURCE Default: ibm-operator-catalog Purpose : Controls which operator catalog is used to locate and install the SLS operator. This determines where OpenShift looks for the operator images and metadata during installation. When to use : - Use default ibm-operator-catalog for production installations with official IBM releases - Use ibm-sls-operators for development builds from Artifactory - Only change if directed to use a custom catalog for specific testing or airgap scenarios Valid values : Any valid CatalogSource name present in the openshift-marketplace namespace (typically ibm-operator-catalog or ibm-sls-operators ) Impact : Changing this value affects which operator versions are available for installation. An invalid catalog source will cause the subscription to fail. Development catalogs require additional authentication via artifactory_username and artifactory_token . Related variables : - sls_channel : Works together to determine the specific operator version installed - artifactory_username and artifactory_token : Required when using ibm-sls-operators Note : For development catalogs, you must also provide Artifactory credentials. sls_channel \u00a4 Specifies the SLS operator subscription channel, which determines the version stream you'll receive updates from. Optional Environment Variable: SLS_CHANNEL Default: 3.x Purpose : Controls which version of SLS will be installed and which updates will be automatically applied. The channel corresponds to major version releases and determines the feature set and compatibility level of your SLS installation. When to use : - Use default 3.x for latest SLS 3.x releases (recommended for most deployments) - Use specific older channels only if required for compatibility with older MAS versions - Consult the MAS compatibility matrix before selecting a channel Valid values : 3.x (check the IBM Operator Catalog for currently available channels) Impact : The channel determines which SLS version is installed and which automatic updates are applied. Changing channels after installation will trigger an upgrade to the latest version in that channel. Related variables : Works with sls_catalog_source to determine available channels. Note : SLS 3.8.0+ no longer requires IBM entitlement keys as images moved to public registry. Review the SLS release notes before changing this value. sls_namespace \u00a4 OpenShift namespace where the SLS operator and instance will be deployed. Optional Environment Variable: SLS_NAMESPACE Default: ibm-sls Purpose : Defines the Kubernetes namespace for SLS resources, providing isolation and organization for the SLS deployment within the cluster. When to use : - Use default ibm-sls for standard deployments - Change only if you need multiple SLS instances or have namespace naming requirements - Ensure namespace doesn't conflict with existing deployments Valid values : Any valid Kubernetes namespace name (lowercase alphanumeric and hyphens) Impact : All SLS resources (operator, pods, secrets, services, ConfigMaps) will be created in this namespace. Changing this after deployment requires reinstallation. Related variables : Used in SLSCfg generation to reference SLS service endpoints. Note : The namespace will be created if it doesn't exist. Ensure you have permissions to create namespaces in the cluster. sls_icr_cpopen \u00a4 Container registry source for SLS operator and component images. Optional Environment Variable: SLS_ICR_CPOPEN Default: icr.io/cpopen Purpose : Specifies the container registry from which SLS pulls all operator and component images. From SLS 3.8.0 onwards, this is the primary registry variable as images moved to the public IBM Container Registry. When to use : - Use default icr.io/cpopen for production installations (SLS 3.8.0+) - Override only for development images or airgap/mirror scenarios - No authentication required for default public registry Valid values : Any valid container registry URL (e.g., icr.io/cpopen , my-registry.com/sls ) Impact : All SLS images will be pulled from this registry. An incorrect or inaccessible registry will cause image pull failures. The default public registry requires no entitlement key (SLS 3.8.0+). Related variables : - sls_icr_cp : Deprecated in SLS 3.8.0, only needed for SLS 3.7.0 and earlier - ibm_entitlement_key : Not required when using default public registry (SLS 3.8.0+) Note : SLS 3.8.0+ uses public registry - no entitlement key needed. For SLS 3.7.0 and earlier, use sls_icr_cp and provide ibm_entitlement_key . sls_instance_name \u00a4 Unique identifier for this SLS installation within the cluster. Optional Environment Variable: SLS_INSTANCE_NAME Default: sls Purpose : Provides a unique identifier that distinguishes this SLS installation from others that may exist in the same cluster. This ID is used to generate resource names throughout the installation. When to use : - Use default sls for standard single-SLS deployments - Change only if you need multiple SLS instances in the same cluster - Must be unique within the cluster if running multiple SLS instances Valid values : Lowercase alphanumeric string (e.g., sls , sls-prod , sls-dev ) Impact : This ID becomes part of resource names and is embedded in many SLS resources. It cannot be changed after installation without reinstalling. Related variables : Used in SLSCfg generation to identify the SLS instance. Note : Choose carefully as this cannot be changed after installation. Most deployments use the default sls value. sls_icr_cp \u00a4 Container registry source for SLS images (SLS 3.7.0 and earlier only). Optional Environment Variable: SLS_ICR_CP Default: cp.icr.io/cp Purpose : Specifies the container registry for SLS images in versions 3.7.0 and earlier. This registry required IBM entitlement key authentication. When to use : - Only required for SLS versions 3.7.0 and earlier - Not needed for SLS 3.8.0+ (images moved to public registry) - Override only for development images or airgap scenarios with older SLS versions Valid values : Any valid container registry URL (e.g., cp.icr.io/cp , my-registry.com/sls ) Impact : For SLS 3.7.0 and earlier, all SLS images will be pulled from this registry. Requires ibm_entitlement_key for authentication. Related variables : - sls_icr_cpopen : Use this instead for SLS 3.8.0+ - ibm_entitlement_key : Required when using this registry (SLS 3.7.0 and earlier) Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry ( icr.io/cpopen ). This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, use sls_icr_cpopen instead (no entitlement key needed). ibm_entitlement_key \u00a4 IBM entitlement key for accessing IBM Container Registry images (SLS 3.7.0 and earlier only). Required for SLS 3.7.0 and earlier (unless sls_url is provided) Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Authenticates access to IBM's entitled container registry to pull SLS operator and component images. This key is tied to your IBM Cloud account and product entitlements. When to use : - Required for SLS versions 3.7.0 and earlier - Not required for SLS 3.8.0 and later (images moved to public registry) - Obtain from IBM Container Library Valid values : IBM entitlement key string (typically starts with \"eyJ...\") Impact : Invalid or expired keys will cause image pull failures for SLS 3.7.0 and earlier. The key is stored in a Kubernetes secret in the SLS namespace. Related variables : - sls_entitlement_username : Username paired with this key (default: cp ) - sls_icr_cp : Registry requiring this authentication (SLS 3.7.0 and earlier) Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry ( icr.io/cpopen ). This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, no entitlement key is needed. sls_entitlement_username \u00a4 Username for authenticating to IBM Container Registry (SLS 3.7.0 and earlier only). Optional Environment Variable: SLS_ENTITLEMENT_USERNAME Default: cp Purpose : Provides the username credential for authenticating with IBM's entitled container registry when pulling SLS images for versions 3.7.0 and earlier. When to use : - Only needed for SLS versions 3.7.0 and earlier - Not required for SLS 3.8.0+ (images moved to public registry) - Usually can be left at default cp for production installations Valid values : cp (for production installations with IBM entitlement key) Impact : Used together with ibm_entitlement_key to create image pull secrets for the SLS namespace. Incorrect username will cause image pull authentication failures. Related variables : - ibm_entitlement_key : Must be set together with this username (SLS 3.7.0 and earlier) - sls_entitlement_key : Alternative entitlement key specific for SLS Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry. This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, no authentication is needed. sls_entitlement_key \u00a4 SLS-specific IBM entitlement key override (SLS 3.7.0 and earlier only). Optional Environment Variable: SLS_ENTITLEMENT_KEY Default: None (falls back to ibm_entitlement_key ) Purpose : Provides an SLS-specific IBM entitlement key that overrides the global ibm_entitlement_key variable. Primarily used in development scenarios where different keys are needed for different components. When to use : - Only needed for SLS versions 3.7.0 and earlier - Use when you need a different entitlement key specifically for SLS - Leave unset to use the global ibm_entitlement_key value - Not required for SLS 3.8.0+ (images moved to public registry) Valid values : IBM entitlement key string (typically starts with \"eyJ...\") Impact : When set, this key is used instead of ibm_entitlement_key for SLS image authentication. Invalid keys will cause image pull failures. Related variables : - ibm_entitlement_key : Global entitlement key that this variable overrides - sls_entitlement_username : Username paired with this key Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry. This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, no entitlement key is needed. Role Variables - Configuration \u00a4 sls_domain \u00a4 Custom domain name for external access to SLS via OpenShift routes. Optional Environment Variable: SLS_DOMAIN Default: None (SLS is only accessible within the cluster) Purpose : Enables external access to SLS through a custom domain route. This is essential when SLS needs to serve multiple MAS instances deployed in separate OpenShift clusters. When to use : - Required when SLS serves MAS instances in different OpenShift clusters - Required for multi-cluster SLS deployments - Leave unset for single-cluster deployments where SLS and MAS are in the same cluster Valid values : Any valid DNS domain name (e.g., sls.mycompany.com , license.example.org ) Impact : When set, SLS creates an external route accessible at the specified domain. You must ensure DNS is properly configured to resolve this domain to your cluster. When not set, SLS is only accessible via internal cluster services. Related variables : Affects how MAS instances connect to SLS (internal service vs external route). Note : Ensure proper DNS configuration and SSL certificates for the domain. External access requires appropriate network security controls. sls_auth_enforce \u00a4 Controls whether SLS enforces client authentication via mutual TLS (mTLS). Optional Environment Variable: SLS_AUTH_ENFORCE Default: True Purpose : Determines whether SLS requires clients to authenticate using mTLS certificates generated through the client registration flow. This provides secure authentication for SLS API access. When to use : - Use True (default) for production environments to enforce secure authentication - Set to False only in development/testing environments where simplified access is needed - Leave as default for standard secure deployments Valid values : True , False Impact : - When True : Clients must register with SLS and use mTLS certificates for API calls (secure) - When False : Authentication is not enforced, allowing unauthenticated API access (insecure) Related variables : Works with sls_registration_open to control client registration and access. Note : Setting to False removes authentication requirements and should only be used in non-production environments. Production deployments should always enforce authentication. sls_mongo_retrywrites \u00a4 Controls whether SLS uses MongoDB retryable writes feature. Optional Environment Variable: SLS_MONGO_RETRYWRITES Default: true Purpose : Determines whether SLS enables MongoDB's retryable writes feature, which automatically retries certain write operations that fail due to transient network errors or replica set elections. When to use : - Use true (default) when using MongoDB Community Edition or IBM Cloud Databases for MongoDB - Set to false when using AWS DocumentDB (which doesn't support retryable writes) - Set to false for any MongoDB-compatible database that doesn't support this feature Valid values : true , false Impact : - When true : Enables automatic retry of failed write operations (improves reliability) - When false : Disables retryable writes (required for DocumentDB and similar services) Related variables : - sls_mongodb_cfg_file or sls_mongodb.* : Determines which MongoDB service is used - Must align with the capabilities of your MongoDB provider Note : AWS DocumentDB does not support retryable writes. Set to false when using DocumentDB to avoid connection errors. sls_compliance_enforce \u00a4 Controls whether SLS enforces license token compliance. Optional Environment Variable: SLS_COMPLIANCE_ENFORCE Default: True Purpose : Determines whether SLS blocks license checkout requests when insufficient tokens are available. This controls whether license limits are strictly enforced or merely tracked. When to use : - Use True (default) for production to enforce license compliance and prevent over-consumption - Set to False only in development/testing environments or during grace periods - Leave as default for standard compliant deployments Valid values : True , False Impact : - When True : License checkout requests are denied when insufficient tokens are available (enforces compliance) - When False : License checkout requests succeed even without available tokens (tracks but doesn't enforce) Related variables : Works with license entitlement configuration to control token availability. Note : Setting to False allows operation beyond licensed capacity and may result in license compliance issues. Use only in non-production environments or with explicit approval. sls_registration_open \u00a4 Controls whether SLS allows new client self-registration. Optional Environment Variable: SLS_REGISTRATION_OPEN Default: True Purpose : Determines whether clients can register themselves with SLS to obtain certificates for API access. This controls the initial onboarding process for new SLS clients. When to use : - Use True (default) to allow MAS instances and other clients to self-register - Set to False after all clients are registered to prevent unauthorized registrations - Leave as default for standard deployments where clients need to register Valid values : True , False Impact : - When True : Clients can register themselves and obtain mTLS certificates for SLS API access - When False : New client registrations are blocked; only pre-registered clients can access SLS Related variables : Works with sls_auth_enforce to control authentication and registration flow. Note : For production environments, consider setting to False after all legitimate clients are registered to prevent unauthorized access. sls_mongodb_cfg_file \u00a4 Local file path to a MongoCfg YAML file for SLS MongoDB configuration. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: SLS_MONGODB_CFG_FILE Default: None Purpose : Provides MongoDB connection details to SLS by referencing a MongoCfg file generated by the mongodb role. This simplifies configuration by reusing existing MongoDB setup information. When to use : - Use when you've deployed MongoDB using the mongodb role (which generates this file) - Preferred method as it ensures consistency with MongoDB deployment - Alternative to manually specifying sls_mongodb.* variables Valid values : Any valid local filesystem path to a MongoCfg YAML file (e.g., /home/user/masconfig/mongocfg-mongoce-system.yaml ) Impact : The role extracts MongoDB connection details (hosts, certificates, credentials) from this file to configure SLS. Invalid or missing file will cause installation to fail. Related variables : - sls_mongodb.* : Alternative method to specify MongoDB connection details - mas_config_dir : Directory where mongodb role generates this file Note : Either this variable or the sls_mongodb object must be provided. Using this file is recommended for consistency with MongoDB deployment. sls_mongodb.hosts \u00a4 List of MongoDB host:port pairs for SLS database connection. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Specifies the MongoDB replica set members that SLS will connect to for license data storage. Multiple hosts provide high availability and failover capability. When to use : - Use when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Required for all MongoDB deployment types (Community, IBM Cloud, AWS DocumentDB) - Provide all replica set members for high availability Valid values : Array of strings in format [\"host1:port1\", \"host2:port2\", \"host3:port3\"] (e.g., [\"mongo-0.mongo:27017\", \"mongo-1.mongo:27017\", \"mongo-2.mongo:27017\"] ) Impact : SLS uses these hosts to establish database connections. Incorrect hosts will cause SLS installation to fail. Multiple hosts enable automatic failover. Related variables : - sls_mongodb.certificates : TLS certificates for secure connection - sls_mongodb.username and sls_mongodb.password : Authentication credentials - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file . sls_mongodb.certificates \u00a4 List of TLS/SSL certificates for secure MongoDB connections. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Provides the TLS/SSL certificates needed to establish secure, encrypted connections between SLS and MongoDB. These certificates verify the MongoDB server's identity and enable encrypted communication. When to use : - Required when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Needed for all MongoDB deployments using TLS/SSL (recommended for production) - Obtain from your MongoDB deployment (CA certificate or server certificate) Valid values : Array of certificate strings in PEM format (e.g., [\"-----BEGIN CERTIFICATE-----\\n...\\n-----END CERTIFICATE-----\"] ) Impact : Without valid certificates, SLS cannot establish secure connections to MongoDB. Invalid certificates will cause connection failures. Related variables : - sls_mongodb.hosts : MongoDB servers these certificates authenticate - sls_mongodb.username and sls_mongodb.password : Used with certificates for authentication - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file . Certificates must match the MongoDB deployment's TLS configuration. sls_mongodb.username \u00a4 MongoDB username for SLS database authentication. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Provides the username credential for SLS to authenticate with MongoDB. This user must have appropriate permissions to read and write to the SLS database. When to use : - Required when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Must correspond to a valid MongoDB user with SLS database permissions - Obtain from your MongoDB deployment configuration Valid values : Valid MongoDB username string Impact : SLS uses this username to authenticate with MongoDB. Invalid username will cause authentication failures and prevent SLS from accessing the database. Related variables : - sls_mongodb.password : Password for this username - sls_mongodb.hosts : MongoDB servers to authenticate against - sls_mongodb.certificates : TLS certificates for secure authentication - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file . Ensure the user has appropriate database permissions. sls_mongodb.password \u00a4 MongoDB password for SLS database authentication. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Provides the password credential for SLS to authenticate with MongoDB. Used together with the username to establish secure database connections. When to use : - Required when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Must correspond to the password for the specified MongoDB username - Obtain from your MongoDB deployment configuration Valid values : Valid MongoDB password string Impact : SLS uses this password to authenticate with MongoDB. Invalid password will cause authentication failures and prevent SLS from accessing the database. Related variables : - sls_mongodb.username : Username for this password - sls_mongodb.hosts : MongoDB servers to authenticate against - sls_mongodb.certificates : TLS certificates for secure authentication - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file . Keep this password secure and do not commit to source control. mas_pod_templates_dir \u00a4 Local directory path containing pod template customization files for SLS. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Allows customization of pod specifications for SLS workloads, enabling control over resource limits, node affinity, tolerations, and other Kubernetes pod settings. This is essential for production deployments with specific infrastructure requirements. When to use : - Use to set custom resource limits (CPU, memory) for SLS pods - Use to configure node affinity or anti-affinity rules - Use to add tolerations for tainted nodes - Use to apply custom security contexts or service accounts - Leave unset for default pod configurations Valid values : Any valid local filesystem path containing ibm-sls-licenseservice.yml Impact : The pod template file will be applied to the LicenseService Custom Resource under spec.podTemplates . Invalid templates can cause pod scheduling failures. Related variables : None Note : This role looks for a file named ibm-sls-licenseservice.yml in the specified directory. The file content should be the YAML block to insert under podTemplates: {object} . For examples, see the BestEfforts reference configuration . For full documentation, refer to Customizing Pod Templates . Bootstrap Variables (SLS 3.7.0 and higher) \u00a4 entitlement_file \u00a4 Local file path to the IBM license entitlement file for bootstrapping SLS. Optional Environment Variable: SLS_ENTITLEMENT_FILE Default: None Purpose : Provides the license entitlement file that defines the MAS product licenses and token allocations available in SLS. This file is obtained from IBM and contains your license entitlements. When to use : - Use to automatically bootstrap SLS with license entitlements during installation - Leave unset if you plan to upload license entitlements manually after SLS installation - Required for fully automated SLS deployments Valid values : Any valid local filesystem path to an IBM license entitlement file (e.g., /home/user/licenses/entitlement.dat ) Impact : When provided, SLS is automatically configured with the license entitlements from this file. Without this, you must manually upload license entitlements through the SLS UI or API after installation. Related variables : None Note : Application Support: SLS 3.7.0 and higher . The license file is obtained from IBM and contains your MAS product entitlements. Keep this file secure as it represents your license rights. Role Variables - Bootstrap [Partly deprecated in SLS 3.7.0] \u00a4 bootstrap.license_file \u00a4 Deprecated in SLS 3.7.0 Defines the License File to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on. Note: this variable used to be called bootstrap.entitlement_file and defaulted to {{mas_config_dir}}/entitlement.lic , this is no longer the case and SLS_LICENSE_FILE has to be set in order to bootstrap. This is now deprecated in SLS 3.7.0. Use this only for versions up to 3.6.0. Optional Environment Variable: SLS_LICENSE_FILE Default: None bootstrap.license_id \u00a4 Deprecated in SLS 3.7.0 Defines the License Id to be used to bootstrap SLS. This must be set when bootstrap.license_file is also set and should match the licenseId from the license file. Don't set if you wish to setup entitlement later on. Note: this is now deprecated in SLS 3.7.0. Use this only for versions up to 3.6.0. Optional unless bootstrap.license_file is set Environment Variable: SLS_LICENSE_ID Default: None bootstrap.registration_key \u00a4 Defines the Registration Key to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on Optional Environment Variable: SLS_REGISTRATION_KEY Default: None SLSCfg Variables \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the SlsCfg configuration will target. Optional, if this or mas_config_dir are not set then the role will not generate a SlsCfg template Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated SlsCfg resource definition. This can be used to manually configure a MAS instance to connect to SLS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a SlsCfg template. Optional (if this or mas_config_dir are not set then the role will not generate a SlsCfg template) Environment Variable: MAS_CONFIG_DIR Default Value: None sls_url \u00a4 The URL of the LicenseService to be called when the Maximo Application Suite is registered with SLS. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_URL Default Value: None mas_license_sync_frequency \u00a4 The sync frequency of user license sync cronjob between Maximo Application Suite and SLS. Optional Environment Variable: MAS_LICENSE_SYNC_FREQUENCY Default Value: */30 * * * * sls_tls_crt \u00a4 The TLS CA certificate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. Takes precedence over sls_tls_crt_local_file_path . Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_TLS_CERT Default Value: None sls_tls_crt_local_file_path \u00a4 The path on the local system to a file containing the TLS CA certificate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. This variable is only used if sls_tls_crt has not been set. Optional (used to instruct the role to set up MAS for an existing SLS instance) Environment Variable: SLS_TLS_CERT_LOCAL_FILE_PATH Default Value: None sls_registration_key \u00a4 The Registration key of the LicenseService instance to be used when the Maximo Application Suite is registered with SLS. Optional Environment Variable: SLS_REGISTRATION_KEY Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None mas_pod_templates_dir \u00a4 Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-slscfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the SlsCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Example Playbook \u00a4 Install and generate a configuration [up to SLS 3.6.0] \u00a4 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxx mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" bootstrap: license_id: \"aa78dd65ef10\" license_file: \"/etc/mas/entitlement.lic\" registration_key: xxxx roles: - ibm.mas_devops.sls Install and upload license file [SLS 3.7.0] \u00a4 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxx mas_instance_id: inst1 sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" entitlement_file: \"/etc/mas/entitlement.lic\" roles: - ibm.mas_devops.sls Install and upload license file [from SLS 3.8.0] \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" entitlement_file: \"/etc/mas/entitlement.lic\" roles: - ibm.mas_devops.sls Generate a configuration for an existing installation \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_action: gencfg sls_tls_crt_local_file_path: \"/home/me/sls.crt\" slscfg_url: \"https://xxx\" slscfg_registration_key: \"xxx\" roles: - ibm.mas_devops.sls Run Role Playbook \u00a4 export SLS_MONGODB_CFG_FILE=/etc/mas/mongodb.yml export SLS_ENTITLEMENT_FILE=/etc/mas/entitlement.lic export MAS_INSTANCE_ID=inst1 ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"sls"},{"location":"roles/sls/#sls","text":"Install IBM Suite License Service and generate a configuration that can be directly applied to IBM Maximo Application Suite. The role assumes that you have already installed the Certificate Manager in the target cluster. This action is performed by the cert_manager role if you want to use this collection to install the cert-manager operator.","title":"sls"},{"location":"roles/sls/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/sls/#general-variables","text":"","title":"General Variables"},{"location":"roles/sls/#sls_action","text":"Specifies which operation to perform on the Suite License Service (SLS) instance. Optional Environment Variable: SLS_ACTION Default: install Purpose : Controls what action the role executes against the SLS instance. This allows the same role to handle installation, configuration generation, and removal of SLS. When to use : - Use install for initial SLS deployment or updates - Use gencfg to generate SLS configuration for MAS without installing SLS (when using existing SLS) - Use uninstall to remove SLS instance (use with caution) Valid values : install , gencfg , uninstall Impact : - install : Deploys or updates SLS operator and instance - gencfg : Only generates SLSCfg resource for MAS integration - uninstall : Removes SLS instance and operator (destructive operation) Related variables : When using gencfg , requires sls_url to be set to point to existing SLS instance. Note : Always backup license data before using uninstall . The gencfg action is useful when SLS is shared across multiple MAS instances.","title":"sls_action"},{"location":"roles/sls/#role-variables-installation","text":"If sls_url is set then the role will skip the installation of an SLS instance and simply generate the SLSCfg resource for the SLS instance defined.","title":"Role Variables - Installation"},{"location":"roles/sls/#artifactory_username","text":"Username for authenticating to IBM Artifactory to access development builds of SLS. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None Purpose : Provides authentication credentials to pull development/pre-release SLS operator images from IBM's Artifactory registry. Required only when installing development builds for testing or early access. When to use : - Required when sls_catalog_source is set to ibm-sls-operators (development catalog) - Not needed for production installations using ibm-operator-catalog - Use your IBM w3Id username for development builds Valid values : Valid IBM Artifactory username (typically your w3Id) Impact : Without valid credentials, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_token : Must be set together with this username - sls_catalog_source : Determines if Artifactory credentials are needed Note : Only required for development/pre-release builds. Production installations use ibm_entitlement_key instead (for SLS 3.7.0 and earlier).","title":"artifactory_username"},{"location":"roles/sls/#artifactory_token","text":"API token for authenticating to IBM Artifactory to access development builds of SLS. Optional Environment Variable: ARTIFACTORY_TOKEN Default: None Purpose : Provides the API token/password credential to authenticate with IBM's Artifactory registry when pulling development/pre-release SLS operator images. When to use : - Required when sls_catalog_source is set to ibm-sls-operators (development catalog) - Not needed for production installations using ibm-operator-catalog - Use your IBM Artifactory API key for development builds Valid values : Valid IBM Artifactory API token Impact : Without a valid token, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_username : Must be set together with this token - sls_catalog_source : Determines if Artifactory credentials are needed Note : Only required for development/pre-release builds. Keep this token secure and do not commit to source control.","title":"artifactory_token"},{"location":"roles/sls/#sls_catalog_source","text":"Specifies the OpenShift operator catalog source containing the SLS operator subscription. Optional Environment Variable: SLS_CATALOG_SOURCE Default: ibm-operator-catalog Purpose : Controls which operator catalog is used to locate and install the SLS operator. This determines where OpenShift looks for the operator images and metadata during installation. When to use : - Use default ibm-operator-catalog for production installations with official IBM releases - Use ibm-sls-operators for development builds from Artifactory - Only change if directed to use a custom catalog for specific testing or airgap scenarios Valid values : Any valid CatalogSource name present in the openshift-marketplace namespace (typically ibm-operator-catalog or ibm-sls-operators ) Impact : Changing this value affects which operator versions are available for installation. An invalid catalog source will cause the subscription to fail. Development catalogs require additional authentication via artifactory_username and artifactory_token . Related variables : - sls_channel : Works together to determine the specific operator version installed - artifactory_username and artifactory_token : Required when using ibm-sls-operators Note : For development catalogs, you must also provide Artifactory credentials.","title":"sls_catalog_source"},{"location":"roles/sls/#sls_channel","text":"Specifies the SLS operator subscription channel, which determines the version stream you'll receive updates from. Optional Environment Variable: SLS_CHANNEL Default: 3.x Purpose : Controls which version of SLS will be installed and which updates will be automatically applied. The channel corresponds to major version releases and determines the feature set and compatibility level of your SLS installation. When to use : - Use default 3.x for latest SLS 3.x releases (recommended for most deployments) - Use specific older channels only if required for compatibility with older MAS versions - Consult the MAS compatibility matrix before selecting a channel Valid values : 3.x (check the IBM Operator Catalog for currently available channels) Impact : The channel determines which SLS version is installed and which automatic updates are applied. Changing channels after installation will trigger an upgrade to the latest version in that channel. Related variables : Works with sls_catalog_source to determine available channels. Note : SLS 3.8.0+ no longer requires IBM entitlement keys as images moved to public registry. Review the SLS release notes before changing this value.","title":"sls_channel"},{"location":"roles/sls/#sls_namespace","text":"OpenShift namespace where the SLS operator and instance will be deployed. Optional Environment Variable: SLS_NAMESPACE Default: ibm-sls Purpose : Defines the Kubernetes namespace for SLS resources, providing isolation and organization for the SLS deployment within the cluster. When to use : - Use default ibm-sls for standard deployments - Change only if you need multiple SLS instances or have namespace naming requirements - Ensure namespace doesn't conflict with existing deployments Valid values : Any valid Kubernetes namespace name (lowercase alphanumeric and hyphens) Impact : All SLS resources (operator, pods, secrets, services, ConfigMaps) will be created in this namespace. Changing this after deployment requires reinstallation. Related variables : Used in SLSCfg generation to reference SLS service endpoints. Note : The namespace will be created if it doesn't exist. Ensure you have permissions to create namespaces in the cluster.","title":"sls_namespace"},{"location":"roles/sls/#sls_icr_cpopen","text":"Container registry source for SLS operator and component images. Optional Environment Variable: SLS_ICR_CPOPEN Default: icr.io/cpopen Purpose : Specifies the container registry from which SLS pulls all operator and component images. From SLS 3.8.0 onwards, this is the primary registry variable as images moved to the public IBM Container Registry. When to use : - Use default icr.io/cpopen for production installations (SLS 3.8.0+) - Override only for development images or airgap/mirror scenarios - No authentication required for default public registry Valid values : Any valid container registry URL (e.g., icr.io/cpopen , my-registry.com/sls ) Impact : All SLS images will be pulled from this registry. An incorrect or inaccessible registry will cause image pull failures. The default public registry requires no entitlement key (SLS 3.8.0+). Related variables : - sls_icr_cp : Deprecated in SLS 3.8.0, only needed for SLS 3.7.0 and earlier - ibm_entitlement_key : Not required when using default public registry (SLS 3.8.0+) Note : SLS 3.8.0+ uses public registry - no entitlement key needed. For SLS 3.7.0 and earlier, use sls_icr_cp and provide ibm_entitlement_key .","title":"sls_icr_cpopen"},{"location":"roles/sls/#sls_instance_name","text":"Unique identifier for this SLS installation within the cluster. Optional Environment Variable: SLS_INSTANCE_NAME Default: sls Purpose : Provides a unique identifier that distinguishes this SLS installation from others that may exist in the same cluster. This ID is used to generate resource names throughout the installation. When to use : - Use default sls for standard single-SLS deployments - Change only if you need multiple SLS instances in the same cluster - Must be unique within the cluster if running multiple SLS instances Valid values : Lowercase alphanumeric string (e.g., sls , sls-prod , sls-dev ) Impact : This ID becomes part of resource names and is embedded in many SLS resources. It cannot be changed after installation without reinstalling. Related variables : Used in SLSCfg generation to identify the SLS instance. Note : Choose carefully as this cannot be changed after installation. Most deployments use the default sls value.","title":"sls_instance_name"},{"location":"roles/sls/#sls_icr_cp","text":"Container registry source for SLS images (SLS 3.7.0 and earlier only). Optional Environment Variable: SLS_ICR_CP Default: cp.icr.io/cp Purpose : Specifies the container registry for SLS images in versions 3.7.0 and earlier. This registry required IBM entitlement key authentication. When to use : - Only required for SLS versions 3.7.0 and earlier - Not needed for SLS 3.8.0+ (images moved to public registry) - Override only for development images or airgap scenarios with older SLS versions Valid values : Any valid container registry URL (e.g., cp.icr.io/cp , my-registry.com/sls ) Impact : For SLS 3.7.0 and earlier, all SLS images will be pulled from this registry. Requires ibm_entitlement_key for authentication. Related variables : - sls_icr_cpopen : Use this instead for SLS 3.8.0+ - ibm_entitlement_key : Required when using this registry (SLS 3.7.0 and earlier) Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry ( icr.io/cpopen ). This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, use sls_icr_cpopen instead (no entitlement key needed).","title":"sls_icr_cp"},{"location":"roles/sls/#ibm_entitlement_key","text":"IBM entitlement key for accessing IBM Container Registry images (SLS 3.7.0 and earlier only). Required for SLS 3.7.0 and earlier (unless sls_url is provided) Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Authenticates access to IBM's entitled container registry to pull SLS operator and component images. This key is tied to your IBM Cloud account and product entitlements. When to use : - Required for SLS versions 3.7.0 and earlier - Not required for SLS 3.8.0 and later (images moved to public registry) - Obtain from IBM Container Library Valid values : IBM entitlement key string (typically starts with \"eyJ...\") Impact : Invalid or expired keys will cause image pull failures for SLS 3.7.0 and earlier. The key is stored in a Kubernetes secret in the SLS namespace. Related variables : - sls_entitlement_username : Username paired with this key (default: cp ) - sls_icr_cp : Registry requiring this authentication (SLS 3.7.0 and earlier) Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry ( icr.io/cpopen ). This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, no entitlement key is needed.","title":"ibm_entitlement_key"},{"location":"roles/sls/#sls_entitlement_username","text":"Username for authenticating to IBM Container Registry (SLS 3.7.0 and earlier only). Optional Environment Variable: SLS_ENTITLEMENT_USERNAME Default: cp Purpose : Provides the username credential for authenticating with IBM's entitled container registry when pulling SLS images for versions 3.7.0 and earlier. When to use : - Only needed for SLS versions 3.7.0 and earlier - Not required for SLS 3.8.0+ (images moved to public registry) - Usually can be left at default cp for production installations Valid values : cp (for production installations with IBM entitlement key) Impact : Used together with ibm_entitlement_key to create image pull secrets for the SLS namespace. Incorrect username will cause image pull authentication failures. Related variables : - ibm_entitlement_key : Must be set together with this username (SLS 3.7.0 and earlier) - sls_entitlement_key : Alternative entitlement key specific for SLS Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry. This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, no authentication is needed.","title":"sls_entitlement_username"},{"location":"roles/sls/#sls_entitlement_key","text":"SLS-specific IBM entitlement key override (SLS 3.7.0 and earlier only). Optional Environment Variable: SLS_ENTITLEMENT_KEY Default: None (falls back to ibm_entitlement_key ) Purpose : Provides an SLS-specific IBM entitlement key that overrides the global ibm_entitlement_key variable. Primarily used in development scenarios where different keys are needed for different components. When to use : - Only needed for SLS versions 3.7.0 and earlier - Use when you need a different entitlement key specifically for SLS - Leave unset to use the global ibm_entitlement_key value - Not required for SLS 3.8.0+ (images moved to public registry) Valid values : IBM entitlement key string (typically starts with \"eyJ...\") Impact : When set, this key is used instead of ibm_entitlement_key for SLS image authentication. Invalid keys will cause image pull failures. Related variables : - ibm_entitlement_key : Global entitlement key that this variable overrides - sls_entitlement_username : Username paired with this key Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry. This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, no entitlement key is needed.","title":"sls_entitlement_key"},{"location":"roles/sls/#role-variables-configuration","text":"","title":"Role Variables - Configuration"},{"location":"roles/sls/#sls_domain","text":"Custom domain name for external access to SLS via OpenShift routes. Optional Environment Variable: SLS_DOMAIN Default: None (SLS is only accessible within the cluster) Purpose : Enables external access to SLS through a custom domain route. This is essential when SLS needs to serve multiple MAS instances deployed in separate OpenShift clusters. When to use : - Required when SLS serves MAS instances in different OpenShift clusters - Required for multi-cluster SLS deployments - Leave unset for single-cluster deployments where SLS and MAS are in the same cluster Valid values : Any valid DNS domain name (e.g., sls.mycompany.com , license.example.org ) Impact : When set, SLS creates an external route accessible at the specified domain. You must ensure DNS is properly configured to resolve this domain to your cluster. When not set, SLS is only accessible via internal cluster services. Related variables : Affects how MAS instances connect to SLS (internal service vs external route). Note : Ensure proper DNS configuration and SSL certificates for the domain. External access requires appropriate network security controls.","title":"sls_domain"},{"location":"roles/sls/#sls_auth_enforce","text":"Controls whether SLS enforces client authentication via mutual TLS (mTLS). Optional Environment Variable: SLS_AUTH_ENFORCE Default: True Purpose : Determines whether SLS requires clients to authenticate using mTLS certificates generated through the client registration flow. This provides secure authentication for SLS API access. When to use : - Use True (default) for production environments to enforce secure authentication - Set to False only in development/testing environments where simplified access is needed - Leave as default for standard secure deployments Valid values : True , False Impact : - When True : Clients must register with SLS and use mTLS certificates for API calls (secure) - When False : Authentication is not enforced, allowing unauthenticated API access (insecure) Related variables : Works with sls_registration_open to control client registration and access. Note : Setting to False removes authentication requirements and should only be used in non-production environments. Production deployments should always enforce authentication.","title":"sls_auth_enforce"},{"location":"roles/sls/#sls_mongo_retrywrites","text":"Controls whether SLS uses MongoDB retryable writes feature. Optional Environment Variable: SLS_MONGO_RETRYWRITES Default: true Purpose : Determines whether SLS enables MongoDB's retryable writes feature, which automatically retries certain write operations that fail due to transient network errors or replica set elections. When to use : - Use true (default) when using MongoDB Community Edition or IBM Cloud Databases for MongoDB - Set to false when using AWS DocumentDB (which doesn't support retryable writes) - Set to false for any MongoDB-compatible database that doesn't support this feature Valid values : true , false Impact : - When true : Enables automatic retry of failed write operations (improves reliability) - When false : Disables retryable writes (required for DocumentDB and similar services) Related variables : - sls_mongodb_cfg_file or sls_mongodb.* : Determines which MongoDB service is used - Must align with the capabilities of your MongoDB provider Note : AWS DocumentDB does not support retryable writes. Set to false when using DocumentDB to avoid connection errors.","title":"sls_mongo_retrywrites"},{"location":"roles/sls/#sls_compliance_enforce","text":"Controls whether SLS enforces license token compliance. Optional Environment Variable: SLS_COMPLIANCE_ENFORCE Default: True Purpose : Determines whether SLS blocks license checkout requests when insufficient tokens are available. This controls whether license limits are strictly enforced or merely tracked. When to use : - Use True (default) for production to enforce license compliance and prevent over-consumption - Set to False only in development/testing environments or during grace periods - Leave as default for standard compliant deployments Valid values : True , False Impact : - When True : License checkout requests are denied when insufficient tokens are available (enforces compliance) - When False : License checkout requests succeed even without available tokens (tracks but doesn't enforce) Related variables : Works with license entitlement configuration to control token availability. Note : Setting to False allows operation beyond licensed capacity and may result in license compliance issues. Use only in non-production environments or with explicit approval.","title":"sls_compliance_enforce"},{"location":"roles/sls/#sls_registration_open","text":"Controls whether SLS allows new client self-registration. Optional Environment Variable: SLS_REGISTRATION_OPEN Default: True Purpose : Determines whether clients can register themselves with SLS to obtain certificates for API access. This controls the initial onboarding process for new SLS clients. When to use : - Use True (default) to allow MAS instances and other clients to self-register - Set to False after all clients are registered to prevent unauthorized registrations - Leave as default for standard deployments where clients need to register Valid values : True , False Impact : - When True : Clients can register themselves and obtain mTLS certificates for SLS API access - When False : New client registrations are blocked; only pre-registered clients can access SLS Related variables : Works with sls_auth_enforce to control authentication and registration flow. Note : For production environments, consider setting to False after all legitimate clients are registered to prevent unauthorized access.","title":"sls_registration_open"},{"location":"roles/sls/#sls_mongodb_cfg_file","text":"Local file path to a MongoCfg YAML file for SLS MongoDB configuration. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: SLS_MONGODB_CFG_FILE Default: None Purpose : Provides MongoDB connection details to SLS by referencing a MongoCfg file generated by the mongodb role. This simplifies configuration by reusing existing MongoDB setup information. When to use : - Use when you've deployed MongoDB using the mongodb role (which generates this file) - Preferred method as it ensures consistency with MongoDB deployment - Alternative to manually specifying sls_mongodb.* variables Valid values : Any valid local filesystem path to a MongoCfg YAML file (e.g., /home/user/masconfig/mongocfg-mongoce-system.yaml ) Impact : The role extracts MongoDB connection details (hosts, certificates, credentials) from this file to configure SLS. Invalid or missing file will cause installation to fail. Related variables : - sls_mongodb.* : Alternative method to specify MongoDB connection details - mas_config_dir : Directory where mongodb role generates this file Note : Either this variable or the sls_mongodb object must be provided. Using this file is recommended for consistency with MongoDB deployment.","title":"sls_mongodb_cfg_file"},{"location":"roles/sls/#sls_mongodbhosts","text":"List of MongoDB host:port pairs for SLS database connection. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Specifies the MongoDB replica set members that SLS will connect to for license data storage. Multiple hosts provide high availability and failover capability. When to use : - Use when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Required for all MongoDB deployment types (Community, IBM Cloud, AWS DocumentDB) - Provide all replica set members for high availability Valid values : Array of strings in format [\"host1:port1\", \"host2:port2\", \"host3:port3\"] (e.g., [\"mongo-0.mongo:27017\", \"mongo-1.mongo:27017\", \"mongo-2.mongo:27017\"] ) Impact : SLS uses these hosts to establish database connections. Incorrect hosts will cause SLS installation to fail. Multiple hosts enable automatic failover. Related variables : - sls_mongodb.certificates : TLS certificates for secure connection - sls_mongodb.username and sls_mongodb.password : Authentication credentials - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file .","title":"sls_mongodb.hosts"},{"location":"roles/sls/#sls_mongodbcertificates","text":"List of TLS/SSL certificates for secure MongoDB connections. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Provides the TLS/SSL certificates needed to establish secure, encrypted connections between SLS and MongoDB. These certificates verify the MongoDB server's identity and enable encrypted communication. When to use : - Required when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Needed for all MongoDB deployments using TLS/SSL (recommended for production) - Obtain from your MongoDB deployment (CA certificate or server certificate) Valid values : Array of certificate strings in PEM format (e.g., [\"-----BEGIN CERTIFICATE-----\\n...\\n-----END CERTIFICATE-----\"] ) Impact : Without valid certificates, SLS cannot establish secure connections to MongoDB. Invalid certificates will cause connection failures. Related variables : - sls_mongodb.hosts : MongoDB servers these certificates authenticate - sls_mongodb.username and sls_mongodb.password : Used with certificates for authentication - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file . Certificates must match the MongoDB deployment's TLS configuration.","title":"sls_mongodb.certificates"},{"location":"roles/sls/#sls_mongodbusername","text":"MongoDB username for SLS database authentication. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Provides the username credential for SLS to authenticate with MongoDB. This user must have appropriate permissions to read and write to the SLS database. When to use : - Required when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Must correspond to a valid MongoDB user with SLS database permissions - Obtain from your MongoDB deployment configuration Valid values : Valid MongoDB username string Impact : SLS uses this username to authenticate with MongoDB. Invalid username will cause authentication failures and prevent SLS from accessing the database. Related variables : - sls_mongodb.password : Password for this username - sls_mongodb.hosts : MongoDB servers to authenticate against - sls_mongodb.certificates : TLS certificates for secure authentication - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file . Ensure the user has appropriate database permissions.","title":"sls_mongodb.username"},{"location":"roles/sls/#sls_mongodbpassword","text":"MongoDB password for SLS database authentication. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Provides the password credential for SLS to authenticate with MongoDB. Used together with the username to establish secure database connections. When to use : - Required when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Must correspond to the password for the specified MongoDB username - Obtain from your MongoDB deployment configuration Valid values : Valid MongoDB password string Impact : SLS uses this password to authenticate with MongoDB. Invalid password will cause authentication failures and prevent SLS from accessing the database. Related variables : - sls_mongodb.username : Username for this password - sls_mongodb.hosts : MongoDB servers to authenticate against - sls_mongodb.certificates : TLS certificates for secure authentication - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file . Keep this password secure and do not commit to source control.","title":"sls_mongodb.password"},{"location":"roles/sls/#mas_pod_templates_dir","text":"Local directory path containing pod template customization files for SLS. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Allows customization of pod specifications for SLS workloads, enabling control over resource limits, node affinity, tolerations, and other Kubernetes pod settings. This is essential for production deployments with specific infrastructure requirements. When to use : - Use to set custom resource limits (CPU, memory) for SLS pods - Use to configure node affinity or anti-affinity rules - Use to add tolerations for tainted nodes - Use to apply custom security contexts or service accounts - Leave unset for default pod configurations Valid values : Any valid local filesystem path containing ibm-sls-licenseservice.yml Impact : The pod template file will be applied to the LicenseService Custom Resource under spec.podTemplates . Invalid templates can cause pod scheduling failures. Related variables : None Note : This role looks for a file named ibm-sls-licenseservice.yml in the specified directory. The file content should be the YAML block to insert under podTemplates: {object} . For examples, see the BestEfforts reference configuration . For full documentation, refer to Customizing Pod Templates .","title":"mas_pod_templates_dir"},{"location":"roles/sls/#bootstrap-variables-sls-370-and-higher","text":"","title":"Bootstrap Variables (SLS 3.7.0 and higher)"},{"location":"roles/sls/#entitlement_file","text":"Local file path to the IBM license entitlement file for bootstrapping SLS. Optional Environment Variable: SLS_ENTITLEMENT_FILE Default: None Purpose : Provides the license entitlement file that defines the MAS product licenses and token allocations available in SLS. This file is obtained from IBM and contains your license entitlements. When to use : - Use to automatically bootstrap SLS with license entitlements during installation - Leave unset if you plan to upload license entitlements manually after SLS installation - Required for fully automated SLS deployments Valid values : Any valid local filesystem path to an IBM license entitlement file (e.g., /home/user/licenses/entitlement.dat ) Impact : When provided, SLS is automatically configured with the license entitlements from this file. Without this, you must manually upload license entitlements through the SLS UI or API after installation. Related variables : None Note : Application Support: SLS 3.7.0 and higher . The license file is obtained from IBM and contains your MAS product entitlements. Keep this file secure as it represents your license rights.","title":"entitlement_file"},{"location":"roles/sls/#role-variables-bootstrap-partly-deprecated-in-sls-370","text":"","title":"Role Variables - Bootstrap [Partly deprecated in SLS 3.7.0]"},{"location":"roles/sls/#bootstraplicense_file","text":"Deprecated in SLS 3.7.0 Defines the License File to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on. Note: this variable used to be called bootstrap.entitlement_file and defaulted to {{mas_config_dir}}/entitlement.lic , this is no longer the case and SLS_LICENSE_FILE has to be set in order to bootstrap. This is now deprecated in SLS 3.7.0. Use this only for versions up to 3.6.0. Optional Environment Variable: SLS_LICENSE_FILE Default: None","title":"bootstrap.license_file"},{"location":"roles/sls/#bootstraplicense_id","text":"Deprecated in SLS 3.7.0 Defines the License Id to be used to bootstrap SLS. This must be set when bootstrap.license_file is also set and should match the licenseId from the license file. Don't set if you wish to setup entitlement later on. Note: this is now deprecated in SLS 3.7.0. Use this only for versions up to 3.6.0. Optional unless bootstrap.license_file is set Environment Variable: SLS_LICENSE_ID Default: None","title":"bootstrap.license_id"},{"location":"roles/sls/#bootstrapregistration_key","text":"Defines the Registration Key to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on Optional Environment Variable: SLS_REGISTRATION_KEY Default: None","title":"bootstrap.registration_key"},{"location":"roles/sls/#slscfg-variables","text":"","title":"SLSCfg Variables"},{"location":"roles/sls/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the SlsCfg configuration will target. Optional, if this or mas_config_dir are not set then the role will not generate a SlsCfg template Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/sls/#mas_config_dir","text":"Local directory to save the generated SlsCfg resource definition. This can be used to manually configure a MAS instance to connect to SLS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a SlsCfg template. Optional (if this or mas_config_dir are not set then the role will not generate a SlsCfg template) Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/sls/#sls_url","text":"The URL of the LicenseService to be called when the Maximo Application Suite is registered with SLS. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_URL Default Value: None","title":"sls_url"},{"location":"roles/sls/#mas_license_sync_frequency","text":"The sync frequency of user license sync cronjob between Maximo Application Suite and SLS. Optional Environment Variable: MAS_LICENSE_SYNC_FREQUENCY Default Value: */30 * * * *","title":"mas_license_sync_frequency"},{"location":"roles/sls/#sls_tls_crt","text":"The TLS CA certificate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. Takes precedence over sls_tls_crt_local_file_path . Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_TLS_CERT Default Value: None","title":"sls_tls_crt"},{"location":"roles/sls/#sls_tls_crt_local_file_path","text":"The path on the local system to a file containing the TLS CA certificate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. This variable is only used if sls_tls_crt has not been set. Optional (used to instruct the role to set up MAS for an existing SLS instance) Environment Variable: SLS_TLS_CERT_LOCAL_FILE_PATH Default Value: None","title":"sls_tls_crt_local_file_path"},{"location":"roles/sls/#sls_registration_key","text":"The Registration key of the LicenseService instance to be used when the Maximo Application Suite is registered with SLS. Optional Environment Variable: SLS_REGISTRATION_KEY Default Value: None","title":"sls_registration_key"},{"location":"roles/sls/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/sls/#mas_pod_templates_dir_1","text":"Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-slscfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the SlsCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None","title":"mas_pod_templates_dir"},{"location":"roles/sls/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/sls/#install-and-generate-a-configuration-up-to-sls-360","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxx mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" bootstrap: license_id: \"aa78dd65ef10\" license_file: \"/etc/mas/entitlement.lic\" registration_key: xxxx roles: - ibm.mas_devops.sls","title":"Install and generate a configuration [up to SLS 3.6.0]"},{"location":"roles/sls/#install-and-upload-license-file-sls-370","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxx mas_instance_id: inst1 sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" entitlement_file: \"/etc/mas/entitlement.lic\" roles: - ibm.mas_devops.sls","title":"Install and upload license file [SLS 3.7.0]"},{"location":"roles/sls/#install-and-upload-license-file-from-sls-380","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" entitlement_file: \"/etc/mas/entitlement.lic\" roles: - ibm.mas_devops.sls","title":"Install and upload license file [from SLS 3.8.0]"},{"location":"roles/sls/#generate-a-configuration-for-an-existing-installation","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_action: gencfg sls_tls_crt_local_file_path: \"/home/me/sls.crt\" slscfg_url: \"https://xxx\" slscfg_registration_key: \"xxx\" roles: - ibm.mas_devops.sls","title":"Generate a configuration for an existing installation"},{"location":"roles/sls/#run-role-playbook","text":"export SLS_MONGODB_CFG_FILE=/etc/mas/mongodb.yml export SLS_ENTITLEMENT_FILE=/etc/mas/entitlement.lic export MAS_INSTANCE_ID=inst1 ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/sls/#license","text":"EPL-2.0","title":"License"},{"location":"roles/smtp/","text":"smtp \u00a4 Generate an SMTP configuration that can be directly applied to IBM Maximo Application Suite. The role supports the Twilio SendGrid email provider. Twilio SendGrid Prior to running this role, you must create an account with SendGrid. The SendGrid account needs to support creating subusers. Tip The role will generate a yaml file containing the definition of a Secret and SmtpCfg resource that can be used to configure the smtp email provider for MAS. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/smtp-$MAS_INSTANCE_ID.yml\" or used in conjunction with the suite_config role. This role will create a subuser that must be validated. An email with a validation link will be sent to the primary email address. You need to validate the subuser using this link. If validation fails, you can resend the email using the SendGrid admin UI. Role Variables \u00a4 smtp_type \u00a4 SMTP email provider type for MAS email notifications. Required Environment Variable: SMTP_TYPE Default: None Purpose : Specifies which SMTP email service provider to use for sending MAS notifications and alerts. When to use : Always required when configuring SMTP for MAS. Currently only SendGrid is supported. Valid values : sendgrid (only supported provider) Impact : Determines which SMTP provider configuration will be created and which API endpoints will be used. Related variables : sendgrid_primary_apikey , sendgrid_hostname Notes : - SendGrid is currently the only supported SMTP provider - Requires an active SendGrid account with subuser support - Future versions may support additional SMTP providers mas_instance_id \u00a4 MAS instance identifier for SMTP configuration. Required (if generating SmtpCfg) Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance the SMTP configuration will be applied to. When to use : Required when generating SmtpCfg resource. If not set along with mas_config_dir , the role will not generate configuration files. Valid values : Valid MAS instance ID (3-12 lowercase alphanumeric characters, e.g., prod , dev , masinst1 ) Impact : Used to name the generated SmtpCfg resource and target the correct MAS instance. Related variables : mas_config_dir , sendgrid_subuser_username Notes : - Must match the instance ID used during MAS installation - Used in default subuser username: ibm-mas_{mas_instance_id} - Both this and mas_config_dir must be set to generate configuration mas_config_dir \u00a4 Local directory for generated SMTP configuration files. Required (if generating SmtpCfg) Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies where to save the generated SmtpCfg resource YAML file containing SMTP configuration and credentials. When to use : Required when generating SmtpCfg resource. If not set along with mas_instance_id , the role will not generate configuration files. Valid values : Valid local filesystem path (e.g., ~/masconfig , /opt/mas/config ) Impact : SmtpCfg YAML file will be created in this directory as smtp-{mas_instance_id}.yml . Related variables : mas_instance_id Notes : - Can be applied manually: oc apply -f $MAS_CONFIG_DIR/smtp-$MAS_INSTANCE_ID.yml - Can be used with suite_config role for automated configuration - Both this and mas_instance_id must be set to generate configuration - Use consistent directory across all MAS DevOps roles mas_pod_templates_dir \u00a4 Directory containing pod template customizations for SMTP. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Provides custom pod template configurations for the SMTP service pods, allowing resource limits, node selectors, and other Kubernetes pod specifications. When to use : Set when you need to customize SMTP pod resources, scheduling, or other pod-level configurations. Valid values : Valid directory path containing ibm-mas-smtpcfg.yml file Impact : Pod template configuration is inserted into SmtpCfg spec under podTemplates element. Related variables : None Notes : - File must be named ibm-mas-smtpcfg.yml - See BestEfforts reference - Full documentation: Customizing Pod Templates - Optional - only needed for custom resource requirements sendgrid_primary_username \u00a4 Primary SendGrid account username. Required Environment Variable: SMTP_PRIMARY_USERNAME Default: None Purpose : Identifies the primary SendGrid account that will be used to create a subuser for MAS email operations. When to use : Always required. Must be the username of an existing SendGrid account with subuser creation permissions. Valid values : Valid SendGrid account username Impact : Used to authenticate to SendGrid API for subuser creation and management. Related variables : sendgrid_primary_apikey , sendgrid_primary_email Notes : - Must have permissions to create subusers in SendGrid - Primary account credentials are only used during setup, not stored in MAS - Subuser will be created under this primary account sendgrid_primary_email \u00a4 Primary SendGrid account email address. Required Environment Variable: SMTP_PRIMARY_EMAIL Default: None Purpose : Provides the email address associated with the primary SendGrid account. Validation emails will be sent to this address. When to use : Always required. Must match the email registered with the SendGrid primary account. Valid values : Valid email address registered with SendGrid account Impact : Subuser validation emails will be sent to this address. You must validate the subuser using the link in the email. Related variables : sendgrid_primary_username , sendgrid_subuser_email Notes : - Validation email will be sent after subuser creation - Must validate subuser before it can send emails - Can resend validation email from SendGrid admin UI if needed sendgrid_subuser_email \u00a4 Email address for the SendGrid subuser. Required Environment Variable: SMTP_SUBUSER_EMAIL Default: None Purpose : Specifies the email address for the SendGrid subuser that will be created for MAS email operations. When to use : Always required. This subuser will be dedicated to sending MAS notifications. Valid values : Valid email address (can be same as primary or different) Impact : This email will be associated with the subuser account and used for subuser-specific operations. Related variables : sendgrid_subuser_username , sendgrid_primary_email Notes : - Subuser provides isolation from primary account - See SendGrid subusers documentation - Can use same email as primary account if desired sendgrid_defaultrecipientemail \u00a4 Default recipient email address for MAS notifications. Required Environment Variable: SENDGRID_DEFAULTRECIPIENTEMAIL Default: None Purpose : Specifies the default destination email address for MAS notifications when no specific recipient is configured. When to use : Always required. This is the fallback email address for system notifications. Valid values : Valid email address Impact : MAS will send notifications to this address when no other recipient is specified in the notification configuration. Related variables : None Notes : - Acts as fallback for unconfigured notifications - Can be overridden per notification type in MAS - Typically set to admin or operations team email sendgrid_primary_apikey \u00a4 API key for the primary SendGrid account. Required Environment Variable: SENDGRID_PRIMARY_APIKEY Default: None Purpose : Provides authentication credentials for SendGrid API to create and configure the subuser. When to use : Always required. Must be an API key from the primary SendGrid account with subuser management permissions. Valid values : Valid SendGrid API key with appropriate permissions Impact : Used to authenticate SendGrid API calls for subuser creation and IP assignment. Related variables : sendgrid_primary_username Notes : - Store securely, never commit to version control - Requires permissions: subuser creation, IP management - Only used during setup, not stored in MAS configuration - Generate from SendGrid console: Settings > API Keys sendgrid_ips \u00a4 SendGrid IP addresses to assign to the subuser. Required Environment Variable: SENDGRID_IPS Default: None Purpose : Specifies which SendGrid IP addresses should be assigned to the subuser for sending emails. When to use : Always required. Must be IP addresses already allocated to your primary SendGrid account. Valid values : JSON array of IP addresses (e.g., '[\"192.0.2.1\", \"192.0.2.2\"]' ) Impact : Subuser will send emails from these IP addresses. Affects email deliverability and reputation. Related variables : sendgrid_primary_apikey Notes : - IPs must already be allocated to primary account - Find IPs in SendGrid console: Settings > IP Addresses - Format as JSON array string - IP reputation affects email deliverability sendgrid_subuser_username \u00a4 Username for the SendGrid subuser. Optional Environment Variable: SMTP_SUBUSER_USERNAME Default: ibm-mas_{mas_instance_id} Purpose : Specifies the username for the SendGrid subuser that will be created for MAS. When to use : Override default if you need a specific naming convention or have multiple MAS instances sharing a SendGrid account. Valid values : Valid SendGrid username (alphanumeric, hyphens, underscores) Impact : Subuser will be created with this username in SendGrid. Related variables : mas_instance_id , sendgrid_subuser_email Notes : - Default includes MAS instance ID for uniqueness - Must be unique within the SendGrid account - See SendGrid subusers documentation sendgrid_defaultsendername \u00a4 Display name for emails sent by MAS. Optional Environment Variable: SENDGRID_DEFAULTSENDERNAME Default: Empty string Purpose : Provides a human-readable sender name that appears in email clients when MAS sends notifications. When to use : Set to provide a friendly sender name like \"MAS Production Alerts\" or \"Maximo Notifications\". Valid values : Any string (e.g., MAS Notifications , Maximo Production ) Impact : This name appears as the sender in email clients, making emails more recognizable to recipients. Related variables : None Notes : - Improves email recognition and trust - Recommended for production deployments - Can be left empty for default behavior sendgrid_defaultshouldemailpasswords \u00a4 Enable password delivery via email. Optional Environment Variable: SENDGRID_DEFAULTSHOULDEMAILPASSWORDS Default: false Purpose : Controls whether MAS should send user passwords via email when creating or resetting accounts. When to use : Set to true only if your security policy allows password transmission via email. Valid values : - true - Send passwords via email - false - Do not send passwords via email (recommended) Impact : When true , new user passwords and password resets will be emailed. When false , passwords must be communicated through other means. Related variables : None Notes : - Security : Sending passwords via email is generally not recommended - Default false follows security best practices - Consider alternative password delivery methods sendgrid_configscope \u00a4 Configuration scope for SMTP settings. Optional Environment Variable: SENDGRID_CONFIGSCOPE Default: system Purpose : Defines the scope level for SMTP configuration in MAS. When to use : Use default system for system-wide SMTP configuration. Other scopes may be supported in future versions. Valid values : system (currently only supported value) Impact : Determines which MAS components can use this SMTP configuration. Related variables : None Notes : - System scope makes SMTP available to all MAS components - Future versions may support workspace or application scopes sendgrid_hostname \u00a4 SendGrid SMTP server hostname. Optional Environment Variable: SENDGRID_HOSTNAME Default: smtp.sendgrid.net Purpose : Specifies the SendGrid SMTP server hostname for email transmission. When to use : Use default unless SendGrid changes their SMTP endpoint or you're using a custom configuration. Valid values : Valid SendGrid SMTP hostname Impact : MAS will connect to this hostname for sending emails via SMTP protocol. Related variables : sendgrid_port , sendgrid_security Notes : - Default is SendGrid's standard SMTP endpoint - Rarely needs to be changed - Verify with SendGrid documentation if unsure sendgrid_port \u00a4 SendGrid SMTP server port. Optional Environment Variable: SENDGRID_PORT Default: 465 Purpose : Specifies the port number for SMTP connections to SendGrid. When to use : Use default 465 for SSL/TLS connections. Change only if using different security configuration. Valid values : - 465 - SMTP with SSL/TLS (recommended, default) - 587 - SMTP with STARTTLS - 25 - Unencrypted SMTP (not recommended) Impact : Determines which port MAS uses to connect to SendGrid SMTP server. Related variables : sendgrid_hostname , sendgrid_security Notes : - Port 465 is recommended for secure connections - Must match the security setting - Port 25 is often blocked by ISPs sendgrid_security \u00a4 SMTP connection security protocol. Optional Environment Variable: SENDGRID_SECURITY Default: SSL Purpose : Specifies the security protocol for SMTP connections to SendGrid. When to use : Use default SSL for secure connections on port 465. Change to STARTTLS if using port 587. Valid values : - SSL - SSL/TLS encryption (recommended, default) - STARTTLS - Upgrade to TLS after initial connection - NONE - No encryption (not recommended) Impact : Determines how SMTP connections are encrypted. Related variables : sendgrid_port , sendgrid_hostname Notes : - SSL on port 465 is recommended - STARTTLS on port 587 is also secure - Never use NONE in production sendgrid_authentication \u00a4 Enable SMTP authentication. Optional Environment Variable: SENDGRID_AUTHENTICATION Default: true Purpose : Controls whether SMTP authentication is required for SendGrid connections. When to use : Always use default true . Authentication is required by SendGrid. Valid values : - true - Enable authentication (required) - false - Disable authentication (will fail) Impact : When true , MAS will authenticate using subuser credentials. When false , connection will fail. Related variables : sendgrid_subuser_username Notes : - Must be true for SendGrid - Authentication uses subuser credentials - Do not change from default sendgrid_api_url \u00a4 SendGrid API endpoint URL. Optional Environment Variable: SENDGRID_API_URL Default: api.sendgrid.com Purpose : Specifies the SendGrid API endpoint for REST API calls during subuser creation and management. When to use : Use default unless SendGrid changes their API endpoint or you're using a custom configuration. Valid values : Valid SendGrid API hostname Impact : Role will make REST API calls to this endpoint for subuser management. Related variables : sendgrid_primary_apikey Notes : - Default is SendGrid's standard API endpoint - Used during role execution, not by MAS - Rarely needs to be changed custom_labels \u00a4 Custom labels for MAS SMTP resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Allows adding custom key-value labels to SMTP-related Kubernetes resources for organization and automation. When to use : Set when you need to tag resources for cost tracking, environment identification, or automation purposes. Valid values : Comma-separated key=value pairs (e.g., environment=production,team=platform,cost-center=engineering ) Impact : Labels are applied to generated SmtpCfg and related resources. Related variables : None Notes : - Useful for resource organization and tracking - Can be used by automation tools for resource discovery - Follow Kubernetes label naming conventions sendgrid_debug \u00a4 Enable debug logging for SendGrid API calls. Optional Environment Variable: SENDGRID_DEBUG Default: False Purpose : Controls whether detailed SendGrid REST API call results are displayed in Ansible logs. When to use : Set to True when troubleshooting SendGrid integration issues or subuser creation problems. Valid values : - True - Display API call results in logs - False - Normal logging (default) Impact : When True , detailed API responses are logged, which may include sensitive information. Related variables : None Notes : - Use only for troubleshooting - May expose sensitive data in logs - Disable after troubleshooting is complete Example Playbook \u00a4 Generate a configuration \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me/masconfig smtp_type: sendgrid sendgrid_primary_username: myusername sendgrid_primary_email: myemail@mydomain sendgrid_subuser_username: mysubusername sendgrid_subuser_email: mysubuser@mydomain sendgrid_defaultrecipientemail: myemail@mydomain sendgrid_defaultsendername: 'My Name' sendgrid_primary_apikey: xxxx sendgrid_ips: '[\"XXX.XXX.XXX.XXX\"]' roles: - ibm.mas_devops.smtp License \u00a4 EPL-2.0","title":"smtp"},{"location":"roles/smtp/#smtp","text":"Generate an SMTP configuration that can be directly applied to IBM Maximo Application Suite. The role supports the Twilio SendGrid email provider. Twilio SendGrid Prior to running this role, you must create an account with SendGrid. The SendGrid account needs to support creating subusers. Tip The role will generate a yaml file containing the definition of a Secret and SmtpCfg resource that can be used to configure the smtp email provider for MAS. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/smtp-$MAS_INSTANCE_ID.yml\" or used in conjunction with the suite_config role. This role will create a subuser that must be validated. An email with a validation link will be sent to the primary email address. You need to validate the subuser using this link. If validation fails, you can resend the email using the SendGrid admin UI.","title":"smtp"},{"location":"roles/smtp/#role-variables","text":"","title":"Role Variables"},{"location":"roles/smtp/#smtp_type","text":"SMTP email provider type for MAS email notifications. Required Environment Variable: SMTP_TYPE Default: None Purpose : Specifies which SMTP email service provider to use for sending MAS notifications and alerts. When to use : Always required when configuring SMTP for MAS. Currently only SendGrid is supported. Valid values : sendgrid (only supported provider) Impact : Determines which SMTP provider configuration will be created and which API endpoints will be used. Related variables : sendgrid_primary_apikey , sendgrid_hostname Notes : - SendGrid is currently the only supported SMTP provider - Requires an active SendGrid account with subuser support - Future versions may support additional SMTP providers","title":"smtp_type"},{"location":"roles/smtp/#mas_instance_id","text":"MAS instance identifier for SMTP configuration. Required (if generating SmtpCfg) Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance the SMTP configuration will be applied to. When to use : Required when generating SmtpCfg resource. If not set along with mas_config_dir , the role will not generate configuration files. Valid values : Valid MAS instance ID (3-12 lowercase alphanumeric characters, e.g., prod , dev , masinst1 ) Impact : Used to name the generated SmtpCfg resource and target the correct MAS instance. Related variables : mas_config_dir , sendgrid_subuser_username Notes : - Must match the instance ID used during MAS installation - Used in default subuser username: ibm-mas_{mas_instance_id} - Both this and mas_config_dir must be set to generate configuration","title":"mas_instance_id"},{"location":"roles/smtp/#mas_config_dir","text":"Local directory for generated SMTP configuration files. Required (if generating SmtpCfg) Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies where to save the generated SmtpCfg resource YAML file containing SMTP configuration and credentials. When to use : Required when generating SmtpCfg resource. If not set along with mas_instance_id , the role will not generate configuration files. Valid values : Valid local filesystem path (e.g., ~/masconfig , /opt/mas/config ) Impact : SmtpCfg YAML file will be created in this directory as smtp-{mas_instance_id}.yml . Related variables : mas_instance_id Notes : - Can be applied manually: oc apply -f $MAS_CONFIG_DIR/smtp-$MAS_INSTANCE_ID.yml - Can be used with suite_config role for automated configuration - Both this and mas_instance_id must be set to generate configuration - Use consistent directory across all MAS DevOps roles","title":"mas_config_dir"},{"location":"roles/smtp/#mas_pod_templates_dir","text":"Directory containing pod template customizations for SMTP. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Provides custom pod template configurations for the SMTP service pods, allowing resource limits, node selectors, and other Kubernetes pod specifications. When to use : Set when you need to customize SMTP pod resources, scheduling, or other pod-level configurations. Valid values : Valid directory path containing ibm-mas-smtpcfg.yml file Impact : Pod template configuration is inserted into SmtpCfg spec under podTemplates element. Related variables : None Notes : - File must be named ibm-mas-smtpcfg.yml - See BestEfforts reference - Full documentation: Customizing Pod Templates - Optional - only needed for custom resource requirements","title":"mas_pod_templates_dir"},{"location":"roles/smtp/#sendgrid_primary_username","text":"Primary SendGrid account username. Required Environment Variable: SMTP_PRIMARY_USERNAME Default: None Purpose : Identifies the primary SendGrid account that will be used to create a subuser for MAS email operations. When to use : Always required. Must be the username of an existing SendGrid account with subuser creation permissions. Valid values : Valid SendGrid account username Impact : Used to authenticate to SendGrid API for subuser creation and management. Related variables : sendgrid_primary_apikey , sendgrid_primary_email Notes : - Must have permissions to create subusers in SendGrid - Primary account credentials are only used during setup, not stored in MAS - Subuser will be created under this primary account","title":"sendgrid_primary_username"},{"location":"roles/smtp/#sendgrid_primary_email","text":"Primary SendGrid account email address. Required Environment Variable: SMTP_PRIMARY_EMAIL Default: None Purpose : Provides the email address associated with the primary SendGrid account. Validation emails will be sent to this address. When to use : Always required. Must match the email registered with the SendGrid primary account. Valid values : Valid email address registered with SendGrid account Impact : Subuser validation emails will be sent to this address. You must validate the subuser using the link in the email. Related variables : sendgrid_primary_username , sendgrid_subuser_email Notes : - Validation email will be sent after subuser creation - Must validate subuser before it can send emails - Can resend validation email from SendGrid admin UI if needed","title":"sendgrid_primary_email"},{"location":"roles/smtp/#sendgrid_subuser_email","text":"Email address for the SendGrid subuser. Required Environment Variable: SMTP_SUBUSER_EMAIL Default: None Purpose : Specifies the email address for the SendGrid subuser that will be created for MAS email operations. When to use : Always required. This subuser will be dedicated to sending MAS notifications. Valid values : Valid email address (can be same as primary or different) Impact : This email will be associated with the subuser account and used for subuser-specific operations. Related variables : sendgrid_subuser_username , sendgrid_primary_email Notes : - Subuser provides isolation from primary account - See SendGrid subusers documentation - Can use same email as primary account if desired","title":"sendgrid_subuser_email"},{"location":"roles/smtp/#sendgrid_defaultrecipientemail","text":"Default recipient email address for MAS notifications. Required Environment Variable: SENDGRID_DEFAULTRECIPIENTEMAIL Default: None Purpose : Specifies the default destination email address for MAS notifications when no specific recipient is configured. When to use : Always required. This is the fallback email address for system notifications. Valid values : Valid email address Impact : MAS will send notifications to this address when no other recipient is specified in the notification configuration. Related variables : None Notes : - Acts as fallback for unconfigured notifications - Can be overridden per notification type in MAS - Typically set to admin or operations team email","title":"sendgrid_defaultrecipientemail"},{"location":"roles/smtp/#sendgrid_primary_apikey","text":"API key for the primary SendGrid account. Required Environment Variable: SENDGRID_PRIMARY_APIKEY Default: None Purpose : Provides authentication credentials for SendGrid API to create and configure the subuser. When to use : Always required. Must be an API key from the primary SendGrid account with subuser management permissions. Valid values : Valid SendGrid API key with appropriate permissions Impact : Used to authenticate SendGrid API calls for subuser creation and IP assignment. Related variables : sendgrid_primary_username Notes : - Store securely, never commit to version control - Requires permissions: subuser creation, IP management - Only used during setup, not stored in MAS configuration - Generate from SendGrid console: Settings > API Keys","title":"sendgrid_primary_apikey"},{"location":"roles/smtp/#sendgrid_ips","text":"SendGrid IP addresses to assign to the subuser. Required Environment Variable: SENDGRID_IPS Default: None Purpose : Specifies which SendGrid IP addresses should be assigned to the subuser for sending emails. When to use : Always required. Must be IP addresses already allocated to your primary SendGrid account. Valid values : JSON array of IP addresses (e.g., '[\"192.0.2.1\", \"192.0.2.2\"]' ) Impact : Subuser will send emails from these IP addresses. Affects email deliverability and reputation. Related variables : sendgrid_primary_apikey Notes : - IPs must already be allocated to primary account - Find IPs in SendGrid console: Settings > IP Addresses - Format as JSON array string - IP reputation affects email deliverability","title":"sendgrid_ips"},{"location":"roles/smtp/#sendgrid_subuser_username","text":"Username for the SendGrid subuser. Optional Environment Variable: SMTP_SUBUSER_USERNAME Default: ibm-mas_{mas_instance_id} Purpose : Specifies the username for the SendGrid subuser that will be created for MAS. When to use : Override default if you need a specific naming convention or have multiple MAS instances sharing a SendGrid account. Valid values : Valid SendGrid username (alphanumeric, hyphens, underscores) Impact : Subuser will be created with this username in SendGrid. Related variables : mas_instance_id , sendgrid_subuser_email Notes : - Default includes MAS instance ID for uniqueness - Must be unique within the SendGrid account - See SendGrid subusers documentation","title":"sendgrid_subuser_username"},{"location":"roles/smtp/#sendgrid_defaultsendername","text":"Display name for emails sent by MAS. Optional Environment Variable: SENDGRID_DEFAULTSENDERNAME Default: Empty string Purpose : Provides a human-readable sender name that appears in email clients when MAS sends notifications. When to use : Set to provide a friendly sender name like \"MAS Production Alerts\" or \"Maximo Notifications\". Valid values : Any string (e.g., MAS Notifications , Maximo Production ) Impact : This name appears as the sender in email clients, making emails more recognizable to recipients. Related variables : None Notes : - Improves email recognition and trust - Recommended for production deployments - Can be left empty for default behavior","title":"sendgrid_defaultsendername"},{"location":"roles/smtp/#sendgrid_defaultshouldemailpasswords","text":"Enable password delivery via email. Optional Environment Variable: SENDGRID_DEFAULTSHOULDEMAILPASSWORDS Default: false Purpose : Controls whether MAS should send user passwords via email when creating or resetting accounts. When to use : Set to true only if your security policy allows password transmission via email. Valid values : - true - Send passwords via email - false - Do not send passwords via email (recommended) Impact : When true , new user passwords and password resets will be emailed. When false , passwords must be communicated through other means. Related variables : None Notes : - Security : Sending passwords via email is generally not recommended - Default false follows security best practices - Consider alternative password delivery methods","title":"sendgrid_defaultshouldemailpasswords"},{"location":"roles/smtp/#sendgrid_configscope","text":"Configuration scope for SMTP settings. Optional Environment Variable: SENDGRID_CONFIGSCOPE Default: system Purpose : Defines the scope level for SMTP configuration in MAS. When to use : Use default system for system-wide SMTP configuration. Other scopes may be supported in future versions. Valid values : system (currently only supported value) Impact : Determines which MAS components can use this SMTP configuration. Related variables : None Notes : - System scope makes SMTP available to all MAS components - Future versions may support workspace or application scopes","title":"sendgrid_configscope"},{"location":"roles/smtp/#sendgrid_hostname","text":"SendGrid SMTP server hostname. Optional Environment Variable: SENDGRID_HOSTNAME Default: smtp.sendgrid.net Purpose : Specifies the SendGrid SMTP server hostname for email transmission. When to use : Use default unless SendGrid changes their SMTP endpoint or you're using a custom configuration. Valid values : Valid SendGrid SMTP hostname Impact : MAS will connect to this hostname for sending emails via SMTP protocol. Related variables : sendgrid_port , sendgrid_security Notes : - Default is SendGrid's standard SMTP endpoint - Rarely needs to be changed - Verify with SendGrid documentation if unsure","title":"sendgrid_hostname"},{"location":"roles/smtp/#sendgrid_port","text":"SendGrid SMTP server port. Optional Environment Variable: SENDGRID_PORT Default: 465 Purpose : Specifies the port number for SMTP connections to SendGrid. When to use : Use default 465 for SSL/TLS connections. Change only if using different security configuration. Valid values : - 465 - SMTP with SSL/TLS (recommended, default) - 587 - SMTP with STARTTLS - 25 - Unencrypted SMTP (not recommended) Impact : Determines which port MAS uses to connect to SendGrid SMTP server. Related variables : sendgrid_hostname , sendgrid_security Notes : - Port 465 is recommended for secure connections - Must match the security setting - Port 25 is often blocked by ISPs","title":"sendgrid_port"},{"location":"roles/smtp/#sendgrid_security","text":"SMTP connection security protocol. Optional Environment Variable: SENDGRID_SECURITY Default: SSL Purpose : Specifies the security protocol for SMTP connections to SendGrid. When to use : Use default SSL for secure connections on port 465. Change to STARTTLS if using port 587. Valid values : - SSL - SSL/TLS encryption (recommended, default) - STARTTLS - Upgrade to TLS after initial connection - NONE - No encryption (not recommended) Impact : Determines how SMTP connections are encrypted. Related variables : sendgrid_port , sendgrid_hostname Notes : - SSL on port 465 is recommended - STARTTLS on port 587 is also secure - Never use NONE in production","title":"sendgrid_security"},{"location":"roles/smtp/#sendgrid_authentication","text":"Enable SMTP authentication. Optional Environment Variable: SENDGRID_AUTHENTICATION Default: true Purpose : Controls whether SMTP authentication is required for SendGrid connections. When to use : Always use default true . Authentication is required by SendGrid. Valid values : - true - Enable authentication (required) - false - Disable authentication (will fail) Impact : When true , MAS will authenticate using subuser credentials. When false , connection will fail. Related variables : sendgrid_subuser_username Notes : - Must be true for SendGrid - Authentication uses subuser credentials - Do not change from default","title":"sendgrid_authentication"},{"location":"roles/smtp/#sendgrid_api_url","text":"SendGrid API endpoint URL. Optional Environment Variable: SENDGRID_API_URL Default: api.sendgrid.com Purpose : Specifies the SendGrid API endpoint for REST API calls during subuser creation and management. When to use : Use default unless SendGrid changes their API endpoint or you're using a custom configuration. Valid values : Valid SendGrid API hostname Impact : Role will make REST API calls to this endpoint for subuser management. Related variables : sendgrid_primary_apikey Notes : - Default is SendGrid's standard API endpoint - Used during role execution, not by MAS - Rarely needs to be changed","title":"sendgrid_api_url"},{"location":"roles/smtp/#custom_labels","text":"Custom labels for MAS SMTP resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Allows adding custom key-value labels to SMTP-related Kubernetes resources for organization and automation. When to use : Set when you need to tag resources for cost tracking, environment identification, or automation purposes. Valid values : Comma-separated key=value pairs (e.g., environment=production,team=platform,cost-center=engineering ) Impact : Labels are applied to generated SmtpCfg and related resources. Related variables : None Notes : - Useful for resource organization and tracking - Can be used by automation tools for resource discovery - Follow Kubernetes label naming conventions","title":"custom_labels"},{"location":"roles/smtp/#sendgrid_debug","text":"Enable debug logging for SendGrid API calls. Optional Environment Variable: SENDGRID_DEBUG Default: False Purpose : Controls whether detailed SendGrid REST API call results are displayed in Ansible logs. When to use : Set to True when troubleshooting SendGrid integration issues or subuser creation problems. Valid values : - True - Display API call results in logs - False - Normal logging (default) Impact : When True , detailed API responses are logged, which may include sensitive information. Related variables : None Notes : - Use only for troubleshooting - May expose sensitive data in logs - Disable after troubleshooting is complete","title":"sendgrid_debug"},{"location":"roles/smtp/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/smtp/#generate-a-configuration","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me/masconfig smtp_type: sendgrid sendgrid_primary_username: myusername sendgrid_primary_email: myemail@mydomain sendgrid_subuser_username: mysubusername sendgrid_subuser_email: mysubuser@mydomain sendgrid_defaultrecipientemail: myemail@mydomain sendgrid_defaultsendername: 'My Name' sendgrid_primary_apikey: xxxx sendgrid_ips: '[\"XXX.XXX.XXX.XXX\"]' roles: - ibm.mas_devops.smtp","title":"Generate a configuration"},{"location":"roles/smtp/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_backup_restore/","text":"suite_app_backup_restore \u00a4 Overview \u00a4 This role supports backing up and restoring the data for below MAS applications: manage : Manage namespace resources, persistent volume data (e.g. attachments) iot : IoT namespace resources monitor : Monitor namespace resources health : Health namespace resources, Watson Studio project asset optimizer : Optimizer namespace resources visualinspection : Visual Inspection namespace resources, persistent volume data (e.g. image datasets, models) Supports creating on-demand or scheduled backup jobs for taking full or incremental backups, and optionally creating Kubernetes jobs for running the backup/restore process. Important An application backup can only be restored to an instance with the same MAS instance ID. Role Variables \u00a4 General Variables \u00a4 masbr_action \u00a4 Action to perform on MAS application data. Required Environment Variable: MAS_BR_ACTION Default: None Purpose : Specifies whether to create a backup of MAS application data or restore from a previous backup. When to use : - Set to backup to create a backup of application data - Set to restore to restore application data from a backup - Always required to indicate the operation type Valid values : backup , restore Impact : - backup : Creates backup job (on-demand or scheduled) for application data - restore : Restores application data from specified backup version Related variables : - masbr_restore_from_version : Required when action is restore - masbr_backup_schedule : Optional for scheduled backups - mas_app_id : Application to backup/restore Note : IMPORTANT - This role handles application-specific data (namespace resources, PV data, Watson Studio assets). Database data (Db2, MongoDB) must be backed up/restored separately. An application backup can only be restored to an instance with the same MAS instance ID. mas_app_id \u00a4 MAS application identifier for backup/restore operations. Required Environment Variable: MAS_APP_ID Default: None Purpose : Identifies which MAS application to backup or restore. Different applications support different data types (namespace, PV, Watson Studio). When to use : - Always required for application backup/restore operations - Must match an installed application in the instance - Determines which data types are available for backup Valid values : manage , iot , monitor , health , optimizer , visualinspection Impact : Determines which application's data will be backed up or restored. Each application supports different data types: - manage : namespace, pv (attachments) - iot : namespace - monitor : namespace - health : namespace, wsl (Watson Studio) - optimizer : namespace - visualinspection : namespace, pv (datasets, models) Related variables : - masbr_backup_data / masbr_restore_data : Data types to backup/restore - mas_instance_id : Instance containing this application - mas_workspace_id : Workspace containing this application Note : Database data (Db2, MongoDB) is not included in application backups and must be backed up separately using dedicated roles. mas_instance_id \u00a4 MAS instance identifier for application backup/restore. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the application to backup or restore. Used to locate application resources and ensure restore compatibility. When to use : - Always required for application backup and restore operations - Must match the instance ID from MAS installation - Critical for restore operations (must match original backup instance ID) Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , main ) Impact : Determines which MAS instance's application will be backed up or restored. CRITICAL - An application backup can only be restored to an instance with the same MAS instance ID. Related variables : - mas_app_id : Application within this instance - mas_workspace_id : Workspace within this instance - masbr_restore_from_version : Backup version to restore (for restore action) Note : IMPORTANT - The instance ID must match between backup and restore operations. Attempting to restore a backup to an instance with a different ID will fail. mas_workspace_id \u00a4 Workspace identifier for application backup/restore. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the application to backup or restore. Used to locate application resources. When to use : - Always required for application backup and restore operations - Must match the workspace ID from application installation - Used to construct resource names and locate application data Valid values : Lowercase alphanumeric string (e.g., ws1 , prod , test ) Impact : Determines which workspace's application data will be backed up or restored. Incorrect workspace ID will cause operations to fail. Related variables : - mas_instance_id : Instance containing this workspace - mas_app_id : Application within this workspace Note : The workspace must contain the specified application. Application data is workspace-specific and cannot be restored to a different workspace. masbr_storage_local_folder \u00a4 Local filesystem path for backup storage. Required Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None Purpose : Specifies the local filesystem path where application backup files are stored (for backups) or retrieved from (for restores). This is the persistent storage location for backup data. When to use : - Always required for backup and restore operations - Must be accessible from the system running the role - Should have sufficient space for application data backups - Must be persistent across operations for restore capability Valid values : Absolute filesystem path (e.g., /tmp/masbr , /backup/mas-apps , /mnt/backup ) Impact : Backup files are written to or read from this location. Insufficient space will cause backup failures. Path must exist and be writable. Related variables : - masbr_copy_timeout_sec : Timeout for transferring files to/from this location - masbr_restore_from_version : Backup version stored in this location Note : Ensure the path has sufficient disk space for application backups (especially for Manage attachments and Visual Inspection datasets). For production, use a dedicated backup volume with appropriate retention policies. masbr_confirm_cluster \u00a4 Confirm cluster connection before backup/restore. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false Purpose : Controls whether the role prompts for confirmation of the currently connected cluster before executing backup or restore operations. Safety feature to prevent accidental operations on wrong cluster. When to use : - Set to true for interactive confirmation (recommended for production) - Leave as false (default) for automated/non-interactive operations - Use true when manually running backup/restore to verify correct cluster Valid values : true , false Impact : - true : Role prompts for cluster confirmation before proceeding - false : Role proceeds without confirmation (suitable for automation) Related variables : - masbr_action : Operation requiring cluster confirmation Note : Enabling cluster confirmation is recommended for manual operations, especially in production environments, to prevent accidental backup/restore on the wrong cluster. masbr_copy_timeout_sec \u00a4 File transfer timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) Purpose : Specifies the maximum time allowed for transferring application backup files between cluster and local storage. Prevents operations from hanging indefinitely. When to use : - Use default (12 hours) for most deployments - Increase for very large backups (e.g., Manage attachments, Visual Inspection datasets) - Decrease for smaller backups to fail faster on issues Valid values : Positive integer (seconds), e.g., 3600 (1 hour), 43200 (12 hours), 86400 (24 hours) Impact : Operations exceeding this timeout will fail. Insufficient timeout for large backups will cause failures. Excessive timeout delays error detection. Related variables : - masbr_storage_local_folder : Destination for file transfers Note : The default 12 hours is suitable for most deployments. Adjust based on backup size (especially for Manage attachments and Visual Inspection datasets) and network speed. masbr_job_timezone \u00a4 Time zone for scheduled backup jobs. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: UTC Purpose : Specifies the time zone for scheduled backup CronJobs. Ensures backups run at the intended local time rather than UTC. When to use : - Leave unset to use UTC (default) - Set when you need backups to run at specific local times - Only applies to scheduled backups (when masbr_backup_schedule is set) Valid values : Valid tz database time zone (e.g., America/New_York , Europe/London , Asia/Tokyo ) Impact : Determines when scheduled backups execute. Incorrect time zone may cause backups to run at unexpected times. Related variables : - masbr_backup_schedule : Cron expression interpreted in this time zone Note : Only relevant for scheduled backups. On-demand backups ignore this setting. Use standard tz database names (e.g., America/New_York , not EST ). Backup Variables \u00a4 masbr_backup_type \u00a4 Backup type: full or incremental. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full Purpose : Specifies whether to create a full backup or incremental backup. Incremental backups only capture changes since the last full backup, reducing backup time and storage. When to use : - Use full (default) for complete backups - Use incr for incremental backups of persistent volume data - Incremental backups require a previous full backup Valid values : full , incr Impact : - full : Creates complete backup of all data - incr : Creates incremental backup of PV data only (namespace data is always full) Related variables : - masbr_backup_from_version : Full backup version for incremental backup - masbr_backup_data : Data types to backup Note : IMPORTANT - Incremental backups only apply to persistent volume (PV) data. Namespace and Watson Studio data are always backed up in full regardless of this setting. Incremental backups require a previous full backup as a baseline. masbr_backup_data \u00a4 Data types to include in backup. Optional Environment Variable: MASBR_BACKUP_DATA Default: All supported data types for the application Purpose : Specifies which types of data to backup. Allows selective backup of namespace resources, persistent volumes, or Watson Studio assets. When to use : - Leave unset to backup all supported data types (recommended) - Set to backup specific data types only - Use comma-separated list for multiple types (e.g., namespace,pv ) Valid values : Comma-separated list of: namespace , pv , wsl - namespace : Kubernetes namespace resources - pv : Persistent volume data (attachments, datasets, models) - wsl : Watson Studio project assets (Health only) Impact : Only specified data types are backed up. Unspecified types are excluded from backup. Related variables : - mas_app_id : Determines which data types are supported - masbr_backup_type : Full or incremental (applies to PV data only) Note : Supported data types vary by application: - Manage: namespace , pv - IoT/Monitor/Optimizer: namespace only - Health: namespace , wsl - Visual Inspection: namespace , pv The data types supported by each MAS applications: MAS App Name MAS App ID Data types Manage manage namespace , pv IoT iot namespace Monitor monitor namespace Health health namespace , wsl Optimizer optimizer namespace Visual Inspection visualinspection namespace , pv masbr_backup_from_version \u00a4 Base full backup version for incremental backups. Optional (when masbr_backup_type=incr ) Environment Variable: MASBR_BACKUP_FROM_VERSION Default: Latest full backup (auto-detected) Purpose : Specifies which full backup to use as the baseline for an incremental backup. Incremental backups capture only changes since this version. When to use : - Only applies when masbr_backup_type=incr - Leave unset to automatically use the latest full backup (recommended) - Set explicitly to use a specific full backup as baseline Valid values : Timestamp in YYYYMMDDHHMMSS format (e.g., 20240621021316 for June 21, 2024 at 02:13:16) Impact : Determines which full backup is used as the baseline. Incremental backup captures changes since this version. If not set, automatically uses the latest full backup. Related variables : - masbr_backup_type : Must be incr for this variable to be used - masbr_storage_local_folder : Location where full backup versions are stored Note : Only valid for incremental backups. The specified version must be a full backup (not incremental). Auto-detection finds the latest full backup in storage. masbr_backup_schedule \u00a4 Cron expression for scheduled backups. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None (on-demand backup) Purpose : Defines a schedule for automatic recurring backups using Cron syntax. When set, creates a Kubernetes CronJob for automated backups. When to use : - Leave unset for on-demand backups (manual execution) - Set to create scheduled/recurring backups - Use for automated backup strategies Valid values : Valid Cron expression (e.g., 0 2 * * * for daily at 2 AM, 0 2 * * 0 for weekly on Sunday at 2 AM) Impact : - When set: Creates a Kubernetes CronJob that runs backups automatically on schedule - When unset: Creates an on-demand backup job that runs immediately Related variables : - masbr_job_timezone : Time zone for interpreting the cron schedule - masbr_action : Must be backup for scheduled backups Note : Scheduled backups only apply when masbr_action=backup . The cron expression is interpreted in the time zone specified by masbr_job_timezone (defaults to UTC). Common patterns: 0 2 * * * (daily 2 AM), 0 2 * * 0 (weekly Sunday 2 AM), 0 2 1 * * (monthly 1st at 2 AM). Restore Variables \u00a4 masbr_restore_from_version \u00a4 Backup version timestamp for restore operations. Required (when masbr_action=restore ) Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Purpose : Specifies which backup version to restore from. The version is a timestamp identifying a specific backup. When to use : - Required when masbr_action=restore - Not used for backup operations - Must match an existing backup version in storage Valid values : Timestamp in YYYYMMDDHHMMSS format (e.g., 20240621021316 for June 21, 2024 at 02:13:16) Impact : Determines which backup is restored. Incorrect or non-existent version will cause restore to fail. Related variables : - masbr_action : Must be restore for this variable to be used - masbr_storage_local_folder : Location where backup versions are stored - mas_instance_id : Must match the instance ID from the backup Note : The backup version timestamp is generated automatically during backup creation. List available backups in masbr_storage_local_folder to find valid version timestamps. IMPORTANT - The backup can only be restored to an instance with the same MAS instance ID as the original backup. masbr_restore_data \u00a4 Data types to include in restore. Optional Environment Variable: MASBR_RESTORE_DATA Default: All supported data types for the application Purpose : Specifies which types of data to restore. Allows selective restore of namespace resources, persistent volumes, or Watson Studio assets. When to use : - Leave unset to restore all supported data types (recommended) - Set to restore specific data types only - Use comma-separated list for multiple types (e.g., namespace,pv ) Valid values : Comma-separated list of: namespace , pv , wsl - namespace : Kubernetes namespace resources - pv : Persistent volume data (attachments, datasets, models) - wsl : Watson Studio project assets (Health only) Impact : Only specified data types are restored. Unspecified types remain unchanged. Related variables : - mas_app_id : Determines which data types are supported - masbr_restore_from_version : Backup version containing the data Note : Supported data types vary by application: - Manage: namespace , pv - IoT/Monitor/Optimizer: namespace only - Health: namespace , wsl - Visual Inspection: namespace , pv The data types supported by each MAS applications: MAS App Name MAS App ID Data types Manage manage namespace , pv IoT iot namespace Monitor monitor namespace Health health namespace , wsl Optimizer optimizer namespace Visual Inspection visualinspection namespace , pv Manage Variables \u00a4 masbr_manage_pvc_paths \u00a4 Manage PVC paths for backup/restore (Manage only). Optional Environment Variable: MASBR_MANAGE_PVC_PATHS Default: None Purpose : Specifies which Manage persistent volumes to backup/restore. Defines PVC names, mount paths, and optional subpaths for Manage attachments and custom files. When to use : - Only applies to Manage application ( mas_app_id=manage ) - Required when backing up/restoring Manage PV data - Leave unset to skip Manage PV backup/restore - Set to backup specific Manage PVCs (e.g., attachments, custom files) Valid values : Comma-separated list in format <pvcName>:<mountPath>/<subPath> - Example: manage-doclinks1-pvc:/mnt/doclinks1/attachments - Multiple: manage-doclinks1-pvc:/mnt/doclinks1,manage-doclinks2-pvc:/mnt/doclinks2 Impact : Only specified PVCs are backed up/restored. Unspecified PVCs are excluded. Related variables : - mas_app_id : Must be manage for this variable to apply - masbr_backup_data / masbr_restore_data : Must include pv data type Note : PVC names and mount paths are defined in the ManageWorkspace CR spec.settings.deployment.persistentVolumes . Subpath is optional. If not set, no Manage PV data is backed up/restored. The <pvcName> and <mountPath> are defined in the ManageWorkspace CRD instance spec.settings.deployment.persistentVolumes : persistentVolumes: - accessModes: - ReadWriteMany mountPath: /mnt/doclinks1 pvcName: manage-doclinks1-pvc size: '20' storageClassName: ocs-storagecluster-cephfs volumeName: '' - accessModes: - ReadWriteMany mountPath: /mnt/doclinks2 pvcName: manage-doclinks2-pvc size: '20' storageClassName: ocs-storagecluster-cephfs volumeName: '' If not set a value for this variable, this role will not backup and restore persistent volume data for Manage. Example Playbook \u00a4 Backup \u00a4 Backup Manage attachments, note that this does not include backup of any data in Db2, see the backup action in the db2 role. - hosts: localhost any_errors_fatal: true vars: masbr_action: backup mas_instance_id: main mas_workspace_id: ws1 mas_app_id: manage masbr_backup_data: pv masbr_manage_pvc_paths: \"manage-doclinks1-pvc:/mnt/doclinks1\" masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_app_backup_restore Restore \u00a4 Restore Manage attachments, note that this does not include restore of any data in Db2, see the restore action in the db2 role. - hosts: localhost any_errors_fatal: true vars: masbr_action: restore masbr_restore_from_version: 20240621021316 mas_instance_id: main mas_workspace_id: ws1 mas_app_id: manage masbr_backup_data: pv masbr_manage_pvc_paths: \"manage-doclinks1-pvc:/mnt/doclinks1\" masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_app_backup_restore License \u00a4 EPL-2.0","title":"suite_app_backup_restore"},{"location":"roles/suite_app_backup_restore/#suite_app_backup_restore","text":"","title":"suite_app_backup_restore"},{"location":"roles/suite_app_backup_restore/#overview","text":"This role supports backing up and restoring the data for below MAS applications: manage : Manage namespace resources, persistent volume data (e.g. attachments) iot : IoT namespace resources monitor : Monitor namespace resources health : Health namespace resources, Watson Studio project asset optimizer : Optimizer namespace resources visualinspection : Visual Inspection namespace resources, persistent volume data (e.g. image datasets, models) Supports creating on-demand or scheduled backup jobs for taking full or incremental backups, and optionally creating Kubernetes jobs for running the backup/restore process. Important An application backup can only be restored to an instance with the same MAS instance ID.","title":"Overview"},{"location":"roles/suite_app_backup_restore/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_backup_restore/#general-variables","text":"","title":"General Variables"},{"location":"roles/suite_app_backup_restore/#masbr_action","text":"Action to perform on MAS application data. Required Environment Variable: MAS_BR_ACTION Default: None Purpose : Specifies whether to create a backup of MAS application data or restore from a previous backup. When to use : - Set to backup to create a backup of application data - Set to restore to restore application data from a backup - Always required to indicate the operation type Valid values : backup , restore Impact : - backup : Creates backup job (on-demand or scheduled) for application data - restore : Restores application data from specified backup version Related variables : - masbr_restore_from_version : Required when action is restore - masbr_backup_schedule : Optional for scheduled backups - mas_app_id : Application to backup/restore Note : IMPORTANT - This role handles application-specific data (namespace resources, PV data, Watson Studio assets). Database data (Db2, MongoDB) must be backed up/restored separately. An application backup can only be restored to an instance with the same MAS instance ID.","title":"masbr_action"},{"location":"roles/suite_app_backup_restore/#mas_app_id","text":"MAS application identifier for backup/restore operations. Required Environment Variable: MAS_APP_ID Default: None Purpose : Identifies which MAS application to backup or restore. Different applications support different data types (namespace, PV, Watson Studio). When to use : - Always required for application backup/restore operations - Must match an installed application in the instance - Determines which data types are available for backup Valid values : manage , iot , monitor , health , optimizer , visualinspection Impact : Determines which application's data will be backed up or restored. Each application supports different data types: - manage : namespace, pv (attachments) - iot : namespace - monitor : namespace - health : namespace, wsl (Watson Studio) - optimizer : namespace - visualinspection : namespace, pv (datasets, models) Related variables : - masbr_backup_data / masbr_restore_data : Data types to backup/restore - mas_instance_id : Instance containing this application - mas_workspace_id : Workspace containing this application Note : Database data (Db2, MongoDB) is not included in application backups and must be backed up separately using dedicated roles.","title":"mas_app_id"},{"location":"roles/suite_app_backup_restore/#mas_instance_id","text":"MAS instance identifier for application backup/restore. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the application to backup or restore. Used to locate application resources and ensure restore compatibility. When to use : - Always required for application backup and restore operations - Must match the instance ID from MAS installation - Critical for restore operations (must match original backup instance ID) Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , main ) Impact : Determines which MAS instance's application will be backed up or restored. CRITICAL - An application backup can only be restored to an instance with the same MAS instance ID. Related variables : - mas_app_id : Application within this instance - mas_workspace_id : Workspace within this instance - masbr_restore_from_version : Backup version to restore (for restore action) Note : IMPORTANT - The instance ID must match between backup and restore operations. Attempting to restore a backup to an instance with a different ID will fail.","title":"mas_instance_id"},{"location":"roles/suite_app_backup_restore/#mas_workspace_id","text":"Workspace identifier for application backup/restore. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the application to backup or restore. Used to locate application resources. When to use : - Always required for application backup and restore operations - Must match the workspace ID from application installation - Used to construct resource names and locate application data Valid values : Lowercase alphanumeric string (e.g., ws1 , prod , test ) Impact : Determines which workspace's application data will be backed up or restored. Incorrect workspace ID will cause operations to fail. Related variables : - mas_instance_id : Instance containing this workspace - mas_app_id : Application within this workspace Note : The workspace must contain the specified application. Application data is workspace-specific and cannot be restored to a different workspace.","title":"mas_workspace_id"},{"location":"roles/suite_app_backup_restore/#masbr_storage_local_folder","text":"Local filesystem path for backup storage. Required Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None Purpose : Specifies the local filesystem path where application backup files are stored (for backups) or retrieved from (for restores). This is the persistent storage location for backup data. When to use : - Always required for backup and restore operations - Must be accessible from the system running the role - Should have sufficient space for application data backups - Must be persistent across operations for restore capability Valid values : Absolute filesystem path (e.g., /tmp/masbr , /backup/mas-apps , /mnt/backup ) Impact : Backup files are written to or read from this location. Insufficient space will cause backup failures. Path must exist and be writable. Related variables : - masbr_copy_timeout_sec : Timeout for transferring files to/from this location - masbr_restore_from_version : Backup version stored in this location Note : Ensure the path has sufficient disk space for application backups (especially for Manage attachments and Visual Inspection datasets). For production, use a dedicated backup volume with appropriate retention policies.","title":"masbr_storage_local_folder"},{"location":"roles/suite_app_backup_restore/#masbr_confirm_cluster","text":"Confirm cluster connection before backup/restore. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false Purpose : Controls whether the role prompts for confirmation of the currently connected cluster before executing backup or restore operations. Safety feature to prevent accidental operations on wrong cluster. When to use : - Set to true for interactive confirmation (recommended for production) - Leave as false (default) for automated/non-interactive operations - Use true when manually running backup/restore to verify correct cluster Valid values : true , false Impact : - true : Role prompts for cluster confirmation before proceeding - false : Role proceeds without confirmation (suitable for automation) Related variables : - masbr_action : Operation requiring cluster confirmation Note : Enabling cluster confirmation is recommended for manual operations, especially in production environments, to prevent accidental backup/restore on the wrong cluster.","title":"masbr_confirm_cluster"},{"location":"roles/suite_app_backup_restore/#masbr_copy_timeout_sec","text":"File transfer timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) Purpose : Specifies the maximum time allowed for transferring application backup files between cluster and local storage. Prevents operations from hanging indefinitely. When to use : - Use default (12 hours) for most deployments - Increase for very large backups (e.g., Manage attachments, Visual Inspection datasets) - Decrease for smaller backups to fail faster on issues Valid values : Positive integer (seconds), e.g., 3600 (1 hour), 43200 (12 hours), 86400 (24 hours) Impact : Operations exceeding this timeout will fail. Insufficient timeout for large backups will cause failures. Excessive timeout delays error detection. Related variables : - masbr_storage_local_folder : Destination for file transfers Note : The default 12 hours is suitable for most deployments. Adjust based on backup size (especially for Manage attachments and Visual Inspection datasets) and network speed.","title":"masbr_copy_timeout_sec"},{"location":"roles/suite_app_backup_restore/#masbr_job_timezone","text":"Time zone for scheduled backup jobs. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: UTC Purpose : Specifies the time zone for scheduled backup CronJobs. Ensures backups run at the intended local time rather than UTC. When to use : - Leave unset to use UTC (default) - Set when you need backups to run at specific local times - Only applies to scheduled backups (when masbr_backup_schedule is set) Valid values : Valid tz database time zone (e.g., America/New_York , Europe/London , Asia/Tokyo ) Impact : Determines when scheduled backups execute. Incorrect time zone may cause backups to run at unexpected times. Related variables : - masbr_backup_schedule : Cron expression interpreted in this time zone Note : Only relevant for scheduled backups. On-demand backups ignore this setting. Use standard tz database names (e.g., America/New_York , not EST ).","title":"masbr_job_timezone"},{"location":"roles/suite_app_backup_restore/#backup-variables","text":"","title":"Backup Variables"},{"location":"roles/suite_app_backup_restore/#masbr_backup_type","text":"Backup type: full or incremental. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full Purpose : Specifies whether to create a full backup or incremental backup. Incremental backups only capture changes since the last full backup, reducing backup time and storage. When to use : - Use full (default) for complete backups - Use incr for incremental backups of persistent volume data - Incremental backups require a previous full backup Valid values : full , incr Impact : - full : Creates complete backup of all data - incr : Creates incremental backup of PV data only (namespace data is always full) Related variables : - masbr_backup_from_version : Full backup version for incremental backup - masbr_backup_data : Data types to backup Note : IMPORTANT - Incremental backups only apply to persistent volume (PV) data. Namespace and Watson Studio data are always backed up in full regardless of this setting. Incremental backups require a previous full backup as a baseline.","title":"masbr_backup_type"},{"location":"roles/suite_app_backup_restore/#masbr_backup_data","text":"Data types to include in backup. Optional Environment Variable: MASBR_BACKUP_DATA Default: All supported data types for the application Purpose : Specifies which types of data to backup. Allows selective backup of namespace resources, persistent volumes, or Watson Studio assets. When to use : - Leave unset to backup all supported data types (recommended) - Set to backup specific data types only - Use comma-separated list for multiple types (e.g., namespace,pv ) Valid values : Comma-separated list of: namespace , pv , wsl - namespace : Kubernetes namespace resources - pv : Persistent volume data (attachments, datasets, models) - wsl : Watson Studio project assets (Health only) Impact : Only specified data types are backed up. Unspecified types are excluded from backup. Related variables : - mas_app_id : Determines which data types are supported - masbr_backup_type : Full or incremental (applies to PV data only) Note : Supported data types vary by application: - Manage: namespace , pv - IoT/Monitor/Optimizer: namespace only - Health: namespace , wsl - Visual Inspection: namespace , pv The data types supported by each MAS applications: MAS App Name MAS App ID Data types Manage manage namespace , pv IoT iot namespace Monitor monitor namespace Health health namespace , wsl Optimizer optimizer namespace Visual Inspection visualinspection namespace , pv","title":"masbr_backup_data"},{"location":"roles/suite_app_backup_restore/#masbr_backup_from_version","text":"Base full backup version for incremental backups. Optional (when masbr_backup_type=incr ) Environment Variable: MASBR_BACKUP_FROM_VERSION Default: Latest full backup (auto-detected) Purpose : Specifies which full backup to use as the baseline for an incremental backup. Incremental backups capture only changes since this version. When to use : - Only applies when masbr_backup_type=incr - Leave unset to automatically use the latest full backup (recommended) - Set explicitly to use a specific full backup as baseline Valid values : Timestamp in YYYYMMDDHHMMSS format (e.g., 20240621021316 for June 21, 2024 at 02:13:16) Impact : Determines which full backup is used as the baseline. Incremental backup captures changes since this version. If not set, automatically uses the latest full backup. Related variables : - masbr_backup_type : Must be incr for this variable to be used - masbr_storage_local_folder : Location where full backup versions are stored Note : Only valid for incremental backups. The specified version must be a full backup (not incremental). Auto-detection finds the latest full backup in storage.","title":"masbr_backup_from_version"},{"location":"roles/suite_app_backup_restore/#masbr_backup_schedule","text":"Cron expression for scheduled backups. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None (on-demand backup) Purpose : Defines a schedule for automatic recurring backups using Cron syntax. When set, creates a Kubernetes CronJob for automated backups. When to use : - Leave unset for on-demand backups (manual execution) - Set to create scheduled/recurring backups - Use for automated backup strategies Valid values : Valid Cron expression (e.g., 0 2 * * * for daily at 2 AM, 0 2 * * 0 for weekly on Sunday at 2 AM) Impact : - When set: Creates a Kubernetes CronJob that runs backups automatically on schedule - When unset: Creates an on-demand backup job that runs immediately Related variables : - masbr_job_timezone : Time zone for interpreting the cron schedule - masbr_action : Must be backup for scheduled backups Note : Scheduled backups only apply when masbr_action=backup . The cron expression is interpreted in the time zone specified by masbr_job_timezone (defaults to UTC). Common patterns: 0 2 * * * (daily 2 AM), 0 2 * * 0 (weekly Sunday 2 AM), 0 2 1 * * (monthly 1st at 2 AM).","title":"masbr_backup_schedule"},{"location":"roles/suite_app_backup_restore/#restore-variables","text":"","title":"Restore Variables"},{"location":"roles/suite_app_backup_restore/#masbr_restore_from_version","text":"Backup version timestamp for restore operations. Required (when masbr_action=restore ) Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Purpose : Specifies which backup version to restore from. The version is a timestamp identifying a specific backup. When to use : - Required when masbr_action=restore - Not used for backup operations - Must match an existing backup version in storage Valid values : Timestamp in YYYYMMDDHHMMSS format (e.g., 20240621021316 for June 21, 2024 at 02:13:16) Impact : Determines which backup is restored. Incorrect or non-existent version will cause restore to fail. Related variables : - masbr_action : Must be restore for this variable to be used - masbr_storage_local_folder : Location where backup versions are stored - mas_instance_id : Must match the instance ID from the backup Note : The backup version timestamp is generated automatically during backup creation. List available backups in masbr_storage_local_folder to find valid version timestamps. IMPORTANT - The backup can only be restored to an instance with the same MAS instance ID as the original backup.","title":"masbr_restore_from_version"},{"location":"roles/suite_app_backup_restore/#masbr_restore_data","text":"Data types to include in restore. Optional Environment Variable: MASBR_RESTORE_DATA Default: All supported data types for the application Purpose : Specifies which types of data to restore. Allows selective restore of namespace resources, persistent volumes, or Watson Studio assets. When to use : - Leave unset to restore all supported data types (recommended) - Set to restore specific data types only - Use comma-separated list for multiple types (e.g., namespace,pv ) Valid values : Comma-separated list of: namespace , pv , wsl - namespace : Kubernetes namespace resources - pv : Persistent volume data (attachments, datasets, models) - wsl : Watson Studio project assets (Health only) Impact : Only specified data types are restored. Unspecified types remain unchanged. Related variables : - mas_app_id : Determines which data types are supported - masbr_restore_from_version : Backup version containing the data Note : Supported data types vary by application: - Manage: namespace , pv - IoT/Monitor/Optimizer: namespace only - Health: namespace , wsl - Visual Inspection: namespace , pv The data types supported by each MAS applications: MAS App Name MAS App ID Data types Manage manage namespace , pv IoT iot namespace Monitor monitor namespace Health health namespace , wsl Optimizer optimizer namespace Visual Inspection visualinspection namespace , pv","title":"masbr_restore_data"},{"location":"roles/suite_app_backup_restore/#manage-variables","text":"","title":"Manage Variables"},{"location":"roles/suite_app_backup_restore/#masbr_manage_pvc_paths","text":"Manage PVC paths for backup/restore (Manage only). Optional Environment Variable: MASBR_MANAGE_PVC_PATHS Default: None Purpose : Specifies which Manage persistent volumes to backup/restore. Defines PVC names, mount paths, and optional subpaths for Manage attachments and custom files. When to use : - Only applies to Manage application ( mas_app_id=manage ) - Required when backing up/restoring Manage PV data - Leave unset to skip Manage PV backup/restore - Set to backup specific Manage PVCs (e.g., attachments, custom files) Valid values : Comma-separated list in format <pvcName>:<mountPath>/<subPath> - Example: manage-doclinks1-pvc:/mnt/doclinks1/attachments - Multiple: manage-doclinks1-pvc:/mnt/doclinks1,manage-doclinks2-pvc:/mnt/doclinks2 Impact : Only specified PVCs are backed up/restored. Unspecified PVCs are excluded. Related variables : - mas_app_id : Must be manage for this variable to apply - masbr_backup_data / masbr_restore_data : Must include pv data type Note : PVC names and mount paths are defined in the ManageWorkspace CR spec.settings.deployment.persistentVolumes . Subpath is optional. If not set, no Manage PV data is backed up/restored. The <pvcName> and <mountPath> are defined in the ManageWorkspace CRD instance spec.settings.deployment.persistentVolumes : persistentVolumes: - accessModes: - ReadWriteMany mountPath: /mnt/doclinks1 pvcName: manage-doclinks1-pvc size: '20' storageClassName: ocs-storagecluster-cephfs volumeName: '' - accessModes: - ReadWriteMany mountPath: /mnt/doclinks2 pvcName: manage-doclinks2-pvc size: '20' storageClassName: ocs-storagecluster-cephfs volumeName: '' If not set a value for this variable, this role will not backup and restore persistent volume data for Manage.","title":"masbr_manage_pvc_paths"},{"location":"roles/suite_app_backup_restore/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_app_backup_restore/#backup","text":"Backup Manage attachments, note that this does not include backup of any data in Db2, see the backup action in the db2 role. - hosts: localhost any_errors_fatal: true vars: masbr_action: backup mas_instance_id: main mas_workspace_id: ws1 mas_app_id: manage masbr_backup_data: pv masbr_manage_pvc_paths: \"manage-doclinks1-pvc:/mnt/doclinks1\" masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_app_backup_restore","title":"Backup"},{"location":"roles/suite_app_backup_restore/#restore","text":"Restore Manage attachments, note that this does not include restore of any data in Db2, see the restore action in the db2 role. - hosts: localhost any_errors_fatal: true vars: masbr_action: restore masbr_restore_from_version: 20240621021316 mas_instance_id: main mas_workspace_id: ws1 mas_app_id: manage masbr_backup_data: pv masbr_manage_pvc_paths: \"manage-doclinks1-pvc:/mnt/doclinks1\" masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_app_backup_restore","title":"Restore"},{"location":"roles/suite_app_backup_restore/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_config/","text":"suite_app_config \u00a4 This role is used to configure specific components of the application workspace after the application has been installed in the Maximo Application Suite. Role Variables \u00a4 General Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier for the target installation. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to configure. This must match the instance ID used during MAS Core installation. When to use : - Always required when configuring application workspaces - Must match the instance ID from suite_install role - Used to locate the correct Suite and Workspace resources Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance's application workspace will be configured. Incorrect instance ID will cause configuration to fail. Related variables : - mas_app_id : Specifies which application to configure - mas_workspace_id : Specifies which workspace to configure Note : This must be an existing MAS instance. The role does not create new instances. mas_app_id \u00a4 MAS application to configure. Required Environment Variable: MAS_APP_ID Default: None Purpose : Specifies which MAS application workspace to configure. Different applications have different configuration options and requirements. When to use : - Always required when configuring application workspaces - Must match an installed application in the MAS instance - Determines which application-specific configuration is applied Valid values : assist , iot , facilities , manage , monitor , optimizer , predict , visualinspection Impact : Determines which application workspace is configured and which configuration options are available. Each application has unique configuration requirements. Related variables : - mas_instance_id : The MAS instance containing this application - mas_workspace_id : The workspace to configure - mas_appws_components : Application-specific components to configure Note : The application must already be installed via suite_app_install role before configuration. Different applications support different configuration variables. mas_workspace_id \u00a4 Workspace identifier for the application workspace to configure. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the application to configure. Workspaces allow multiple isolated environments within a single application. When to use : - Always required when configuring application workspaces - Must match an existing workspace created during application installation - Typically matches the workspace ID used in suite_app_install Valid values : Lowercase alphanumeric string (e.g., masdev , prod , test ) Impact : Determines which workspace is configured. Each workspace has its own configuration, data, and users. Related variables : - mas_instance_id : The MAS instance containing this workspace - mas_app_id : The application containing this workspace - mas_appws_components : Components to configure in this workspace Note : The workspace must already exist (created during suite_app_install). This role configures existing workspaces, it does not create new ones. aiservice_instance_id \u00a4 AI Service instance ID for Manage integration (Manage only). Optional Environment Variable: AISERVICE_INSTANCE_ID Default: None Purpose : Enables automatic AI Service integration with Manage application. When set, the role retrieves credentials, configures connection, and imports certificates. When to use : - Only when configuring Manage application ( mas_app_id=manage ) - When you want to integrate AI Service capabilities into Manage - Requires AI Service to be installed and running in the cluster - Must be used together with aiservice_tenant_id Valid values : Valid AI Service instance ID (e.g., aiservice1 , ai-prod ) Impact : When configured, automatically: - Retrieves API key from tenant-specific secret - Extracts AI Service URL from aibroker route - Imports AI Service TLS certificate into Manage - Configures AI Service connection properties - Verifies AI Service health before proceeding Related variables : - aiservice_tenant_id : Required tenant ID for the integration - mas_app_id : Must be manage for this to apply Note : AI Service must be installed and healthy before configuration. The role performs automatic health checks and credential retrieval from cluster resources. aiservice_tenant_id \u00a4 AI Service tenant ID for the integration (Manage only). Optional Environment Variable: AISERVICE_TENANT_ID Default: None Purpose : Specifies which AI Service tenant to use for Manage integration. Combined with instance ID to form the fully qualified tenant identifier. When to use : - Required when aiservice_instance_id is set - Only applies to Manage application - Must match an existing tenant in the AI Service instance Valid values : Valid AI Service tenant ID string (e.g., tenant1 , prod-tenant ) Impact : Combined with aiservice_instance_id to locate tenant-specific resources (API key secret, configuration). Incorrect tenant ID will cause integration to fail. Related variables : - aiservice_instance_id : Required instance ID for the integration - Forms secret name: aiservice-{instance_id}-{tenant_id}----apikey-secret Note : The tenant must already exist in the AI Service instance. The role retrieves credentials from the tenant-specific secret in the cluster. custom_labels \u00a4 Comma-separated list of key=value labels to apply to workspace resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to application workspace resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,app=manage ) Impact : Labels are applied to workspace-specific resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect workspace functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management. Note : Labels help with resource organization and are especially useful in multi-tenant or multi-workspace environments. Workspace Configuration Variables \u00a4 mas_appws_spec \u00a4 Custom workspace deployment specification (overrides component-based configuration). Optional Environment Variable: MAS_APPWS_SPEC Default: Application-specific defaults in vars/defaultspecs/{mas_app_id}.yml Purpose : Provides complete control over workspace deployment specification. Allows advanced customization beyond what component-based configuration offers. When to use : - Use for advanced workspace customization - Use when you need full control over the workspace spec - Use when component-based configuration is insufficient - Leave unset for standard deployments using mas_appws_components Valid values : Valid workspace specification YAML/JSON matching the application's workspace CR schema Impact : WARNING - Overrides all settings from mas_appws_components . Provides complete control but requires deep knowledge of workspace CR structure. Incorrect specs can cause deployment failures. Related variables : - mas_appws_components : Simpler component-based configuration (overridden by this) - Application-specific default specs in vars/defaultspecs/ Note : Use mas_appws_components for standard deployments. Only use this variable when you need advanced customization or have specific requirements not covered by component-based configuration. mas_appws_bindings_jdbc \u00a4 JDBC binding scope for the workspace. Optional Environment Variable: MAS_APPWS_BINDINGS_JDBC Default: system Purpose : Controls the scope of JDBC database binding for the workspace. Different scopes provide different levels of isolation and sharing. When to use : - Use system (default) for shared system-level database configuration - Use application for application-level database configuration - Use workspace for workspace-specific database configuration - Use workspace-application for Maximo Real Estate and Facilities (recommended) Valid values : system , application , workspace , workspace-application Impact : - system : Uses system-level JDBC configuration (shared across all workspaces) - application : Uses application-level JDBC configuration - workspace : Uses workspace-specific JDBC configuration - workspace-application : Combines workspace and application scopes Related variables : JDBC configuration must exist at the specified scope level. Note : IMPORTANT - For Maximo Real Estate and Facilities applications, use workspace-application scope. The default system scope is suitable for most other applications. mas_appws_components \u00a4 Application components and versions to configure in the workspace. Optional Environment Variable: MAS_APPWS_COMPONENTS Default: Application-specific defaults Purpose : Specifies which application components to configure and their versions. Different applications have different available components. When to use : - Use to enable specific application components - Use to control component versions - Leave unset to use application-specific defaults - Overridden by mas_appws_spec if that is set Valid values : Comma-separated component=version pairs (e.g., base=latest,health=latest , base=latest,civil=latest ) Impact : Determines which components are configured in the workspace. Available components vary by application (e.g., Manage has base, health, civil, etc.). Related variables : - mas_appws_spec : Overrides this if set - mas_app_id : Determines available components Note : Component availability depends on the application. For Manage: base , health , civil , etc. Refer to application documentation for available components. Use latest for most recent version or specify exact version. mas_pod_templates_dir \u00a4 Directory containing pod template configuration files (Manage only). Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Specifies a directory containing pod template YAML files for customizing Manage workspace and component workloads. Enables resource requests/limits, node selectors, tolerations, and other pod-level customizations. When to use : - Only for Manage application ( mas_app_id=manage ) - Use to customize pod resources (CPU, memory) - Use to apply node selectors or tolerations - Use to configure pod-level settings beyond defaults Valid values : Absolute path to directory containing pod template files Impact : Pod templates from this directory are merged into the ManageWorkspace CR. Files are applied to specific components or the workspace itself based on filename. Related variables : - mas_app_id : Must be manage for this to apply - Files must follow specific naming convention Note : Expected filenames: - ibm-mas-manage-manageworkspace.yml \u2192 workspace-level pod templates - ibm-mas-manage-imagestitching.yml \u2192 civil component pod templates - ibm-mas-manage-slackproxy.yml \u2192 component pod templates - ibm-mas-manage-healthextworkspace.yml \u2192 health component pod templates Refer to MAS CLI BestEfforts templates and Customizing Pod Templates documentation. Predict Configuration Variables \u00a4 mas_appws_settings_deployment_size \u00a4 Workload size for Predict containers (Predict only). Optional (Predict only) Environment Variable: MAS_APPWS_SETTINGS_DEPLOYMENT_SIZE Default: small Purpose : Controls the deployment size and replica count for Predict application containers. Different sizes provide different levels of availability and performance. When to use : - Only for Predict application ( mas_app_id=predict ) - Use developer for single-node development environments - Use small (default) for standard production deployments - Use medium for higher availability requirements - Use large for maximum availability (if supported) Valid values : developer , small , medium , large Impact : - developer : 1 replica (no high availability) - small : 2 replicas (basic high availability) - medium : 3 replicas (enhanced high availability) - large : Higher replica count (if supported) Related variables : - mas_app_id : Must be predict for this to apply Note : The developer size is suitable only for development/testing. Production environments should use small or larger for high availability. Watson Studio Local Variables \u00a4 These variables are only used when using this role to configure Predict , or Health & Predict Utilities . cpd_wsl_project_id \u00a4 Watson Studio analytics project ID (Predict/HP Utilities only). Required (unless cpd_wsl_project_name and mas_config_dir are set) Environment Variable: CPD_WSL_PROJECT_ID Default: None Purpose : Specifies the ID of the Watson Studio analytics project to use for Predict or Health & Predict Utilities configuration. When to use : - Required for Predict or HP Utilities configuration - Use when you know the project ID directly - Alternative: use cpd_wsl_project_name + mas_config_dir to retrieve ID from saved file Valid values : Valid Watson Studio project ID (UUID format) Impact : Links the MAS application to the specified Watson Studio project for analytics capabilities. Incorrect project ID will cause configuration to fail. Related variables : - cpd_wsl_project_name : Alternative method using project name - mas_config_dir : Required with cpd_wsl_project_name Note : The project must already exist in Watson Studio (created by cp4d_service role). Either provide this ID directly or use the name-based alternative. cpd_wsl_project_name \u00a4 Filename containing Watson Studio project ID (Predict/HP Utilities only). Optional Environment Variable: CPD_WSL_PROJECT_NAME Default: wsl-mas-${mas_instance_id}-hputilities Purpose : Specifies the filename in mas_config_dir where the Watson Studio project ID is saved. Alternative to providing cpd_wsl_project_id directly. When to use : - Use with mas_config_dir as alternative to cpd_wsl_project_id - Use when project ID was saved by cp4d_service role - Allows retrieving project ID from saved configuration Valid values : Filename (without path) where project ID is stored Impact : Role reads the project ID from this file in mas_config_dir . File must exist and contain valid project ID. Related variables : - mas_config_dir : Required directory containing this file - cpd_wsl_project_id : Alternative direct ID specification Note : The default filename matches the pattern used by cp4d_service role. The file should contain the Watson Studio project ID created during CP4D service setup. mas_config_dir \u00a4 Local directory for configuration files (Predict/HP Utilities only). Optional Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies directory containing configuration files, particularly Watson Studio project ID files. Used with cpd_wsl_project_name to retrieve project IDs. When to use : - Use with cpd_wsl_project_name to retrieve Watson Studio project ID - Use when project ID was saved by cp4d_service role - Should match the directory used in cp4d_service role Valid values : Absolute path to existing directory (e.g., /home/user/masconfig , ~/masconfig ) Impact : Role reads Watson Studio project ID from files in this directory. Directory must exist and contain the specified project file. Related variables : - cpd_wsl_project_name : Filename to read from this directory - cpd_wsl_project_id : Alternative direct ID specification Note : Use the same directory across all MAS setup roles for consistency. The cp4d_service role saves project IDs here, and this role retrieves them. Watson Machine Learning Variables \u00a4 These variables are only used when using this role to configure Predict . cpd_product_version \u00a4 Cloud Pak for Data version (Predict only). Required (Predict only) Environment Variable: CPD_PRODUCT_VERSION Default: None Purpose : Specifies the CP4D version to infer the correct Watson Machine Learning version for Predict workspace configuration. When to use : - Required when configuring Predict application - Must match the installed CP4D version in the cluster - Used to determine compatible WML version Valid values : Valid CP4D version string (e.g., 4.8.0 , 4.8.5 , 5.0.0 ) Impact : Determines which Watson Machine Learning version is configured in Predict. Incorrect version may cause compatibility issues. Related variables : - cpd_wml_instance_id : WML instance to configure - cpd_wml_url : WML service URL - mas_app_id : Must be predict for this to apply Note : This must match the actual CP4D version installed in your cluster. The role uses this to select the appropriate WML version for Predict configuration. cpd_wml_instance_id \u00a4 Watson Machine Learning instance identifier (Predict only). Optional (Predict only) Environment Variable: CPD_WML_INSTANCE_ID Default: openshift Purpose : Specifies the Watson Machine Learning instance identifier to configure in Predict workspace. When to use : - Only for Predict application configuration - Use default ( openshift ) for standard deployments - Set custom value if using non-default WML instance Valid values : Valid WML instance identifier string Impact : Identifies which WML instance Predict will use for machine learning operations. Must match an existing WML instance in CP4D. Related variables : - cpd_product_version : CP4D version for WML compatibility - cpd_wml_url : WML service URL - mas_app_id : Must be predict for this to apply Note : The default openshift value is suitable for most deployments. Only change if you have a specific WML instance identifier. cpd_wml_url \u00a4 Watson Machine Learning service URL (Predict only). Optional (Predict only) Environment Variable: CPD_WML_URL Default: https://internal-nginx-svc.ibm-cpd.svc:12443 Purpose : Specifies the URL to access Watson Machine Learning service. Typically the same as the Cloud Pak for Data URL. When to use : - Only for Predict application configuration - Use default if CP4D is in ibm-cpd namespace - Set custom URL if CP4D is in different namespace or uses custom service name Valid values : Valid HTTPS URL to WML service (e.g., https://internal-nginx-svc.{namespace}.svc:12443 ) Impact : Determines how Predict connects to Watson Machine Learning. Incorrect URL will prevent Predict from accessing WML services. Related variables : - cpd_product_version : CP4D version - cpd_wml_instance_id : WML instance identifier - mas_app_id : Must be predict for this to apply Note : The default assumes CP4D is installed in the ibm-cpd namespace. If CP4D is in a different namespace, update the URL accordingly (e.g., https://internal-nginx-svc.my-cpd-namespace.svc:12443 ). Manage Workspace Variables \u00a4 AI Service Integration \u00a4 aiservice_instance_id \u00a4 AI Service instance ID to integrate with Manage application. Optional Environment Variable: AISERVICE_INSTANCE_ID Default: None When configured, the role will: - Retrieve AI Service API key from the tenant-specific secret - Extract AI Service URL from the aibroker route - Import AI Service TLS certificate into Manage - Configure AI Service connection properties in Manage encryption secret - Verify AI Service health status before proceeding aiservice_tenant_id \u00a4 AI Service tenant ID to use for the integration. This is combined with the instance ID to form the fully qualified tenant name. Required when aiservice_instance_id is set Environment Variable: AISERVICE_TENANT_ID Default: None Note: The AI Service integration automatically retrieves the following from the cluster: - API key from secret: aiservice-{instance_id}-{tenant_id}----apikey-secret - Service URL from route: aibroker in namespace aiservice-{instance_id} - TLS certificate from secret: {instance_id}-public-aibroker-tls The integration also performs a health check to verify AI Service is running before completing the configuration. Health Integration \u00a4 mas_appws_bindings_health_wsl_flag \u00a4 Enable Watson Studio binding for Health (Manage only). Optional (Manage Health only) Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL_FLAG Default: false Purpose : Controls whether Watson Studio should be bound to the Manage Health component. Requires a system-level WatsonStudioCfg to be applied in the cluster. When to use : - Only for Manage application with Health component - Set to true to enable Watson Studio integration with Health - Leave as false (default) if Watson Studio integration is not needed Valid values : true , false Impact : When true , binds Watson Studio to Health component, enabling advanced analytics capabilities. Requires Watson Studio to be configured at system level. Related variables : - mas_appws_bindings_health_wsl : Binding scope (typically system ) - mas_app_id : Must be manage with Health component Note : A system-level WatsonStudioCfg must exist in the cluster before enabling this. Watson Studio must be installed and configured via CP4D. mas_appws_bindings_health_wsl \u00a4 Watson Studio binding scope for Health (Manage only). Optional (Manage Health only) Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL Default: None Purpose : Specifies the binding scope for Watson Studio integration with Manage Health component. When to use : - Only for Manage application with Health component - Set to system when Watson Studio is configured at system level - Used together with mas_appws_bindings_health_wsl_flag=true Valid values : system Impact : Binds Watson Studio at the specified scope to Health component, enabling advanced analytics and AI capabilities. Related variables : - mas_appws_bindings_health_wsl_flag : Must be true to enable binding - mas_app_id : Must be manage with Health component Note : Watson Studio must be installed and configured via CP4D with a system-level WatsonStudioCfg before using this binding. - Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL - Default: None mas_app_settings_aio_flag \u00a4 Flag indicating if Asset Investment Optimization (AIO) resource must be loaded or not. It can be loaded only when Optimizer application is installed. Optional , only supported when Optimizer application is installed Environment Variable: MAS_APP_SETTINGS_AIO_FLAG Default: true DB2 Settings \u00a4 mas_app_settings_db_schema \u00a4 Name of the schema where Manage database lives in. Code also supports deprecated mas_app_settings_db2_schema variable name. Optional Environment Variable: MAS_APP_SETTINGS_DB_SCHEMA Default: maximo mas_app_settings_demodata \u00a4 Flag indicating if manage demodata should be loaded or not. Optional Environment Variable: MAS_APP_SETTINGS_DEMODATA Default: false (do not load demodata) mas_app_settings_db2vargraphic \u00a4 Flag indicating if VARGRAPHIC (if true) or VARCHAR (if false) is used. Details: https://www.ibm.com/docs/en/mas-cd/continuous-delivery?topic=deploy-language-support Optional Environment Variable: MAS_APP_SETTINGS_DB2VARGRAPHIC Default: true mas_app_settings_tablespace \u00a4 Name of the Manage database tablespace. Optional Environment Variable: MAS_APP_SETTINGS_TABLESPACE Default: MAXDATA mas_app_settings_indexspace \u00a4 Name of the Manage database indexspace. Optional Environment Variable: MAS_APP_SETTINGS_INDEXSPACE Default: MAXINDEX Persistent Volumes \u00a4 mas_app_settings_persistent_volumes_flag \u00a4 Flag indicating if persistent volumes should be configured by default during Manage Workspace activation. There are two defaulted File Storage Persistent Volumes Claim resources that will be created out of the box for Manage if this flag is set to true : /DOCLINKS : Persistent volume used to store doclinks/attachments /bim : Persistent volume used to store Building Information Models related artifacts (models, docs and import) Optional Environment Variable: MAS_APP_SETTINGS_PERSISTENT_VOLUMES_FLAG Default: false JMS Queues \u00a4 The following properties can be defined to customize the persistent volumes for the JMS queues setup for Manage. mas_app_settings_jms_queue_pvc_storage_class \u00a4 Provide the persistent volume storage class to be used for JMS queue configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) access modes are supported. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes mas_app_settings_jms_queue_pvc_name \u00a4 Provide the persistent volume claim name to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_NAME Default: manage-jms mas_app_settings_jms_queue_pvc_size \u00a4 Provide the persistent volume claim size to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_SIZE Default: 20Gi mas_app_settings_jms_queue_mount_path \u00a4 Provide the persistent volume storage mount path to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_MOUNT_PATH Default: /jms mas_app_settings_jms_queue_pvc_accessmode \u00a4 Provide the persistent volume storage access-mode to be used for JMS queue configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_ACCESSMODE Default: ReadWriteMany mas_app_settings_default_jms \u00a4 Set this to true if you want to have JMS continuous queues configured. Optional Environment Variable: MAS_APP_SETTINGS_DEFAULT_JMS Default: false Doclinks/Attachments \u00a4 The following properties can be defined to customize the persistent volumes for the Doclinks/Attachments setup for Manage. mas_app_settings_doclinks_pvc_storage_class \u00a4 Provide the persistent volume storage class to be used for doclinks/attachments configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes mas_app_settings_doclinks_pvc_name \u00a4 Provide the persistent volume claim name to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_NAME Default: manage-doclinks mas_app_settings_doclinks_pvc_size \u00a4 Provide the persistent volume claim size to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_SIZE Default: 20Gi mas_app_settings_doclinks_mount_path \u00a4 Provide the persistent volume storage mount path to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_MOUNT_PATH Default: /DOCLINKS mas_app_settings_doclinks_pvc_accessmode \u00a4 Provide the persistent volume storage access-mode to be used for doclinks/attachments configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_ACCESSMODE Default: ReadWriteMany BIM (Building Information Models) \u00a4 The following properties can be defined to customize the persistent volumes for the Building Information Models setup for Manage. mas_app_settings_bim_pvc_storage_class \u00a4 Provide the persistent volume storage class to be used for Building Information Models configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes mas_app_settings_bim_pvc_name \u00a4 Provide the persistent volume claim name to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_NAME Default: manage-bim mas_app_settings_bim_pvc_size \u00a4 Provide the persistent volume claim size to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_SIZE Default: 20Gi mas_app_settings_bim_mount_path \u00a4 Provide the persistent volume storage mount path to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_MOUNT_PATH Default: /bim mas_app_settings_bim_pvc_accessmode \u00a4 Provide the persistent volume storage access-mode to be used for Building Information Models configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_ACCESSMODE Default: ReadWriteMany Supported Languages \u00a4 mas_app_settings_base_lang \u00a4 Provide the base language for Manage application. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. Optional Environment Variable: MAS_APP_SETTINGS_BASE_LANG Default: EN (English) mas_app_settings_secondary_langs \u00a4 Provide a list of additional secondary languages for Manage application. Optional Environment Variable: MAS_APP_SETTINGS_SECONDARY_LANGS Default: None Note: The more languages you add, the longer Manage will take to install and activate. Export the MAS_APP_SETTINGS_SECONDARY_LANGS variable with the language codes as comma-separated values. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. For example, use the following to enable Manage application with Arabic, Deutsch and Japanese as secondary languages: export MAS_APP_SETTINGS_SECONDARY_LANGS='AR,DE,JA' Server Bundle Configuration \u00a4 mas_app_settings_server_bundles_size \u00a4 Provides different flavors of server bundle configuration to handle workload for Manage application. Optional Environment Variable: MAS_APP_SETTINGS_SERVER_BUNDLES_SIZE Default: dev For more details about Manage application server bundle configuration, refer to Setting the server bundles for Manage application . Currently supported server bundle sizes are: - dev - Deploys Manage with the default server bundle configuration (i.e just 1 bundle pod handling all Manage application workload) - small - Deploys Manage with the most common deployment configuration (i.e 4 bundle pods, each one handling workload for each main capabilities: mea , cron , report and ui ) - jms - Can be used for Manage 8.4 and above. Same server bundle configuration as small and includes jms bundle pod. Enabling JMS pod workload will also configure Manage to use default JMS messaging queues to be stored in /{{ mas_app_settings_jms_queue_mount_path }}/jmsstore persistent volume mount path - snojms - Can be used for Manage 8.4 and above. Includes all and jms bundle pods. Enabling JMS pod workload will also configure Manage to use default JMS messaging queues to be stored in /{{ mas_app_settings_jms_queue_mount_path }}/jmsstore persistent volume mount path Customization Archive Settings \u00a4 mas_app_settings_customization_archive_url \u00a4 Provide a custom archive/file path to be included as part of Manage deployment. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOMIZATION_ARCHIVE_URL Default: None mas_app_settings_customization_archive_name \u00a4 Provide a custom archive file name to be associated with the archive/file path provided. Only used when mas_app_settings_customization_archive_url is defined. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOMIZATION_ARCHIVE_NAME Default: manage-custom-archive Database Encryption and AI Service Secrets \u00a4 Note: The encryption secret stores both database encryption keys and AI Service integration properties when aiservice_instance_id is configured. The secret will contain: - Database encryption keys: MXE_SECURITY_CRYPTO_KEY , MXE_SECURITY_CRYPTOX_KEY , MXE_SECURITY_OLD_CRYPTO_KEY , MXE_SECURITY_OLD_CRYPTOX_KEY - AI Service connection details: mxe.int.aibrokerapikey , mxe.int.aibrokerapiurl , mxe.int.aibrokertenantid mas_manage_encryptionsecret_crypto_key \u00a4 This defines the MXE_SECURITY_CRYPTO_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Optional Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_CRYPTO_KEY Default: Auto-generated mas_manage_encryptionsecret_cryptox_key \u00a4 This defines the MXE_SECURITY_CRYPTOX_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Required if mas_manage_encryptionsecret_crypto_key is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_CRYPTOX_KEY Default: Auto-generated mas_manage_encryptionsecret_old_crypto_key \u00a4 This defines the MXE_SECURITY_OLD_CRYPTO_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Optional Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_OLD_CRYPTO_KEY Default: None mas_manage_encryptionsecret_old_cryptox_key \u00a4 This defines the MXE_SECURITY_OLD_CRYPTOX_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Required if mas_manage_encryptionsecret_old_crypto_key is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_OLD_CRYPTOX_KEY Default: None mas_manage_encryptionsecret_aiservice_apikey \u00a4 The AI Service API key to configure in the encryption secret. When set along with the URL and FQN, the role will add these properties to the encryption secret. Optional Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_AISERVICE_APIKEY Default: Auto-retrieved from AI Service tenant secret when aiservice_instance_id is configured mas_manage_encryptionsecret_aiservice_url \u00a4 The AI Service broker URL to configure in the encryption secret. Required if mas_manage_encryptionsecret_aiservice_apikey is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_AISERVICE_URL Default: Auto-retrieved from AI Service route when aiservice_instance_id is configured mas_manage_encryptionsecret_aiservice_fqn \u00a4 The fully qualified AI Service tenant name (format: {instance_id}.{tenant_id} ) to configure in the encryption secret. Required if mas_manage_encryptionsecret_aiservice_apikey is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_AISERVICE_FQN Default: Auto-generated from aiservice_instance_id and aiservice_tenant_id when configured Server Timezone Setting \u00a4 mas_app_settings_server_timezone \u00a4 Sets the Manage server timezone. If you also want to have the Manage's DB2 database aligned with the same timezone, you must set DB2_TIMEZONE while provisioning the corresponding DB2 instance using db2 role. Optional Environment Variable: MAS_APP_SETTINGS_SERVER_TIMEZONE Default: GMT Facilities Workspace Variables \u00a4 mas_ws_facilities_size \u00a4 Sets the size of deployment. Optional Environment Variable: MAS_FACILITIES_SIZE Default: small Available options are small , medium and large mas_ws_facilities_pull_policy \u00a4 Sets the imagePullPolicy strategy for all deployments. The default is set to IfNotPresent to reduce the pulling operations in the cluster. Optional Environment Variable: MAS_FACILITIES_PULL_POLICY Default: IfNotPresent mas_ws_facilities_liberty_extension_xml_secret_name \u00a4 Provide the secret name of the secret which contains additional XML tags that needs to be added into the existing Liberty Server XML to configure the application accordingly. NOTE: The Secret name MUST be <workspaceId>-facilities-lexml--sn Optional Environment Variable: MAS_FACILITIES_LIBERTY_EXTENSION_XML_SECRET_NAME Default: None Sample Secret Template: cat <<EOF | oc create -f - kind: Secret apiVersion: v1 metadata: name: <MAS_FACILITIES_LIBERTY_EXTENSION_XML_SECRET_NAME> namespace: mas-<instanceId>-facilities data: extensions.xml: <!-- Custom XML tags --> type: Opaque EOF mas_ws_facilities_vault_secret_name \u00a4 Provide the name of the secret which contains a password to the vault with AES Encryption key. By default, this secret will be generated automatically. NOTE: The Secret name MUST be <workspaceId>-facilities-vs--sn Optional Environment Variable: MAS_FACILITIES_VAULT_SECRET_NAME Default: None Sample Secret Template: cat <<EOF | oc create -f - kind: Secret apiVersion: v1 metadata: name: <MAS_FACILITIES_VAULT_SECRET_NAME> namespace: mas-<instanceId>-facilities data: pwd: <your password> type: Opaque EOF mas_ws_facilities_dwfagents \u00a4 Allows the user to add dedicated workflow agents (DWFA) to MREF. To specify a DWFA it's required to specify a JSON with a unique name and members . Each member has a unique name and class that can be classified as user or group . Below an example of the structure of the JSON: export MAS_FACILITIES_DWFAGENTS='[{\"name\":\"dwfa1\",\"members\":[{\"name\": \"u1\", \"class\": \"user\"}]}, {\"name\":\"dwfa2\",\"members\":[{\"name\": \"u2\", \"class\": \"user\"},{\"name\":\"g1\", \"class\":\"group\"}]}]' Optional Environment Variable: MAS_FACILITIES_DWFAGENTS Default: [] Facilities Database Settings \u00a4 mas_ws_facilities_db_maxconnpoolsize \u00a4 Sets the maximum connection pool size for database. Optional Environment Variable: MAS_FACILITIES_DB_MAX_POOLSIZE Default: 200 Facilities Routes Settings \u00a4 mas_ws_facilities_db_timout \u00a4 Sets the timeout of the application. It is a string with the structure <timeout_value><time_unit> , where timeout_value is any non zero unsigned integer number and the supported time_unit are microseconds (us), milliseconds (ms), seconds (s), minutes (m), hours (h), or days (d). Optional Environment Variable: MAS_FACILITIES_ROUTES_TIMEOUT Default: 600s Facilities Storage Settings \u00a4 mas_ws_facilities_storage_log_class \u00a4 Sets the storage class name for the Log Persistent Volume Claim used for MREF agents. Optional Environment Variable: MAS_FACILITIES_STORAGE_LOG_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes mas_ws_facilities_storage_log_mode \u00a4 Sets the attach mode of the Log PVC. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_FACILITIES_STORAGE_LOG_MODE Default: ReadWriteOnce mas_ws_facilities_storage_log_size \u00a4 Sets the size of the Log PVC. Defaults to 30 Gigabytes. Optional Environment Variable: MAS_FACILITIES_STORAGE_LOG_SIZE Default: 30 mas_ws_facilities_storage_userfiles_class \u00a4 Sets the storage class name for the Userfiles PVC used for MREF agents. Optional Environment Variable: MAS_FACILITIES_STORAGE_USERFILES_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes mas_ws_facilities_storage_userfiles_mode \u00a4 Sets the attach mode of the Userfiles PVC. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_FACILITIES_STORAGE_USERFILES_MODE Default: ReadWriteOnce mas_ws_facilities_storage_userfiles_size \u00a4 Sets the size of the Log PVC. Defaults to 50 Gigabytes. Optional Environment Variable: MAS_FACILITIES_STORAGE_USERFILES_SIZE Default: 50 Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS workspace configuration mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" mas_appws_spec: bindings: jdbc: \"{{ mas_appws_jdbc_binding | default( 'system' , true) }}\" roles: - ibm.mas_devops.suite_app_config License \u00a4 EPL-2.0","title":"suite_app_config"},{"location":"roles/suite_app_config/#suite_app_config","text":"This role is used to configure specific components of the application workspace after the application has been installed in the Maximo Application Suite.","title":"suite_app_config"},{"location":"roles/suite_app_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_config/#general-variables","text":"","title":"General Variables"},{"location":"roles/suite_app_config/#mas_instance_id","text":"MAS instance identifier for the target installation. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to configure. This must match the instance ID used during MAS Core installation. When to use : - Always required when configuring application workspaces - Must match the instance ID from suite_install role - Used to locate the correct Suite and Workspace resources Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance's application workspace will be configured. Incorrect instance ID will cause configuration to fail. Related variables : - mas_app_id : Specifies which application to configure - mas_workspace_id : Specifies which workspace to configure Note : This must be an existing MAS instance. The role does not create new instances.","title":"mas_instance_id"},{"location":"roles/suite_app_config/#mas_app_id","text":"MAS application to configure. Required Environment Variable: MAS_APP_ID Default: None Purpose : Specifies which MAS application workspace to configure. Different applications have different configuration options and requirements. When to use : - Always required when configuring application workspaces - Must match an installed application in the MAS instance - Determines which application-specific configuration is applied Valid values : assist , iot , facilities , manage , monitor , optimizer , predict , visualinspection Impact : Determines which application workspace is configured and which configuration options are available. Each application has unique configuration requirements. Related variables : - mas_instance_id : The MAS instance containing this application - mas_workspace_id : The workspace to configure - mas_appws_components : Application-specific components to configure Note : The application must already be installed via suite_app_install role before configuration. Different applications support different configuration variables.","title":"mas_app_id"},{"location":"roles/suite_app_config/#mas_workspace_id","text":"Workspace identifier for the application workspace to configure. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the application to configure. Workspaces allow multiple isolated environments within a single application. When to use : - Always required when configuring application workspaces - Must match an existing workspace created during application installation - Typically matches the workspace ID used in suite_app_install Valid values : Lowercase alphanumeric string (e.g., masdev , prod , test ) Impact : Determines which workspace is configured. Each workspace has its own configuration, data, and users. Related variables : - mas_instance_id : The MAS instance containing this workspace - mas_app_id : The application containing this workspace - mas_appws_components : Components to configure in this workspace Note : The workspace must already exist (created during suite_app_install). This role configures existing workspaces, it does not create new ones.","title":"mas_workspace_id"},{"location":"roles/suite_app_config/#aiservice_instance_id","text":"AI Service instance ID for Manage integration (Manage only). Optional Environment Variable: AISERVICE_INSTANCE_ID Default: None Purpose : Enables automatic AI Service integration with Manage application. When set, the role retrieves credentials, configures connection, and imports certificates. When to use : - Only when configuring Manage application ( mas_app_id=manage ) - When you want to integrate AI Service capabilities into Manage - Requires AI Service to be installed and running in the cluster - Must be used together with aiservice_tenant_id Valid values : Valid AI Service instance ID (e.g., aiservice1 , ai-prod ) Impact : When configured, automatically: - Retrieves API key from tenant-specific secret - Extracts AI Service URL from aibroker route - Imports AI Service TLS certificate into Manage - Configures AI Service connection properties - Verifies AI Service health before proceeding Related variables : - aiservice_tenant_id : Required tenant ID for the integration - mas_app_id : Must be manage for this to apply Note : AI Service must be installed and healthy before configuration. The role performs automatic health checks and credential retrieval from cluster resources.","title":"aiservice_instance_id"},{"location":"roles/suite_app_config/#aiservice_tenant_id","text":"AI Service tenant ID for the integration (Manage only). Optional Environment Variable: AISERVICE_TENANT_ID Default: None Purpose : Specifies which AI Service tenant to use for Manage integration. Combined with instance ID to form the fully qualified tenant identifier. When to use : - Required when aiservice_instance_id is set - Only applies to Manage application - Must match an existing tenant in the AI Service instance Valid values : Valid AI Service tenant ID string (e.g., tenant1 , prod-tenant ) Impact : Combined with aiservice_instance_id to locate tenant-specific resources (API key secret, configuration). Incorrect tenant ID will cause integration to fail. Related variables : - aiservice_instance_id : Required instance ID for the integration - Forms secret name: aiservice-{instance_id}-{tenant_id}----apikey-secret Note : The tenant must already exist in the AI Service instance. The role retrieves credentials from the tenant-specific secret in the cluster.","title":"aiservice_tenant_id"},{"location":"roles/suite_app_config/#custom_labels","text":"Comma-separated list of key=value labels to apply to workspace resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to application workspace resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,app=manage ) Impact : Labels are applied to workspace-specific resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect workspace functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management. Note : Labels help with resource organization and are especially useful in multi-tenant or multi-workspace environments.","title":"custom_labels"},{"location":"roles/suite_app_config/#workspace-configuration-variables","text":"","title":"Workspace Configuration Variables"},{"location":"roles/suite_app_config/#mas_appws_spec","text":"Custom workspace deployment specification (overrides component-based configuration). Optional Environment Variable: MAS_APPWS_SPEC Default: Application-specific defaults in vars/defaultspecs/{mas_app_id}.yml Purpose : Provides complete control over workspace deployment specification. Allows advanced customization beyond what component-based configuration offers. When to use : - Use for advanced workspace customization - Use when you need full control over the workspace spec - Use when component-based configuration is insufficient - Leave unset for standard deployments using mas_appws_components Valid values : Valid workspace specification YAML/JSON matching the application's workspace CR schema Impact : WARNING - Overrides all settings from mas_appws_components . Provides complete control but requires deep knowledge of workspace CR structure. Incorrect specs can cause deployment failures. Related variables : - mas_appws_components : Simpler component-based configuration (overridden by this) - Application-specific default specs in vars/defaultspecs/ Note : Use mas_appws_components for standard deployments. Only use this variable when you need advanced customization or have specific requirements not covered by component-based configuration.","title":"mas_appws_spec"},{"location":"roles/suite_app_config/#mas_appws_bindings_jdbc","text":"JDBC binding scope for the workspace. Optional Environment Variable: MAS_APPWS_BINDINGS_JDBC Default: system Purpose : Controls the scope of JDBC database binding for the workspace. Different scopes provide different levels of isolation and sharing. When to use : - Use system (default) for shared system-level database configuration - Use application for application-level database configuration - Use workspace for workspace-specific database configuration - Use workspace-application for Maximo Real Estate and Facilities (recommended) Valid values : system , application , workspace , workspace-application Impact : - system : Uses system-level JDBC configuration (shared across all workspaces) - application : Uses application-level JDBC configuration - workspace : Uses workspace-specific JDBC configuration - workspace-application : Combines workspace and application scopes Related variables : JDBC configuration must exist at the specified scope level. Note : IMPORTANT - For Maximo Real Estate and Facilities applications, use workspace-application scope. The default system scope is suitable for most other applications.","title":"mas_appws_bindings_jdbc"},{"location":"roles/suite_app_config/#mas_appws_components","text":"Application components and versions to configure in the workspace. Optional Environment Variable: MAS_APPWS_COMPONENTS Default: Application-specific defaults Purpose : Specifies which application components to configure and their versions. Different applications have different available components. When to use : - Use to enable specific application components - Use to control component versions - Leave unset to use application-specific defaults - Overridden by mas_appws_spec if that is set Valid values : Comma-separated component=version pairs (e.g., base=latest,health=latest , base=latest,civil=latest ) Impact : Determines which components are configured in the workspace. Available components vary by application (e.g., Manage has base, health, civil, etc.). Related variables : - mas_appws_spec : Overrides this if set - mas_app_id : Determines available components Note : Component availability depends on the application. For Manage: base , health , civil , etc. Refer to application documentation for available components. Use latest for most recent version or specify exact version.","title":"mas_appws_components"},{"location":"roles/suite_app_config/#mas_pod_templates_dir","text":"Directory containing pod template configuration files (Manage only). Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Specifies a directory containing pod template YAML files for customizing Manage workspace and component workloads. Enables resource requests/limits, node selectors, tolerations, and other pod-level customizations. When to use : - Only for Manage application ( mas_app_id=manage ) - Use to customize pod resources (CPU, memory) - Use to apply node selectors or tolerations - Use to configure pod-level settings beyond defaults Valid values : Absolute path to directory containing pod template files Impact : Pod templates from this directory are merged into the ManageWorkspace CR. Files are applied to specific components or the workspace itself based on filename. Related variables : - mas_app_id : Must be manage for this to apply - Files must follow specific naming convention Note : Expected filenames: - ibm-mas-manage-manageworkspace.yml \u2192 workspace-level pod templates - ibm-mas-manage-imagestitching.yml \u2192 civil component pod templates - ibm-mas-manage-slackproxy.yml \u2192 component pod templates - ibm-mas-manage-healthextworkspace.yml \u2192 health component pod templates Refer to MAS CLI BestEfforts templates and Customizing Pod Templates documentation.","title":"mas_pod_templates_dir"},{"location":"roles/suite_app_config/#predict-configuration-variables","text":"","title":"Predict Configuration Variables"},{"location":"roles/suite_app_config/#mas_appws_settings_deployment_size","text":"Workload size for Predict containers (Predict only). Optional (Predict only) Environment Variable: MAS_APPWS_SETTINGS_DEPLOYMENT_SIZE Default: small Purpose : Controls the deployment size and replica count for Predict application containers. Different sizes provide different levels of availability and performance. When to use : - Only for Predict application ( mas_app_id=predict ) - Use developer for single-node development environments - Use small (default) for standard production deployments - Use medium for higher availability requirements - Use large for maximum availability (if supported) Valid values : developer , small , medium , large Impact : - developer : 1 replica (no high availability) - small : 2 replicas (basic high availability) - medium : 3 replicas (enhanced high availability) - large : Higher replica count (if supported) Related variables : - mas_app_id : Must be predict for this to apply Note : The developer size is suitable only for development/testing. Production environments should use small or larger for high availability.","title":"mas_appws_settings_deployment_size"},{"location":"roles/suite_app_config/#watson-studio-local-variables","text":"These variables are only used when using this role to configure Predict , or Health & Predict Utilities .","title":"Watson Studio Local Variables"},{"location":"roles/suite_app_config/#cpd_wsl_project_id","text":"Watson Studio analytics project ID (Predict/HP Utilities only). Required (unless cpd_wsl_project_name and mas_config_dir are set) Environment Variable: CPD_WSL_PROJECT_ID Default: None Purpose : Specifies the ID of the Watson Studio analytics project to use for Predict or Health & Predict Utilities configuration. When to use : - Required for Predict or HP Utilities configuration - Use when you know the project ID directly - Alternative: use cpd_wsl_project_name + mas_config_dir to retrieve ID from saved file Valid values : Valid Watson Studio project ID (UUID format) Impact : Links the MAS application to the specified Watson Studio project for analytics capabilities. Incorrect project ID will cause configuration to fail. Related variables : - cpd_wsl_project_name : Alternative method using project name - mas_config_dir : Required with cpd_wsl_project_name Note : The project must already exist in Watson Studio (created by cp4d_service role). Either provide this ID directly or use the name-based alternative.","title":"cpd_wsl_project_id"},{"location":"roles/suite_app_config/#cpd_wsl_project_name","text":"Filename containing Watson Studio project ID (Predict/HP Utilities only). Optional Environment Variable: CPD_WSL_PROJECT_NAME Default: wsl-mas-${mas_instance_id}-hputilities Purpose : Specifies the filename in mas_config_dir where the Watson Studio project ID is saved. Alternative to providing cpd_wsl_project_id directly. When to use : - Use with mas_config_dir as alternative to cpd_wsl_project_id - Use when project ID was saved by cp4d_service role - Allows retrieving project ID from saved configuration Valid values : Filename (without path) where project ID is stored Impact : Role reads the project ID from this file in mas_config_dir . File must exist and contain valid project ID. Related variables : - mas_config_dir : Required directory containing this file - cpd_wsl_project_id : Alternative direct ID specification Note : The default filename matches the pattern used by cp4d_service role. The file should contain the Watson Studio project ID created during CP4D service setup.","title":"cpd_wsl_project_name"},{"location":"roles/suite_app_config/#mas_config_dir","text":"Local directory for configuration files (Predict/HP Utilities only). Optional Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies directory containing configuration files, particularly Watson Studio project ID files. Used with cpd_wsl_project_name to retrieve project IDs. When to use : - Use with cpd_wsl_project_name to retrieve Watson Studio project ID - Use when project ID was saved by cp4d_service role - Should match the directory used in cp4d_service role Valid values : Absolute path to existing directory (e.g., /home/user/masconfig , ~/masconfig ) Impact : Role reads Watson Studio project ID from files in this directory. Directory must exist and contain the specified project file. Related variables : - cpd_wsl_project_name : Filename to read from this directory - cpd_wsl_project_id : Alternative direct ID specification Note : Use the same directory across all MAS setup roles for consistency. The cp4d_service role saves project IDs here, and this role retrieves them.","title":"mas_config_dir"},{"location":"roles/suite_app_config/#watson-machine-learning-variables","text":"These variables are only used when using this role to configure Predict .","title":"Watson Machine Learning Variables"},{"location":"roles/suite_app_config/#cpd_product_version","text":"Cloud Pak for Data version (Predict only). Required (Predict only) Environment Variable: CPD_PRODUCT_VERSION Default: None Purpose : Specifies the CP4D version to infer the correct Watson Machine Learning version for Predict workspace configuration. When to use : - Required when configuring Predict application - Must match the installed CP4D version in the cluster - Used to determine compatible WML version Valid values : Valid CP4D version string (e.g., 4.8.0 , 4.8.5 , 5.0.0 ) Impact : Determines which Watson Machine Learning version is configured in Predict. Incorrect version may cause compatibility issues. Related variables : - cpd_wml_instance_id : WML instance to configure - cpd_wml_url : WML service URL - mas_app_id : Must be predict for this to apply Note : This must match the actual CP4D version installed in your cluster. The role uses this to select the appropriate WML version for Predict configuration.","title":"cpd_product_version"},{"location":"roles/suite_app_config/#cpd_wml_instance_id","text":"Watson Machine Learning instance identifier (Predict only). Optional (Predict only) Environment Variable: CPD_WML_INSTANCE_ID Default: openshift Purpose : Specifies the Watson Machine Learning instance identifier to configure in Predict workspace. When to use : - Only for Predict application configuration - Use default ( openshift ) for standard deployments - Set custom value if using non-default WML instance Valid values : Valid WML instance identifier string Impact : Identifies which WML instance Predict will use for machine learning operations. Must match an existing WML instance in CP4D. Related variables : - cpd_product_version : CP4D version for WML compatibility - cpd_wml_url : WML service URL - mas_app_id : Must be predict for this to apply Note : The default openshift value is suitable for most deployments. Only change if you have a specific WML instance identifier.","title":"cpd_wml_instance_id"},{"location":"roles/suite_app_config/#cpd_wml_url","text":"Watson Machine Learning service URL (Predict only). Optional (Predict only) Environment Variable: CPD_WML_URL Default: https://internal-nginx-svc.ibm-cpd.svc:12443 Purpose : Specifies the URL to access Watson Machine Learning service. Typically the same as the Cloud Pak for Data URL. When to use : - Only for Predict application configuration - Use default if CP4D is in ibm-cpd namespace - Set custom URL if CP4D is in different namespace or uses custom service name Valid values : Valid HTTPS URL to WML service (e.g., https://internal-nginx-svc.{namespace}.svc:12443 ) Impact : Determines how Predict connects to Watson Machine Learning. Incorrect URL will prevent Predict from accessing WML services. Related variables : - cpd_product_version : CP4D version - cpd_wml_instance_id : WML instance identifier - mas_app_id : Must be predict for this to apply Note : The default assumes CP4D is installed in the ibm-cpd namespace. If CP4D is in a different namespace, update the URL accordingly (e.g., https://internal-nginx-svc.my-cpd-namespace.svc:12443 ).","title":"cpd_wml_url"},{"location":"roles/suite_app_config/#manage-workspace-variables","text":"","title":"Manage Workspace Variables"},{"location":"roles/suite_app_config/#ai-service-integration","text":"","title":"AI Service Integration"},{"location":"roles/suite_app_config/#aiservice_instance_id_1","text":"AI Service instance ID to integrate with Manage application. Optional Environment Variable: AISERVICE_INSTANCE_ID Default: None When configured, the role will: - Retrieve AI Service API key from the tenant-specific secret - Extract AI Service URL from the aibroker route - Import AI Service TLS certificate into Manage - Configure AI Service connection properties in Manage encryption secret - Verify AI Service health status before proceeding","title":"aiservice_instance_id"},{"location":"roles/suite_app_config/#aiservice_tenant_id_1","text":"AI Service tenant ID to use for the integration. This is combined with the instance ID to form the fully qualified tenant name. Required when aiservice_instance_id is set Environment Variable: AISERVICE_TENANT_ID Default: None Note: The AI Service integration automatically retrieves the following from the cluster: - API key from secret: aiservice-{instance_id}-{tenant_id}----apikey-secret - Service URL from route: aibroker in namespace aiservice-{instance_id} - TLS certificate from secret: {instance_id}-public-aibroker-tls The integration also performs a health check to verify AI Service is running before completing the configuration.","title":"aiservice_tenant_id"},{"location":"roles/suite_app_config/#health-integration","text":"","title":"Health Integration"},{"location":"roles/suite_app_config/#mas_appws_bindings_health_wsl_flag","text":"Enable Watson Studio binding for Health (Manage only). Optional (Manage Health only) Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL_FLAG Default: false Purpose : Controls whether Watson Studio should be bound to the Manage Health component. Requires a system-level WatsonStudioCfg to be applied in the cluster. When to use : - Only for Manage application with Health component - Set to true to enable Watson Studio integration with Health - Leave as false (default) if Watson Studio integration is not needed Valid values : true , false Impact : When true , binds Watson Studio to Health component, enabling advanced analytics capabilities. Requires Watson Studio to be configured at system level. Related variables : - mas_appws_bindings_health_wsl : Binding scope (typically system ) - mas_app_id : Must be manage with Health component Note : A system-level WatsonStudioCfg must exist in the cluster before enabling this. Watson Studio must be installed and configured via CP4D.","title":"mas_appws_bindings_health_wsl_flag"},{"location":"roles/suite_app_config/#mas_appws_bindings_health_wsl","text":"Watson Studio binding scope for Health (Manage only). Optional (Manage Health only) Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL Default: None Purpose : Specifies the binding scope for Watson Studio integration with Manage Health component. When to use : - Only for Manage application with Health component - Set to system when Watson Studio is configured at system level - Used together with mas_appws_bindings_health_wsl_flag=true Valid values : system Impact : Binds Watson Studio at the specified scope to Health component, enabling advanced analytics and AI capabilities. Related variables : - mas_appws_bindings_health_wsl_flag : Must be true to enable binding - mas_app_id : Must be manage with Health component Note : Watson Studio must be installed and configured via CP4D with a system-level WatsonStudioCfg before using this binding. - Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL - Default: None","title":"mas_appws_bindings_health_wsl"},{"location":"roles/suite_app_config/#mas_app_settings_aio_flag","text":"Flag indicating if Asset Investment Optimization (AIO) resource must be loaded or not. It can be loaded only when Optimizer application is installed. Optional , only supported when Optimizer application is installed Environment Variable: MAS_APP_SETTINGS_AIO_FLAG Default: true","title":"mas_app_settings_aio_flag"},{"location":"roles/suite_app_config/#db2-settings","text":"","title":"DB2 Settings"},{"location":"roles/suite_app_config/#mas_app_settings_db_schema","text":"Name of the schema where Manage database lives in. Code also supports deprecated mas_app_settings_db2_schema variable name. Optional Environment Variable: MAS_APP_SETTINGS_DB_SCHEMA Default: maximo","title":"mas_app_settings_db_schema"},{"location":"roles/suite_app_config/#mas_app_settings_demodata","text":"Flag indicating if manage demodata should be loaded or not. Optional Environment Variable: MAS_APP_SETTINGS_DEMODATA Default: false (do not load demodata)","title":"mas_app_settings_demodata"},{"location":"roles/suite_app_config/#mas_app_settings_db2vargraphic","text":"Flag indicating if VARGRAPHIC (if true) or VARCHAR (if false) is used. Details: https://www.ibm.com/docs/en/mas-cd/continuous-delivery?topic=deploy-language-support Optional Environment Variable: MAS_APP_SETTINGS_DB2VARGRAPHIC Default: true","title":"mas_app_settings_db2vargraphic"},{"location":"roles/suite_app_config/#mas_app_settings_tablespace","text":"Name of the Manage database tablespace. Optional Environment Variable: MAS_APP_SETTINGS_TABLESPACE Default: MAXDATA","title":"mas_app_settings_tablespace"},{"location":"roles/suite_app_config/#mas_app_settings_indexspace","text":"Name of the Manage database indexspace. Optional Environment Variable: MAS_APP_SETTINGS_INDEXSPACE Default: MAXINDEX","title":"mas_app_settings_indexspace"},{"location":"roles/suite_app_config/#persistent-volumes","text":"","title":"Persistent Volumes"},{"location":"roles/suite_app_config/#mas_app_settings_persistent_volumes_flag","text":"Flag indicating if persistent volumes should be configured by default during Manage Workspace activation. There are two defaulted File Storage Persistent Volumes Claim resources that will be created out of the box for Manage if this flag is set to true : /DOCLINKS : Persistent volume used to store doclinks/attachments /bim : Persistent volume used to store Building Information Models related artifacts (models, docs and import) Optional Environment Variable: MAS_APP_SETTINGS_PERSISTENT_VOLUMES_FLAG Default: false","title":"mas_app_settings_persistent_volumes_flag"},{"location":"roles/suite_app_config/#jms-queues","text":"The following properties can be defined to customize the persistent volumes for the JMS queues setup for Manage.","title":"JMS Queues"},{"location":"roles/suite_app_config/#mas_app_settings_jms_queue_pvc_storage_class","text":"Provide the persistent volume storage class to be used for JMS queue configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) access modes are supported. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes","title":"mas_app_settings_jms_queue_pvc_storage_class"},{"location":"roles/suite_app_config/#mas_app_settings_jms_queue_pvc_name","text":"Provide the persistent volume claim name to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_NAME Default: manage-jms","title":"mas_app_settings_jms_queue_pvc_name"},{"location":"roles/suite_app_config/#mas_app_settings_jms_queue_pvc_size","text":"Provide the persistent volume claim size to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_SIZE Default: 20Gi","title":"mas_app_settings_jms_queue_pvc_size"},{"location":"roles/suite_app_config/#mas_app_settings_jms_queue_mount_path","text":"Provide the persistent volume storage mount path to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_MOUNT_PATH Default: /jms","title":"mas_app_settings_jms_queue_mount_path"},{"location":"roles/suite_app_config/#mas_app_settings_jms_queue_pvc_accessmode","text":"Provide the persistent volume storage access-mode to be used for JMS queue configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_ACCESSMODE Default: ReadWriteMany","title":"mas_app_settings_jms_queue_pvc_accessmode"},{"location":"roles/suite_app_config/#mas_app_settings_default_jms","text":"Set this to true if you want to have JMS continuous queues configured. Optional Environment Variable: MAS_APP_SETTINGS_DEFAULT_JMS Default: false","title":"mas_app_settings_default_jms"},{"location":"roles/suite_app_config/#doclinksattachments","text":"The following properties can be defined to customize the persistent volumes for the Doclinks/Attachments setup for Manage.","title":"Doclinks/Attachments"},{"location":"roles/suite_app_config/#mas_app_settings_doclinks_pvc_storage_class","text":"Provide the persistent volume storage class to be used for doclinks/attachments configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes","title":"mas_app_settings_doclinks_pvc_storage_class"},{"location":"roles/suite_app_config/#mas_app_settings_doclinks_pvc_name","text":"Provide the persistent volume claim name to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_NAME Default: manage-doclinks","title":"mas_app_settings_doclinks_pvc_name"},{"location":"roles/suite_app_config/#mas_app_settings_doclinks_pvc_size","text":"Provide the persistent volume claim size to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_SIZE Default: 20Gi","title":"mas_app_settings_doclinks_pvc_size"},{"location":"roles/suite_app_config/#mas_app_settings_doclinks_mount_path","text":"Provide the persistent volume storage mount path to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_MOUNT_PATH Default: /DOCLINKS","title":"mas_app_settings_doclinks_mount_path"},{"location":"roles/suite_app_config/#mas_app_settings_doclinks_pvc_accessmode","text":"Provide the persistent volume storage access-mode to be used for doclinks/attachments configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_ACCESSMODE Default: ReadWriteMany","title":"mas_app_settings_doclinks_pvc_accessmode"},{"location":"roles/suite_app_config/#bim-building-information-models","text":"The following properties can be defined to customize the persistent volumes for the Building Information Models setup for Manage.","title":"BIM (Building Information Models)"},{"location":"roles/suite_app_config/#mas_app_settings_bim_pvc_storage_class","text":"Provide the persistent volume storage class to be used for Building Information Models configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes","title":"mas_app_settings_bim_pvc_storage_class"},{"location":"roles/suite_app_config/#mas_app_settings_bim_pvc_name","text":"Provide the persistent volume claim name to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_NAME Default: manage-bim","title":"mas_app_settings_bim_pvc_name"},{"location":"roles/suite_app_config/#mas_app_settings_bim_pvc_size","text":"Provide the persistent volume claim size to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_SIZE Default: 20Gi","title":"mas_app_settings_bim_pvc_size"},{"location":"roles/suite_app_config/#mas_app_settings_bim_mount_path","text":"Provide the persistent volume storage mount path to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_MOUNT_PATH Default: /bim","title":"mas_app_settings_bim_mount_path"},{"location":"roles/suite_app_config/#mas_app_settings_bim_pvc_accessmode","text":"Provide the persistent volume storage access-mode to be used for Building Information Models configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_ACCESSMODE Default: ReadWriteMany","title":"mas_app_settings_bim_pvc_accessmode"},{"location":"roles/suite_app_config/#supported-languages","text":"","title":"Supported Languages"},{"location":"roles/suite_app_config/#mas_app_settings_base_lang","text":"Provide the base language for Manage application. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. Optional Environment Variable: MAS_APP_SETTINGS_BASE_LANG Default: EN (English)","title":"mas_app_settings_base_lang"},{"location":"roles/suite_app_config/#mas_app_settings_secondary_langs","text":"Provide a list of additional secondary languages for Manage application. Optional Environment Variable: MAS_APP_SETTINGS_SECONDARY_LANGS Default: None Note: The more languages you add, the longer Manage will take to install and activate. Export the MAS_APP_SETTINGS_SECONDARY_LANGS variable with the language codes as comma-separated values. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. For example, use the following to enable Manage application with Arabic, Deutsch and Japanese as secondary languages: export MAS_APP_SETTINGS_SECONDARY_LANGS='AR,DE,JA'","title":"mas_app_settings_secondary_langs"},{"location":"roles/suite_app_config/#server-bundle-configuration","text":"","title":"Server Bundle Configuration"},{"location":"roles/suite_app_config/#mas_app_settings_server_bundles_size","text":"Provides different flavors of server bundle configuration to handle workload for Manage application. Optional Environment Variable: MAS_APP_SETTINGS_SERVER_BUNDLES_SIZE Default: dev For more details about Manage application server bundle configuration, refer to Setting the server bundles for Manage application . Currently supported server bundle sizes are: - dev - Deploys Manage with the default server bundle configuration (i.e just 1 bundle pod handling all Manage application workload) - small - Deploys Manage with the most common deployment configuration (i.e 4 bundle pods, each one handling workload for each main capabilities: mea , cron , report and ui ) - jms - Can be used for Manage 8.4 and above. Same server bundle configuration as small and includes jms bundle pod. Enabling JMS pod workload will also configure Manage to use default JMS messaging queues to be stored in /{{ mas_app_settings_jms_queue_mount_path }}/jmsstore persistent volume mount path - snojms - Can be used for Manage 8.4 and above. Includes all and jms bundle pods. Enabling JMS pod workload will also configure Manage to use default JMS messaging queues to be stored in /{{ mas_app_settings_jms_queue_mount_path }}/jmsstore persistent volume mount path","title":"mas_app_settings_server_bundles_size"},{"location":"roles/suite_app_config/#customization-archive-settings","text":"","title":"Customization Archive Settings"},{"location":"roles/suite_app_config/#mas_app_settings_customization_archive_url","text":"Provide a custom archive/file path to be included as part of Manage deployment. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOMIZATION_ARCHIVE_URL Default: None","title":"mas_app_settings_customization_archive_url"},{"location":"roles/suite_app_config/#mas_app_settings_customization_archive_name","text":"Provide a custom archive file name to be associated with the archive/file path provided. Only used when mas_app_settings_customization_archive_url is defined. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOMIZATION_ARCHIVE_NAME Default: manage-custom-archive","title":"mas_app_settings_customization_archive_name"},{"location":"roles/suite_app_config/#database-encryption-and-ai-service-secrets","text":"Note: The encryption secret stores both database encryption keys and AI Service integration properties when aiservice_instance_id is configured. The secret will contain: - Database encryption keys: MXE_SECURITY_CRYPTO_KEY , MXE_SECURITY_CRYPTOX_KEY , MXE_SECURITY_OLD_CRYPTO_KEY , MXE_SECURITY_OLD_CRYPTOX_KEY - AI Service connection details: mxe.int.aibrokerapikey , mxe.int.aibrokerapiurl , mxe.int.aibrokertenantid","title":"Database Encryption and AI Service Secrets"},{"location":"roles/suite_app_config/#mas_manage_encryptionsecret_crypto_key","text":"This defines the MXE_SECURITY_CRYPTO_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Optional Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_CRYPTO_KEY Default: Auto-generated","title":"mas_manage_encryptionsecret_crypto_key"},{"location":"roles/suite_app_config/#mas_manage_encryptionsecret_cryptox_key","text":"This defines the MXE_SECURITY_CRYPTOX_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Required if mas_manage_encryptionsecret_crypto_key is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_CRYPTOX_KEY Default: Auto-generated","title":"mas_manage_encryptionsecret_cryptox_key"},{"location":"roles/suite_app_config/#mas_manage_encryptionsecret_old_crypto_key","text":"This defines the MXE_SECURITY_OLD_CRYPTO_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Optional Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_OLD_CRYPTO_KEY Default: None","title":"mas_manage_encryptionsecret_old_crypto_key"},{"location":"roles/suite_app_config/#mas_manage_encryptionsecret_old_cryptox_key","text":"This defines the MXE_SECURITY_OLD_CRYPTOX_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Required if mas_manage_encryptionsecret_old_crypto_key is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_OLD_CRYPTOX_KEY Default: None","title":"mas_manage_encryptionsecret_old_cryptox_key"},{"location":"roles/suite_app_config/#mas_manage_encryptionsecret_aiservice_apikey","text":"The AI Service API key to configure in the encryption secret. When set along with the URL and FQN, the role will add these properties to the encryption secret. Optional Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_AISERVICE_APIKEY Default: Auto-retrieved from AI Service tenant secret when aiservice_instance_id is configured","title":"mas_manage_encryptionsecret_aiservice_apikey"},{"location":"roles/suite_app_config/#mas_manage_encryptionsecret_aiservice_url","text":"The AI Service broker URL to configure in the encryption secret. Required if mas_manage_encryptionsecret_aiservice_apikey is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_AISERVICE_URL Default: Auto-retrieved from AI Service route when aiservice_instance_id is configured","title":"mas_manage_encryptionsecret_aiservice_url"},{"location":"roles/suite_app_config/#mas_manage_encryptionsecret_aiservice_fqn","text":"The fully qualified AI Service tenant name (format: {instance_id}.{tenant_id} ) to configure in the encryption secret. Required if mas_manage_encryptionsecret_aiservice_apikey is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_AISERVICE_FQN Default: Auto-generated from aiservice_instance_id and aiservice_tenant_id when configured","title":"mas_manage_encryptionsecret_aiservice_fqn"},{"location":"roles/suite_app_config/#server-timezone-setting","text":"","title":"Server Timezone Setting"},{"location":"roles/suite_app_config/#mas_app_settings_server_timezone","text":"Sets the Manage server timezone. If you also want to have the Manage's DB2 database aligned with the same timezone, you must set DB2_TIMEZONE while provisioning the corresponding DB2 instance using db2 role. Optional Environment Variable: MAS_APP_SETTINGS_SERVER_TIMEZONE Default: GMT","title":"mas_app_settings_server_timezone"},{"location":"roles/suite_app_config/#facilities-workspace-variables","text":"","title":"Facilities Workspace Variables"},{"location":"roles/suite_app_config/#mas_ws_facilities_size","text":"Sets the size of deployment. Optional Environment Variable: MAS_FACILITIES_SIZE Default: small Available options are small , medium and large","title":"mas_ws_facilities_size"},{"location":"roles/suite_app_config/#mas_ws_facilities_pull_policy","text":"Sets the imagePullPolicy strategy for all deployments. The default is set to IfNotPresent to reduce the pulling operations in the cluster. Optional Environment Variable: MAS_FACILITIES_PULL_POLICY Default: IfNotPresent","title":"mas_ws_facilities_pull_policy"},{"location":"roles/suite_app_config/#mas_ws_facilities_liberty_extension_xml_secret_name","text":"Provide the secret name of the secret which contains additional XML tags that needs to be added into the existing Liberty Server XML to configure the application accordingly. NOTE: The Secret name MUST be <workspaceId>-facilities-lexml--sn Optional Environment Variable: MAS_FACILITIES_LIBERTY_EXTENSION_XML_SECRET_NAME Default: None Sample Secret Template: cat <<EOF | oc create -f - kind: Secret apiVersion: v1 metadata: name: <MAS_FACILITIES_LIBERTY_EXTENSION_XML_SECRET_NAME> namespace: mas-<instanceId>-facilities data: extensions.xml: <!-- Custom XML tags --> type: Opaque EOF","title":"mas_ws_facilities_liberty_extension_xml_secret_name"},{"location":"roles/suite_app_config/#mas_ws_facilities_vault_secret_name","text":"Provide the name of the secret which contains a password to the vault with AES Encryption key. By default, this secret will be generated automatically. NOTE: The Secret name MUST be <workspaceId>-facilities-vs--sn Optional Environment Variable: MAS_FACILITIES_VAULT_SECRET_NAME Default: None Sample Secret Template: cat <<EOF | oc create -f - kind: Secret apiVersion: v1 metadata: name: <MAS_FACILITIES_VAULT_SECRET_NAME> namespace: mas-<instanceId>-facilities data: pwd: <your password> type: Opaque EOF","title":"mas_ws_facilities_vault_secret_name"},{"location":"roles/suite_app_config/#mas_ws_facilities_dwfagents","text":"Allows the user to add dedicated workflow agents (DWFA) to MREF. To specify a DWFA it's required to specify a JSON with a unique name and members . Each member has a unique name and class that can be classified as user or group . Below an example of the structure of the JSON: export MAS_FACILITIES_DWFAGENTS='[{\"name\":\"dwfa1\",\"members\":[{\"name\": \"u1\", \"class\": \"user\"}]}, {\"name\":\"dwfa2\",\"members\":[{\"name\": \"u2\", \"class\": \"user\"},{\"name\":\"g1\", \"class\":\"group\"}]}]' Optional Environment Variable: MAS_FACILITIES_DWFAGENTS Default: []","title":"mas_ws_facilities_dwfagents"},{"location":"roles/suite_app_config/#facilities-database-settings","text":"","title":"Facilities Database Settings"},{"location":"roles/suite_app_config/#mas_ws_facilities_db_maxconnpoolsize","text":"Sets the maximum connection pool size for database. Optional Environment Variable: MAS_FACILITIES_DB_MAX_POOLSIZE Default: 200","title":"mas_ws_facilities_db_maxconnpoolsize"},{"location":"roles/suite_app_config/#facilities-routes-settings","text":"","title":"Facilities Routes Settings"},{"location":"roles/suite_app_config/#mas_ws_facilities_db_timout","text":"Sets the timeout of the application. It is a string with the structure <timeout_value><time_unit> , where timeout_value is any non zero unsigned integer number and the supported time_unit are microseconds (us), milliseconds (ms), seconds (s), minutes (m), hours (h), or days (d). Optional Environment Variable: MAS_FACILITIES_ROUTES_TIMEOUT Default: 600s","title":"mas_ws_facilities_db_timout"},{"location":"roles/suite_app_config/#facilities-storage-settings","text":"","title":"Facilities Storage Settings"},{"location":"roles/suite_app_config/#mas_ws_facilities_storage_log_class","text":"Sets the storage class name for the Log Persistent Volume Claim used for MREF agents. Optional Environment Variable: MAS_FACILITIES_STORAGE_LOG_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes","title":"mas_ws_facilities_storage_log_class"},{"location":"roles/suite_app_config/#mas_ws_facilities_storage_log_mode","text":"Sets the attach mode of the Log PVC. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_FACILITIES_STORAGE_LOG_MODE Default: ReadWriteOnce","title":"mas_ws_facilities_storage_log_mode"},{"location":"roles/suite_app_config/#mas_ws_facilities_storage_log_size","text":"Sets the size of the Log PVC. Defaults to 30 Gigabytes. Optional Environment Variable: MAS_FACILITIES_STORAGE_LOG_SIZE Default: 30","title":"mas_ws_facilities_storage_log_size"},{"location":"roles/suite_app_config/#mas_ws_facilities_storage_userfiles_class","text":"Sets the storage class name for the Userfiles PVC used for MREF agents. Optional Environment Variable: MAS_FACILITIES_STORAGE_USERFILES_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes","title":"mas_ws_facilities_storage_userfiles_class"},{"location":"roles/suite_app_config/#mas_ws_facilities_storage_userfiles_mode","text":"Sets the attach mode of the Userfiles PVC. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_FACILITIES_STORAGE_USERFILES_MODE Default: ReadWriteOnce","title":"mas_ws_facilities_storage_userfiles_mode"},{"location":"roles/suite_app_config/#mas_ws_facilities_storage_userfiles_size","text":"Sets the size of the Log PVC. Defaults to 50 Gigabytes. Optional Environment Variable: MAS_FACILITIES_STORAGE_USERFILES_SIZE Default: 50","title":"mas_ws_facilities_storage_userfiles_size"},{"location":"roles/suite_app_config/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS workspace configuration mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" mas_appws_spec: bindings: jdbc: \"{{ mas_appws_jdbc_binding | default( 'system' , true) }}\" roles: - ibm.mas_devops.suite_app_config","title":"Example Playbook"},{"location":"roles/suite_app_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_install/","text":"suite_app_install \u00a4 This role is used to install a specified application in Maximo Application Suite. Role Variables \u00a4 General \u00a4 mas_instance_id \u00a4 Unique identifier for the MAS instance where the application will be installed. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to install the application into. This must match the instance ID used during the MAS core installation to ensure the application is deployed to the correct MAS environment. When to use : - Always required for any MAS application installation - Must match the instance ID from your MAS core installation - Use the same value across all application installations for a given MAS instance Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : The application will be installed into the namespace mas-{mas_instance_id}-{mas_app_id} . An incorrect instance ID will cause the installation to fail or create resources in the wrong namespace. Related variables : Works with mas_app_id to determine the target namespace for the application. Note : This must match the instance ID used when installing MAS core. Cannot be changed after application installation. mas_app_id \u00a4 Specifies which MAS application to install. Required Environment Variable: MAS_APP_ID Default: None Purpose : Identifies the specific MAS application to be installed, determining which operator subscription is created and which application resources are deployed. When to use : - Always required for any MAS application installation - Set to the specific application you want to install - Each application requires a separate role execution Valid values : assist , iot , facilities , manage , monitor , predict , visualinspection , optimizer , arcgis Impact : Determines which application operator is installed and which namespace is created ( mas-{mas_instance_id}-{mas_app_id} ). Different applications have different configuration requirements and dependencies. Related variables : - mas_app_channel : Must be set to a valid channel for the selected application - mas_app_catalog_source : Must contain the operator for the selected application - Application-specific settings variables (e.g., mas_app_settings_iot_* for IoT) Note : Each application has its own set of configuration variables. Refer to the application-specific sections below for additional required variables. mas_app_catalog_source \u00a4 Specifies the OpenShift operator catalog source containing the MAS application operator subscription. Optional Environment Variable: MAS_APP_CATALOG_SOURCE Default: ibm-operator-catalog Purpose : Controls which operator catalog is used to locate and install the MAS application operator. This determines where OpenShift looks for the application operator images and metadata during installation. When to use : - Use default ibm-operator-catalog for production installations with official IBM releases - Use ibm-mas-{mas_app_id}-operators for development builds from Artifactory (e.g., ibm-mas-manage-operators ) - Only change if directed to use a custom catalog for specific testing or airgap scenarios Valid values : Any valid CatalogSource name present in the openshift-marketplace namespace Impact : Changing this value affects which operator versions are available for installation. An invalid catalog source will cause the subscription to fail. Development catalogs require additional authentication via artifactory_username and artifactory_token . Related variables : - mas_app_channel : Works together to determine the specific operator version installed - artifactory_username and artifactory_token : Required when using development catalogs Note : For development catalogs, the naming pattern is ibm-mas-{mas_app_id}-operators where {mas_app_id} is the application name (e.g., manage , monitor ). mas_app_channel \u00a4 Specifies the MAS application operator subscription channel, which determines the version stream you'll receive updates from. Required Environment Variable: MAS_APP_CHANNEL Default: None Purpose : Controls which version of the MAS application will be installed and which updates will be automatically applied. The channel corresponds to major.minor version releases and determines the feature set and compatibility level of your application installation. When to use : - Set to the latest stable channel for new production deployments - Use specific older channels when compatibility with MAS core or other applications requires it - Consult the MAS compatibility matrix before selecting a channel - Change channels only during planned upgrade windows as this triggers version updates Valid values : Application-specific channels (e.g., 8.6.x , 8.7.x , 8.8.x for Manage; check the IBM Operator Catalog for currently available channels for your application) Impact : The channel determines which application version is installed and which automatic updates are applied. Changing channels after installation will trigger an upgrade to the latest version in that channel, which may require application reconfiguration and testing. Related variables : - mas_app_catalog_source : Works together to determine available channels - mas_instance_id : Application must be compatible with the MAS core version Note : Each MAS application has its own set of available channels. Ensure the selected channel is compatible with your MAS core version. Review the application upgrade documentation before changing this value. custom_labels \u00a4 Comma-separated list of key=value labels to apply to MAS application resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to application resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,app=manage ) Impact : Labels are applied to application resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect application functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management. Pre-Release Support \u00a4 artifactory_username \u00a4 Username for authenticating to IBM Artifactory to access development builds of MAS applications. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None Purpose : Provides authentication credentials to pull development/pre-release MAS application operator images from IBM's Artifactory registry. Required only when installing development builds for testing or early access. When to use : - Required when mas_app_catalog_source is set to a development catalog (e.g., ibm-mas-manage-operators ) - Not needed for production installations using ibm-operator-catalog - Use your IBM w3Id username for development builds Valid values : Valid IBM Artifactory username (typically your w3Id) Impact : Without valid credentials, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_token : Must be set together with this username - mas_app_catalog_source : Determines if Artifactory credentials are needed - mas_entitlement_username : Used for production installations instead Note : Only required for development/pre-release builds. Production installations use mas_entitlement_key instead. artifactory_token \u00a4 API token for authenticating to IBM Artifactory to access development builds of MAS applications. Optional Environment Variable: ARTIFACTORY_TOKEN Default: None Purpose : Provides the API token/password credential to authenticate with IBM's Artifactory registry when pulling development/pre-release MAS application operator images. When to use : - Required when mas_app_catalog_source is set to a development catalog (e.g., ibm-mas-manage-operators ) - Not needed for production installations using ibm-operator-catalog - Use your IBM Artifactory API key for development builds Valid values : Valid IBM Artifactory API token Impact : Without a valid token, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_username : Must be set together with this token - mas_app_catalog_source : Determines if Artifactory credentials are needed - mas_entitlement_key : Used for production installations instead Note : Only required for development/pre-release builds. Production installations use mas_entitlement_key instead. Keep this token secure and do not commit to source control. mas_entitlement_username \u00a4 Username for authenticating to IBM Container Registry to pull MAS application images. Optional Environment Variable: MAS_ENTITLEMENT_USERNAME Default: cp for production installations Purpose : Provides the username credential for authenticating with IBM's entitled container registry when pulling MAS application operator and component images. When to use : - Set to cp for production installations using IBM entitlement keys - Set to your w3Id for development builds from Artifactory - Usually can be left at default for production installations Valid values : - cp - For production installations with IBM entitlement key - Your IBM w3Id - For development builds Impact : Used together with mas_entitlement_key to create image pull secrets for the application namespace. Incorrect username will cause image pull authentication failures. Related variables : - mas_entitlement_key : Must be set together with this username - artifactory_username : Alternative for development builds Note : For production installations, the default value cp is typically correct when used with an IBM entitlement key. mas_entitlement_key \u00a4 IBM entitlement key for authenticating access to IBM Container Registry to pull MAS application images. Optional Environment Variable: MAS_ENTITLEMENT_KEY Default: None Purpose : Provides authentication credentials to pull MAS application container images from IBM's entitled registry. This key is tied to your IBM Cloud account and product entitlements, proving you have the right to use MAS application software. When to use : - Required for production installations using official IBM releases - Obtain from IBM Container Library at https://myibm.ibm.com/products-services/containerlibrary - For development builds, use your Artifactory API key instead - Key must be valid and have active MAS application entitlements Valid values : - IBM entitlement key string (typically starts with \"eyJ...\") - Must be a valid, non-expired key with MAS product entitlements - For development: Artifactory API key Impact : Invalid or expired keys will cause image pull failures during application installation. The key is stored in a Kubernetes secret and used to create image pull secrets for all application pods. Related variables : - mas_entitlement_username : Username paired with this key (default: cp ) - artifactory_token : Alternative for development builds Note : Keep this key secure. Do not commit it to source control. Use environment variables or secure secret management. Application Configuration \u00a4 mas_app_spec \u00a4 Custom YAML specification to override default application configuration settings. Optional Environment Variable: None Default: Application-specific defaults in vars/defaultspecs/{{mas_app_id}}.yml Purpose : Allows advanced users to provide a complete custom specification for the application installation, bypassing individual configuration variables. This enables fine-grained control over application settings not exposed through standard variables. When to use : - Use for advanced customization scenarios not covered by standard variables - Use when you need to set application-specific settings not exposed as role variables - Use with caution as it overrides all other configuration variables - Leave unset for standard installations using individual configuration variables Valid values : Valid YAML dictionary matching the application's Custom Resource specification Impact : When set, this completely overrides all other application configuration variables (e.g., mas_app_settings_* , mas_app_bindings_* ). The role will use this specification directly in the application CR. Related variables : Overrides all mas_app_settings_* and mas_app_bindings_* variables when set. Note : Requires deep knowledge of the application's Custom Resource specification. Incorrect specifications can cause installation failures. Use individual configuration variables unless you have specific advanced requirements. mas_app_bindings_jdbc \u00a4 Specifies the scope for JDBC database configuration binding. Optional Environment Variable: MAS_APP_BINDINGS_JDBC Default: system Purpose : Controls whether the application uses system-level JDBC configuration (shared across all applications) or application-specific JDBC configuration. This determines which database configuration the application will use. When to use : - Use system (default) when all applications share the same database configuration - Use application when this application needs its own dedicated database configuration - Most deployments use system for simplified management Valid values : system , application Impact : - system : Application uses the JDBC configuration defined at the MAS instance level (JdbcCfg with scope=system) - application : Application requires its own application-scoped JDBC configuration (JdbcCfg with scope=application) Related variables : Requires corresponding JdbcCfg resource to be configured at the appropriate scope. Note : Changing this after installation may require reconfiguration of database connections. Ensure the appropriate JdbcCfg resource exists before changing this value. mas_app_plan \u00a4 Specifies the licensing plan/tier for the application installation. Optional Environment Variable: MAS_APP_PLAN Default: Application-specific (varies by application) Purpose : Determines which feature set and licensing tier is activated for the application. Different plans may enable or restrict certain features based on your license entitlements. When to use : - Set according to your license entitlements - Consult your IBM license agreement for available plans - Leave as default if you have standard licensing Valid values : Application-specific - Optimizer : full , limited (v8.2+, defaults to full ) - Other applications may have different plan options Impact : The plan determines which features are available in the application. Using a plan not covered by your license may result in compliance issues. Some features may be disabled or unavailable depending on the selected plan. Related variables : Must align with your IBM license entitlements for the application. Note : Ensure the selected plan matches your license agreement. Contact IBM if you're unsure which plan to use. mas_pod_templates_dir \u00a4 Local directory path containing pod template customization files for the application. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Allows customization of pod specifications for application workloads, enabling control over resource limits, node affinity, tolerations, and other Kubernetes pod settings. This is essential for production deployments with specific infrastructure requirements. When to use : - Use to set custom resource limits (CPU, memory) for application pods - Use to configure node affinity or anti-affinity rules - Use to add tolerations for tainted nodes - Use to apply custom security contexts or service accounts - Leave unset for default pod configurations Valid values : Any valid local filesystem path containing application-specific pod template YAML files Impact : Pod template files in this directory will be applied to the application's Custom Resource, affecting how application pods are scheduled and configured. Invalid templates can cause pod scheduling failures. Related variables : - mas_app_id : Determines which pod template files are expected - See application-specific sections below for required file names Note : Each application expects specific file names. Refer to the application-specific mas_pod_templates_dir documentation below for details. For full documentation, see Customizing Pod Templates in the product documentation. Visual Inspection Configuration \u00a4 mas_app_settings_visualinspection_storage_class \u00a4 Storage class for Visual Inspection user data persistent volumes. Optional Environment Variable: MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_CLASS Default: Auto-selected from available ReadWriteMany (RWX) storage classes in the cluster Purpose : Specifies which storage class provides persistent volumes for Visual Inspection user data, including uploaded images, trained models, and inspection results. This storage must support concurrent access from multiple pods. When to use : - Set explicitly when you have multiple RWX storage classes and want to control which is used - Set when the auto-selection doesn't choose your preferred storage class - Leave unset to allow automatic selection of an appropriate RWX storage class Valid values : Any valid storage class name in your cluster that supports ReadWriteMany (RWX) access mode Impact : Affects performance and reliability of Visual Inspection data storage. The storage class must support RWX access mode for Visual Inspection to function correctly. Related variables : mas_app_settings_visualinspection_storage_size - Determines the size of the PVC using this storage class. Note : ReadWriteMany (RWX) support is required . Verify your storage class supports RWX with oc get storageclass before deployment. Common RWX storage classes include NFS, CephFS, and cloud provider file storage. mas_app_settings_visualinspection_storage_size \u00a4 Size of the persistent volume for Visual Inspection user data. Optional Environment Variable: MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for Visual Inspection user data, including uploaded images, trained AI models, inspection results, and datasets. Proper sizing prevents storage exhaustion. When to use : - Increase for production environments with large image datasets - Increase for environments with many trained models or long data retention - Use default (100Gi) for development, testing, or small deployments - Consider your data retention policies and expected growth Valid values : Any valid Kubernetes storage size (e.g., 100Gi , 500Gi , 1Ti , 2Ti ) Impact : Larger values consume more cluster storage resources. Insufficient storage will prevent uploading new images or training models. PVC expansion capability depends on the storage class. Related variables : mas_app_settings_visualinspection_storage_class - Must support volume expansion if you plan to increase size later. Note : Plan for growth when setting initial size. Monitor storage usage to avoid running out of space. Check if your storage class supports PVC expansion before deployment. IoT Configuration \u00a4 mas_app_settings_iot_deployment_size \u00a4 Specifies the deployment size profile for MAS IoT, which determines resource allocations and scaling characteristics. Optional Environment Variable: MAS_APP_SETTINGS_IOT_DEPLOYMENT_SIZE Default: small Purpose : Controls the resource allocation profile for IoT components, affecting CPU, memory, and replica counts. Different sizes are optimized for different workload scales and environments. When to use : - Use dev for development and testing environments with minimal resource requirements - Use small for production environments with moderate device counts and data volumes - Use large for production environments with high device counts, data volumes, or throughput requirements Valid values : dev , small , large Impact : - dev : Minimal resources, suitable for development only - small : Moderate resources, suitable for small to medium production deployments - large : Maximum resources, suitable for large-scale production deployments Related variables : Affects overall cluster resource consumption for IoT components. Note : Application Support: IoT 8.6+ . Ensure your cluster has sufficient resources for the selected size. Cannot be easily changed after deployment without reinstallation. mas_app_settings_iot_fpl_pvc_storage_class \u00a4 Storage class for IoT Function Pipeline (FPL) component persistent volumes. Optional Environment Variable: MAS_APP_SETTINGS_IOT_FPL_PVC_STORAGE_CLASS Default: Auto-selected from available ReadWriteOnce (RWO) storage classes in the cluster Purpose : Specifies which storage class provides persistent volumes for IoT FPL component transient state storage. FPL processes IoT data pipelines and requires persistent storage for state management. When to use : - Set explicitly when you have multiple RWO storage classes and want to control which is used - Set when the auto-selection doesn't choose your preferred storage class - Leave unset to allow automatic selection of an appropriate RWO storage class Valid values : Any valid storage class name in your cluster that supports ReadWriteOnce (RWO) access mode Impact : Affects performance and reliability of IoT data pipeline processing. The storage class determines I/O performance for pipeline state operations. Related variables : - mas_app_settings_iot_fpl_router_pvc_size - Size for FPL router PVC - mas_app_settings_iot_fpl_executor_pvc_size - Size for FPL executor PVC Note : Application Support: IoT 8.6+ . ReadWriteOnce (RWO) access mode is required. Verify with oc get storageclass before deployment. mas_app_settings_iot_fpl_router_pvc_size \u00a4 Size of the persistent volume for IoT FPL pipeline router component. Optional Environment Variable: MAS_APP_SETTINGS_IOT_FPL_ROUTER_PVC_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for the IoT Function Pipeline router component's transient state storage. The router manages pipeline execution and requires persistent storage for state management. When to use : - Increase for production environments with high pipeline throughput - Increase for environments with many concurrent pipelines or complex pipeline logic - Use default (100Gi) for development, testing, or moderate workloads - Consider your pipeline complexity and execution frequency Valid values : Any valid Kubernetes storage size (e.g., 50Gi , 100Gi , 200Gi , 500Gi ) Impact : Larger values consume more cluster storage resources. Insufficient storage may cause pipeline execution failures or state management issues. Related variables : - mas_app_settings_iot_fpl_pvc_storage_class - Storage class for this PVC - mas_app_settings_iot_fpl_executor_pvc_size - Size for FPL executor component Note : Application Support: IoT 8.6+ . Monitor storage usage and adjust as needed. Check if your storage class supports PVC expansion. mas_app_settings_iot_fpl_executor_pvc_size \u00a4 Size of the persistent volume for IoT FPL pipeline executor component. Optional Environment Variable: MAS_APP_SETTINGS_IOT_FPL_EXECUTOR_PVC_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for the IoT Function Pipeline executor component's transient state storage. The executor runs pipeline logic and requires persistent storage for intermediate data and state. When to use : - Increase for production environments with high data processing volumes - Increase for pipelines that process large datasets or generate significant intermediate data - Use default (100Gi) for development, testing, or moderate workloads - Consider your pipeline data processing requirements Valid values : Any valid Kubernetes storage size (e.g., 50Gi , 100Gi , 200Gi , 500Gi ) Impact : Larger values consume more cluster storage resources. Insufficient storage may cause pipeline execution failures or data processing issues. Related variables : - mas_app_settings_iot_fpl_pvc_storage_class - Storage class for this PVC - mas_app_settings_iot_fpl_router_pvc_size - Size for FPL router component Note : Application Support: IoT 8.6+ . Monitor storage usage and adjust as needed. Check if your storage class supports PVC expansion. mas_app_settings_iot_mqttbroker_pvc_storage_class \u00a4 Storage class for IoT MQTT broker (MessageSight) persistent volumes. Optional Environment Variable: MAS_APP_SETTINGS_IOT_MQTTBROKER_PVC_STORAGE_CLASS Default: Auto-selected from available ReadWriteOnce (RWO) storage classes in the cluster Purpose : Specifies which storage class provides persistent volumes for the IoT MQTT broker component. The MQTT broker handles device connectivity and message routing, requiring persistent storage for message queues and broker state. When to use : - Set explicitly when you have multiple RWO storage classes and want to control which is used - Set when the auto-selection doesn't choose your preferred storage class - Set when you need specific performance characteristics for MQTT message handling - Leave unset to allow automatic selection of an appropriate RWO storage class Valid values : Any valid storage class name in your cluster that supports ReadWriteOnce (RWO) access mode Impact : Affects performance and reliability of IoT device connectivity and message routing. The storage class determines I/O performance for message queue operations. Related variables : mas_app_settings_iot_mqttbroker_pvc_size - Determines the size of the PVC using this storage class. Note : Application Support: IoT 8.3+ . ReadWriteOnce (RWO) access mode is required. Verify with oc get storageclass before deployment. mas_app_settings_iot_mqttbroker_pvc_size \u00a4 Size of the persistent volume for IoT MQTT broker (MessageSight) component. Optional Environment Variable: MAS_APP_SETTINGS_IOT_MQTTBROKER_PVC_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for the IoT MQTT broker's persistent storage, including message queues, broker state, and retained messages. Proper sizing ensures reliable device connectivity and message handling. When to use : - Increase for production environments with many connected devices - Increase for environments with high message volumes or long message retention - Increase if devices frequently disconnect and require message queuing - Use default (100Gi) for development, testing, or moderate device counts Valid values : Any valid Kubernetes storage size (e.g., 50Gi , 100Gi , 200Gi , 500Gi ) Impact : Larger values consume more cluster storage resources. Insufficient storage may cause message loss, device connection failures, or broker instability. Related variables : mas_app_settings_iot_mqttbroker_pvc_storage_class - Storage class for this PVC. Note : Application Support: IoT 8.3+ . Monitor storage usage to prevent message queue overflow. Check if your storage class supports PVC expansion. mas_pod_templates_dir (IoT) \u00a4 This role will look for configuration files named: ibm-mas-iot-iot.yml , ibm-mas-iot-actions.yml , ibm-mas-iot-auth.yml , ibm-mas-iot-datapower.yml , ibm-mas-iot-devops.yml , ibm-mas-iot-dm.yml , ibm-mas-iot-dsc.yml , ibm-mas-iot-edgeconfig.yml , ibm-mas-iot-fpl.yml , ibm-mas-iot-guardian.yml , ibm-mas-iot-mbgx.yml , ibm-mas-iot-mfgx.yml , ibm-mas-iot-monitor.yml , ibm-mas-iot-orgmgmt.yml , ibm-mas-iot-provision.yml , ibm-mas-iot-registry.yml , ibm-mas-iot-state.yml , ibm-mas-iot-webui.yml . The content of the configuration file should be the yaml block that you wish to be inserted into the IoT CR. ibm-mas-iot-iot.yml will be inserted into the main IoT CR spec -> podTemplates whereas the component ones e.g, ibm-mas-iot-actions.yml will be under spec -> components -> {componentName} -> podTemplates . The ibm-mas-iot operator will then pass this on to the corresponding component CR when available. This is an example of one of the components (actions) - refer to the BestEfforts reference configuration in the MAS CLI . For full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Manage Configuration \u00a4 mas_pod_templates_dir (Manage) \u00a4 This role will look for a configuration file named ibm-mas-manage-manageapp.yml . The content of the configuration file should be the yaml block that you wish to be inserted into the ManageApp CR. ibm-mas-manage-manageapp.yml will be inserted into the ManageApp CR spec -> podTemplates . The ibm-mas-manage operator will then pass this on to the corresponding deployments when available. For full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None mas_appws_upgrade_type \u00a4 Specifies the upgrade strategy for Manage workspace database schema updates. Optional Environment Variable: MAS_APPWS_UPGRADE_TYPE Default: regularUpgrade Purpose : Controls how Manage performs database schema upgrades during application updates, balancing between system downtime and upgrade complexity. Different strategies offer different trade-offs between availability and upgrade duration. When to use : - Use regularUpgrade for standard upgrades with planned downtime windows - Use onlineUpgrade to minimize downtime during upgrades (requires more resources and time) - Consider your maintenance window constraints and availability requirements Valid values : - regularUpgrade - Standard upgrade with full system downtime - onlineUpgrade - Minimized downtime upgrade (requires additional resources) Impact : - regularUpgrade : Shorter upgrade time but requires full system downtime - onlineUpgrade : Longer upgrade time but minimizes system downtime; requires additional database resources during upgrade Related variables : Applies to Manage application upgrades only. Note : For full documentation of upgrade types and requirements, refer to the Manage Upgrade information in the product documentation. Online upgrades require careful planning and additional resources. Monitor Configuration \u00a4 mas_app_settings_monitor_deployment_size \u00a4 Specifies the deployment size profile for MAS Monitor, which determines resource allocations and scaling characteristics. Optional Environment Variable: MAS_APP_SETTINGS_MONITOR_DEPLOYMENT_SIZE Default: dev Purpose : Controls the resource allocation profile for Monitor components, affecting CPU, memory, and replica counts. Different sizes are optimized for different workload scales and environments. When to use : - Use dev for development and testing environments with minimal resource requirements - Use small for production environments with moderate monitoring requirements - Use large for production environments with extensive monitoring, dashboards, or high data volumes Valid values : dev , small , large Impact : - dev : Minimal resources, suitable for development only - small : Moderate resources, suitable for small to medium production deployments - large : Maximum resources, suitable for large-scale production deployments with extensive monitoring Related variables : Affects overall cluster resource consumption for Monitor components. Note : Application Support: Monitor 8.6+ . Ensure your cluster has sufficient resources for the selected size. Cannot be easily changed after deployment without reinstallation. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # Choose which catalog source to use for the MAS install, default to the IBM operator catalog mas_app_catalog_source: \"{{ lookup('env', 'MAS_APP_CATALOG_SOURCE') | default('ibm-operator-catalog', true) }}\" # Which MAS channel to subscribe to mas_app_channel: \"{{ lookup('env', 'MAS_APP_CHANNEL') | default('8.x', true) }}\" # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS configuration - Entitlement mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') | default('cp', true) }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" # Application Configuration - Spec mas_app_spec: bindings: jdbc: system mongo: system kafka: system settings: messagesight: storage: class: block1000p size: 100Gi deployment: size: medium # Application Configuration - Install Plan mas_app_plan: \"{{ lookup('env', 'MAS_APP_PLAN') | default('full', true) }}\" roles: - ibm.mas_devops.suite_app_install License \u00a4 EPL-2.0","title":"suite_app_install"},{"location":"roles/suite_app_install/#suite_app_install","text":"This role is used to install a specified application in Maximo Application Suite.","title":"suite_app_install"},{"location":"roles/suite_app_install/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_install/#general","text":"","title":"General"},{"location":"roles/suite_app_install/#mas_instance_id","text":"Unique identifier for the MAS instance where the application will be installed. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to install the application into. This must match the instance ID used during the MAS core installation to ensure the application is deployed to the correct MAS environment. When to use : - Always required for any MAS application installation - Must match the instance ID from your MAS core installation - Use the same value across all application installations for a given MAS instance Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : The application will be installed into the namespace mas-{mas_instance_id}-{mas_app_id} . An incorrect instance ID will cause the installation to fail or create resources in the wrong namespace. Related variables : Works with mas_app_id to determine the target namespace for the application. Note : This must match the instance ID used when installing MAS core. Cannot be changed after application installation.","title":"mas_instance_id"},{"location":"roles/suite_app_install/#mas_app_id","text":"Specifies which MAS application to install. Required Environment Variable: MAS_APP_ID Default: None Purpose : Identifies the specific MAS application to be installed, determining which operator subscription is created and which application resources are deployed. When to use : - Always required for any MAS application installation - Set to the specific application you want to install - Each application requires a separate role execution Valid values : assist , iot , facilities , manage , monitor , predict , visualinspection , optimizer , arcgis Impact : Determines which application operator is installed and which namespace is created ( mas-{mas_instance_id}-{mas_app_id} ). Different applications have different configuration requirements and dependencies. Related variables : - mas_app_channel : Must be set to a valid channel for the selected application - mas_app_catalog_source : Must contain the operator for the selected application - Application-specific settings variables (e.g., mas_app_settings_iot_* for IoT) Note : Each application has its own set of configuration variables. Refer to the application-specific sections below for additional required variables.","title":"mas_app_id"},{"location":"roles/suite_app_install/#mas_app_catalog_source","text":"Specifies the OpenShift operator catalog source containing the MAS application operator subscription. Optional Environment Variable: MAS_APP_CATALOG_SOURCE Default: ibm-operator-catalog Purpose : Controls which operator catalog is used to locate and install the MAS application operator. This determines where OpenShift looks for the application operator images and metadata during installation. When to use : - Use default ibm-operator-catalog for production installations with official IBM releases - Use ibm-mas-{mas_app_id}-operators for development builds from Artifactory (e.g., ibm-mas-manage-operators ) - Only change if directed to use a custom catalog for specific testing or airgap scenarios Valid values : Any valid CatalogSource name present in the openshift-marketplace namespace Impact : Changing this value affects which operator versions are available for installation. An invalid catalog source will cause the subscription to fail. Development catalogs require additional authentication via artifactory_username and artifactory_token . Related variables : - mas_app_channel : Works together to determine the specific operator version installed - artifactory_username and artifactory_token : Required when using development catalogs Note : For development catalogs, the naming pattern is ibm-mas-{mas_app_id}-operators where {mas_app_id} is the application name (e.g., manage , monitor ).","title":"mas_app_catalog_source"},{"location":"roles/suite_app_install/#mas_app_channel","text":"Specifies the MAS application operator subscription channel, which determines the version stream you'll receive updates from. Required Environment Variable: MAS_APP_CHANNEL Default: None Purpose : Controls which version of the MAS application will be installed and which updates will be automatically applied. The channel corresponds to major.minor version releases and determines the feature set and compatibility level of your application installation. When to use : - Set to the latest stable channel for new production deployments - Use specific older channels when compatibility with MAS core or other applications requires it - Consult the MAS compatibility matrix before selecting a channel - Change channels only during planned upgrade windows as this triggers version updates Valid values : Application-specific channels (e.g., 8.6.x , 8.7.x , 8.8.x for Manage; check the IBM Operator Catalog for currently available channels for your application) Impact : The channel determines which application version is installed and which automatic updates are applied. Changing channels after installation will trigger an upgrade to the latest version in that channel, which may require application reconfiguration and testing. Related variables : - mas_app_catalog_source : Works together to determine available channels - mas_instance_id : Application must be compatible with the MAS core version Note : Each MAS application has its own set of available channels. Ensure the selected channel is compatible with your MAS core version. Review the application upgrade documentation before changing this value.","title":"mas_app_channel"},{"location":"roles/suite_app_install/#custom_labels","text":"Comma-separated list of key=value labels to apply to MAS application resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to application resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,app=manage ) Impact : Labels are applied to application resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect application functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management.","title":"custom_labels"},{"location":"roles/suite_app_install/#pre-release-support","text":"","title":"Pre-Release Support"},{"location":"roles/suite_app_install/#artifactory_username","text":"Username for authenticating to IBM Artifactory to access development builds of MAS applications. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None Purpose : Provides authentication credentials to pull development/pre-release MAS application operator images from IBM's Artifactory registry. Required only when installing development builds for testing or early access. When to use : - Required when mas_app_catalog_source is set to a development catalog (e.g., ibm-mas-manage-operators ) - Not needed for production installations using ibm-operator-catalog - Use your IBM w3Id username for development builds Valid values : Valid IBM Artifactory username (typically your w3Id) Impact : Without valid credentials, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_token : Must be set together with this username - mas_app_catalog_source : Determines if Artifactory credentials are needed - mas_entitlement_username : Used for production installations instead Note : Only required for development/pre-release builds. Production installations use mas_entitlement_key instead.","title":"artifactory_username"},{"location":"roles/suite_app_install/#artifactory_token","text":"API token for authenticating to IBM Artifactory to access development builds of MAS applications. Optional Environment Variable: ARTIFACTORY_TOKEN Default: None Purpose : Provides the API token/password credential to authenticate with IBM's Artifactory registry when pulling development/pre-release MAS application operator images. When to use : - Required when mas_app_catalog_source is set to a development catalog (e.g., ibm-mas-manage-operators ) - Not needed for production installations using ibm-operator-catalog - Use your IBM Artifactory API key for development builds Valid values : Valid IBM Artifactory API token Impact : Without a valid token, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_username : Must be set together with this token - mas_app_catalog_source : Determines if Artifactory credentials are needed - mas_entitlement_key : Used for production installations instead Note : Only required for development/pre-release builds. Production installations use mas_entitlement_key instead. Keep this token secure and do not commit to source control.","title":"artifactory_token"},{"location":"roles/suite_app_install/#mas_entitlement_username","text":"Username for authenticating to IBM Container Registry to pull MAS application images. Optional Environment Variable: MAS_ENTITLEMENT_USERNAME Default: cp for production installations Purpose : Provides the username credential for authenticating with IBM's entitled container registry when pulling MAS application operator and component images. When to use : - Set to cp for production installations using IBM entitlement keys - Set to your w3Id for development builds from Artifactory - Usually can be left at default for production installations Valid values : - cp - For production installations with IBM entitlement key - Your IBM w3Id - For development builds Impact : Used together with mas_entitlement_key to create image pull secrets for the application namespace. Incorrect username will cause image pull authentication failures. Related variables : - mas_entitlement_key : Must be set together with this username - artifactory_username : Alternative for development builds Note : For production installations, the default value cp is typically correct when used with an IBM entitlement key.","title":"mas_entitlement_username"},{"location":"roles/suite_app_install/#mas_entitlement_key","text":"IBM entitlement key for authenticating access to IBM Container Registry to pull MAS application images. Optional Environment Variable: MAS_ENTITLEMENT_KEY Default: None Purpose : Provides authentication credentials to pull MAS application container images from IBM's entitled registry. This key is tied to your IBM Cloud account and product entitlements, proving you have the right to use MAS application software. When to use : - Required for production installations using official IBM releases - Obtain from IBM Container Library at https://myibm.ibm.com/products-services/containerlibrary - For development builds, use your Artifactory API key instead - Key must be valid and have active MAS application entitlements Valid values : - IBM entitlement key string (typically starts with \"eyJ...\") - Must be a valid, non-expired key with MAS product entitlements - For development: Artifactory API key Impact : Invalid or expired keys will cause image pull failures during application installation. The key is stored in a Kubernetes secret and used to create image pull secrets for all application pods. Related variables : - mas_entitlement_username : Username paired with this key (default: cp ) - artifactory_token : Alternative for development builds Note : Keep this key secure. Do not commit it to source control. Use environment variables or secure secret management.","title":"mas_entitlement_key"},{"location":"roles/suite_app_install/#application-configuration","text":"","title":"Application Configuration"},{"location":"roles/suite_app_install/#mas_app_spec","text":"Custom YAML specification to override default application configuration settings. Optional Environment Variable: None Default: Application-specific defaults in vars/defaultspecs/{{mas_app_id}}.yml Purpose : Allows advanced users to provide a complete custom specification for the application installation, bypassing individual configuration variables. This enables fine-grained control over application settings not exposed through standard variables. When to use : - Use for advanced customization scenarios not covered by standard variables - Use when you need to set application-specific settings not exposed as role variables - Use with caution as it overrides all other configuration variables - Leave unset for standard installations using individual configuration variables Valid values : Valid YAML dictionary matching the application's Custom Resource specification Impact : When set, this completely overrides all other application configuration variables (e.g., mas_app_settings_* , mas_app_bindings_* ). The role will use this specification directly in the application CR. Related variables : Overrides all mas_app_settings_* and mas_app_bindings_* variables when set. Note : Requires deep knowledge of the application's Custom Resource specification. Incorrect specifications can cause installation failures. Use individual configuration variables unless you have specific advanced requirements.","title":"mas_app_spec"},{"location":"roles/suite_app_install/#mas_app_bindings_jdbc","text":"Specifies the scope for JDBC database configuration binding. Optional Environment Variable: MAS_APP_BINDINGS_JDBC Default: system Purpose : Controls whether the application uses system-level JDBC configuration (shared across all applications) or application-specific JDBC configuration. This determines which database configuration the application will use. When to use : - Use system (default) when all applications share the same database configuration - Use application when this application needs its own dedicated database configuration - Most deployments use system for simplified management Valid values : system , application Impact : - system : Application uses the JDBC configuration defined at the MAS instance level (JdbcCfg with scope=system) - application : Application requires its own application-scoped JDBC configuration (JdbcCfg with scope=application) Related variables : Requires corresponding JdbcCfg resource to be configured at the appropriate scope. Note : Changing this after installation may require reconfiguration of database connections. Ensure the appropriate JdbcCfg resource exists before changing this value.","title":"mas_app_bindings_jdbc"},{"location":"roles/suite_app_install/#mas_app_plan","text":"Specifies the licensing plan/tier for the application installation. Optional Environment Variable: MAS_APP_PLAN Default: Application-specific (varies by application) Purpose : Determines which feature set and licensing tier is activated for the application. Different plans may enable or restrict certain features based on your license entitlements. When to use : - Set according to your license entitlements - Consult your IBM license agreement for available plans - Leave as default if you have standard licensing Valid values : Application-specific - Optimizer : full , limited (v8.2+, defaults to full ) - Other applications may have different plan options Impact : The plan determines which features are available in the application. Using a plan not covered by your license may result in compliance issues. Some features may be disabled or unavailable depending on the selected plan. Related variables : Must align with your IBM license entitlements for the application. Note : Ensure the selected plan matches your license agreement. Contact IBM if you're unsure which plan to use.","title":"mas_app_plan"},{"location":"roles/suite_app_install/#mas_pod_templates_dir","text":"Local directory path containing pod template customization files for the application. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Allows customization of pod specifications for application workloads, enabling control over resource limits, node affinity, tolerations, and other Kubernetes pod settings. This is essential for production deployments with specific infrastructure requirements. When to use : - Use to set custom resource limits (CPU, memory) for application pods - Use to configure node affinity or anti-affinity rules - Use to add tolerations for tainted nodes - Use to apply custom security contexts or service accounts - Leave unset for default pod configurations Valid values : Any valid local filesystem path containing application-specific pod template YAML files Impact : Pod template files in this directory will be applied to the application's Custom Resource, affecting how application pods are scheduled and configured. Invalid templates can cause pod scheduling failures. Related variables : - mas_app_id : Determines which pod template files are expected - See application-specific sections below for required file names Note : Each application expects specific file names. Refer to the application-specific mas_pod_templates_dir documentation below for details. For full documentation, see Customizing Pod Templates in the product documentation.","title":"mas_pod_templates_dir"},{"location":"roles/suite_app_install/#visual-inspection-configuration","text":"","title":"Visual Inspection Configuration"},{"location":"roles/suite_app_install/#mas_app_settings_visualinspection_storage_class","text":"Storage class for Visual Inspection user data persistent volumes. Optional Environment Variable: MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_CLASS Default: Auto-selected from available ReadWriteMany (RWX) storage classes in the cluster Purpose : Specifies which storage class provides persistent volumes for Visual Inspection user data, including uploaded images, trained models, and inspection results. This storage must support concurrent access from multiple pods. When to use : - Set explicitly when you have multiple RWX storage classes and want to control which is used - Set when the auto-selection doesn't choose your preferred storage class - Leave unset to allow automatic selection of an appropriate RWX storage class Valid values : Any valid storage class name in your cluster that supports ReadWriteMany (RWX) access mode Impact : Affects performance and reliability of Visual Inspection data storage. The storage class must support RWX access mode for Visual Inspection to function correctly. Related variables : mas_app_settings_visualinspection_storage_size - Determines the size of the PVC using this storage class. Note : ReadWriteMany (RWX) support is required . Verify your storage class supports RWX with oc get storageclass before deployment. Common RWX storage classes include NFS, CephFS, and cloud provider file storage.","title":"mas_app_settings_visualinspection_storage_class"},{"location":"roles/suite_app_install/#mas_app_settings_visualinspection_storage_size","text":"Size of the persistent volume for Visual Inspection user data. Optional Environment Variable: MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for Visual Inspection user data, including uploaded images, trained AI models, inspection results, and datasets. Proper sizing prevents storage exhaustion. When to use : - Increase for production environments with large image datasets - Increase for environments with many trained models or long data retention - Use default (100Gi) for development, testing, or small deployments - Consider your data retention policies and expected growth Valid values : Any valid Kubernetes storage size (e.g., 100Gi , 500Gi , 1Ti , 2Ti ) Impact : Larger values consume more cluster storage resources. Insufficient storage will prevent uploading new images or training models. PVC expansion capability depends on the storage class. Related variables : mas_app_settings_visualinspection_storage_class - Must support volume expansion if you plan to increase size later. Note : Plan for growth when setting initial size. Monitor storage usage to avoid running out of space. Check if your storage class supports PVC expansion before deployment.","title":"mas_app_settings_visualinspection_storage_size"},{"location":"roles/suite_app_install/#iot-configuration","text":"","title":"IoT Configuration"},{"location":"roles/suite_app_install/#mas_app_settings_iot_deployment_size","text":"Specifies the deployment size profile for MAS IoT, which determines resource allocations and scaling characteristics. Optional Environment Variable: MAS_APP_SETTINGS_IOT_DEPLOYMENT_SIZE Default: small Purpose : Controls the resource allocation profile for IoT components, affecting CPU, memory, and replica counts. Different sizes are optimized for different workload scales and environments. When to use : - Use dev for development and testing environments with minimal resource requirements - Use small for production environments with moderate device counts and data volumes - Use large for production environments with high device counts, data volumes, or throughput requirements Valid values : dev , small , large Impact : - dev : Minimal resources, suitable for development only - small : Moderate resources, suitable for small to medium production deployments - large : Maximum resources, suitable for large-scale production deployments Related variables : Affects overall cluster resource consumption for IoT components. Note : Application Support: IoT 8.6+ . Ensure your cluster has sufficient resources for the selected size. Cannot be easily changed after deployment without reinstallation.","title":"mas_app_settings_iot_deployment_size"},{"location":"roles/suite_app_install/#mas_app_settings_iot_fpl_pvc_storage_class","text":"Storage class for IoT Function Pipeline (FPL) component persistent volumes. Optional Environment Variable: MAS_APP_SETTINGS_IOT_FPL_PVC_STORAGE_CLASS Default: Auto-selected from available ReadWriteOnce (RWO) storage classes in the cluster Purpose : Specifies which storage class provides persistent volumes for IoT FPL component transient state storage. FPL processes IoT data pipelines and requires persistent storage for state management. When to use : - Set explicitly when you have multiple RWO storage classes and want to control which is used - Set when the auto-selection doesn't choose your preferred storage class - Leave unset to allow automatic selection of an appropriate RWO storage class Valid values : Any valid storage class name in your cluster that supports ReadWriteOnce (RWO) access mode Impact : Affects performance and reliability of IoT data pipeline processing. The storage class determines I/O performance for pipeline state operations. Related variables : - mas_app_settings_iot_fpl_router_pvc_size - Size for FPL router PVC - mas_app_settings_iot_fpl_executor_pvc_size - Size for FPL executor PVC Note : Application Support: IoT 8.6+ . ReadWriteOnce (RWO) access mode is required. Verify with oc get storageclass before deployment.","title":"mas_app_settings_iot_fpl_pvc_storage_class"},{"location":"roles/suite_app_install/#mas_app_settings_iot_fpl_router_pvc_size","text":"Size of the persistent volume for IoT FPL pipeline router component. Optional Environment Variable: MAS_APP_SETTINGS_IOT_FPL_ROUTER_PVC_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for the IoT Function Pipeline router component's transient state storage. The router manages pipeline execution and requires persistent storage for state management. When to use : - Increase for production environments with high pipeline throughput - Increase for environments with many concurrent pipelines or complex pipeline logic - Use default (100Gi) for development, testing, or moderate workloads - Consider your pipeline complexity and execution frequency Valid values : Any valid Kubernetes storage size (e.g., 50Gi , 100Gi , 200Gi , 500Gi ) Impact : Larger values consume more cluster storage resources. Insufficient storage may cause pipeline execution failures or state management issues. Related variables : - mas_app_settings_iot_fpl_pvc_storage_class - Storage class for this PVC - mas_app_settings_iot_fpl_executor_pvc_size - Size for FPL executor component Note : Application Support: IoT 8.6+ . Monitor storage usage and adjust as needed. Check if your storage class supports PVC expansion.","title":"mas_app_settings_iot_fpl_router_pvc_size"},{"location":"roles/suite_app_install/#mas_app_settings_iot_fpl_executor_pvc_size","text":"Size of the persistent volume for IoT FPL pipeline executor component. Optional Environment Variable: MAS_APP_SETTINGS_IOT_FPL_EXECUTOR_PVC_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for the IoT Function Pipeline executor component's transient state storage. The executor runs pipeline logic and requires persistent storage for intermediate data and state. When to use : - Increase for production environments with high data processing volumes - Increase for pipelines that process large datasets or generate significant intermediate data - Use default (100Gi) for development, testing, or moderate workloads - Consider your pipeline data processing requirements Valid values : Any valid Kubernetes storage size (e.g., 50Gi , 100Gi , 200Gi , 500Gi ) Impact : Larger values consume more cluster storage resources. Insufficient storage may cause pipeline execution failures or data processing issues. Related variables : - mas_app_settings_iot_fpl_pvc_storage_class - Storage class for this PVC - mas_app_settings_iot_fpl_router_pvc_size - Size for FPL router component Note : Application Support: IoT 8.6+ . Monitor storage usage and adjust as needed. Check if your storage class supports PVC expansion.","title":"mas_app_settings_iot_fpl_executor_pvc_size"},{"location":"roles/suite_app_install/#mas_app_settings_iot_mqttbroker_pvc_storage_class","text":"Storage class for IoT MQTT broker (MessageSight) persistent volumes. Optional Environment Variable: MAS_APP_SETTINGS_IOT_MQTTBROKER_PVC_STORAGE_CLASS Default: Auto-selected from available ReadWriteOnce (RWO) storage classes in the cluster Purpose : Specifies which storage class provides persistent volumes for the IoT MQTT broker component. The MQTT broker handles device connectivity and message routing, requiring persistent storage for message queues and broker state. When to use : - Set explicitly when you have multiple RWO storage classes and want to control which is used - Set when the auto-selection doesn't choose your preferred storage class - Set when you need specific performance characteristics for MQTT message handling - Leave unset to allow automatic selection of an appropriate RWO storage class Valid values : Any valid storage class name in your cluster that supports ReadWriteOnce (RWO) access mode Impact : Affects performance and reliability of IoT device connectivity and message routing. The storage class determines I/O performance for message queue operations. Related variables : mas_app_settings_iot_mqttbroker_pvc_size - Determines the size of the PVC using this storage class. Note : Application Support: IoT 8.3+ . ReadWriteOnce (RWO) access mode is required. Verify with oc get storageclass before deployment.","title":"mas_app_settings_iot_mqttbroker_pvc_storage_class"},{"location":"roles/suite_app_install/#mas_app_settings_iot_mqttbroker_pvc_size","text":"Size of the persistent volume for IoT MQTT broker (MessageSight) component. Optional Environment Variable: MAS_APP_SETTINGS_IOT_MQTTBROKER_PVC_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for the IoT MQTT broker's persistent storage, including message queues, broker state, and retained messages. Proper sizing ensures reliable device connectivity and message handling. When to use : - Increase for production environments with many connected devices - Increase for environments with high message volumes or long message retention - Increase if devices frequently disconnect and require message queuing - Use default (100Gi) for development, testing, or moderate device counts Valid values : Any valid Kubernetes storage size (e.g., 50Gi , 100Gi , 200Gi , 500Gi ) Impact : Larger values consume more cluster storage resources. Insufficient storage may cause message loss, device connection failures, or broker instability. Related variables : mas_app_settings_iot_mqttbroker_pvc_storage_class - Storage class for this PVC. Note : Application Support: IoT 8.3+ . Monitor storage usage to prevent message queue overflow. Check if your storage class supports PVC expansion.","title":"mas_app_settings_iot_mqttbroker_pvc_size"},{"location":"roles/suite_app_install/#mas_pod_templates_dir-iot","text":"This role will look for configuration files named: ibm-mas-iot-iot.yml , ibm-mas-iot-actions.yml , ibm-mas-iot-auth.yml , ibm-mas-iot-datapower.yml , ibm-mas-iot-devops.yml , ibm-mas-iot-dm.yml , ibm-mas-iot-dsc.yml , ibm-mas-iot-edgeconfig.yml , ibm-mas-iot-fpl.yml , ibm-mas-iot-guardian.yml , ibm-mas-iot-mbgx.yml , ibm-mas-iot-mfgx.yml , ibm-mas-iot-monitor.yml , ibm-mas-iot-orgmgmt.yml , ibm-mas-iot-provision.yml , ibm-mas-iot-registry.yml , ibm-mas-iot-state.yml , ibm-mas-iot-webui.yml . The content of the configuration file should be the yaml block that you wish to be inserted into the IoT CR. ibm-mas-iot-iot.yml will be inserted into the main IoT CR spec -> podTemplates whereas the component ones e.g, ibm-mas-iot-actions.yml will be under spec -> components -> {componentName} -> podTemplates . The ibm-mas-iot operator will then pass this on to the corresponding component CR when available. This is an example of one of the components (actions) - refer to the BestEfforts reference configuration in the MAS CLI . For full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None","title":"mas_pod_templates_dir (IoT)"},{"location":"roles/suite_app_install/#manage-configuration","text":"","title":"Manage Configuration"},{"location":"roles/suite_app_install/#mas_pod_templates_dir-manage","text":"This role will look for a configuration file named ibm-mas-manage-manageapp.yml . The content of the configuration file should be the yaml block that you wish to be inserted into the ManageApp CR. ibm-mas-manage-manageapp.yml will be inserted into the ManageApp CR spec -> podTemplates . The ibm-mas-manage operator will then pass this on to the corresponding deployments when available. For full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None","title":"mas_pod_templates_dir (Manage)"},{"location":"roles/suite_app_install/#mas_appws_upgrade_type","text":"Specifies the upgrade strategy for Manage workspace database schema updates. Optional Environment Variable: MAS_APPWS_UPGRADE_TYPE Default: regularUpgrade Purpose : Controls how Manage performs database schema upgrades during application updates, balancing between system downtime and upgrade complexity. Different strategies offer different trade-offs between availability and upgrade duration. When to use : - Use regularUpgrade for standard upgrades with planned downtime windows - Use onlineUpgrade to minimize downtime during upgrades (requires more resources and time) - Consider your maintenance window constraints and availability requirements Valid values : - regularUpgrade - Standard upgrade with full system downtime - onlineUpgrade - Minimized downtime upgrade (requires additional resources) Impact : - regularUpgrade : Shorter upgrade time but requires full system downtime - onlineUpgrade : Longer upgrade time but minimizes system downtime; requires additional database resources during upgrade Related variables : Applies to Manage application upgrades only. Note : For full documentation of upgrade types and requirements, refer to the Manage Upgrade information in the product documentation. Online upgrades require careful planning and additional resources.","title":"mas_appws_upgrade_type"},{"location":"roles/suite_app_install/#monitor-configuration","text":"","title":"Monitor Configuration"},{"location":"roles/suite_app_install/#mas_app_settings_monitor_deployment_size","text":"Specifies the deployment size profile for MAS Monitor, which determines resource allocations and scaling characteristics. Optional Environment Variable: MAS_APP_SETTINGS_MONITOR_DEPLOYMENT_SIZE Default: dev Purpose : Controls the resource allocation profile for Monitor components, affecting CPU, memory, and replica counts. Different sizes are optimized for different workload scales and environments. When to use : - Use dev for development and testing environments with minimal resource requirements - Use small for production environments with moderate monitoring requirements - Use large for production environments with extensive monitoring, dashboards, or high data volumes Valid values : dev , small , large Impact : - dev : Minimal resources, suitable for development only - small : Moderate resources, suitable for small to medium production deployments - large : Maximum resources, suitable for large-scale production deployments with extensive monitoring Related variables : Affects overall cluster resource consumption for Monitor components. Note : Application Support: Monitor 8.6+ . Ensure your cluster has sufficient resources for the selected size. Cannot be easily changed after deployment without reinstallation.","title":"mas_app_settings_monitor_deployment_size"},{"location":"roles/suite_app_install/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Choose which catalog source to use for the MAS install, default to the IBM operator catalog mas_app_catalog_source: \"{{ lookup('env', 'MAS_APP_CATALOG_SOURCE') | default('ibm-operator-catalog', true) }}\" # Which MAS channel to subscribe to mas_app_channel: \"{{ lookup('env', 'MAS_APP_CHANNEL') | default('8.x', true) }}\" # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS configuration - Entitlement mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') | default('cp', true) }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" # Application Configuration - Spec mas_app_spec: bindings: jdbc: system mongo: system kafka: system settings: messagesight: storage: class: block1000p size: 100Gi deployment: size: medium # Application Configuration - Install Plan mas_app_plan: \"{{ lookup('env', 'MAS_APP_PLAN') | default('full', true) }}\" roles: - ibm.mas_devops.suite_app_install","title":"Example Playbook"},{"location":"roles/suite_app_install/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_rollback/","text":"suite_app_rollback \u00a4 Roll back Maximo Application Suite applications to earlier versions when issues are encountered after upgrades. This role provides a safe rollback mechanism with built-in compatibility validation to ensure the target version is supported for rollback from the current version and compatible with the running MAS core platform. Important : Currently designed for Manage application only. Rollback capability is available in MAS 8.7 and later versions. Each version defines a set of supported rollback targets (e.g., from 8.7.x to 8.7.0). The role performs comprehensive validation before rollback and monitors the reconciliation process to ensure successful completion at the target version. Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier where the application rollback will be performed. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies the target MAS instance containing the application to be rolled back. When to use : Always required. Must match an existing MAS instance with the application installed. Valid values : Valid MAS instance ID (typically 3-12 lowercase alphanumeric characters). Impact : The rollback operation will target the application in the mas-<instance-id>-<app-id> namespace. Incorrect instance ID will cause the role to fail. Related variables : mas_app_id , mas_app_version Notes : - Must match the instance ID used during MAS installation - Case-sensitive value - Ensure you have proper backups before performing rollback operations mas_app_id \u00a4 MAS application identifier to be rolled back. Required Environment Variable: MAS_APP_ID Default: None Purpose : Specifies which MAS application will be rolled back to an earlier version. When to use : Always required. Currently only manage is supported. Valid values : - manage - Maximo Manage application (only supported value currently) Impact : Determines which application namespace and resources will be targeted for rollback. Only Manage application rollback is currently implemented. Related variables : mas_instance_id , mas_app_version Notes : - Critical : Only manage is supported in current implementation - Future versions may support additional applications - Application must be installed and running before rollback - Rollback capability requires MAS 8.7 or later mas_app_version \u00a4 Target version to roll back to. Required when rollback_mas_app or verify_app_version is true Environment Variable: MAS_APP_VERSION Default: None Purpose : Specifies the exact version to which the application should be rolled back. When to use : Required for rollback operations or version verification. Must be a version that is supported for rollback from the current version. Valid values : Valid MAS application version string (e.g., 8.7.0 , 8.7.1 , 8.8.0 ). Must be: - A version supported for rollback from the current version - Compatible with the running MAS core platform version - Available in the configured catalog source Impact : The application will be rolled back to this specific version. Built-in validation prevents unsupported rollback paths. Related variables : mas_app_id , rollback_mas_app , skip_compatibility_check Notes : - Each MAS version defines supported rollback targets - Cannot roll back across major version boundaries in most cases - Verify rollback compatibility in IBM documentation before proceeding - Consider database schema compatibility when rolling back - Always backup before rollback operations rollback_mas_app \u00a4 Enable or disable the actual rollback operation. Optional Environment Variable: ROLLBACK_MAS_APP Default: true Purpose : Controls whether the role performs the actual rollback operation or just validation checks. When to use : Set to true to perform rollback, false to only validate without making changes. Valid values : - true - Perform the rollback operation (default) - false - Validation only, no changes made Impact : When false , the role will validate compatibility but not modify the application. Useful for testing rollback feasibility before execution. Related variables : verify_app_version , mas_app_version Notes : - Default true means rollback will execute if validation passes - Set to false for dry-run validation before actual rollback - Combine with verify_app_version for post-rollback verification - Validation includes compatibility checks and version availability verify_app_version \u00a4 Enable verification that the application is at the specified version. Optional Environment Variable: VERIFY_APP_VERSION Default: false Purpose : Checks whether the current application version matches the specified target version, useful for post-rollback verification. When to use : Set to true to verify the application version without performing rollback. Useful after rollback to confirm success. Valid values : - true - Verify current version matches mas_app_version - false - Skip version verification (default) Impact : When true , the role will check if the application is at the specified version and report success or failure. No changes are made to the application. Related variables : mas_app_version , rollback_mas_app Notes : - Useful for post-rollback validation in automation pipelines - Can be used independently of rollback operation - Set rollback_mas_app=false and verify_app_version=true for verification-only mode - Helps confirm rollback completed successfully mas_rollback_dryrun \u00a4 Enable dry-run mode for rollback operations. Optional Environment Variable: MAS_ROLLBACK_DRYRUN Default: false Purpose : Performs all validation checks without actually executing the rollback, allowing you to test the rollback process safely. When to use : Set to true when you want to validate rollback feasibility without making any changes to the system. Valid values : - true - Dry-run mode, validation only - false - Normal operation (default) Impact : When true , all compatibility checks and validations are performed, but no changes are made to the application or cluster. Related variables : rollback_mas_app , skip_compatibility_check Notes : - Recommended to run in dry-run mode first before actual rollback - Helps identify potential issues before committing to rollback - All validation errors will be reported without risk - Does not affect the running application skip_compatibility_check \u00a4 Bypass compatibility validation checks. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: false Purpose : Allows skipping the built-in compatibility validation between the target rollback version and the current MAS core platform version. When to use : Use with extreme caution . Only skip checks when you have verified compatibility through other means or are following IBM support guidance. Valid values : - true - Skip compatibility validation (dangerous) - false - Perform all compatibility checks (default, recommended) Impact : When true , the role will not validate version compatibility, potentially allowing unsupported rollback operations that could cause system instability or data corruption. Related variables : mas_app_version , rollback_mas_app Notes : - Warning : Skipping compatibility checks can lead to unsupported configurations - Only use when explicitly instructed by IBM support - May result in application failures or data issues - Default false is strongly recommended for production environments - Compatibility checks protect against unsupported rollback paths Example Playbook \u00a4 Automatic Target Selection \u00a4 Running this playbook will rollback Manage Application to the 8.7.1 version. If you run this playbook when you are already on the same version it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: manage mas_app_version: 8.7.1 roles: - ibm.mas_devops.suite_app_rollback Verify Manage App Version \u00a4 Running this playbook will attempt to verify the current version of Manage Application matches with the specified version. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: manage mas_app_version: 8.7.1 rollback_mas_app: False verify_app_version: True roles: - ibm.mas_devops.suite_app_rollback License \u00a4 EPL-2.0","title":"suite_app_rollback"},{"location":"roles/suite_app_rollback/#suite_app_rollback","text":"Roll back Maximo Application Suite applications to earlier versions when issues are encountered after upgrades. This role provides a safe rollback mechanism with built-in compatibility validation to ensure the target version is supported for rollback from the current version and compatible with the running MAS core platform. Important : Currently designed for Manage application only. Rollback capability is available in MAS 8.7 and later versions. Each version defines a set of supported rollback targets (e.g., from 8.7.x to 8.7.0). The role performs comprehensive validation before rollback and monitors the reconciliation process to ensure successful completion at the target version.","title":"suite_app_rollback"},{"location":"roles/suite_app_rollback/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_rollback/#mas_instance_id","text":"MAS instance identifier where the application rollback will be performed. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies the target MAS instance containing the application to be rolled back. When to use : Always required. Must match an existing MAS instance with the application installed. Valid values : Valid MAS instance ID (typically 3-12 lowercase alphanumeric characters). Impact : The rollback operation will target the application in the mas-<instance-id>-<app-id> namespace. Incorrect instance ID will cause the role to fail. Related variables : mas_app_id , mas_app_version Notes : - Must match the instance ID used during MAS installation - Case-sensitive value - Ensure you have proper backups before performing rollback operations","title":"mas_instance_id"},{"location":"roles/suite_app_rollback/#mas_app_id","text":"MAS application identifier to be rolled back. Required Environment Variable: MAS_APP_ID Default: None Purpose : Specifies which MAS application will be rolled back to an earlier version. When to use : Always required. Currently only manage is supported. Valid values : - manage - Maximo Manage application (only supported value currently) Impact : Determines which application namespace and resources will be targeted for rollback. Only Manage application rollback is currently implemented. Related variables : mas_instance_id , mas_app_version Notes : - Critical : Only manage is supported in current implementation - Future versions may support additional applications - Application must be installed and running before rollback - Rollback capability requires MAS 8.7 or later","title":"mas_app_id"},{"location":"roles/suite_app_rollback/#mas_app_version","text":"Target version to roll back to. Required when rollback_mas_app or verify_app_version is true Environment Variable: MAS_APP_VERSION Default: None Purpose : Specifies the exact version to which the application should be rolled back. When to use : Required for rollback operations or version verification. Must be a version that is supported for rollback from the current version. Valid values : Valid MAS application version string (e.g., 8.7.0 , 8.7.1 , 8.8.0 ). Must be: - A version supported for rollback from the current version - Compatible with the running MAS core platform version - Available in the configured catalog source Impact : The application will be rolled back to this specific version. Built-in validation prevents unsupported rollback paths. Related variables : mas_app_id , rollback_mas_app , skip_compatibility_check Notes : - Each MAS version defines supported rollback targets - Cannot roll back across major version boundaries in most cases - Verify rollback compatibility in IBM documentation before proceeding - Consider database schema compatibility when rolling back - Always backup before rollback operations","title":"mas_app_version"},{"location":"roles/suite_app_rollback/#rollback_mas_app","text":"Enable or disable the actual rollback operation. Optional Environment Variable: ROLLBACK_MAS_APP Default: true Purpose : Controls whether the role performs the actual rollback operation or just validation checks. When to use : Set to true to perform rollback, false to only validate without making changes. Valid values : - true - Perform the rollback operation (default) - false - Validation only, no changes made Impact : When false , the role will validate compatibility but not modify the application. Useful for testing rollback feasibility before execution. Related variables : verify_app_version , mas_app_version Notes : - Default true means rollback will execute if validation passes - Set to false for dry-run validation before actual rollback - Combine with verify_app_version for post-rollback verification - Validation includes compatibility checks and version availability","title":"rollback_mas_app"},{"location":"roles/suite_app_rollback/#verify_app_version","text":"Enable verification that the application is at the specified version. Optional Environment Variable: VERIFY_APP_VERSION Default: false Purpose : Checks whether the current application version matches the specified target version, useful for post-rollback verification. When to use : Set to true to verify the application version without performing rollback. Useful after rollback to confirm success. Valid values : - true - Verify current version matches mas_app_version - false - Skip version verification (default) Impact : When true , the role will check if the application is at the specified version and report success or failure. No changes are made to the application. Related variables : mas_app_version , rollback_mas_app Notes : - Useful for post-rollback validation in automation pipelines - Can be used independently of rollback operation - Set rollback_mas_app=false and verify_app_version=true for verification-only mode - Helps confirm rollback completed successfully","title":"verify_app_version"},{"location":"roles/suite_app_rollback/#mas_rollback_dryrun","text":"Enable dry-run mode for rollback operations. Optional Environment Variable: MAS_ROLLBACK_DRYRUN Default: false Purpose : Performs all validation checks without actually executing the rollback, allowing you to test the rollback process safely. When to use : Set to true when you want to validate rollback feasibility without making any changes to the system. Valid values : - true - Dry-run mode, validation only - false - Normal operation (default) Impact : When true , all compatibility checks and validations are performed, but no changes are made to the application or cluster. Related variables : rollback_mas_app , skip_compatibility_check Notes : - Recommended to run in dry-run mode first before actual rollback - Helps identify potential issues before committing to rollback - All validation errors will be reported without risk - Does not affect the running application","title":"mas_rollback_dryrun"},{"location":"roles/suite_app_rollback/#skip_compatibility_check","text":"Bypass compatibility validation checks. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: false Purpose : Allows skipping the built-in compatibility validation between the target rollback version and the current MAS core platform version. When to use : Use with extreme caution . Only skip checks when you have verified compatibility through other means or are following IBM support guidance. Valid values : - true - Skip compatibility validation (dangerous) - false - Perform all compatibility checks (default, recommended) Impact : When true , the role will not validate version compatibility, potentially allowing unsupported rollback operations that could cause system instability or data corruption. Related variables : mas_app_version , rollback_mas_app Notes : - Warning : Skipping compatibility checks can lead to unsupported configurations - Only use when explicitly instructed by IBM support - May result in application failures or data issues - Default false is strongly recommended for production environments - Compatibility checks protect against unsupported rollback paths","title":"skip_compatibility_check"},{"location":"roles/suite_app_rollback/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_app_rollback/#automatic-target-selection","text":"Running this playbook will rollback Manage Application to the 8.7.1 version. If you run this playbook when you are already on the same version it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: manage mas_app_version: 8.7.1 roles: - ibm.mas_devops.suite_app_rollback","title":"Automatic Target Selection"},{"location":"roles/suite_app_rollback/#verify-manage-app-version","text":"Running this playbook will attempt to verify the current version of Manage Application matches with the specified version. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: manage mas_app_version: 8.7.1 rollback_mas_app: False verify_app_version: True roles: - ibm.mas_devops.suite_app_rollback","title":"Verify Manage App Version"},{"location":"roles/suite_app_rollback/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_uninstall/","text":"suite_app_uninstall \u00a4 Uninstall Maximo Application Suite applications from a MAS instance. This role provides a clean removal process for MAS applications, removing all application-specific resources, configurations, and namespaces while preserving the MAS core platform and other applications. The role supports uninstallation of all MAS applications including Assist, Health, IoT, Manage, Monitor, Optimizer, Predict, Visual Inspection, and Facilities (TRIRIGA). Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier from which the application will be uninstalled. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies the target MAS instance containing the application to be removed. When to use : Always required. Must match an existing MAS instance with the application installed. Valid values : Valid MAS instance ID (typically 3-12 lowercase alphanumeric characters). Impact : The uninstall operation will target the application in the mas-<instance-id>-<app-id> namespace. All resources in this namespace will be removed. Related variables : mas_app_id Notes : - Must match the instance ID used during MAS installation - Case-sensitive value - Critical : Uninstallation is permanent and cannot be undone - Always backup application data before uninstalling - Other applications in the same instance are not affected mas_app_id \u00a4 MAS application identifier to be uninstalled. Required Environment Variable: MAS_APP_ID Default: None Purpose : Specifies which MAS application will be removed from the instance. When to use : Always required. Must be a valid, installed application. Valid values : - assist - Maximo Assist - health - Maximo Health (formerly Predict) - iot - Maximo IoT - manage - Maximo Manage - monitor - Maximo Monitor - optimizer - Maximo Optimizer - predict - Maximo Predict - visualinspection - Maximo Visual Inspection - facilities - Maximo Facilities (TRIRIGA) Impact : The specified application and all its resources will be permanently removed. This includes: - Application namespace and all contained resources - Application custom resources (e.g., ManageApp, IoTApp) - Application-specific configurations - Application workspaces (if any) - Application data stored in the namespace Related variables : mas_instance_id Notes : - Warning : Uninstallation is irreversible - Application data in external databases is NOT removed - Backup all critical data before uninstalling - Application must be installed before it can be uninstalled - Uninstalling Manage does not remove the Manage database - Consider using application deactivation instead of uninstall if you may need to reinstall - Uninstalling an application does not affect other applications in the same instance Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" roles: - ibm.mas_devops.suite_app_uninstall License \u00a4 EPL-2.0","title":"suite_app_uninstall"},{"location":"roles/suite_app_uninstall/#suite_app_uninstall","text":"Uninstall Maximo Application Suite applications from a MAS instance. This role provides a clean removal process for MAS applications, removing all application-specific resources, configurations, and namespaces while preserving the MAS core platform and other applications. The role supports uninstallation of all MAS applications including Assist, Health, IoT, Manage, Monitor, Optimizer, Predict, Visual Inspection, and Facilities (TRIRIGA).","title":"suite_app_uninstall"},{"location":"roles/suite_app_uninstall/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_uninstall/#mas_instance_id","text":"MAS instance identifier from which the application will be uninstalled. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies the target MAS instance containing the application to be removed. When to use : Always required. Must match an existing MAS instance with the application installed. Valid values : Valid MAS instance ID (typically 3-12 lowercase alphanumeric characters). Impact : The uninstall operation will target the application in the mas-<instance-id>-<app-id> namespace. All resources in this namespace will be removed. Related variables : mas_app_id Notes : - Must match the instance ID used during MAS installation - Case-sensitive value - Critical : Uninstallation is permanent and cannot be undone - Always backup application data before uninstalling - Other applications in the same instance are not affected","title":"mas_instance_id"},{"location":"roles/suite_app_uninstall/#mas_app_id","text":"MAS application identifier to be uninstalled. Required Environment Variable: MAS_APP_ID Default: None Purpose : Specifies which MAS application will be removed from the instance. When to use : Always required. Must be a valid, installed application. Valid values : - assist - Maximo Assist - health - Maximo Health (formerly Predict) - iot - Maximo IoT - manage - Maximo Manage - monitor - Maximo Monitor - optimizer - Maximo Optimizer - predict - Maximo Predict - visualinspection - Maximo Visual Inspection - facilities - Maximo Facilities (TRIRIGA) Impact : The specified application and all its resources will be permanently removed. This includes: - Application namespace and all contained resources - Application custom resources (e.g., ManageApp, IoTApp) - Application-specific configurations - Application workspaces (if any) - Application data stored in the namespace Related variables : mas_instance_id Notes : - Warning : Uninstallation is irreversible - Application data in external databases is NOT removed - Backup all critical data before uninstalling - Application must be installed before it can be uninstalled - Uninstalling Manage does not remove the Manage database - Consider using application deactivation instead of uninstall if you may need to reinstall - Uninstalling an application does not affect other applications in the same instance","title":"mas_app_id"},{"location":"roles/suite_app_uninstall/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" roles: - ibm.mas_devops.suite_app_uninstall","title":"Example Playbook"},{"location":"roles/suite_app_uninstall/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_upgrade/","text":"suite_app_upgrade \u00a4 This role will upgrade the subscription channel for an installed MAS application after validating: That the application is installed and in a healthy state That the new version of the application can be upgraded to from the existing version That the new version of the application is compatible with the running MAS core platform Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier for application upgrade. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the application to upgrade. Used to locate and validate the application installation. When to use : - Always required for application upgrades - Must match the instance ID from MAS installation - Used to validate application health before upgrade Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance's application will be upgraded. Incorrect instance ID will cause upgrade to fail. Related variables : - mas_app_id : Application to upgrade in this instance - mas_app_channel : Target upgrade channel Note : The role validates that the application is installed and healthy in this instance before proceeding with the upgrade. mas_app_id \u00a4 MAS application identifier to upgrade. Required Environment Variable: MAS_APP_ID Default: None Purpose : Specifies which MAS application to upgrade. The role validates the application is installed and healthy before upgrading. When to use : - Always required for application upgrades - Must match an installed application in the instance - Application must be in healthy state for upgrade Valid values : Valid MAS application ID (e.g., iot , manage , monitor , predict , health , assist , visualinspection , optimizer ) Impact : Determines which application's subscription channel will be upgraded. The role validates compatibility and upgrade path before proceeding. Related variables : - mas_instance_id : Instance containing this application - mas_app_channel : Target upgrade channel for this application Note : The role performs comprehensive validation including application health, upgrade path compatibility, and MAS core platform compatibility before upgrading. mas_app_channel \u00a4 Target subscription channel for application upgrade. Required Environment Variable: MAS_APP_CHANNEL Default: None Purpose : Specifies the target subscription channel to upgrade the application to. The role validates that a supported upgrade path exists from the current version. When to use : - Always required for application upgrades - Must be a valid channel for the application - Should represent a newer version than currently installed Valid values : Valid subscription channel for the application (e.g., 8.5.x , 8.6.x , 8.7.x for IoT; 8.4.x , 8.5.x , 8.6.x for Manage) Impact : Determines the target version for the application upgrade. The role validates: - Upgrade path compatibility (can upgrade from current to target) - MAS core platform compatibility (target version works with current MAS) - Application health before proceeding Related variables : - mas_app_id : Application being upgraded - skip_compatibility_check : Whether to skip validation (not recommended) Note : Built-in validation ensures safe upgrades. The role will fail if the upgrade path is not supported or if the target version is incompatible with the current MAS core platform. mas_upgrade_dryrun \u00a4 Dry-run mode for upgrade validation only. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: false Purpose : Enables dry-run mode where the role performs all validation checks without making any changes to the installation. Useful for testing upgrade paths. When to use : - Set to true to validate upgrade without executing it - Use for testing and planning upgrade paths - Recommended before production upgrades - Leave as false (default) to perform actual upgrade Valid values : true , false Impact : - true : Performs validation only (health check, compatibility check, upgrade path validation) without modifying the installation - false : Performs validation and executes the upgrade if validation passes Related variables : - skip_compatibility_check : Controls whether compatibility validation is performed Note : Dry-run mode is highly recommended before production upgrades to identify potential issues. All validation checks are performed, but no changes are made to the subscription channel or application. skip_compatibility_check \u00a4 Skip compatibility validation before upgrade. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: false Purpose : Controls whether compatibility validation is performed before upgrade. Validation checks if the target channel is compatible with current MAS and application versions. When to use : - Leave as false (default) for safe upgrades with validation - Set to true only in exceptional cases (not recommended) - Use only when you have verified compatibility manually Valid values : true , false Impact : - false (default): Performs compatibility validation before upgrade (recommended) - true : Skips compatibility validation, allowing potentially incompatible upgrades Related variables : - mas_upgrade_dryrun : Controls whether upgrade is executed or only validated - mas_app_channel : Target channel being validated Note : WARNING - Skipping compatibility checks can lead to failed upgrades or unstable installations. Only skip validation if you have manually verified the upgrade path is supported. The default validation protects against incompatible upgrades. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: iot mas_app_channel: 8.5.x roles: - ibm.mas_devops.suite_app_upgrade License \u00a4 EPL-2.0","title":"suite_app_upgrade"},{"location":"roles/suite_app_upgrade/#suite_app_upgrade","text":"This role will upgrade the subscription channel for an installed MAS application after validating: That the application is installed and in a healthy state That the new version of the application can be upgraded to from the existing version That the new version of the application is compatible with the running MAS core platform","title":"suite_app_upgrade"},{"location":"roles/suite_app_upgrade/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_upgrade/#mas_instance_id","text":"MAS instance identifier for application upgrade. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the application to upgrade. Used to locate and validate the application installation. When to use : - Always required for application upgrades - Must match the instance ID from MAS installation - Used to validate application health before upgrade Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance's application will be upgraded. Incorrect instance ID will cause upgrade to fail. Related variables : - mas_app_id : Application to upgrade in this instance - mas_app_channel : Target upgrade channel Note : The role validates that the application is installed and healthy in this instance before proceeding with the upgrade.","title":"mas_instance_id"},{"location":"roles/suite_app_upgrade/#mas_app_id","text":"MAS application identifier to upgrade. Required Environment Variable: MAS_APP_ID Default: None Purpose : Specifies which MAS application to upgrade. The role validates the application is installed and healthy before upgrading. When to use : - Always required for application upgrades - Must match an installed application in the instance - Application must be in healthy state for upgrade Valid values : Valid MAS application ID (e.g., iot , manage , monitor , predict , health , assist , visualinspection , optimizer ) Impact : Determines which application's subscription channel will be upgraded. The role validates compatibility and upgrade path before proceeding. Related variables : - mas_instance_id : Instance containing this application - mas_app_channel : Target upgrade channel for this application Note : The role performs comprehensive validation including application health, upgrade path compatibility, and MAS core platform compatibility before upgrading.","title":"mas_app_id"},{"location":"roles/suite_app_upgrade/#mas_app_channel","text":"Target subscription channel for application upgrade. Required Environment Variable: MAS_APP_CHANNEL Default: None Purpose : Specifies the target subscription channel to upgrade the application to. The role validates that a supported upgrade path exists from the current version. When to use : - Always required for application upgrades - Must be a valid channel for the application - Should represent a newer version than currently installed Valid values : Valid subscription channel for the application (e.g., 8.5.x , 8.6.x , 8.7.x for IoT; 8.4.x , 8.5.x , 8.6.x for Manage) Impact : Determines the target version for the application upgrade. The role validates: - Upgrade path compatibility (can upgrade from current to target) - MAS core platform compatibility (target version works with current MAS) - Application health before proceeding Related variables : - mas_app_id : Application being upgraded - skip_compatibility_check : Whether to skip validation (not recommended) Note : Built-in validation ensures safe upgrades. The role will fail if the upgrade path is not supported or if the target version is incompatible with the current MAS core platform.","title":"mas_app_channel"},{"location":"roles/suite_app_upgrade/#mas_upgrade_dryrun","text":"Dry-run mode for upgrade validation only. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: false Purpose : Enables dry-run mode where the role performs all validation checks without making any changes to the installation. Useful for testing upgrade paths. When to use : - Set to true to validate upgrade without executing it - Use for testing and planning upgrade paths - Recommended before production upgrades - Leave as false (default) to perform actual upgrade Valid values : true , false Impact : - true : Performs validation only (health check, compatibility check, upgrade path validation) without modifying the installation - false : Performs validation and executes the upgrade if validation passes Related variables : - skip_compatibility_check : Controls whether compatibility validation is performed Note : Dry-run mode is highly recommended before production upgrades to identify potential issues. All validation checks are performed, but no changes are made to the subscription channel or application.","title":"mas_upgrade_dryrun"},{"location":"roles/suite_app_upgrade/#skip_compatibility_check","text":"Skip compatibility validation before upgrade. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: false Purpose : Controls whether compatibility validation is performed before upgrade. Validation checks if the target channel is compatible with current MAS and application versions. When to use : - Leave as false (default) for safe upgrades with validation - Set to true only in exceptional cases (not recommended) - Use only when you have verified compatibility manually Valid values : true , false Impact : - false (default): Performs compatibility validation before upgrade (recommended) - true : Skips compatibility validation, allowing potentially incompatible upgrades Related variables : - mas_upgrade_dryrun : Controls whether upgrade is executed or only validated - mas_app_channel : Target channel being validated Note : WARNING - Skipping compatibility checks can lead to failed upgrades or unstable installations. Only skip validation if you have manually verified the upgrade path is supported. The default validation protects against incompatible upgrades.","title":"skip_compatibility_check"},{"location":"roles/suite_app_upgrade/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: iot mas_app_channel: 8.5.x roles: - ibm.mas_devops.suite_app_upgrade","title":"Example Playbook"},{"location":"roles/suite_app_upgrade/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_backup_restore/","text":"suite_backup_restore \u00a4 This role supports backing up and restoring MAS Core namespace resources; supports creating on-demand or scheduled backup jobs for taking full or incremental backups, and optionally creating Kubernetes jobs for running the backup/restore process. Important A backup can only be restored to an instance with the same MAS instance ID. Role Variables \u00a4 General \u00a4 masbr_action \u00a4 Action to perform on MAS Core namespace. Required Environment Variable: MASBR_ACTION Default: None Purpose : Specifies whether to create a backup of MAS Core namespace resources or restore from a previous backup. When to use : - Set to backup to create a backup of MAS Core namespace resources - Set to restore to restore MAS Core namespace from a backup - Always required to indicate the operation type Valid values : backup , restore Impact : - backup : Creates backup job (on-demand or scheduled) for MAS Core namespace resources - restore : Restores MAS Core namespace from specified backup version Related variables : - masbr_restore_from_version : Required when action is restore - masbr_backup_schedule : Optional for scheduled backups - mas_instance_id : Instance to backup/restore Note : IMPORTANT - This role handles MAS Core namespace resources only. MongoDB data must be backed up/restored separately using the mongodb role. A backup can only be restored to an instance with the same MAS instance ID. mas_instance_id \u00a4 MAS instance identifier for backup/restore operations. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to backup or restore. Used to locate MAS Core namespace resources and ensure restore compatibility. When to use : - Always required for backup and restore operations - Must match the instance ID from MAS installation - Critical for restore operations (must match original backup instance ID) Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , main ) Impact : Determines which MAS instance's Core namespace will be backed up or restored. CRITICAL - A backup can only be restored to an instance with the same MAS instance ID. Related variables : - masbr_action : Whether backing up or restoring this instance - masbr_restore_from_version : Backup version to restore (for restore action) Note : IMPORTANT - The instance ID must match between backup and restore operations. Attempting to restore a backup to an instance with a different ID will fail. masbr_confirm_cluster \u00a4 Confirm cluster connection before backup/restore. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false Purpose : Controls whether the role prompts for confirmation of the currently connected cluster before executing backup or restore operations. Safety feature to prevent accidental operations on wrong cluster. When to use : - Set to true for interactive confirmation (recommended for production) - Leave as false (default) for automated/non-interactive operations - Use true when manually running backup/restore to verify correct cluster Valid values : true , false Impact : - true : Role prompts for cluster confirmation before proceeding - false : Role proceeds without confirmation (suitable for automation) Related variables : - masbr_action : Operation requiring cluster confirmation Note : Enabling cluster confirmation is recommended for manual operations, especially in production environments, to prevent accidental backup/restore on the wrong cluster. masbr_copy_timeout_sec \u00a4 File transfer timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) Purpose : Specifies the maximum time allowed for transferring backup files between cluster and local storage. Prevents operations from hanging indefinitely. When to use : - Use default (12 hours) for most deployments - Increase for very large backups or slow network connections - Decrease for smaller backups to fail faster on issues Valid values : Positive integer (seconds), e.g., 3600 (1 hour), 43200 (12 hours), 86400 (24 hours) Impact : Operations exceeding this timeout will fail. Insufficient timeout for large backups will cause failures. Excessive timeout delays error detection. Related variables : - masbr_storage_local_folder : Destination for file transfers Note : The default 12 hours is suitable for most deployments. Adjust based on backup size and network speed. Monitor actual transfer times to optimize this setting. masbr_job_timezone \u00a4 Time zone for scheduled backup jobs. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: UTC Purpose : Specifies the time zone for scheduled backup CronJobs. Ensures backups run at the intended local time rather than UTC. When to use : - Leave unset to use UTC (default) - Set when you need backups to run at specific local times - Only applies to scheduled backups (when masbr_backup_schedule is set) Valid values : Valid tz database time zone (e.g., America/New_York , Europe/London , Asia/Tokyo ) Impact : Determines when scheduled backups execute. Incorrect time zone may cause backups to run at unexpected times. Related variables : - masbr_backup_schedule : Cron expression interpreted in this time zone Note : Only relevant for scheduled backups. On-demand backups ignore this setting. Use standard tz database names (e.g., America/New_York , not EST ). masbr_storage_local_folder \u00a4 Local filesystem path for backup storage. Required Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None Purpose : Specifies the local filesystem path where backup files are stored (for backups) or retrieved from (for restores). This is the persistent storage location for backup data. When to use : - Always required for backup and restore operations - Must be accessible from the system running the role - Should have sufficient space for backup files - Must be persistent across operations for restore capability Valid values : Absolute filesystem path (e.g., /tmp/masbr , /backup/mas , /mnt/backup ) Impact : Backup files are written to or read from this location. Insufficient space will cause backup failures. Path must exist and be writable. Related variables : - masbr_copy_timeout_sec : Timeout for transferring files to/from this location - masbr_restore_from_version : Backup version stored in this location Note : Ensure the path has sufficient disk space for backups. For production, use a dedicated backup volume with appropriate retention policies. The path must be accessible during both backup and restore operations. Backup \u00a4 masbr_backup_schedule \u00a4 Cron expression for scheduled backups. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None (on-demand backup) Purpose : Defines a schedule for automatic recurring backups using Cron syntax. When set, creates a Kubernetes CronJob for automated backups. When to use : - Leave unset for on-demand backups (manual execution) - Set to create scheduled/recurring backups - Use for automated backup strategies Valid values : Valid Cron expression (e.g., 0 2 * * * for daily at 2 AM, 0 2 * * 0 for weekly on Sunday at 2 AM) Impact : - When set: Creates a Kubernetes CronJob that runs backups automatically on schedule - When unset: Creates an on-demand backup job that runs immediately Related variables : - masbr_job_timezone : Time zone for interpreting the cron schedule - masbr_action : Must be backup for scheduled backups Note : Scheduled backups only apply when masbr_action=backup . The cron expression is interpreted in the time zone specified by masbr_job_timezone (defaults to UTC). Common patterns: 0 2 * * * (daily 2 AM), 0 2 * * 0 (weekly Sunday 2 AM), 0 2 1 * * (monthly 1st at 2 AM). Restore \u00a4 masbr_restore_from_version \u00a4 Backup version timestamp for restore operations. Required (when masbr_action=restore ) Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Purpose : Specifies which backup version to restore from. The version is a timestamp identifying a specific backup. When to use : - Required when masbr_action=restore - Not used for backup operations - Must match an existing backup version in storage Valid values : Timestamp in YYYYMMDDHHMMSS format (e.g., 20240621021316 for June 21, 2024 at 02:13:16) Impact : Determines which backup is restored. Incorrect or non-existent version will cause restore to fail. Related variables : - masbr_action : Must be restore for this variable to be used - masbr_storage_local_folder : Location where backup versions are stored - mas_instance_id : Must match the instance ID from the backup Note : The backup version timestamp is generated automatically during backup creation. List available backups in masbr_storage_local_folder to find valid version timestamps. IMPORTANT - The backup can only be restored to an instance with the same MAS instance ID as the original backup. Example Playbook \u00a4 Backup \u00a4 Backup MAS Core namespace resources, note that this does not include backup of any data in MongoDb, see the backup action in the mongodb role. - hosts: localhost any_errors_fatal: true vars: masbr_action: backup mas_instance_id: main masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_backup_restore Restore \u00a4 Restore MAS Core namespace resources, note that this does not include backup of any data in MongoDb, see the restore action in the mongodb role. - hosts: localhost any_errors_fatal: true vars: masbr_action: restore masbr_restore_from_version: 20240621021316 mas_instance_id: main masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_backup_restore License \u00a4 EPL-2.0","title":"suite_backup_restore"},{"location":"roles/suite_backup_restore/#suite_backup_restore","text":"This role supports backing up and restoring MAS Core namespace resources; supports creating on-demand or scheduled backup jobs for taking full or incremental backups, and optionally creating Kubernetes jobs for running the backup/restore process. Important A backup can only be restored to an instance with the same MAS instance ID.","title":"suite_backup_restore"},{"location":"roles/suite_backup_restore/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_backup_restore/#general","text":"","title":"General"},{"location":"roles/suite_backup_restore/#masbr_action","text":"Action to perform on MAS Core namespace. Required Environment Variable: MASBR_ACTION Default: None Purpose : Specifies whether to create a backup of MAS Core namespace resources or restore from a previous backup. When to use : - Set to backup to create a backup of MAS Core namespace resources - Set to restore to restore MAS Core namespace from a backup - Always required to indicate the operation type Valid values : backup , restore Impact : - backup : Creates backup job (on-demand or scheduled) for MAS Core namespace resources - restore : Restores MAS Core namespace from specified backup version Related variables : - masbr_restore_from_version : Required when action is restore - masbr_backup_schedule : Optional for scheduled backups - mas_instance_id : Instance to backup/restore Note : IMPORTANT - This role handles MAS Core namespace resources only. MongoDB data must be backed up/restored separately using the mongodb role. A backup can only be restored to an instance with the same MAS instance ID.","title":"masbr_action"},{"location":"roles/suite_backup_restore/#mas_instance_id","text":"MAS instance identifier for backup/restore operations. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to backup or restore. Used to locate MAS Core namespace resources and ensure restore compatibility. When to use : - Always required for backup and restore operations - Must match the instance ID from MAS installation - Critical for restore operations (must match original backup instance ID) Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , main ) Impact : Determines which MAS instance's Core namespace will be backed up or restored. CRITICAL - A backup can only be restored to an instance with the same MAS instance ID. Related variables : - masbr_action : Whether backing up or restoring this instance - masbr_restore_from_version : Backup version to restore (for restore action) Note : IMPORTANT - The instance ID must match between backup and restore operations. Attempting to restore a backup to an instance with a different ID will fail.","title":"mas_instance_id"},{"location":"roles/suite_backup_restore/#masbr_confirm_cluster","text":"Confirm cluster connection before backup/restore. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false Purpose : Controls whether the role prompts for confirmation of the currently connected cluster before executing backup or restore operations. Safety feature to prevent accidental operations on wrong cluster. When to use : - Set to true for interactive confirmation (recommended for production) - Leave as false (default) for automated/non-interactive operations - Use true when manually running backup/restore to verify correct cluster Valid values : true , false Impact : - true : Role prompts for cluster confirmation before proceeding - false : Role proceeds without confirmation (suitable for automation) Related variables : - masbr_action : Operation requiring cluster confirmation Note : Enabling cluster confirmation is recommended for manual operations, especially in production environments, to prevent accidental backup/restore on the wrong cluster.","title":"masbr_confirm_cluster"},{"location":"roles/suite_backup_restore/#masbr_copy_timeout_sec","text":"File transfer timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) Purpose : Specifies the maximum time allowed for transferring backup files between cluster and local storage. Prevents operations from hanging indefinitely. When to use : - Use default (12 hours) for most deployments - Increase for very large backups or slow network connections - Decrease for smaller backups to fail faster on issues Valid values : Positive integer (seconds), e.g., 3600 (1 hour), 43200 (12 hours), 86400 (24 hours) Impact : Operations exceeding this timeout will fail. Insufficient timeout for large backups will cause failures. Excessive timeout delays error detection. Related variables : - masbr_storage_local_folder : Destination for file transfers Note : The default 12 hours is suitable for most deployments. Adjust based on backup size and network speed. Monitor actual transfer times to optimize this setting.","title":"masbr_copy_timeout_sec"},{"location":"roles/suite_backup_restore/#masbr_job_timezone","text":"Time zone for scheduled backup jobs. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: UTC Purpose : Specifies the time zone for scheduled backup CronJobs. Ensures backups run at the intended local time rather than UTC. When to use : - Leave unset to use UTC (default) - Set when you need backups to run at specific local times - Only applies to scheduled backups (when masbr_backup_schedule is set) Valid values : Valid tz database time zone (e.g., America/New_York , Europe/London , Asia/Tokyo ) Impact : Determines when scheduled backups execute. Incorrect time zone may cause backups to run at unexpected times. Related variables : - masbr_backup_schedule : Cron expression interpreted in this time zone Note : Only relevant for scheduled backups. On-demand backups ignore this setting. Use standard tz database names (e.g., America/New_York , not EST ).","title":"masbr_job_timezone"},{"location":"roles/suite_backup_restore/#masbr_storage_local_folder","text":"Local filesystem path for backup storage. Required Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None Purpose : Specifies the local filesystem path where backup files are stored (for backups) or retrieved from (for restores). This is the persistent storage location for backup data. When to use : - Always required for backup and restore operations - Must be accessible from the system running the role - Should have sufficient space for backup files - Must be persistent across operations for restore capability Valid values : Absolute filesystem path (e.g., /tmp/masbr , /backup/mas , /mnt/backup ) Impact : Backup files are written to or read from this location. Insufficient space will cause backup failures. Path must exist and be writable. Related variables : - masbr_copy_timeout_sec : Timeout for transferring files to/from this location - masbr_restore_from_version : Backup version stored in this location Note : Ensure the path has sufficient disk space for backups. For production, use a dedicated backup volume with appropriate retention policies. The path must be accessible during both backup and restore operations.","title":"masbr_storage_local_folder"},{"location":"roles/suite_backup_restore/#backup","text":"","title":"Backup"},{"location":"roles/suite_backup_restore/#masbr_backup_schedule","text":"Cron expression for scheduled backups. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None (on-demand backup) Purpose : Defines a schedule for automatic recurring backups using Cron syntax. When set, creates a Kubernetes CronJob for automated backups. When to use : - Leave unset for on-demand backups (manual execution) - Set to create scheduled/recurring backups - Use for automated backup strategies Valid values : Valid Cron expression (e.g., 0 2 * * * for daily at 2 AM, 0 2 * * 0 for weekly on Sunday at 2 AM) Impact : - When set: Creates a Kubernetes CronJob that runs backups automatically on schedule - When unset: Creates an on-demand backup job that runs immediately Related variables : - masbr_job_timezone : Time zone for interpreting the cron schedule - masbr_action : Must be backup for scheduled backups Note : Scheduled backups only apply when masbr_action=backup . The cron expression is interpreted in the time zone specified by masbr_job_timezone (defaults to UTC). Common patterns: 0 2 * * * (daily 2 AM), 0 2 * * 0 (weekly Sunday 2 AM), 0 2 1 * * (monthly 1st at 2 AM).","title":"masbr_backup_schedule"},{"location":"roles/suite_backup_restore/#restore","text":"","title":"Restore"},{"location":"roles/suite_backup_restore/#masbr_restore_from_version","text":"Backup version timestamp for restore operations. Required (when masbr_action=restore ) Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Purpose : Specifies which backup version to restore from. The version is a timestamp identifying a specific backup. When to use : - Required when masbr_action=restore - Not used for backup operations - Must match an existing backup version in storage Valid values : Timestamp in YYYYMMDDHHMMSS format (e.g., 20240621021316 for June 21, 2024 at 02:13:16) Impact : Determines which backup is restored. Incorrect or non-existent version will cause restore to fail. Related variables : - masbr_action : Must be restore for this variable to be used - masbr_storage_local_folder : Location where backup versions are stored - mas_instance_id : Must match the instance ID from the backup Note : The backup version timestamp is generated automatically during backup creation. List available backups in masbr_storage_local_folder to find valid version timestamps. IMPORTANT - The backup can only be restored to an instance with the same MAS instance ID as the original backup.","title":"masbr_restore_from_version"},{"location":"roles/suite_backup_restore/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_backup_restore/#backup_1","text":"Backup MAS Core namespace resources, note that this does not include backup of any data in MongoDb, see the backup action in the mongodb role. - hosts: localhost any_errors_fatal: true vars: masbr_action: backup mas_instance_id: main masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_backup_restore","title":"Backup"},{"location":"roles/suite_backup_restore/#restore_1","text":"Restore MAS Core namespace resources, note that this does not include backup of any data in MongoDb, see the restore action in the mongodb role. - hosts: localhost any_errors_fatal: true vars: masbr_action: restore masbr_restore_from_version: 20240621021316 mas_instance_id: main masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_backup_restore","title":"Restore"},{"location":"roles/suite_backup_restore/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_certs/","text":"suite_certs \u00a4 This role manages TLS certificates for Maximo Application Suite core platform and applications. It processes certificate files organized in a directory structure and creates TLS secrets in the appropriate namespaces. The role supports manual certificate management and optional integration with IBM Cloud Internet Services (CIS) for DNS management. Certificate Directory Structure Certificates must be organized in subdirectories named after the MAS component ( core , monitor , manage , iot , etc.). Each subdirectory must contain three mandatory files: tls.crt , tls.key , and ca.crt . What This Role Does \u00a4 Iterates through certificate subdirectories in $MAS_CONFIG_DIR/certs Validates presence of required certificate files ( tls.crt , tls.key , ca.crt ) Creates TLS secrets in appropriate MAS namespaces Optionally manages DNS CNAME records in IBM Cloud Internet Services Supports GitOps mode for generating configurations without applying them Certificate Directory Structure \u00a4 Required Structure \u00a4 $MAS_CONFIG_DIR/certs/ \u251c\u2500\u2500 core/ \u2502 \u251c\u2500\u2500 tls.crt \u2502 \u251c\u2500\u2500 tls.key \u2502 \u2514\u2500\u2500 ca.crt \u251c\u2500\u2500 monitor/ \u2502 \u251c\u2500\u2500 tls.crt \u2502 \u251c\u2500\u2500 tls.key \u2502 \u2514\u2500\u2500 ca.crt \u251c\u2500\u2500 manage/ \u2502 \u251c\u2500\u2500 tls.crt \u2502 \u251c\u2500\u2500 tls.key \u2502 \u2514\u2500\u2500 ca.crt \u2514\u2500\u2500 <app>/ \u251c\u2500\u2500 tls.crt \u251c\u2500\u2500 tls.key \u2514\u2500\u2500 ca.crt Supported Applications \u00a4 core - MAS core platform iot - IoT application monitor - Monitor application manage - Manage application health - Health application predict - Predict application assist - Assist application optimizer - Optimizer application visualinspection - Visual Inspection application facilities - Facilities application add - Add application arcgis - ArcGIS integration Warning All three certificate files ( tls.crt , tls.key , ca.crt ) are mandatory in each subdirectory. The role will fail if a subdirectory is empty or missing any required file. Secret Names TLS secret names for each application are defined in suite_certs/defaults/main.yml . Changes to secret names or adding new applications requires updating this file. Role Variables - General \u00a4 mas_instance_id \u00a4 MAS instance identifier for certificate management. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance the certificates are for. Used to construct namespace names and secret names. When to use : - Always required for certificate management - Must match the instance ID from MAS installation - Used in namespace and secret name construction Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines target namespaces for certificate secrets. Namespace format is mas-{instance_id}-{app} (e.g., mas-prod-core , mas-prod-monitor ). Related variables : - mas_config_dir : Root directory containing certificate subdirectories - mas_workspace_id : Required for workspace-specific apps (manage, health, facilities) Note : The instance ID is used to construct both namespace names and TLS secret names according to MAS naming conventions. mas_manual_cert_mgmt \u00a4 Enable manual certificate management mode. Optional Environment Variable: MAS_MANUAL_CERT_MGMT Default: false Purpose : Enables manual certificate management mode, allowing you to provide custom certificates instead of using cert-manager generated certificates. When to use : - Set to true when using custom/purchased certificates - Required for environments without cert-manager - Use when corporate policy requires specific certificate authorities - Necessary for custom domain certificates Valid values : true , false Impact : - true : Uses certificates from $MAS_CONFIG_DIR/certs directories - false : Relies on cert-manager for automatic certificate generation (default) Related variables : - mas_config_dir : Directory containing certificate files - dns_provider : Optional DNS management integration Note : When enabled, you must provide all required certificate files in the correct directory structure. This mode is essential for production environments with specific certificate requirements. mas_config_dir \u00a4 Configuration directory path. Required Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the root directory containing the certs subdirectory with certificate files organized by application. When to use : - Always required when using manual certificate management - Should point to a directory containing a certs subdirectory - Typically organized by instance ID Valid values : Valid local filesystem path (e.g., /home/user/masconfig , ~/masconfig/inst1 ) Impact : The role looks for certificates in $MAS_CONFIG_DIR/certs/{app}/ directories. All certificate processing is relative to this path. Related variables : - mas_instance_id : Instance these certificates are for - mas_manual_cert_mgmt : Must be enabled to use certificates Note : Organize by instance for clarity (e.g., /masconfig/inst1/certs/ , /masconfig/inst2/certs/ ). The certs subdirectory is automatically appended to this path. gitops \u00a4 Enable GitOps mode (generate only, no apply). Optional Environment Variable: GITOPS Default: false Purpose : When enabled, generates certificate secret configurations without applying them to the cluster. Useful for GitOps workflows where configurations are committed to Git and applied separately. When to use : - Set to true for GitOps/declarative workflows - Use when configurations should be reviewed before application - Helpful for version-controlled infrastructure - Required for environments with strict change control Valid values : true , false Impact : - true : Generates YAML configurations but doesn't create resources on cluster - false : Generates and applies configurations directly to cluster (default) Related variables : - mas_config_dir : Where generated configurations are saved Note : In GitOps mode, generated YAML files can be committed to Git and applied through your GitOps tooling (ArgoCD, Flux, etc.). Role Variables - CIS DNS Provider (Optional) \u00a4 Optional variables for users managing DNS with IBM Cloud Internet Services. When configured, this role automatically creates or updates CNAME records for MAS routes in your CIS instance. dns_provider \u00a4 DNS provider type. Optional Environment Variable: DNS_PROVIDER Default: None Purpose : Specifies the DNS provider for automatic DNS record management. Currently only supports IBM Cloud Internet Services (CIS). When to use : - Set to cis when using IBM Cloud Internet Services for DNS - Leave unset if not using automatic DNS management - Required for automatic CNAME creation Valid values : cis (only supported value), or empty/unset Impact : When set to cis , enables automatic CNAME record creation in IBM Cloud Internet Services for MAS routes. Related variables : - cis_crn : Required when this is cis - cis_apikey : Required when this is cis - cis_subdomain : Required when this is cis - mas_workspace_id : Required when this is cis Note : Any value other than cis or empty will result in an error. This feature is optional and only needed if you want automatic DNS management. mas_workspace_id \u00a4 Workspace identifier for DNS records. Required (when dns_provider=cis ) Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Specifies the workspace ID used in CNAME record construction when using CIS as DNS provider. Required for workspace-specific applications. When to use : - Required when dns_provider=cis - Used for workspace-specific apps (manage, health, facilities) - Part of the DNS hostname construction Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , masdev ) Impact : Used to construct DNS CNAME records for workspace-specific applications. Format: {workspace_id}.{subdomain}.{domain} Related variables : - dns_provider : Must be cis - cis_subdomain : Combined with workspace ID for DNS names - mas_instance_id : Instance containing this workspace Note : Only needed when using CIS DNS integration. Not required for basic certificate management. cis_crn \u00a4 IBM Cloud Internet Services CRN. Required (when dns_provider=cis ) Environment Variable: CIS_CRN Default: None Purpose : Provides the Cloud Resource Name (CRN) that uniquely identifies your CIS instance in IBM Cloud. Used for API authentication and resource targeting. When to use : - Required when dns_provider=cis - Found in your CIS instance details page in IBM Cloud - Used to target the correct CIS instance Valid values : Valid IBM Cloud CRN string (format: crn:v1:bluemix:public:internet-svcs:... ) Impact : Identifies which CIS instance to manage DNS records in. All CNAME operations target this CIS instance. Related variables : - dns_provider : Must be cis - cis_apikey : Required for authentication - cis_subdomain : Domain managed in this CIS instance Note : SECURITY - The CRN is not sensitive but should be kept with your infrastructure configuration. Find it in IBM Cloud Console under your CIS instance details. cis_apikey \u00a4 IBM Cloud API key for CIS access. Required (when dns_provider=cis ) Environment Variable: CIS_APIKEY Default: None Purpose : Provides IBM Cloud API key for authenticating with CIS to manage DNS records. When to use : - Required when dns_provider=cis - Must have permissions to manage DNS in the CIS instance - Used for all CIS API operations Valid values : Valid IBM Cloud API key string Impact : Used to authenticate with IBM Cloud and manage DNS records in the specified CIS instance. Must have appropriate IAM permissions. Related variables : - dns_provider : Must be cis - cis_crn : CIS instance to manage - cis_subdomain : Domain to manage records in Note : SECURITY - API key should be kept secure and not committed to version control. Requires IAM permissions for DNS management in CIS. Obtain from IBM Cloud IAM. cis_subdomain \u00a4 CIS subdomain for CNAME records. Required (when dns_provider=cis ) Environment Variable: CIS_SUBDOMAIN Default: None Purpose : Specifies the subdomain within your CIS-managed domain where MAS CNAME records will be created. When to use : - Required when dns_provider=cis - Should match your MAS deployment subdomain - Part of the full DNS hostname Valid values : Valid subdomain string (e.g., mas , maximo , apps ) Impact : Used to construct full DNS names for MAS routes. Format: {component}.{subdomain}.{domain} or {workspace}.{subdomain}.{domain} Related variables : - dns_provider : Must be cis - mas_workspace_id : Combined with subdomain for workspace apps - cis_crn : CIS instance managing this subdomain Note : The subdomain should already exist in your CIS domain. This role creates CNAME records within the subdomain, not the subdomain itself. cis_proxy \u00a4 Enable CIS proxy for CNAME records. Optional Environment Variable: CIS_PROXY Default: false Purpose : Enables IBM Cloud Internet Services proxy mode for created CNAME records, allowing CIS security features (WAF, DDoS protection, rate limiting) to be applied. When to use : - Set to true to enable CIS security features - Use for production environments requiring additional protection - Only applies when dns_provider=cis - Requires CIS security features to be configured Valid values : true , false Impact : - true : CNAME records are proxied through CIS, enabling security features - false : CNAME records are DNS-only (default) Related variables : - dns_provider : Must be cis - cis_crn : CIS instance with security features Note : When enabled, traffic flows through CIS infrastructure, allowing WAF rules, DDoS protection, and other security features to be applied. May add latency but provides additional security layers. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_manual_cert_mgmt: true mas_config_dir: /home/user/masconfig roles: - ibm.mas_devops.suite_certs License \u00a4 EPL-2.0","title":"suite_certs"},{"location":"roles/suite_certs/#suite_certs","text":"This role manages TLS certificates for Maximo Application Suite core platform and applications. It processes certificate files organized in a directory structure and creates TLS secrets in the appropriate namespaces. The role supports manual certificate management and optional integration with IBM Cloud Internet Services (CIS) for DNS management. Certificate Directory Structure Certificates must be organized in subdirectories named after the MAS component ( core , monitor , manage , iot , etc.). Each subdirectory must contain three mandatory files: tls.crt , tls.key , and ca.crt .","title":"suite_certs"},{"location":"roles/suite_certs/#what-this-role-does","text":"Iterates through certificate subdirectories in $MAS_CONFIG_DIR/certs Validates presence of required certificate files ( tls.crt , tls.key , ca.crt ) Creates TLS secrets in appropriate MAS namespaces Optionally manages DNS CNAME records in IBM Cloud Internet Services Supports GitOps mode for generating configurations without applying them","title":"What This Role Does"},{"location":"roles/suite_certs/#certificate-directory-structure","text":"","title":"Certificate Directory Structure"},{"location":"roles/suite_certs/#required-structure","text":"$MAS_CONFIG_DIR/certs/ \u251c\u2500\u2500 core/ \u2502 \u251c\u2500\u2500 tls.crt \u2502 \u251c\u2500\u2500 tls.key \u2502 \u2514\u2500\u2500 ca.crt \u251c\u2500\u2500 monitor/ \u2502 \u251c\u2500\u2500 tls.crt \u2502 \u251c\u2500\u2500 tls.key \u2502 \u2514\u2500\u2500 ca.crt \u251c\u2500\u2500 manage/ \u2502 \u251c\u2500\u2500 tls.crt \u2502 \u251c\u2500\u2500 tls.key \u2502 \u2514\u2500\u2500 ca.crt \u2514\u2500\u2500 <app>/ \u251c\u2500\u2500 tls.crt \u251c\u2500\u2500 tls.key \u2514\u2500\u2500 ca.crt","title":"Required Structure"},{"location":"roles/suite_certs/#supported-applications","text":"core - MAS core platform iot - IoT application monitor - Monitor application manage - Manage application health - Health application predict - Predict application assist - Assist application optimizer - Optimizer application visualinspection - Visual Inspection application facilities - Facilities application add - Add application arcgis - ArcGIS integration Warning All three certificate files ( tls.crt , tls.key , ca.crt ) are mandatory in each subdirectory. The role will fail if a subdirectory is empty or missing any required file. Secret Names TLS secret names for each application are defined in suite_certs/defaults/main.yml . Changes to secret names or adding new applications requires updating this file.","title":"Supported Applications"},{"location":"roles/suite_certs/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/suite_certs/#mas_instance_id","text":"MAS instance identifier for certificate management. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance the certificates are for. Used to construct namespace names and secret names. When to use : - Always required for certificate management - Must match the instance ID from MAS installation - Used in namespace and secret name construction Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines target namespaces for certificate secrets. Namespace format is mas-{instance_id}-{app} (e.g., mas-prod-core , mas-prod-monitor ). Related variables : - mas_config_dir : Root directory containing certificate subdirectories - mas_workspace_id : Required for workspace-specific apps (manage, health, facilities) Note : The instance ID is used to construct both namespace names and TLS secret names according to MAS naming conventions.","title":"mas_instance_id"},{"location":"roles/suite_certs/#mas_manual_cert_mgmt","text":"Enable manual certificate management mode. Optional Environment Variable: MAS_MANUAL_CERT_MGMT Default: false Purpose : Enables manual certificate management mode, allowing you to provide custom certificates instead of using cert-manager generated certificates. When to use : - Set to true when using custom/purchased certificates - Required for environments without cert-manager - Use when corporate policy requires specific certificate authorities - Necessary for custom domain certificates Valid values : true , false Impact : - true : Uses certificates from $MAS_CONFIG_DIR/certs directories - false : Relies on cert-manager for automatic certificate generation (default) Related variables : - mas_config_dir : Directory containing certificate files - dns_provider : Optional DNS management integration Note : When enabled, you must provide all required certificate files in the correct directory structure. This mode is essential for production environments with specific certificate requirements.","title":"mas_manual_cert_mgmt"},{"location":"roles/suite_certs/#mas_config_dir","text":"Configuration directory path. Required Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the root directory containing the certs subdirectory with certificate files organized by application. When to use : - Always required when using manual certificate management - Should point to a directory containing a certs subdirectory - Typically organized by instance ID Valid values : Valid local filesystem path (e.g., /home/user/masconfig , ~/masconfig/inst1 ) Impact : The role looks for certificates in $MAS_CONFIG_DIR/certs/{app}/ directories. All certificate processing is relative to this path. Related variables : - mas_instance_id : Instance these certificates are for - mas_manual_cert_mgmt : Must be enabled to use certificates Note : Organize by instance for clarity (e.g., /masconfig/inst1/certs/ , /masconfig/inst2/certs/ ). The certs subdirectory is automatically appended to this path.","title":"mas_config_dir"},{"location":"roles/suite_certs/#gitops","text":"Enable GitOps mode (generate only, no apply). Optional Environment Variable: GITOPS Default: false Purpose : When enabled, generates certificate secret configurations without applying them to the cluster. Useful for GitOps workflows where configurations are committed to Git and applied separately. When to use : - Set to true for GitOps/declarative workflows - Use when configurations should be reviewed before application - Helpful for version-controlled infrastructure - Required for environments with strict change control Valid values : true , false Impact : - true : Generates YAML configurations but doesn't create resources on cluster - false : Generates and applies configurations directly to cluster (default) Related variables : - mas_config_dir : Where generated configurations are saved Note : In GitOps mode, generated YAML files can be committed to Git and applied through your GitOps tooling (ArgoCD, Flux, etc.).","title":"gitops"},{"location":"roles/suite_certs/#role-variables-cis-dns-provider-optional","text":"Optional variables for users managing DNS with IBM Cloud Internet Services. When configured, this role automatically creates or updates CNAME records for MAS routes in your CIS instance.","title":"Role Variables - CIS DNS Provider (Optional)"},{"location":"roles/suite_certs/#dns_provider","text":"DNS provider type. Optional Environment Variable: DNS_PROVIDER Default: None Purpose : Specifies the DNS provider for automatic DNS record management. Currently only supports IBM Cloud Internet Services (CIS). When to use : - Set to cis when using IBM Cloud Internet Services for DNS - Leave unset if not using automatic DNS management - Required for automatic CNAME creation Valid values : cis (only supported value), or empty/unset Impact : When set to cis , enables automatic CNAME record creation in IBM Cloud Internet Services for MAS routes. Related variables : - cis_crn : Required when this is cis - cis_apikey : Required when this is cis - cis_subdomain : Required when this is cis - mas_workspace_id : Required when this is cis Note : Any value other than cis or empty will result in an error. This feature is optional and only needed if you want automatic DNS management.","title":"dns_provider"},{"location":"roles/suite_certs/#mas_workspace_id","text":"Workspace identifier for DNS records. Required (when dns_provider=cis ) Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Specifies the workspace ID used in CNAME record construction when using CIS as DNS provider. Required for workspace-specific applications. When to use : - Required when dns_provider=cis - Used for workspace-specific apps (manage, health, facilities) - Part of the DNS hostname construction Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , masdev ) Impact : Used to construct DNS CNAME records for workspace-specific applications. Format: {workspace_id}.{subdomain}.{domain} Related variables : - dns_provider : Must be cis - cis_subdomain : Combined with workspace ID for DNS names - mas_instance_id : Instance containing this workspace Note : Only needed when using CIS DNS integration. Not required for basic certificate management.","title":"mas_workspace_id"},{"location":"roles/suite_certs/#cis_crn","text":"IBM Cloud Internet Services CRN. Required (when dns_provider=cis ) Environment Variable: CIS_CRN Default: None Purpose : Provides the Cloud Resource Name (CRN) that uniquely identifies your CIS instance in IBM Cloud. Used for API authentication and resource targeting. When to use : - Required when dns_provider=cis - Found in your CIS instance details page in IBM Cloud - Used to target the correct CIS instance Valid values : Valid IBM Cloud CRN string (format: crn:v1:bluemix:public:internet-svcs:... ) Impact : Identifies which CIS instance to manage DNS records in. All CNAME operations target this CIS instance. Related variables : - dns_provider : Must be cis - cis_apikey : Required for authentication - cis_subdomain : Domain managed in this CIS instance Note : SECURITY - The CRN is not sensitive but should be kept with your infrastructure configuration. Find it in IBM Cloud Console under your CIS instance details.","title":"cis_crn"},{"location":"roles/suite_certs/#cis_apikey","text":"IBM Cloud API key for CIS access. Required (when dns_provider=cis ) Environment Variable: CIS_APIKEY Default: None Purpose : Provides IBM Cloud API key for authenticating with CIS to manage DNS records. When to use : - Required when dns_provider=cis - Must have permissions to manage DNS in the CIS instance - Used for all CIS API operations Valid values : Valid IBM Cloud API key string Impact : Used to authenticate with IBM Cloud and manage DNS records in the specified CIS instance. Must have appropriate IAM permissions. Related variables : - dns_provider : Must be cis - cis_crn : CIS instance to manage - cis_subdomain : Domain to manage records in Note : SECURITY - API key should be kept secure and not committed to version control. Requires IAM permissions for DNS management in CIS. Obtain from IBM Cloud IAM.","title":"cis_apikey"},{"location":"roles/suite_certs/#cis_subdomain","text":"CIS subdomain for CNAME records. Required (when dns_provider=cis ) Environment Variable: CIS_SUBDOMAIN Default: None Purpose : Specifies the subdomain within your CIS-managed domain where MAS CNAME records will be created. When to use : - Required when dns_provider=cis - Should match your MAS deployment subdomain - Part of the full DNS hostname Valid values : Valid subdomain string (e.g., mas , maximo , apps ) Impact : Used to construct full DNS names for MAS routes. Format: {component}.{subdomain}.{domain} or {workspace}.{subdomain}.{domain} Related variables : - dns_provider : Must be cis - mas_workspace_id : Combined with subdomain for workspace apps - cis_crn : CIS instance managing this subdomain Note : The subdomain should already exist in your CIS domain. This role creates CNAME records within the subdomain, not the subdomain itself.","title":"cis_subdomain"},{"location":"roles/suite_certs/#cis_proxy","text":"Enable CIS proxy for CNAME records. Optional Environment Variable: CIS_PROXY Default: false Purpose : Enables IBM Cloud Internet Services proxy mode for created CNAME records, allowing CIS security features (WAF, DDoS protection, rate limiting) to be applied. When to use : - Set to true to enable CIS security features - Use for production environments requiring additional protection - Only applies when dns_provider=cis - Requires CIS security features to be configured Valid values : true , false Impact : - true : CNAME records are proxied through CIS, enabling security features - false : CNAME records are DNS-only (default) Related variables : - dns_provider : Must be cis - cis_crn : CIS instance with security features Note : When enabled, traffic flows through CIS infrastructure, allowing WAF rules, DDoS protection, and other security features to be applied. May add latency but provides additional security layers.","title":"cis_proxy"},{"location":"roles/suite_certs/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_manual_cert_mgmt: true mas_config_dir: /home/user/masconfig roles: - ibm.mas_devops.suite_certs","title":"Example Playbook"},{"location":"roles/suite_certs/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_config/","text":"suite_config \u00a4 This role applies configuration files to a Maximo Application Suite installation. It searches for YAML configuration files in a specified directory and applies them to the cluster using the Kubernetes API. This is typically used after installing MAS to configure various aspects of the suite such as workspace configurations, JDBC configurations, SMTP settings, and other custom resources. Role Variables \u00a4 mas_instance_id \u00a4 Unique identifier for the MAS instance to apply configurations to. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to target when applying configuration files. This ensures configurations are applied to the correct MAS installation when multiple instances exist in the cluster. When to use : - Always required for any MAS configuration operation - Must match the instance ID used during MAS core installation - Use the same value across all configuration operations for a given MAS instance Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Configurations will be applied to the namespace mas-{mas_instance_id}-core and related application namespaces. An incorrect instance ID will cause configurations to be applied to the wrong MAS instance or fail if the instance doesn't exist. Related variables : Works with mas_config_dir to determine which configuration files to apply. Note : This must match the instance ID used when installing MAS core. Verify the instance ID before applying configurations to avoid misconfiguration. mas_config_dir \u00a4 Local directory path containing MAS configuration YAML files to apply to the cluster. Required Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the directory containing MAS configuration files (MongoCfg, JdbcCfg, BASCfg, SLSCfg, etc.) that will be automatically applied to configure the MAS instance. This enables automated configuration of MAS dependencies and settings. When to use : - Always required when using this role to apply configurations - Use the same directory where dependency roles (mongodb, db2, sls) generate their configuration files - Typically set to a consistent location across all MAS setup roles (e.g., /home/user/masconfig ) Valid values : Any valid local filesystem path containing YAML configuration files (e.g., /home/user/masconfig , ~/masconfig , ./config ) Impact : The role recursively searches this directory for all *.yaml and *.yml files and applies them to the cluster using oc apply . Invalid YAML files or incorrect configurations will cause the role to fail. Files matching jdbc-aiservice*.yml or jdbc-aiservice*.yaml patterns are automatically excluded from processing. Related variables : - mas_instance_id : Determines which MAS instance receives the configurations - Used by dependency roles (mongodb, db2, sls) as output directory for generated configs Note : Ensure all YAML files in this directory are valid Kubernetes resources intended for this MAS instance. The role applies all matching files, so remove or move any files not intended for application. AI Service JDBC configurations are excluded as they require special handling. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_config_dir: \"/home/user/masconfig\" roles: - ibm.mas_devops.suite_config License \u00a4 EPL-2.0","title":"suite_config"},{"location":"roles/suite_config/#suite_config","text":"This role applies configuration files to a Maximo Application Suite installation. It searches for YAML configuration files in a specified directory and applies them to the cluster using the Kubernetes API. This is typically used after installing MAS to configure various aspects of the suite such as workspace configurations, JDBC configurations, SMTP settings, and other custom resources.","title":"suite_config"},{"location":"roles/suite_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_config/#mas_instance_id","text":"Unique identifier for the MAS instance to apply configurations to. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to target when applying configuration files. This ensures configurations are applied to the correct MAS installation when multiple instances exist in the cluster. When to use : - Always required for any MAS configuration operation - Must match the instance ID used during MAS core installation - Use the same value across all configuration operations for a given MAS instance Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Configurations will be applied to the namespace mas-{mas_instance_id}-core and related application namespaces. An incorrect instance ID will cause configurations to be applied to the wrong MAS instance or fail if the instance doesn't exist. Related variables : Works with mas_config_dir to determine which configuration files to apply. Note : This must match the instance ID used when installing MAS core. Verify the instance ID before applying configurations to avoid misconfiguration.","title":"mas_instance_id"},{"location":"roles/suite_config/#mas_config_dir","text":"Local directory path containing MAS configuration YAML files to apply to the cluster. Required Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the directory containing MAS configuration files (MongoCfg, JdbcCfg, BASCfg, SLSCfg, etc.) that will be automatically applied to configure the MAS instance. This enables automated configuration of MAS dependencies and settings. When to use : - Always required when using this role to apply configurations - Use the same directory where dependency roles (mongodb, db2, sls) generate their configuration files - Typically set to a consistent location across all MAS setup roles (e.g., /home/user/masconfig ) Valid values : Any valid local filesystem path containing YAML configuration files (e.g., /home/user/masconfig , ~/masconfig , ./config ) Impact : The role recursively searches this directory for all *.yaml and *.yml files and applies them to the cluster using oc apply . Invalid YAML files or incorrect configurations will cause the role to fail. Files matching jdbc-aiservice*.yml or jdbc-aiservice*.yaml patterns are automatically excluded from processing. Related variables : - mas_instance_id : Determines which MAS instance receives the configurations - Used by dependency roles (mongodb, db2, sls) as output directory for generated configs Note : Ensure all YAML files in this directory are valid Kubernetes resources intended for this MAS instance. The role applies all matching files, so remove or move any files not intended for application. AI Service JDBC configurations are excluded as they require special handling.","title":"mas_config_dir"},{"location":"roles/suite_config/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_config_dir: \"/home/user/masconfig\" roles: - ibm.mas_devops.suite_config","title":"Example Playbook"},{"location":"roles/suite_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_db2_setup_for_facilities/","text":"suite_db2_setup_for_facilities \u00a4 This role performs initial Db2 database setup required for Maximo Facilities (Real Estate and Facilities Management) application. It configures database parameters, creates tablespaces, and applies performance optimizations that the Facilities operator cannot yet handle automatically. Temporary Role This role exists as a workaround until the Facilities operator can perform these setup tasks automatically. It handles Facilities-specific database requirements. What This Role Does \u00a4 Installs Db2 with required properties for Facilities Copies setup scripts into Db2 pod Executes database configuration changes inside container Creates and configures tablespaces for Facilities Applies enhanced Db2 performance parameters Optionally restarts Db2 instance to apply configuration Downtime Risk Setting enforce_db2_config=true will restart the Db2 instance, causing downtime. Schedule during maintenance windows or use with newly created instances. Role Variables \u00a4 db2_instance_name \u00a4 Db2 instance name for Facilities setup. Required Environment Variable: DB2_INSTANCE_NAME Default: None Purpose : Identifies the Db2 Warehouse instance where Facilities database setup will be performed. When to use : - Always required for Facilities database setup - Must match the Db2 instance name for Facilities - Used to locate the correct Db2 pod Valid values : Valid Db2 instance name (e.g., db2w-facilities , db2u-db01 ) Impact : Determines which Db2 instance receives the Facilities-specific configuration and tablespace setup. Related variables : - db2_namespace : Namespace containing this instance - db2_dbname : Database within this instance Note : To find the instance name, go to the Db2 namespace and look for pods with label=engine . Describe the pod and find the app label value. db2_namespace \u00a4 Db2 instance namespace. Optional Environment Variable: DB2_NAMESPACE Default: db2u Purpose : Specifies the OpenShift namespace where the Db2 instance is deployed. When to use : - Use default ( db2u ) for standard Db2 deployments - Override if Db2 is deployed in a custom namespace - Required to locate the Db2 pod Valid values : Valid Kubernetes namespace name Impact : Determines where to find the Db2 instance for setup script execution. Related variables : - db2_instance_name : Instance to find in this namespace Note : The default db2u namespace is used by most Db2 Warehouse deployments. db2_username \u00a4 Database connection username. Optional Environment Variable: None Default: tridata Purpose : Specifies the username for connecting to the Db2 database during Facilities setup script execution. When to use : - Use default ( tridata ) for standard Facilities deployments - Override if using custom database user - Must have appropriate database privileges Valid values : Valid Db2 username with admin privileges Impact : Determines which user account executes the setup script and database configuration changes. Related variables : - db2_dbname : Database to connect to - db2_schema : Schema to configure (TRIDATA) Note : The default tridata user is specific to Facilities/TRIRIGA. The user must have sufficient privileges to create tablespaces and modify database configuration. db2_dbname \u00a4 Database name within Db2 instance. Optional Environment Variable: None Default: BLUDB Purpose : Specifies the database name within the Db2 instance where Facilities setup will be performed. When to use : - Use default ( BLUDB ) for standard Facilities deployments - Override if using custom database name - Must exist before running this role Valid values : Valid Db2 database name Impact : Determines which database receives the Facilities tablespace and configuration setup. Related variables : - db2_instance_name : Instance containing this database - db2_schema : Schema within this database (TRIDATA) Note : BLUDB is the default database name for Facilities deployments. db2_schema \u00a4 Facilities database schema name. Optional Environment Variable: None Default: TRIDATA Purpose : Specifies the schema name where Facilities (TRIRIGA) tables and objects will be created. When to use : - Use default ( TRIDATA ) for standard Facilities deployments - Override if using custom schema name - Must match Facilities configuration Valid values : Valid Db2 schema name Impact : Determines which schema receives the tablespace configuration and Facilities-specific setup. Related variables : - db2_dbname : Database containing this schema - db2_username : User accessing this schema (tridata) Note : The default TRIDATA schema is standard for Facilities/TRIRIGA deployments. This differs from Manage which uses maximo schema. Ensure this matches your Facilities database configuration. db2_tablespace_data_size \u00a4 Data tablespace size. Optional Environment Variable: DB2_TABLESPACE_DATA_SIZE Default: 5000 M Purpose : Specifies the size of the data tablespace created for Facilities application data. When to use : - Use default ( 5000 M ) for small to medium deployments - Increase for large Facilities deployments with extensive real estate data - Consider data growth over time Valid values : Db2 size format (e.g., 5000 M , 10 G , 50 G ) Impact : Determines how much data can be stored in Facilities tables. Insufficient size will prevent data insertion. Related variables : - db2_tablespace_index_size : Related index tablespace size Note : Size requirements depend on: - Number of buildings and spaces - Historical data retention - Document attachments - Custom fields and extensions Start with 5GB and monitor usage. Tablespaces can be expanded later if needed. db2_tablespace_index_size \u00a4 Index tablespace size. Optional Environment Variable: DB2_TABLESPACE_INDEX_SIZE Default: 5000 M Purpose : Specifies the size of the index tablespace created for Facilities database indexes. When to use : - Use default ( 5000 M ) for small to medium deployments - Increase for large Facilities deployments with many indexes - Typically 20-30% of data tablespace size Valid values : Db2 size format (e.g., 5000 M , 10 G , 20 G ) Impact : Determines how many indexes can be created. Insufficient size will prevent index creation and impact performance. Related variables : - db2_tablespace_data_size : Related data tablespace size Note : Indexes improve query performance but consume space. A good rule of thumb is to allocate 20-30% of the data tablespace size for indexes. Monitor usage and adjust as needed. db2_config_version \u00a4 Db2 configuration parameter version. Optional Environment Variable: DB2_CONFIG_VERSION Default: 1.0.0 Purpose : Specifies the version of enhanced Db2 performance parameters to apply during Facilities setup. When to use : - Use default ( 1.0.0 ) for current parameter set - Override only if specific version required - Different versions may have different parameter sets Valid values : 1.0.0 (currently supported version) Impact : Determines which set of Db2 performance parameters are applied to optimize for Facilities workloads. Related variables : - enforce_db2_config : Controls whether parameters are applied with restart Note : The parameter set includes optimizations for Facilities' specific database access patterns. Future versions may include additional optimizations. enforce_db2_config \u00a4 Force Db2 configuration with restart. Optional Environment Variable: ENFORCE_DB2_CONFIG Default: true Purpose : Controls whether enhanced Db2 parameters are applied with a database restart. Restart is required for parameters to take effect but causes downtime. When to use : - Set to true (default) for new Db2 instances - Set to true during scheduled maintenance windows - Set to false to skip parameter application (not recommended) Valid values : true , false Impact : - true : Applies enhanced parameters and restarts Db2 instance (causes downtime) - false : Skips enhanced parameter application (suboptimal performance) Related variables : - db2_config_version : Version of parameters to apply Note : CRITICAL - Setting to true will restart the Db2 instance, causing downtime for all applications using the database. Schedule during maintenance windows. For production systems, coordinate with stakeholders. For new instances, this is safe as no applications are using the database yet. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # Configuration for the Db2 cluster db2_namespace: db2u db2_instance_name: db2u-db01 db2_dbname: BLUDB roles: - ibm.mas_devops.suite_db2_setup_for_facilities License \u00a4 EPL-2.0","title":"suite_db2_setup_for_facilities"},{"location":"roles/suite_db2_setup_for_facilities/#suite_db2_setup_for_facilities","text":"This role performs initial Db2 database setup required for Maximo Facilities (Real Estate and Facilities Management) application. It configures database parameters, creates tablespaces, and applies performance optimizations that the Facilities operator cannot yet handle automatically. Temporary Role This role exists as a workaround until the Facilities operator can perform these setup tasks automatically. It handles Facilities-specific database requirements.","title":"suite_db2_setup_for_facilities"},{"location":"roles/suite_db2_setup_for_facilities/#what-this-role-does","text":"Installs Db2 with required properties for Facilities Copies setup scripts into Db2 pod Executes database configuration changes inside container Creates and configures tablespaces for Facilities Applies enhanced Db2 performance parameters Optionally restarts Db2 instance to apply configuration Downtime Risk Setting enforce_db2_config=true will restart the Db2 instance, causing downtime. Schedule during maintenance windows or use with newly created instances.","title":"What This Role Does"},{"location":"roles/suite_db2_setup_for_facilities/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_db2_setup_for_facilities/#db2_instance_name","text":"Db2 instance name for Facilities setup. Required Environment Variable: DB2_INSTANCE_NAME Default: None Purpose : Identifies the Db2 Warehouse instance where Facilities database setup will be performed. When to use : - Always required for Facilities database setup - Must match the Db2 instance name for Facilities - Used to locate the correct Db2 pod Valid values : Valid Db2 instance name (e.g., db2w-facilities , db2u-db01 ) Impact : Determines which Db2 instance receives the Facilities-specific configuration and tablespace setup. Related variables : - db2_namespace : Namespace containing this instance - db2_dbname : Database within this instance Note : To find the instance name, go to the Db2 namespace and look for pods with label=engine . Describe the pod and find the app label value.","title":"db2_instance_name"},{"location":"roles/suite_db2_setup_for_facilities/#db2_namespace","text":"Db2 instance namespace. Optional Environment Variable: DB2_NAMESPACE Default: db2u Purpose : Specifies the OpenShift namespace where the Db2 instance is deployed. When to use : - Use default ( db2u ) for standard Db2 deployments - Override if Db2 is deployed in a custom namespace - Required to locate the Db2 pod Valid values : Valid Kubernetes namespace name Impact : Determines where to find the Db2 instance for setup script execution. Related variables : - db2_instance_name : Instance to find in this namespace Note : The default db2u namespace is used by most Db2 Warehouse deployments.","title":"db2_namespace"},{"location":"roles/suite_db2_setup_for_facilities/#db2_username","text":"Database connection username. Optional Environment Variable: None Default: tridata Purpose : Specifies the username for connecting to the Db2 database during Facilities setup script execution. When to use : - Use default ( tridata ) for standard Facilities deployments - Override if using custom database user - Must have appropriate database privileges Valid values : Valid Db2 username with admin privileges Impact : Determines which user account executes the setup script and database configuration changes. Related variables : - db2_dbname : Database to connect to - db2_schema : Schema to configure (TRIDATA) Note : The default tridata user is specific to Facilities/TRIRIGA. The user must have sufficient privileges to create tablespaces and modify database configuration.","title":"db2_username"},{"location":"roles/suite_db2_setup_for_facilities/#db2_dbname","text":"Database name within Db2 instance. Optional Environment Variable: None Default: BLUDB Purpose : Specifies the database name within the Db2 instance where Facilities setup will be performed. When to use : - Use default ( BLUDB ) for standard Facilities deployments - Override if using custom database name - Must exist before running this role Valid values : Valid Db2 database name Impact : Determines which database receives the Facilities tablespace and configuration setup. Related variables : - db2_instance_name : Instance containing this database - db2_schema : Schema within this database (TRIDATA) Note : BLUDB is the default database name for Facilities deployments.","title":"db2_dbname"},{"location":"roles/suite_db2_setup_for_facilities/#db2_schema","text":"Facilities database schema name. Optional Environment Variable: None Default: TRIDATA Purpose : Specifies the schema name where Facilities (TRIRIGA) tables and objects will be created. When to use : - Use default ( TRIDATA ) for standard Facilities deployments - Override if using custom schema name - Must match Facilities configuration Valid values : Valid Db2 schema name Impact : Determines which schema receives the tablespace configuration and Facilities-specific setup. Related variables : - db2_dbname : Database containing this schema - db2_username : User accessing this schema (tridata) Note : The default TRIDATA schema is standard for Facilities/TRIRIGA deployments. This differs from Manage which uses maximo schema. Ensure this matches your Facilities database configuration.","title":"db2_schema"},{"location":"roles/suite_db2_setup_for_facilities/#db2_tablespace_data_size","text":"Data tablespace size. Optional Environment Variable: DB2_TABLESPACE_DATA_SIZE Default: 5000 M Purpose : Specifies the size of the data tablespace created for Facilities application data. When to use : - Use default ( 5000 M ) for small to medium deployments - Increase for large Facilities deployments with extensive real estate data - Consider data growth over time Valid values : Db2 size format (e.g., 5000 M , 10 G , 50 G ) Impact : Determines how much data can be stored in Facilities tables. Insufficient size will prevent data insertion. Related variables : - db2_tablespace_index_size : Related index tablespace size Note : Size requirements depend on: - Number of buildings and spaces - Historical data retention - Document attachments - Custom fields and extensions Start with 5GB and monitor usage. Tablespaces can be expanded later if needed.","title":"db2_tablespace_data_size"},{"location":"roles/suite_db2_setup_for_facilities/#db2_tablespace_index_size","text":"Index tablespace size. Optional Environment Variable: DB2_TABLESPACE_INDEX_SIZE Default: 5000 M Purpose : Specifies the size of the index tablespace created for Facilities database indexes. When to use : - Use default ( 5000 M ) for small to medium deployments - Increase for large Facilities deployments with many indexes - Typically 20-30% of data tablespace size Valid values : Db2 size format (e.g., 5000 M , 10 G , 20 G ) Impact : Determines how many indexes can be created. Insufficient size will prevent index creation and impact performance. Related variables : - db2_tablespace_data_size : Related data tablespace size Note : Indexes improve query performance but consume space. A good rule of thumb is to allocate 20-30% of the data tablespace size for indexes. Monitor usage and adjust as needed.","title":"db2_tablespace_index_size"},{"location":"roles/suite_db2_setup_for_facilities/#db2_config_version","text":"Db2 configuration parameter version. Optional Environment Variable: DB2_CONFIG_VERSION Default: 1.0.0 Purpose : Specifies the version of enhanced Db2 performance parameters to apply during Facilities setup. When to use : - Use default ( 1.0.0 ) for current parameter set - Override only if specific version required - Different versions may have different parameter sets Valid values : 1.0.0 (currently supported version) Impact : Determines which set of Db2 performance parameters are applied to optimize for Facilities workloads. Related variables : - enforce_db2_config : Controls whether parameters are applied with restart Note : The parameter set includes optimizations for Facilities' specific database access patterns. Future versions may include additional optimizations.","title":"db2_config_version"},{"location":"roles/suite_db2_setup_for_facilities/#enforce_db2_config","text":"Force Db2 configuration with restart. Optional Environment Variable: ENFORCE_DB2_CONFIG Default: true Purpose : Controls whether enhanced Db2 parameters are applied with a database restart. Restart is required for parameters to take effect but causes downtime. When to use : - Set to true (default) for new Db2 instances - Set to true during scheduled maintenance windows - Set to false to skip parameter application (not recommended) Valid values : true , false Impact : - true : Applies enhanced parameters and restarts Db2 instance (causes downtime) - false : Skips enhanced parameter application (suboptimal performance) Related variables : - db2_config_version : Version of parameters to apply Note : CRITICAL - Setting to true will restart the Db2 instance, causing downtime for all applications using the database. Schedule during maintenance windows. For production systems, coordinate with stakeholders. For new instances, this is safe as no applications are using the database yet.","title":"enforce_db2_config"},{"location":"roles/suite_db2_setup_for_facilities/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Configuration for the Db2 cluster db2_namespace: db2u db2_instance_name: db2u-db01 db2_dbname: BLUDB roles: - ibm.mas_devops.suite_db2_setup_for_facilities","title":"Example Playbook"},{"location":"roles/suite_db2_setup_for_facilities/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_db2_setup_for_manage/","text":"suite_db2_setup_for_manage \u00a4 This role performs initial Db2 database setup required for Maximo Manage application. It configures database parameters, creates tablespaces, and applies performance optimizations that the Manage operator cannot yet handle automatically. Temporary Role This role exists as a workaround until the Manage operator can perform these setup tasks automatically. It supports both CP4D version 3.5 and 4.0. What This Role Does \u00a4 Copies setup script ( setupdb.sh ) into Db2 pod Executes database configuration changes inside container Creates and configures tablespaces for Manage Applies enhanced Db2 performance parameters Optionally restarts Db2 instance to apply configuration Downtime Risk Setting enforce_db2_config=true will restart the Db2 instance, causing downtime. Schedule during maintenance windows or use with newly created instances. Role Variables \u00a4 db2_instance_name \u00a4 Db2 instance name for Manage setup. Required Environment Variable: DB2_INSTANCE_NAME Default: None Purpose : Identifies the Db2 Warehouse instance where Manage database setup will be performed. When to use : - Always required for Manage database setup - Must match the Db2 instance name for Manage - Used to locate the correct Db2 pod Valid values : Valid Db2 instance name (e.g., db2w-manage , db2u-manage ) Impact : Determines which Db2 instance receives the Manage-specific configuration and tablespace setup. Related variables : - db2_namespace : Namespace containing this instance - db2_dbname : Database within this instance Note : To find the instance name, go to the Db2 namespace and look for pods with label=engine . Describe the pod and find the app label value. db2_namespace \u00a4 Db2 instance namespace. Optional Environment Variable: DB2_NAMESPACE Default: db2u Purpose : Specifies the OpenShift namespace where the Db2 instance is deployed. When to use : - Use default ( db2u ) for standard Db2 deployments - Override if Db2 is deployed in a custom namespace - Required to locate the Db2 pod Valid values : Valid Kubernetes namespace name Impact : Determines where to find the Db2 instance for setup script execution. Related variables : - db2_instance_name : Instance to find in this namespace Note : The default db2u namespace is used by most Db2 Warehouse deployments. db2_username \u00a4 Database connection username. Optional Environment Variable: None Default: db2inst1 Purpose : Specifies the username for connecting to the Db2 database during setup script execution. When to use : - Use default ( db2inst1 ) for standard Db2 deployments - Override if using custom database user - Must have appropriate database privileges Valid values : Valid Db2 username with admin privileges Impact : Determines which user account executes the setup script and database configuration changes. Related variables : - db2_dbname : Database to connect to - db2_schema : Schema to configure Note : The user must have sufficient privileges to create tablespaces and modify database configuration. db2_dbname \u00a4 Database name within Db2 instance. Optional Environment Variable: None Default: BLUDB Purpose : Specifies the database name within the Db2 instance where Manage setup will be performed. When to use : - Use default ( BLUDB ) for standard Manage deployments - Override if using custom database name - Must exist before running this role Valid values : Valid Db2 database name Impact : Determines which database receives the Manage tablespace and configuration setup. Related variables : - db2_instance_name : Instance containing this database - db2_schema : Schema within this database Note : BLUDB is the default database name for Manage deployments. db2_schema \u00a4 Manage database schema name. Optional Environment Variable: None Default: maximo Purpose : Specifies the schema name where Manage tables and objects will be created. When to use : - Use default ( maximo ) for standard Manage deployments - Override if using custom schema name - Must match Manage configuration Valid values : Valid Db2 schema name Impact : Determines which schema receives the tablespace configuration and Manage-specific setup. Related variables : - db2_dbname : Database containing this schema - db2_username : User accessing this schema Note : The default maximo schema is standard for Manage deployments. Ensure this matches your Manage database configuration. db2_tablespace_data_size \u00a4 Data tablespace size. Optional Environment Variable: DB2_TABLESPACE_DATA_SIZE Default: 5000 M Purpose : Specifies the size of the data tablespace created for Manage application data. When to use : - Use default ( 5000 M ) for small to medium deployments - Increase for large Manage deployments with extensive data - Consider data growth over time Valid values : Db2 size format (e.g., 5000 M , 10 G , 50 G ) Impact : Determines how much data can be stored in Manage tables. Insufficient size will prevent data insertion. Related variables : - db2_tablespace_index_size : Related index tablespace size Note : Size requirements depend on: - Number of assets and work orders - Historical data retention - Attachment storage (if using database) - Custom fields and extensions Start with 5GB and monitor usage. Tablespaces can be expanded later if needed. db2_tablespace_index_size \u00a4 Index tablespace size. Optional Environment Variable: DB2_TABLESPACE_INDEX_SIZE Default: 5000 M Purpose : Specifies the size of the index tablespace created for Manage database indexes. When to use : - Use default ( 5000 M ) for small to medium deployments - Increase for large Manage deployments with many indexes - Typically 20-30% of data tablespace size Valid values : Db2 size format (e.g., 5000 M , 10 G , 20 G ) Impact : Determines how many indexes can be created. Insufficient size will prevent index creation and impact performance. Related variables : - db2_tablespace_data_size : Related data tablespace size Note : Indexes improve query performance but consume space. A good rule of thumb is to allocate 20-30% of the data tablespace size for indexes. Monitor usage and adjust as needed. db2_config_version \u00a4 Db2 configuration parameter version. Optional Environment Variable: DB2_CONFIG_VERSION Default: 1.0.0 Purpose : Specifies the version of enhanced Db2 performance parameters to apply during setup. When to use : - Use default ( 1.0.0 ) for current parameter set - Override only if specific version required - Different versions may have different parameter sets Valid values : 1.0.0 (currently supported version) Impact : Determines which set of Db2 performance parameters are applied to optimize for Manage workloads. Related variables : - enforce_db2_config : Controls whether parameters are applied with restart Note : The parameter set includes optimizations for Manage's specific database access patterns. Future versions may include additional optimizations. enforce_db2_config \u00a4 Force Db2 configuration with restart. Optional Environment Variable: ENFORCE_DB2_CONFIG Default: true Purpose : Controls whether enhanced Db2 parameters are applied with a database restart. Restart is required for parameters to take effect but causes downtime. When to use : - Set to true (default) for new Db2 instances - Set to true during scheduled maintenance windows - Set to false to skip parameter application (not recommended) Valid values : true , false Impact : - true : Applies enhanced parameters and restarts Db2 instance (causes downtime) - false : Skips enhanced parameter application (suboptimal performance) Related variables : - db2_config_version : Version of parameters to apply Note : CRITICAL - Setting to true will restart the Db2 instance, causing downtime for all applications using the database. Schedule during maintenance windows. For production systems, coordinate with stakeholders. For new instances, this is safe as no applications are using the database yet. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: db2_instancename: mydb2 db2_namespace: db2u db2_config_version: \"1.0.0\" # It will cause downtime if set to true, please be careful. enforce_db2_config: true roles: - ibm.mas_devops.suite_db2_setup_for_manage License \u00a4 EPL-2.0","title":"suite_db2_setup_for_manage"},{"location":"roles/suite_db2_setup_for_manage/#suite_db2_setup_for_manage","text":"This role performs initial Db2 database setup required for Maximo Manage application. It configures database parameters, creates tablespaces, and applies performance optimizations that the Manage operator cannot yet handle automatically. Temporary Role This role exists as a workaround until the Manage operator can perform these setup tasks automatically. It supports both CP4D version 3.5 and 4.0.","title":"suite_db2_setup_for_manage"},{"location":"roles/suite_db2_setup_for_manage/#what-this-role-does","text":"Copies setup script ( setupdb.sh ) into Db2 pod Executes database configuration changes inside container Creates and configures tablespaces for Manage Applies enhanced Db2 performance parameters Optionally restarts Db2 instance to apply configuration Downtime Risk Setting enforce_db2_config=true will restart the Db2 instance, causing downtime. Schedule during maintenance windows or use with newly created instances.","title":"What This Role Does"},{"location":"roles/suite_db2_setup_for_manage/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_db2_setup_for_manage/#db2_instance_name","text":"Db2 instance name for Manage setup. Required Environment Variable: DB2_INSTANCE_NAME Default: None Purpose : Identifies the Db2 Warehouse instance where Manage database setup will be performed. When to use : - Always required for Manage database setup - Must match the Db2 instance name for Manage - Used to locate the correct Db2 pod Valid values : Valid Db2 instance name (e.g., db2w-manage , db2u-manage ) Impact : Determines which Db2 instance receives the Manage-specific configuration and tablespace setup. Related variables : - db2_namespace : Namespace containing this instance - db2_dbname : Database within this instance Note : To find the instance name, go to the Db2 namespace and look for pods with label=engine . Describe the pod and find the app label value.","title":"db2_instance_name"},{"location":"roles/suite_db2_setup_for_manage/#db2_namespace","text":"Db2 instance namespace. Optional Environment Variable: DB2_NAMESPACE Default: db2u Purpose : Specifies the OpenShift namespace where the Db2 instance is deployed. When to use : - Use default ( db2u ) for standard Db2 deployments - Override if Db2 is deployed in a custom namespace - Required to locate the Db2 pod Valid values : Valid Kubernetes namespace name Impact : Determines where to find the Db2 instance for setup script execution. Related variables : - db2_instance_name : Instance to find in this namespace Note : The default db2u namespace is used by most Db2 Warehouse deployments.","title":"db2_namespace"},{"location":"roles/suite_db2_setup_for_manage/#db2_username","text":"Database connection username. Optional Environment Variable: None Default: db2inst1 Purpose : Specifies the username for connecting to the Db2 database during setup script execution. When to use : - Use default ( db2inst1 ) for standard Db2 deployments - Override if using custom database user - Must have appropriate database privileges Valid values : Valid Db2 username with admin privileges Impact : Determines which user account executes the setup script and database configuration changes. Related variables : - db2_dbname : Database to connect to - db2_schema : Schema to configure Note : The user must have sufficient privileges to create tablespaces and modify database configuration.","title":"db2_username"},{"location":"roles/suite_db2_setup_for_manage/#db2_dbname","text":"Database name within Db2 instance. Optional Environment Variable: None Default: BLUDB Purpose : Specifies the database name within the Db2 instance where Manage setup will be performed. When to use : - Use default ( BLUDB ) for standard Manage deployments - Override if using custom database name - Must exist before running this role Valid values : Valid Db2 database name Impact : Determines which database receives the Manage tablespace and configuration setup. Related variables : - db2_instance_name : Instance containing this database - db2_schema : Schema within this database Note : BLUDB is the default database name for Manage deployments.","title":"db2_dbname"},{"location":"roles/suite_db2_setup_for_manage/#db2_schema","text":"Manage database schema name. Optional Environment Variable: None Default: maximo Purpose : Specifies the schema name where Manage tables and objects will be created. When to use : - Use default ( maximo ) for standard Manage deployments - Override if using custom schema name - Must match Manage configuration Valid values : Valid Db2 schema name Impact : Determines which schema receives the tablespace configuration and Manage-specific setup. Related variables : - db2_dbname : Database containing this schema - db2_username : User accessing this schema Note : The default maximo schema is standard for Manage deployments. Ensure this matches your Manage database configuration.","title":"db2_schema"},{"location":"roles/suite_db2_setup_for_manage/#db2_tablespace_data_size","text":"Data tablespace size. Optional Environment Variable: DB2_TABLESPACE_DATA_SIZE Default: 5000 M Purpose : Specifies the size of the data tablespace created for Manage application data. When to use : - Use default ( 5000 M ) for small to medium deployments - Increase for large Manage deployments with extensive data - Consider data growth over time Valid values : Db2 size format (e.g., 5000 M , 10 G , 50 G ) Impact : Determines how much data can be stored in Manage tables. Insufficient size will prevent data insertion. Related variables : - db2_tablespace_index_size : Related index tablespace size Note : Size requirements depend on: - Number of assets and work orders - Historical data retention - Attachment storage (if using database) - Custom fields and extensions Start with 5GB and monitor usage. Tablespaces can be expanded later if needed.","title":"db2_tablespace_data_size"},{"location":"roles/suite_db2_setup_for_manage/#db2_tablespace_index_size","text":"Index tablespace size. Optional Environment Variable: DB2_TABLESPACE_INDEX_SIZE Default: 5000 M Purpose : Specifies the size of the index tablespace created for Manage database indexes. When to use : - Use default ( 5000 M ) for small to medium deployments - Increase for large Manage deployments with many indexes - Typically 20-30% of data tablespace size Valid values : Db2 size format (e.g., 5000 M , 10 G , 20 G ) Impact : Determines how many indexes can be created. Insufficient size will prevent index creation and impact performance. Related variables : - db2_tablespace_data_size : Related data tablespace size Note : Indexes improve query performance but consume space. A good rule of thumb is to allocate 20-30% of the data tablespace size for indexes. Monitor usage and adjust as needed.","title":"db2_tablespace_index_size"},{"location":"roles/suite_db2_setup_for_manage/#db2_config_version","text":"Db2 configuration parameter version. Optional Environment Variable: DB2_CONFIG_VERSION Default: 1.0.0 Purpose : Specifies the version of enhanced Db2 performance parameters to apply during setup. When to use : - Use default ( 1.0.0 ) for current parameter set - Override only if specific version required - Different versions may have different parameter sets Valid values : 1.0.0 (currently supported version) Impact : Determines which set of Db2 performance parameters are applied to optimize for Manage workloads. Related variables : - enforce_db2_config : Controls whether parameters are applied with restart Note : The parameter set includes optimizations for Manage's specific database access patterns. Future versions may include additional optimizations.","title":"db2_config_version"},{"location":"roles/suite_db2_setup_for_manage/#enforce_db2_config","text":"Force Db2 configuration with restart. Optional Environment Variable: ENFORCE_DB2_CONFIG Default: true Purpose : Controls whether enhanced Db2 parameters are applied with a database restart. Restart is required for parameters to take effect but causes downtime. When to use : - Set to true (default) for new Db2 instances - Set to true during scheduled maintenance windows - Set to false to skip parameter application (not recommended) Valid values : true , false Impact : - true : Applies enhanced parameters and restarts Db2 instance (causes downtime) - false : Skips enhanced parameter application (suboptimal performance) Related variables : - db2_config_version : Version of parameters to apply Note : CRITICAL - Setting to true will restart the Db2 instance, causing downtime for all applications using the database. Schedule during maintenance windows. For production systems, coordinate with stakeholders. For new instances, this is safe as no applications are using the database yet.","title":"enforce_db2_config"},{"location":"roles/suite_db2_setup_for_manage/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: db2_instancename: mydb2 db2_namespace: db2u db2_config_version: \"1.0.0\" # It will cause downtime if set to true, please be careful. enforce_db2_config: true roles: - ibm.mas_devops.suite_db2_setup_for_manage","title":"Example Playbook"},{"location":"roles/suite_db2_setup_for_manage/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_dns/","text":"suite_dns \u00a4 This role will manage MAS and DNS provider integration. IBM Cloud Internet Services, Cloudflare, and AWS Route 53 are the supported DNS providers. It will also create a secure route (https://cp4d. ) to the CP4D web client using the custom domain used in this role. Note : this role will take no action when mas_manual_cert_mgmt is set to True DNS Management \u00a4 There are two different ways this role controls DNS entries in the provider: Top Level DNS Entries \u00a4 This mode will create the entries directly under your DNS zone. Use this when the DNS zone matches the MAS domain exactly. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mymas.mycompany.com then you will be creating top-level DNS entries for MAS, e.g. admin , home , & api . Subdomain DNS Entries \u00a4 This mode will create DNS entries in the zone under a subdomain. Use this when your DNS zone will be used for more than just one MAS instance. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mycompany.com then you will be creating subdomain DNS entries for MAS, e.g. admin.mymas , home.mymas , & api.mymas . CIS and Cloudflare integrations support both modes of DNS management. A single optional variable is required to enable subdomain DNS management, in the examples above you would set these to mymas : cis_subdomain cloudflare_subdomain Let's Encrypt Integration \u00a4 Both the CIS and Cloudflare options also enable integration with Let's Encrypt for automatic certificate management via IBM Certificate Manager. Each will create a new ClusterIssuer which can be used when installing Maximo Application Suite: Cloudflare Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cloudflare-le-prod IBM Cloud Internet Services Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cis-le-prod If you want to use Let's Encrypt certificates in your MAS installation you will need to configure the mas_cluster_issuer variable in the suite_install role, setting it to the name of the ClusterIssuer as documented above. Note There are issues with how cert-manager works with LetsEncrypt staging servers. It creates a secret for the certificate that doesn't contain the LetsEncrypt CA, but the staging service does not use a well known cert so we end up with MAS unable to trust the certificates generated by LetsEncrypt staging. At present there is no workaround for this, so do not use the LetsEncrypt staging certificate issuer. Role Variables \u00a4 General \u00a4 dns_provider \u00a4 Specifies which DNS provider to use for managing MAS domain DNS entries. Required Environment Variable: DNS_PROVIDER Default: None Purpose : Determines which DNS service will be used to create and manage DNS records for MAS routes and endpoints. Different providers offer different features and integration capabilities. When to use : - Set to cloudflare when using Cloudflare DNS service - Set to cis when using IBM Cloud Internet Services - Set to route53 when using AWS Route 53 - Choose based on your organization's DNS infrastructure Valid values : cis , cloudflare , route53 Impact : The selected provider determines which additional variables are required and which features are available (e.g., Let's Encrypt integration is available for CIS and Cloudflare but not Route53). Related variables : - When cloudflare : Requires cloudflare_email , cloudflare_apitoken , cloudflare_zone - When cis : Requires cis_email , cis_apikey , cis_crn - When route53 : Requires AWS Route53 specific variables Note : This role takes no action when mas_manual_cert_mgmt is set to True . CIS and Cloudflare support Let's Encrypt integration for automatic certificate management. mas_instance_id \u00a4 Unique identifier for the MAS instance requiring DNS configuration. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to configure DNS entries for. This ID is used to generate DNS record names and ClusterIssuer resources specific to this MAS installation. When to use : - Always required for DNS configuration - Must match the instance ID used during MAS core installation - Used to create unique ClusterIssuer names for Let's Encrypt integration Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : This ID is embedded in DNS record names and ClusterIssuer names (e.g., {mas_instance_id}-cloudflare-le-prod ). Incorrect ID will create DNS entries for the wrong instance. Related variables : Works with mas_domain to construct full DNS names for MAS routes. Note : Must match the instance ID from MAS core installation. Used in ClusterIssuer names for Let's Encrypt integration. mas_workspace_id \u00a4 Workspace identifier for the MAS installation requiring DNS configuration. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which MAS workspace to configure DNS entries for. Workspaces are logical divisions within a MAS instance that can have separate configurations and applications. When to use : - Always required for DNS configuration - Must match a workspace ID configured in your MAS instance - Typically set to the primary workspace ID (e.g., masdev , prod ) Valid values : Lowercase alphanumeric string (e.g., masdev , prod , ws1 ) Impact : This ID is used to generate workspace-specific DNS entries and routes. Incorrect workspace ID may result in DNS entries that don't match your MAS workspace configuration. Related variables : Works with mas_instance_id and mas_domain to construct workspace-specific DNS names. Note : Must match an existing workspace in your MAS instance. Each workspace can have its own DNS configuration. mas_domain \u00a4 Custom domain name for accessing MAS web interfaces and APIs. Required Environment Variable: MAS_DOMAIN Default: None Purpose : Defines the base domain used to construct all MAS URLs and DNS entries (e.g., admin.{mas_domain}, home.{mas_domain}, api.{mas_domain}). This domain must be managed by your chosen DNS provider. When to use : - Always required for DNS configuration - Must match the domain configured in your DNS provider's zone - Should align with your organization's domain naming conventions - Must be the same domain used in MAS core installation Valid values : Any valid DNS domain name (e.g., mas.mycompany.com , prod-mas.example.org ) Impact : DNS entries will be created for this domain in your DNS provider. The domain must be properly configured in your DNS provider's zone. All MAS routes will use this domain. Related variables : - Must align with DNS provider zone configuration ( cloudflare_zone , cis_crn , etc.) - Works with mas_instance_id to create unique DNS entries - May use subdomain mode with cloudflare_subdomain or cis_subdomain Note : Ensure this domain is properly configured in your DNS provider before running this role. DNS propagation may take time after entries are created. ocp_ingress \u00a4 Override for the OpenShift cluster ingress domain used in DNS entries. Optional Environment Variable: OCP_INGRESS Default: None (automatically detected from cluster) Purpose : Allows manual specification of the OpenShift cluster's ingress domain when automatic detection fails or when you need to use a specific ingress controller. The ingress domain is used as the target for DNS CNAME records. When to use : - Leave unset to allow automatic detection (recommended) - Set only when automatic detection fails - Set when using a non-default ingress controller - Set in airgap or restricted network environments where detection may not work Valid values : Valid OpenShift ingress domain (e.g., apps.cluster-name.domain.com ) Impact : When set, this value is used as the target for DNS CNAME records instead of the automatically detected ingress. Incorrect value will cause DNS entries to point to the wrong location. Related variables : Works with DNS provider variables to create CNAME records pointing to this ingress. Note : Automatic detection is usually sufficient. Only override if you encounter issues or have specific ingress requirements. cert_manager_namespace \u00a4 OpenShift namespace where Certificate Manager is installed. Optional Environment Variable: CERT_MANAGER_NAMESPACE Default: None (automatically detected) Purpose : Specifies the namespace where Certificate Manager is deployed. This is needed to create ClusterIssuer resources for Let's Encrypt integration in the correct location. When to use : - Leave unset to allow automatic detection (recommended) - Set only if Certificate Manager is installed in a non-standard namespace - Set if automatic detection fails Valid values : Any valid Kubernetes namespace name where Certificate Manager is installed Impact : ClusterIssuer resources for Let's Encrypt will be created referencing this namespace. Incorrect namespace will cause ClusterIssuer creation to fail. Related variables : Used when creating ClusterIssuer for Cloudflare or CIS Let's Encrypt integration. Note : Automatic detection typically finds Certificate Manager in standard namespaces. Only override if using a custom installation. custom_labels \u00a4 Comma-separated list of key=value labels to apply to DNS-related resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to DNS-related resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,component=dns ) Impact : Labels are applied to DNS-related resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect DNS functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management. mas_manual_cert_mgmt \u00a4 Controls whether to disable automatic DNS and certificate management. Optional Environment Variable: MAS_MANUAL_CERT_MGMT Default: false Purpose : Allows you to opt out of automatic DNS entry creation and certificate management when you prefer to manage these manually or through alternative methods. When to use : - Set to true when managing DNS entries manually - Set to true when using custom certificate management solutions - Set to true when DNS provider integration is not available - Leave as false (default) for automated DNS and certificate management Valid values : true , false Impact : - When true : This role takes no action; DNS entries and certificates must be managed manually - When false : Role automatically creates DNS entries and optionally configures Let's Encrypt integration Related variables : When true , all other DNS provider variables are ignored. Note : Setting to true means you are responsible for creating all required DNS entries and managing certificates manually. Ensure proper DNS configuration before MAS installation. output_dir \u00a4 Local directory path where the edge routes output file will be saved. Optional Environment Variable: OUTPUT_DIR Default: . (current directory) Purpose : Specifies where to save the edge-routes-{mas_instance_id}.txt file containing information about the DNS entries and routes created by this role. This file is useful for verification and troubleshooting. When to use : - Set to a specific directory for organized output file management - Use default (current directory) for simple deployments - Set to a shared location for team access to route information Valid values : Any valid local filesystem path (e.g., /home/user/mas-output , ~/masconfig , ./output ) Impact : The edge routes file will be created in this directory. The file contains DNS entry details and can be used to verify DNS configuration. Related variables : Output filename includes mas_instance_id for identification. Note : Ensure the directory exists and is writable. The file provides useful information for verifying DNS setup and troubleshooting connectivity issues. Cloudflare DNS Integration \u00a4 cloudflare_email \u00a4 Email address associated with your Cloudflare account. Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_EMAIL Default: None Purpose : Provides the email address for authenticating with Cloudflare API. This email must be associated with a Cloudflare account that has access to manage the DNS zone. When to use : - Required when dns_provider=cloudflare - Must be the email address used to log into Cloudflare - Account must have permissions to manage DNS records in the target zone Valid values : Valid email address associated with a Cloudflare account Impact : Used together with cloudflare_apitoken to authenticate API requests. Incorrect email will cause authentication failures. Related variables : - cloudflare_apitoken : Must be set together with this email - cloudflare_zone : Zone that this account has access to manage Note : Ensure the Cloudflare account has appropriate permissions to create and manage DNS records in the target zone. cloudflare_apitoken \u00a4 API token for authenticating with Cloudflare API. Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_APITOKEN Default: None Purpose : Provides the API token credential for authenticating with Cloudflare to create and manage DNS records. This token must have appropriate permissions for DNS management. When to use : - Required when dns_provider=cloudflare - Generate from Cloudflare dashboard with DNS edit permissions - Token must have access to the target DNS zone Valid values : Valid Cloudflare API token string Impact : Used together with cloudflare_email to authenticate API requests. Invalid or insufficient permissions will cause DNS operations to fail. Related variables : - cloudflare_email : Must be set together with this token - cloudflare_zone : Zone that this token has permissions to manage Note : Generate the API token following the Cloudflare documentation . Ensure the token has DNS edit permissions for the target zone. Keep this token secure and do not commit to source control. cloudflare_zone \u00a4 DNS zone name managed by Cloudflare where MAS DNS entries will be created. Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_ZONE Default: None Purpose : Specifies the Cloudflare DNS zone (domain) where MAS DNS records will be created. This must be a domain that is already configured and active in your Cloudflare account. When to use : - Required when dns_provider=cloudflare - Must match a zone configured in your Cloudflare account - Should be the parent domain of mas_domain (or equal to it for top-level entries) Valid values : Valid domain name managed in Cloudflare (e.g., mydomain.com , example.org ) Impact : DNS entries will be created in this zone. The zone must exist in Cloudflare and be accessible with the provided credentials. Incorrect zone will cause DNS operations to fail. Related variables : - mas_domain : Should be within this zone (e.g., if zone is mydomain.com , mas_domain could be mas.mydomain.com ) - cloudflare_subdomain : Optional subdomain within this zone for MAS entries - cloudflare_email and cloudflare_apitoken : Must have access to this zone Note : Ensure the zone is active in Cloudflare and DNS is properly delegated before running this role. cloudflare_subdomain \u00a4 Subdomain within the Cloudflare zone for MAS DNS entries (subdomain mode). Optional Environment Variable: CLOUDFLARE_SUBDOMAIN Default: None (uses top-level DNS entries) Purpose : Enables subdomain mode for DNS management, where MAS DNS entries are created under a subdomain rather than at the top level of the zone. This allows multiple MAS instances or other services to share the same DNS zone. When to use : - Use when your DNS zone will host multiple MAS instances or other services - Use when mas_domain is a subdomain of cloudflare_zone - Leave unset when mas_domain equals cloudflare_zone (top-level mode) Valid values : Subdomain name (e.g., if cloudflare_zone=mycompany.com and mas_domain=mas.mycompany.com , set to mas ) Impact : - When set: DNS entries created as admin.{cloudflare_subdomain} , home.{cloudflare_subdomain} , etc. - When unset: DNS entries created as admin , home , etc. directly in the zone Related variables : - cloudflare_zone : Parent zone for this subdomain - mas_domain : Should equal {cloudflare_subdomain}.{cloudflare_zone} Note : The relationship must be: cloudflare_subdomain.cloudflare_zone = mas_domain . For example, if zone is mycompany.com and mas_domain is mas.mycompany.com , set subdomain to mas . IBM Cloud Internet Services DNS Integration \u00a4 Note When using CIS integration, some resources will be installed in the cluster such as RBACs, API Services and CIS Webhook deployments. In OCP 4.12+, to avoid CIS webhook deployment failure at start up, this role will grant anyuid permission to cert-manager-webhook-ibm-cis service account so it can fully access the cert-manager-webhook-ibm-cis deployment pod as a workaround: oc adm policy add-scc-to-user anyuid -z cert-manager-webhook-ibm-cis -n ibm-common-services cis_email \u00a4 Email address for Let's Encrypt ClusterIssuer registration when using IBM CIS. Required if dns_provider is set to cis Environment Variable: CIS_EMAIL Default: None Purpose : Provides the email address that will be registered with Let's Encrypt when creating the ClusterIssuer for automatic certificate management. Let's Encrypt uses this email for certificate expiration notifications and account recovery. When to use : - Required when dns_provider=cis - Use a monitored email address to receive Let's Encrypt notifications - Typically use a team or service email rather than personal email Valid values : Valid email address Impact : This email is embedded in the ClusterIssuer resource and registered with Let's Encrypt. You'll receive notifications about certificate renewals and any issues at this address. Related variables : - cis_apikey and cis_crn : Required together for CIS integration - Used in ClusterIssuer: {mas_instance_id}-cis-le-prod Note : Use a monitored email address as Let's Encrypt sends important notifications about certificate expiration and renewal issues. cis_apikey \u00a4 IBM Cloud API key for authenticating with IBM Cloud Internet Services. Required if dns_provider is set to cis Environment Variable: CIS_APIKEY Default: None Purpose : Provides the IBM Cloud API key credential for authenticating with IBM Cloud Internet Services to create and manage DNS records and configure Let's Encrypt integration. When to use : - Required when dns_provider=cis - Generate from IBM Cloud IAM with appropriate CIS permissions - Key must have access to the CIS instance specified by cis_crn Valid values : Valid IBM Cloud API key string Impact : Used to authenticate all CIS API operations including DNS record management and ClusterIssuer configuration. Invalid key or insufficient permissions will cause operations to fail. Related variables : - cis_crn : CIS instance that this API key has access to - cis_email : Email for Let's Encrypt registration Note : Generate the API key from IBM Cloud IAM. Ensure it has appropriate permissions for the CIS instance. Keep this key secure and do not commit to source control. cis_crn \u00a4 Cloud Resource Name (CRN) identifying the IBM Cloud Internet Services instance. Required if dns_provider is set to cis Environment Variable: CIS_CRN Default: None Purpose : Uniquely identifies the specific IBM Cloud Internet Services instance where DNS records will be managed. The CRN ensures operations target the correct CIS instance when multiple instances exist. When to use : - Required when dns_provider=cis - Obtain from IBM Cloud CIS instance details - Must correspond to the CIS instance containing your DNS zone Valid values : Valid IBM Cloud CRN string (format: crn:v1:bluemix:public:internet-svcs:... ) Impact : All DNS operations will target this CIS instance. Incorrect CRN will cause operations to fail or target the wrong instance. Related variables : - cis_apikey : Must have permissions for this CIS instance - DNS zone must be configured in this CIS instance Note : Find the CRN in the IBM Cloud console under your CIS instance details. Ensure the CRN matches the instance containing your target DNS zone. cis_subdomain \u00a4 Subdomain within the CIS zone for MAS DNS entries (subdomain mode). Optional Environment Variable: CIS_SUBDOMAIN Default: None (uses top-level DNS entries) Purpose : Enables subdomain mode for DNS management, where MAS DNS entries are created under a subdomain rather than at the top level of the CIS zone. This allows multiple MAS instances or other services to share the same DNS zone. When to use : - Use when your CIS zone will host multiple MAS instances or other services - Use when mas_domain is a subdomain of your CIS zone domain - Leave unset when mas_domain equals the CIS zone domain (top-level mode) Valid values : Subdomain name (e.g., if CIS zone is mycompany.com and mas_domain=mas.mycompany.com , set to mas ) Impact : - When set: DNS entries created as admin.{cis_subdomain} , home.{cis_subdomain} , etc. - When unset: DNS entries created as admin , home , etc. directly in the zone Related variables : - CIS zone domain: Parent zone for this subdomain - mas_domain : Should equal {cis_subdomain}.{cis_zone_domain} Note : The relationship must be: cis_subdomain.{cis_zone_domain} = mas_domain . For example, if CIS zone is mycompany.com and mas_domain is mas.mycompany.com , set subdomain to mas . cis_enhanced_security \u00a4 Enables enhanced security features for IBM CIS DNS integration. Optional Environment Variable: CIS_ENHANCED_SECURITY Default: false Purpose : Activates a comprehensive set of security enhancements for CIS including WAF configuration, proxy mode, expanded DNS entries, wildcard prevention, and edge certificates. This provides optimal security for MAS deployments using CIS. When to use : - Set to true for production environments requiring enhanced security - Set to true when security compliance requires WAF and proxy protection - Leave as false for development/test environments or when enhanced features aren't needed Valid values : true , false Impact : When enabled, this configures: - WAF firewall with rules optimized for MAS - Proxy mode for DNS entries (traffic routed through CIS) - Expanded list of DNS entries for comprehensive coverage - Prevention of wildcard DNS entries - Edge certificates in CIS instance Related variables : - cis_waf : Controls WAF specifically (when enhanced_security is true) - cis_proxy : Controls proxy mode (when enhanced_security is true) Note : See IBM Cloud CIS security documentation for details. Enhanced security may impact performance due to proxy routing. Enhanced IBM CIS DNS Integration Security \u00a4 See the cis_enhanced_security variable above for details. cis_waf \u00a4 Controls Web Application Firewall (WAF) configuration for CIS. Optional Environment Variable: CIS_WAF Default: true Purpose : Enables or disables WAF configuration with rules optimized for MAS application functionality. WAF provides protection against common web attacks while ensuring MAS features work correctly. When to use : - Leave as true (default) for production environments requiring WAF protection - Set to false only if WAF causes issues or conflicts with other security tools - Typically used in conjunction with cis_enhanced_security Valid values : true , false Impact : - When true : Configures WAF with rules that protect MAS while allowing required functionality - When false : WAF is not configured (less security protection) Related variables : - cis_enhanced_security : When true, this setting is part of enhanced security features - cis_proxy : Often used together for comprehensive security Note : WAF rules are specifically tuned to avoid blocking legitimate MAS application traffic. Default is true for security best practices. cis_proxy \u00a4 Controls whether DNS entries use CIS proxy mode (traffic routed through CIS). Optional Environment Variable: CIS_PROXY Default: false Purpose : Enables proxy mode for DNS entries, routing traffic through IBM CIS infrastructure. This provides additional security, DDoS protection, and performance optimization but adds latency. When to use : - Set to true for production environments requiring DDoS protection - Set to true when using CIS security features (WAF, rate limiting) - Set to false for direct routing (lower latency, less protection) - Typically enabled with cis_enhanced_security Valid values : true , false Impact : - When true : Traffic routes through CIS (orange cloud icon in CIS console), enabling security features but adding latency - When false : DNS entries point directly to OpenShift ingress (grey cloud icon), lower latency but no CIS protection Related variables : - cis_enhanced_security : When true, proxy is typically enabled - cis_waf : Requires proxy mode to function Note : Proxy mode is required for WAF and other CIS security features to work. Consider the latency vs. security tradeoff for your use case. cis_service_name \u00a4 Custom name for the CIS service resources created in the cluster. Optional Environment Variable: CIS_SERVICE_NAME Default: {ClusterName}-cis-{mas_instance_id} (auto-generated) Purpose : Allows customization of the CIS service name used for resources created in the OpenShift cluster. The default name is automatically generated from the cluster name and MAS instance ID. When to use : - Leave unset to use the auto-generated name (recommended) - Set only when you need a specific naming convention - Set when the auto-generated name conflicts with existing resources Valid values : Valid Kubernetes resource name (lowercase alphanumeric with hyphens) Impact : This name is used for CIS-related resources in the cluster. Changing it after initial deployment may cause issues with resource management. Related variables : Works with mas_instance_id in the default naming pattern. Note : The default auto-generated name is usually sufficient. Only override if you have specific naming requirements or conflicts. - Default: None update_dns \u00a4 Set this to false if you want to not update DNS entries if they already exist. Optional Environment Variable: UPDATE_DNS_ENTRIES Default: true delete_wildcards \u00a4 Set this to true to force deletion of wildcard dns entries in cis. Optional Environment Variable: DELETE_WILDCARDS Default: false override_edge_certs \u00a4 Set this to false to not override and delete any existing edge certificates in cis instance when creating new edge certificates. Optional Environment Variable: OVERRIDE_EDGE_CERTS Default: true cis_entries_to_add \u00a4 Comma separated list of entries to add for edge certificates. These are broken down into functional areas of MAS. The options are: all (include all entries - default), core (MAS Core), health (MAS Health App), iot (MAS IoT app), manage (MAS Manage app), monitor (MAS Monitor app), predict (MAS Predict app), visualinspection (MAS VisualInspection app), optimizer (MAS Optimizer app), assist (MAS Assist app), arcgis (MAS Arcgis), reportdb (MAS ReportDB), facilities (MAS Facilities app). Optional Environment Variable: CIS_ENTRIES_TO_ADD Default: all AWS Route 53 \u00a4 Prerequisites: To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Note In some cases, the Route 53 zone may not resolve the certificate challenges generated by IBM Certificate Manager, which could cause a problem while issuing the public certificates via Let's Encrypt. In this case, a manual workaround might be needed in cert-manager-controller pod to enable recursive nameservers. For more details on how to apply this workaround, refer to this documentation . This workaround is not automated, as most of the times this is not needed, but when it is, it requires the cert-manager-controller pod to be stopped, thus we want to avoid making this behavior as standard approach. route53_hosted_zone_name \u00a4 AWS Route53 Hosted Zone name. Required if dns_provider is set to route53 Environment Variable: ROUTE53_HOSTED_ZONE_NAME Default: None route53_hosted_zone_region \u00a4 AWS Route53 Hosted Zone region. Optional Environment Variable: ROUTE53_HOSTED_ZONE_REGION Default: Same value as defined in AWS_REGION , or if none defined, then us-east-2 is the defaulted region route53_subdomain \u00a4 If a subdomain is defined, this will be used to create the corresponding CNAME entries in the targeted Route53 hosted zone instance. Therefore, the Route53 subdomain + the Route53 hosted zone name defined, when combined, needs to match with the chosen MAS Domain, otherwise the DNS records won't be able to get resolved. Example: MAS Top Level Domain my-mas-instance.mycompany.com , AWS Route53 hosted zone name mycompany.com , AWS Route53 subdomain my-mas-instance . Optional Environment Variable: ROUTE53_SUBDOMAIN Default: None route53_email \u00a4 AWS Route53 contact e-mail. Will be set in the cluster issuer created in order to receive alerts. Optional Environment Variable: ROUTE53_EMAIL Default: None aws_access_key_id \u00a4 AWS access key ID for authentication. Required if dns_provider is set to route53 Environment Variable: AWS_ACCESS_KEY_ID Default: None aws_secret_access_key \u00a4 AWS secret access key for authentication. Required if dns_provider is set to route53 Environment Variable: AWS_SECRET_ACCESS_KEY Default: None CloudPak for Data \u00a4 cpd_instance_namespace \u00a4 Namespace where the Cloud Pak for Data is installed and the cpd route exists. If set, then this role will also attempt to configure public certificates to the CPD route using the DNS provider defined. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default: None cpd_prod_issuer_name \u00a4 Define the certificate issuer responsible for generating the public certificate for your CPD route. If not set, then it will use same issuer set for MAS instance. Optional Environment Variable: CPD_PROD_ISSUER_NAME Default: Same certificate issuer used for MAS instance cpd_custom_domain \u00a4 Define the custom domain for your CPD route. If not set, then it will use same domain set for MAS instance. Optional Environment Variable: CPD_CUSTOM_DOMAIN Default: cp4d.{{ mas_domain }} Example Playbook \u00a4 CIS or Cloudflare \u00a4 - hosts: localhost any_errors_fatal: true vars: dns_provider: cis # or cloudflare mas_instance_id: inst1 mas_domain: mydomain.com cis_crn: xxx cis_apikey: xxx cis_email: xxx roles: - ibm.mas_devops.suite_dns AWS Route 53 \u00a4 - hosts: localhost any_errors_fatal: true vars: dns_provider: route53 mas_instance_id: inst1 mas_domain: inst1.mydomain.com aws_access_key_id: xxx aws_secret_access_key: xxx route53_hosted_zone_name: mydomain.com route53_hosted_zone_region: us-east-2 route53_subdomain: inst1 route53_email: anyemail@test.com roles: - ibm.mas_devops.suite_dns License \u00a4 EPL-2.0","title":"suite_dns"},{"location":"roles/suite_dns/#suite_dns","text":"This role will manage MAS and DNS provider integration. IBM Cloud Internet Services, Cloudflare, and AWS Route 53 are the supported DNS providers. It will also create a secure route (https://cp4d. ) to the CP4D web client using the custom domain used in this role. Note : this role will take no action when mas_manual_cert_mgmt is set to True","title":"suite_dns"},{"location":"roles/suite_dns/#dns-management","text":"There are two different ways this role controls DNS entries in the provider:","title":"DNS Management"},{"location":"roles/suite_dns/#top-level-dns-entries","text":"This mode will create the entries directly under your DNS zone. Use this when the DNS zone matches the MAS domain exactly. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mymas.mycompany.com then you will be creating top-level DNS entries for MAS, e.g. admin , home , & api .","title":"Top Level DNS Entries"},{"location":"roles/suite_dns/#subdomain-dns-entries","text":"This mode will create DNS entries in the zone under a subdomain. Use this when your DNS zone will be used for more than just one MAS instance. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mycompany.com then you will be creating subdomain DNS entries for MAS, e.g. admin.mymas , home.mymas , & api.mymas . CIS and Cloudflare integrations support both modes of DNS management. A single optional variable is required to enable subdomain DNS management, in the examples above you would set these to mymas : cis_subdomain cloudflare_subdomain","title":"Subdomain DNS Entries"},{"location":"roles/suite_dns/#lets-encrypt-integration","text":"Both the CIS and Cloudflare options also enable integration with Let's Encrypt for automatic certificate management via IBM Certificate Manager. Each will create a new ClusterIssuer which can be used when installing Maximo Application Suite: Cloudflare Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cloudflare-le-prod IBM Cloud Internet Services Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cis-le-prod If you want to use Let's Encrypt certificates in your MAS installation you will need to configure the mas_cluster_issuer variable in the suite_install role, setting it to the name of the ClusterIssuer as documented above. Note There are issues with how cert-manager works with LetsEncrypt staging servers. It creates a secret for the certificate that doesn't contain the LetsEncrypt CA, but the staging service does not use a well known cert so we end up with MAS unable to trust the certificates generated by LetsEncrypt staging. At present there is no workaround for this, so do not use the LetsEncrypt staging certificate issuer.","title":"Let's Encrypt Integration"},{"location":"roles/suite_dns/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_dns/#general","text":"","title":"General"},{"location":"roles/suite_dns/#dns_provider","text":"Specifies which DNS provider to use for managing MAS domain DNS entries. Required Environment Variable: DNS_PROVIDER Default: None Purpose : Determines which DNS service will be used to create and manage DNS records for MAS routes and endpoints. Different providers offer different features and integration capabilities. When to use : - Set to cloudflare when using Cloudflare DNS service - Set to cis when using IBM Cloud Internet Services - Set to route53 when using AWS Route 53 - Choose based on your organization's DNS infrastructure Valid values : cis , cloudflare , route53 Impact : The selected provider determines which additional variables are required and which features are available (e.g., Let's Encrypt integration is available for CIS and Cloudflare but not Route53). Related variables : - When cloudflare : Requires cloudflare_email , cloudflare_apitoken , cloudflare_zone - When cis : Requires cis_email , cis_apikey , cis_crn - When route53 : Requires AWS Route53 specific variables Note : This role takes no action when mas_manual_cert_mgmt is set to True . CIS and Cloudflare support Let's Encrypt integration for automatic certificate management.","title":"dns_provider"},{"location":"roles/suite_dns/#mas_instance_id","text":"Unique identifier for the MAS instance requiring DNS configuration. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to configure DNS entries for. This ID is used to generate DNS record names and ClusterIssuer resources specific to this MAS installation. When to use : - Always required for DNS configuration - Must match the instance ID used during MAS core installation - Used to create unique ClusterIssuer names for Let's Encrypt integration Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : This ID is embedded in DNS record names and ClusterIssuer names (e.g., {mas_instance_id}-cloudflare-le-prod ). Incorrect ID will create DNS entries for the wrong instance. Related variables : Works with mas_domain to construct full DNS names for MAS routes. Note : Must match the instance ID from MAS core installation. Used in ClusterIssuer names for Let's Encrypt integration.","title":"mas_instance_id"},{"location":"roles/suite_dns/#mas_workspace_id","text":"Workspace identifier for the MAS installation requiring DNS configuration. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which MAS workspace to configure DNS entries for. Workspaces are logical divisions within a MAS instance that can have separate configurations and applications. When to use : - Always required for DNS configuration - Must match a workspace ID configured in your MAS instance - Typically set to the primary workspace ID (e.g., masdev , prod ) Valid values : Lowercase alphanumeric string (e.g., masdev , prod , ws1 ) Impact : This ID is used to generate workspace-specific DNS entries and routes. Incorrect workspace ID may result in DNS entries that don't match your MAS workspace configuration. Related variables : Works with mas_instance_id and mas_domain to construct workspace-specific DNS names. Note : Must match an existing workspace in your MAS instance. Each workspace can have its own DNS configuration.","title":"mas_workspace_id"},{"location":"roles/suite_dns/#mas_domain","text":"Custom domain name for accessing MAS web interfaces and APIs. Required Environment Variable: MAS_DOMAIN Default: None Purpose : Defines the base domain used to construct all MAS URLs and DNS entries (e.g., admin.{mas_domain}, home.{mas_domain}, api.{mas_domain}). This domain must be managed by your chosen DNS provider. When to use : - Always required for DNS configuration - Must match the domain configured in your DNS provider's zone - Should align with your organization's domain naming conventions - Must be the same domain used in MAS core installation Valid values : Any valid DNS domain name (e.g., mas.mycompany.com , prod-mas.example.org ) Impact : DNS entries will be created for this domain in your DNS provider. The domain must be properly configured in your DNS provider's zone. All MAS routes will use this domain. Related variables : - Must align with DNS provider zone configuration ( cloudflare_zone , cis_crn , etc.) - Works with mas_instance_id to create unique DNS entries - May use subdomain mode with cloudflare_subdomain or cis_subdomain Note : Ensure this domain is properly configured in your DNS provider before running this role. DNS propagation may take time after entries are created.","title":"mas_domain"},{"location":"roles/suite_dns/#ocp_ingress","text":"Override for the OpenShift cluster ingress domain used in DNS entries. Optional Environment Variable: OCP_INGRESS Default: None (automatically detected from cluster) Purpose : Allows manual specification of the OpenShift cluster's ingress domain when automatic detection fails or when you need to use a specific ingress controller. The ingress domain is used as the target for DNS CNAME records. When to use : - Leave unset to allow automatic detection (recommended) - Set only when automatic detection fails - Set when using a non-default ingress controller - Set in airgap or restricted network environments where detection may not work Valid values : Valid OpenShift ingress domain (e.g., apps.cluster-name.domain.com ) Impact : When set, this value is used as the target for DNS CNAME records instead of the automatically detected ingress. Incorrect value will cause DNS entries to point to the wrong location. Related variables : Works with DNS provider variables to create CNAME records pointing to this ingress. Note : Automatic detection is usually sufficient. Only override if you encounter issues or have specific ingress requirements.","title":"ocp_ingress"},{"location":"roles/suite_dns/#cert_manager_namespace","text":"OpenShift namespace where Certificate Manager is installed. Optional Environment Variable: CERT_MANAGER_NAMESPACE Default: None (automatically detected) Purpose : Specifies the namespace where Certificate Manager is deployed. This is needed to create ClusterIssuer resources for Let's Encrypt integration in the correct location. When to use : - Leave unset to allow automatic detection (recommended) - Set only if Certificate Manager is installed in a non-standard namespace - Set if automatic detection fails Valid values : Any valid Kubernetes namespace name where Certificate Manager is installed Impact : ClusterIssuer resources for Let's Encrypt will be created referencing this namespace. Incorrect namespace will cause ClusterIssuer creation to fail. Related variables : Used when creating ClusterIssuer for Cloudflare or CIS Let's Encrypt integration. Note : Automatic detection typically finds Certificate Manager in standard namespaces. Only override if using a custom installation.","title":"cert_manager_namespace"},{"location":"roles/suite_dns/#custom_labels","text":"Comma-separated list of key=value labels to apply to DNS-related resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to DNS-related resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,component=dns ) Impact : Labels are applied to DNS-related resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect DNS functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management.","title":"custom_labels"},{"location":"roles/suite_dns/#mas_manual_cert_mgmt","text":"Controls whether to disable automatic DNS and certificate management. Optional Environment Variable: MAS_MANUAL_CERT_MGMT Default: false Purpose : Allows you to opt out of automatic DNS entry creation and certificate management when you prefer to manage these manually or through alternative methods. When to use : - Set to true when managing DNS entries manually - Set to true when using custom certificate management solutions - Set to true when DNS provider integration is not available - Leave as false (default) for automated DNS and certificate management Valid values : true , false Impact : - When true : This role takes no action; DNS entries and certificates must be managed manually - When false : Role automatically creates DNS entries and optionally configures Let's Encrypt integration Related variables : When true , all other DNS provider variables are ignored. Note : Setting to true means you are responsible for creating all required DNS entries and managing certificates manually. Ensure proper DNS configuration before MAS installation.","title":"mas_manual_cert_mgmt"},{"location":"roles/suite_dns/#output_dir","text":"Local directory path where the edge routes output file will be saved. Optional Environment Variable: OUTPUT_DIR Default: . (current directory) Purpose : Specifies where to save the edge-routes-{mas_instance_id}.txt file containing information about the DNS entries and routes created by this role. This file is useful for verification and troubleshooting. When to use : - Set to a specific directory for organized output file management - Use default (current directory) for simple deployments - Set to a shared location for team access to route information Valid values : Any valid local filesystem path (e.g., /home/user/mas-output , ~/masconfig , ./output ) Impact : The edge routes file will be created in this directory. The file contains DNS entry details and can be used to verify DNS configuration. Related variables : Output filename includes mas_instance_id for identification. Note : Ensure the directory exists and is writable. The file provides useful information for verifying DNS setup and troubleshooting connectivity issues.","title":"output_dir"},{"location":"roles/suite_dns/#cloudflare-dns-integration","text":"","title":"Cloudflare DNS Integration"},{"location":"roles/suite_dns/#cloudflare_email","text":"Email address associated with your Cloudflare account. Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_EMAIL Default: None Purpose : Provides the email address for authenticating with Cloudflare API. This email must be associated with a Cloudflare account that has access to manage the DNS zone. When to use : - Required when dns_provider=cloudflare - Must be the email address used to log into Cloudflare - Account must have permissions to manage DNS records in the target zone Valid values : Valid email address associated with a Cloudflare account Impact : Used together with cloudflare_apitoken to authenticate API requests. Incorrect email will cause authentication failures. Related variables : - cloudflare_apitoken : Must be set together with this email - cloudflare_zone : Zone that this account has access to manage Note : Ensure the Cloudflare account has appropriate permissions to create and manage DNS records in the target zone.","title":"cloudflare_email"},{"location":"roles/suite_dns/#cloudflare_apitoken","text":"API token for authenticating with Cloudflare API. Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_APITOKEN Default: None Purpose : Provides the API token credential for authenticating with Cloudflare to create and manage DNS records. This token must have appropriate permissions for DNS management. When to use : - Required when dns_provider=cloudflare - Generate from Cloudflare dashboard with DNS edit permissions - Token must have access to the target DNS zone Valid values : Valid Cloudflare API token string Impact : Used together with cloudflare_email to authenticate API requests. Invalid or insufficient permissions will cause DNS operations to fail. Related variables : - cloudflare_email : Must be set together with this token - cloudflare_zone : Zone that this token has permissions to manage Note : Generate the API token following the Cloudflare documentation . Ensure the token has DNS edit permissions for the target zone. Keep this token secure and do not commit to source control.","title":"cloudflare_apitoken"},{"location":"roles/suite_dns/#cloudflare_zone","text":"DNS zone name managed by Cloudflare where MAS DNS entries will be created. Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_ZONE Default: None Purpose : Specifies the Cloudflare DNS zone (domain) where MAS DNS records will be created. This must be a domain that is already configured and active in your Cloudflare account. When to use : - Required when dns_provider=cloudflare - Must match a zone configured in your Cloudflare account - Should be the parent domain of mas_domain (or equal to it for top-level entries) Valid values : Valid domain name managed in Cloudflare (e.g., mydomain.com , example.org ) Impact : DNS entries will be created in this zone. The zone must exist in Cloudflare and be accessible with the provided credentials. Incorrect zone will cause DNS operations to fail. Related variables : - mas_domain : Should be within this zone (e.g., if zone is mydomain.com , mas_domain could be mas.mydomain.com ) - cloudflare_subdomain : Optional subdomain within this zone for MAS entries - cloudflare_email and cloudflare_apitoken : Must have access to this zone Note : Ensure the zone is active in Cloudflare and DNS is properly delegated before running this role.","title":"cloudflare_zone"},{"location":"roles/suite_dns/#cloudflare_subdomain","text":"Subdomain within the Cloudflare zone for MAS DNS entries (subdomain mode). Optional Environment Variable: CLOUDFLARE_SUBDOMAIN Default: None (uses top-level DNS entries) Purpose : Enables subdomain mode for DNS management, where MAS DNS entries are created under a subdomain rather than at the top level of the zone. This allows multiple MAS instances or other services to share the same DNS zone. When to use : - Use when your DNS zone will host multiple MAS instances or other services - Use when mas_domain is a subdomain of cloudflare_zone - Leave unset when mas_domain equals cloudflare_zone (top-level mode) Valid values : Subdomain name (e.g., if cloudflare_zone=mycompany.com and mas_domain=mas.mycompany.com , set to mas ) Impact : - When set: DNS entries created as admin.{cloudflare_subdomain} , home.{cloudflare_subdomain} , etc. - When unset: DNS entries created as admin , home , etc. directly in the zone Related variables : - cloudflare_zone : Parent zone for this subdomain - mas_domain : Should equal {cloudflare_subdomain}.{cloudflare_zone} Note : The relationship must be: cloudflare_subdomain.cloudflare_zone = mas_domain . For example, if zone is mycompany.com and mas_domain is mas.mycompany.com , set subdomain to mas .","title":"cloudflare_subdomain"},{"location":"roles/suite_dns/#ibm-cloud-internet-services-dns-integration","text":"Note When using CIS integration, some resources will be installed in the cluster such as RBACs, API Services and CIS Webhook deployments. In OCP 4.12+, to avoid CIS webhook deployment failure at start up, this role will grant anyuid permission to cert-manager-webhook-ibm-cis service account so it can fully access the cert-manager-webhook-ibm-cis deployment pod as a workaround: oc adm policy add-scc-to-user anyuid -z cert-manager-webhook-ibm-cis -n ibm-common-services","title":"IBM Cloud Internet Services DNS Integration"},{"location":"roles/suite_dns/#cis_email","text":"Email address for Let's Encrypt ClusterIssuer registration when using IBM CIS. Required if dns_provider is set to cis Environment Variable: CIS_EMAIL Default: None Purpose : Provides the email address that will be registered with Let's Encrypt when creating the ClusterIssuer for automatic certificate management. Let's Encrypt uses this email for certificate expiration notifications and account recovery. When to use : - Required when dns_provider=cis - Use a monitored email address to receive Let's Encrypt notifications - Typically use a team or service email rather than personal email Valid values : Valid email address Impact : This email is embedded in the ClusterIssuer resource and registered with Let's Encrypt. You'll receive notifications about certificate renewals and any issues at this address. Related variables : - cis_apikey and cis_crn : Required together for CIS integration - Used in ClusterIssuer: {mas_instance_id}-cis-le-prod Note : Use a monitored email address as Let's Encrypt sends important notifications about certificate expiration and renewal issues.","title":"cis_email"},{"location":"roles/suite_dns/#cis_apikey","text":"IBM Cloud API key for authenticating with IBM Cloud Internet Services. Required if dns_provider is set to cis Environment Variable: CIS_APIKEY Default: None Purpose : Provides the IBM Cloud API key credential for authenticating with IBM Cloud Internet Services to create and manage DNS records and configure Let's Encrypt integration. When to use : - Required when dns_provider=cis - Generate from IBM Cloud IAM with appropriate CIS permissions - Key must have access to the CIS instance specified by cis_crn Valid values : Valid IBM Cloud API key string Impact : Used to authenticate all CIS API operations including DNS record management and ClusterIssuer configuration. Invalid key or insufficient permissions will cause operations to fail. Related variables : - cis_crn : CIS instance that this API key has access to - cis_email : Email for Let's Encrypt registration Note : Generate the API key from IBM Cloud IAM. Ensure it has appropriate permissions for the CIS instance. Keep this key secure and do not commit to source control.","title":"cis_apikey"},{"location":"roles/suite_dns/#cis_crn","text":"Cloud Resource Name (CRN) identifying the IBM Cloud Internet Services instance. Required if dns_provider is set to cis Environment Variable: CIS_CRN Default: None Purpose : Uniquely identifies the specific IBM Cloud Internet Services instance where DNS records will be managed. The CRN ensures operations target the correct CIS instance when multiple instances exist. When to use : - Required when dns_provider=cis - Obtain from IBM Cloud CIS instance details - Must correspond to the CIS instance containing your DNS zone Valid values : Valid IBM Cloud CRN string (format: crn:v1:bluemix:public:internet-svcs:... ) Impact : All DNS operations will target this CIS instance. Incorrect CRN will cause operations to fail or target the wrong instance. Related variables : - cis_apikey : Must have permissions for this CIS instance - DNS zone must be configured in this CIS instance Note : Find the CRN in the IBM Cloud console under your CIS instance details. Ensure the CRN matches the instance containing your target DNS zone.","title":"cis_crn"},{"location":"roles/suite_dns/#cis_subdomain","text":"Subdomain within the CIS zone for MAS DNS entries (subdomain mode). Optional Environment Variable: CIS_SUBDOMAIN Default: None (uses top-level DNS entries) Purpose : Enables subdomain mode for DNS management, where MAS DNS entries are created under a subdomain rather than at the top level of the CIS zone. This allows multiple MAS instances or other services to share the same DNS zone. When to use : - Use when your CIS zone will host multiple MAS instances or other services - Use when mas_domain is a subdomain of your CIS zone domain - Leave unset when mas_domain equals the CIS zone domain (top-level mode) Valid values : Subdomain name (e.g., if CIS zone is mycompany.com and mas_domain=mas.mycompany.com , set to mas ) Impact : - When set: DNS entries created as admin.{cis_subdomain} , home.{cis_subdomain} , etc. - When unset: DNS entries created as admin , home , etc. directly in the zone Related variables : - CIS zone domain: Parent zone for this subdomain - mas_domain : Should equal {cis_subdomain}.{cis_zone_domain} Note : The relationship must be: cis_subdomain.{cis_zone_domain} = mas_domain . For example, if CIS zone is mycompany.com and mas_domain is mas.mycompany.com , set subdomain to mas .","title":"cis_subdomain"},{"location":"roles/suite_dns/#cis_enhanced_security","text":"Enables enhanced security features for IBM CIS DNS integration. Optional Environment Variable: CIS_ENHANCED_SECURITY Default: false Purpose : Activates a comprehensive set of security enhancements for CIS including WAF configuration, proxy mode, expanded DNS entries, wildcard prevention, and edge certificates. This provides optimal security for MAS deployments using CIS. When to use : - Set to true for production environments requiring enhanced security - Set to true when security compliance requires WAF and proxy protection - Leave as false for development/test environments or when enhanced features aren't needed Valid values : true , false Impact : When enabled, this configures: - WAF firewall with rules optimized for MAS - Proxy mode for DNS entries (traffic routed through CIS) - Expanded list of DNS entries for comprehensive coverage - Prevention of wildcard DNS entries - Edge certificates in CIS instance Related variables : - cis_waf : Controls WAF specifically (when enhanced_security is true) - cis_proxy : Controls proxy mode (when enhanced_security is true) Note : See IBM Cloud CIS security documentation for details. Enhanced security may impact performance due to proxy routing.","title":"cis_enhanced_security"},{"location":"roles/suite_dns/#enhanced-ibm-cis-dns-integration-security","text":"See the cis_enhanced_security variable above for details.","title":"Enhanced IBM CIS DNS Integration Security"},{"location":"roles/suite_dns/#cis_waf","text":"Controls Web Application Firewall (WAF) configuration for CIS. Optional Environment Variable: CIS_WAF Default: true Purpose : Enables or disables WAF configuration with rules optimized for MAS application functionality. WAF provides protection against common web attacks while ensuring MAS features work correctly. When to use : - Leave as true (default) for production environments requiring WAF protection - Set to false only if WAF causes issues or conflicts with other security tools - Typically used in conjunction with cis_enhanced_security Valid values : true , false Impact : - When true : Configures WAF with rules that protect MAS while allowing required functionality - When false : WAF is not configured (less security protection) Related variables : - cis_enhanced_security : When true, this setting is part of enhanced security features - cis_proxy : Often used together for comprehensive security Note : WAF rules are specifically tuned to avoid blocking legitimate MAS application traffic. Default is true for security best practices.","title":"cis_waf"},{"location":"roles/suite_dns/#cis_proxy","text":"Controls whether DNS entries use CIS proxy mode (traffic routed through CIS). Optional Environment Variable: CIS_PROXY Default: false Purpose : Enables proxy mode for DNS entries, routing traffic through IBM CIS infrastructure. This provides additional security, DDoS protection, and performance optimization but adds latency. When to use : - Set to true for production environments requiring DDoS protection - Set to true when using CIS security features (WAF, rate limiting) - Set to false for direct routing (lower latency, less protection) - Typically enabled with cis_enhanced_security Valid values : true , false Impact : - When true : Traffic routes through CIS (orange cloud icon in CIS console), enabling security features but adding latency - When false : DNS entries point directly to OpenShift ingress (grey cloud icon), lower latency but no CIS protection Related variables : - cis_enhanced_security : When true, proxy is typically enabled - cis_waf : Requires proxy mode to function Note : Proxy mode is required for WAF and other CIS security features to work. Consider the latency vs. security tradeoff for your use case.","title":"cis_proxy"},{"location":"roles/suite_dns/#cis_service_name","text":"Custom name for the CIS service resources created in the cluster. Optional Environment Variable: CIS_SERVICE_NAME Default: {ClusterName}-cis-{mas_instance_id} (auto-generated) Purpose : Allows customization of the CIS service name used for resources created in the OpenShift cluster. The default name is automatically generated from the cluster name and MAS instance ID. When to use : - Leave unset to use the auto-generated name (recommended) - Set only when you need a specific naming convention - Set when the auto-generated name conflicts with existing resources Valid values : Valid Kubernetes resource name (lowercase alphanumeric with hyphens) Impact : This name is used for CIS-related resources in the cluster. Changing it after initial deployment may cause issues with resource management. Related variables : Works with mas_instance_id in the default naming pattern. Note : The default auto-generated name is usually sufficient. Only override if you have specific naming requirements or conflicts. - Default: None","title":"cis_service_name"},{"location":"roles/suite_dns/#update_dns","text":"Set this to false if you want to not update DNS entries if they already exist. Optional Environment Variable: UPDATE_DNS_ENTRIES Default: true","title":"update_dns"},{"location":"roles/suite_dns/#delete_wildcards","text":"Set this to true to force deletion of wildcard dns entries in cis. Optional Environment Variable: DELETE_WILDCARDS Default: false","title":"delete_wildcards"},{"location":"roles/suite_dns/#override_edge_certs","text":"Set this to false to not override and delete any existing edge certificates in cis instance when creating new edge certificates. Optional Environment Variable: OVERRIDE_EDGE_CERTS Default: true","title":"override_edge_certs"},{"location":"roles/suite_dns/#cis_entries_to_add","text":"Comma separated list of entries to add for edge certificates. These are broken down into functional areas of MAS. The options are: all (include all entries - default), core (MAS Core), health (MAS Health App), iot (MAS IoT app), manage (MAS Manage app), monitor (MAS Monitor app), predict (MAS Predict app), visualinspection (MAS VisualInspection app), optimizer (MAS Optimizer app), assist (MAS Assist app), arcgis (MAS Arcgis), reportdb (MAS ReportDB), facilities (MAS Facilities app). Optional Environment Variable: CIS_ENTRIES_TO_ADD Default: all","title":"cis_entries_to_add"},{"location":"roles/suite_dns/#aws-route-53","text":"Prerequisites: To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Note In some cases, the Route 53 zone may not resolve the certificate challenges generated by IBM Certificate Manager, which could cause a problem while issuing the public certificates via Let's Encrypt. In this case, a manual workaround might be needed in cert-manager-controller pod to enable recursive nameservers. For more details on how to apply this workaround, refer to this documentation . This workaround is not automated, as most of the times this is not needed, but when it is, it requires the cert-manager-controller pod to be stopped, thus we want to avoid making this behavior as standard approach.","title":"AWS Route 53"},{"location":"roles/suite_dns/#route53_hosted_zone_name","text":"AWS Route53 Hosted Zone name. Required if dns_provider is set to route53 Environment Variable: ROUTE53_HOSTED_ZONE_NAME Default: None","title":"route53_hosted_zone_name"},{"location":"roles/suite_dns/#route53_hosted_zone_region","text":"AWS Route53 Hosted Zone region. Optional Environment Variable: ROUTE53_HOSTED_ZONE_REGION Default: Same value as defined in AWS_REGION , or if none defined, then us-east-2 is the defaulted region","title":"route53_hosted_zone_region"},{"location":"roles/suite_dns/#route53_subdomain","text":"If a subdomain is defined, this will be used to create the corresponding CNAME entries in the targeted Route53 hosted zone instance. Therefore, the Route53 subdomain + the Route53 hosted zone name defined, when combined, needs to match with the chosen MAS Domain, otherwise the DNS records won't be able to get resolved. Example: MAS Top Level Domain my-mas-instance.mycompany.com , AWS Route53 hosted zone name mycompany.com , AWS Route53 subdomain my-mas-instance . Optional Environment Variable: ROUTE53_SUBDOMAIN Default: None","title":"route53_subdomain"},{"location":"roles/suite_dns/#route53_email","text":"AWS Route53 contact e-mail. Will be set in the cluster issuer created in order to receive alerts. Optional Environment Variable: ROUTE53_EMAIL Default: None","title":"route53_email"},{"location":"roles/suite_dns/#aws_access_key_id","text":"AWS access key ID for authentication. Required if dns_provider is set to route53 Environment Variable: AWS_ACCESS_KEY_ID Default: None","title":"aws_access_key_id"},{"location":"roles/suite_dns/#aws_secret_access_key","text":"AWS secret access key for authentication. Required if dns_provider is set to route53 Environment Variable: AWS_SECRET_ACCESS_KEY Default: None","title":"aws_secret_access_key"},{"location":"roles/suite_dns/#cloudpak-for-data","text":"","title":"CloudPak for Data"},{"location":"roles/suite_dns/#cpd_instance_namespace","text":"Namespace where the Cloud Pak for Data is installed and the cpd route exists. If set, then this role will also attempt to configure public certificates to the CPD route using the DNS provider defined. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default: None","title":"cpd_instance_namespace"},{"location":"roles/suite_dns/#cpd_prod_issuer_name","text":"Define the certificate issuer responsible for generating the public certificate for your CPD route. If not set, then it will use same issuer set for MAS instance. Optional Environment Variable: CPD_PROD_ISSUER_NAME Default: Same certificate issuer used for MAS instance","title":"cpd_prod_issuer_name"},{"location":"roles/suite_dns/#cpd_custom_domain","text":"Define the custom domain for your CPD route. If not set, then it will use same domain set for MAS instance. Optional Environment Variable: CPD_CUSTOM_DOMAIN Default: cp4d.{{ mas_domain }}","title":"cpd_custom_domain"},{"location":"roles/suite_dns/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_dns/#cis-or-cloudflare","text":"- hosts: localhost any_errors_fatal: true vars: dns_provider: cis # or cloudflare mas_instance_id: inst1 mas_domain: mydomain.com cis_crn: xxx cis_apikey: xxx cis_email: xxx roles: - ibm.mas_devops.suite_dns","title":"CIS or Cloudflare"},{"location":"roles/suite_dns/#aws-route-53_1","text":"- hosts: localhost any_errors_fatal: true vars: dns_provider: route53 mas_instance_id: inst1 mas_domain: inst1.mydomain.com aws_access_key_id: xxx aws_secret_access_key: xxx route53_hosted_zone_name: mydomain.com route53_hosted_zone_region: us-east-2 route53_subdomain: inst1 route53_email: anyemail@test.com roles: - ibm.mas_devops.suite_dns","title":"AWS Route 53"},{"location":"roles/suite_dns/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_install/","text":"suite_install \u00a4 This role installs Maximo Application Suite. It internally resolves the namespace based on the mas_instance_id as mas-{mas_instance_id}-core . Role Variables \u00a4 Basic Install \u00a4 mas_catalog_source \u00a4 Specifies the OpenShift operator catalog source containing the MAS operator subscription. Optional Environment Variable: MAS_CATALOG_SOURCE Default: ibm-operator-catalog Purpose : Controls which operator catalog is used to locate and install the MAS operator. This determines where OpenShift looks for the operator images and metadata during installation. When to use : - Use default ibm-operator-catalog for production installations with official IBM releases - Use ibm-operator-catalog for development installations as well (supports both use cases) - Only change if directed to use a custom catalog for specific testing or airgap scenarios Valid values : Any valid CatalogSource name present in the openshift-marketplace namespace (typically ibm-operator-catalog ) Impact : Changing this value affects which operator versions are available for installation. An invalid catalog source will cause the subscription to fail. Related variables : Works with mas_channel to determine the specific operator version installed. mas_channel \u00a4 Specifies the MAS operator subscription channel, which determines the version stream you'll receive updates from. Required Environment Variable: MAS_CHANNEL Default: None Purpose : Controls which version of MAS will be installed and which updates will be automatically applied. The channel corresponds to major.minor version releases (e.g., 8.11.x , 9.0.x ) and determines the feature set and compatibility level of your MAS installation. When to use : - Set to the latest stable channel for new production deployments to receive the newest features - Use specific older channels when compatibility with existing applications or dependencies requires it - Consult the MAS compatibility matrix before selecting a channel to ensure compatibility with your applications - Change channels only during planned upgrade windows as this triggers version updates Valid values : 8.9.x , 8.10.x , 8.11.x , 9.0.x (check the IBM Operator Catalog for currently available channels) Impact : The channel determines which MAS version is installed and which automatic updates are applied. Changing channels after installation will trigger an upgrade to the latest version in that channel, which may require application reconfiguration and testing. Related variables : Works with mas_catalog_source to determine available channels. Note : Once installed, changing channels requires careful planning. Review the MAS upgrade documentation before changing this value. Basic Configuration \u00a4 mas_domain \u00a4 Specifies the custom domain name for accessing MAS web interfaces and APIs. Optional Environment Variable: MAS_DOMAIN Default: None (uses cluster default subdomain) Purpose : Defines the base domain used to construct all MAS URLs (e.g., admin.{mas_domain}, home.{mas_domain}). This allows you to use a custom domain instead of the default OpenShift cluster subdomain, which is important for production environments with specific DNS requirements. When to use : - Set for production environments where you need branded or corporate domain names - Set when integrating with external DNS providers (Cloudflare, Route53, IBM CIS) - Set when using custom SSL certificates tied to specific domains - Leave unset for development/testing to use the default cluster subdomain automatically Valid values : Any valid DNS domain name (e.g., mas.mycompany.com , prod-mas.example.org ) Impact : When set, MAS will use this domain for all routes and certificates. You must ensure DNS is properly configured to resolve this domain to your cluster. When not set, MAS automatically uses the cluster's default ingress subdomain. Related variables : - Used by suite_dns role to configure DNS entries - Affects certificate generation when using mas_cluster_issuer - Must align with mas_routing_mode configuration mas_instance_id \u00a4 Unique identifier for this MAS installation within the cluster. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Provides a unique identifier that distinguishes this MAS installation from others that may exist in the same cluster. This ID is used to generate namespace names, resource names, and configuration identifiers throughout the installation. When to use : - Always required for any MAS installation - Use short, descriptive names (e.g., prod , dev , test , inst1 ) - Must be unique within the cluster if running multiple MAS instances - Cannot be changed after installation without reinstalling Valid values : Lowercase alphanumeric string, 3-12 characters, starting with a letter (e.g., prod , dev01 , mastest ) Impact : This ID becomes part of the core namespace name ( mas-{mas_instance_id}-core ) and is embedded in many resource names. It cannot be changed after installation. All MAS configurations and applications will reference this instance ID. Related variables : - Used by all MAS configuration roles to target the correct instance - Referenced in application installation roles - Used in backup/restore operations to identify the instance Note : Choose carefully as this cannot be changed after installation. Use consistent naming across environments (e.g., dev , test , prod ). mas_entitlement_key \u00a4 IBM entitlement key for authenticating access to IBM Container Registry. Required Environment Variable: MAS_ENTITLEMENT_KEY Default: Value of IBM_ENTITLEMENT_KEY if set Purpose : Provides authentication credentials to pull MAS container images from IBM's entitled registry. This key is tied to your IBM Cloud account and product entitlements, proving you have the right to use MAS software. When to use : - Required for all production installations using official IBM releases - Obtain from IBM Container Library at https://myibm.ibm.com/products-services/containerlibrary - For development builds on Artifactory, use your Artifactory API key instead - Key must be valid and have active MAS entitlements Valid values : - IBM entitlement key string (typically starts with \"eyJ...\") - Must be a valid, non-expired key with MAS product entitlements - For development: Artifactory API key Impact : Invalid or expired keys will cause image pull failures during installation. The key is stored in a Kubernetes secret and used to create image pull secrets for all MAS pods. Without a valid key, the installation cannot proceed. Related variables : - Falls back to IBM_ENTITLEMENT_KEY environment variable if not set - Used with mas_entitlement_username for registry authentication - Related to mas_icr_cp and mas_icr_cpopen registry settings Note : Keep this key secure. Do not commit it to source control. Use environment variables or secure secret management. mas_config_dir \u00a4 Local directory path containing additional Kubernetes configuration files to apply during MAS installation. Optional Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Allows you to provide custom Kubernetes resources (YAML files) that will be automatically applied to the cluster during MAS installation. This enables advanced configuration scenarios and customization beyond the standard role variables. When to use : - Use to apply MAS configuration resources (MongoCfg, BASCfg, JdbcCfg, etc.) generated by other roles - Use to apply custom ConfigMaps, Secrets, or other Kubernetes resources - Use to pre-configure MAS settings before the suite becomes fully operational - Leave unset if you plan to apply configurations manually after installation Valid values : Any valid local filesystem path (e.g., /home/user/masconfig , ~/mas-configs , ./config ) Impact : All *.yaml and *.yml files in this directory will be applied to the cluster using oc apply . Files are processed in alphabetical order. Invalid YAML or resources will cause the role to fail. Related variables : - Used by mongodb , db2 , sls and other dependency roles to output configuration files - Used by suite_config role to apply configurations - Must be set if using generated configuration files from dependency roles Note : Ensure all files in this directory are valid Kubernetes resources. The role does not validate file contents before applying. Advanced Configuration \u00a4 mas_annotations \u00a4 Comma-separated list of key=value annotations to apply to all MAS resources. Optional Environment Variable: MAS_ANNOTATIONS Default: None Purpose : Adds Kubernetes annotations to all resources created by the MAS operator. Annotations are used to attach metadata that can control operator behavior, enable specific features, or provide information to other tools and operators. When to use : - Set to mas.ibm.com/operationalMode=nonproduction for non-production environments (reduces resource requirements) - Use to add custom metadata for organizational tracking or automation - Use to enable specific MAS features controlled by annotations - Leave unset for default production configuration Valid values : Comma-separated list of key=value pairs (e.g., key1=value1,key2=value2 ) Impact : Annotations affect how the MAS operator configures resources. The operationalMode=nonproduction annotation significantly reduces CPU and memory requirements but is not suitable for production workloads. Related variables : Works alongside custom_labels for resource metadata. Note : The operationalMode=nonproduction annotation should only be used in development/test environments. It reduces resource allocations below production requirements. mas_img_pull_policy \u00a4 Controls the image pull policy for all MAS container images. Optional Environment Variable: MAS_IMG_PULL_POLICY Default: None (uses operator default) Purpose : Determines when Kubernetes will pull container images from the registry. This affects deployment speed, network usage, and whether you get the latest image updates. When to use : - Use Always in development to ensure latest images are pulled on every pod restart - Use IfNotPresent in production to reduce network traffic and improve pod startup time - Use Never in airgap environments where images are pre-loaded - Leave unset to use the operator's default policy (typically IfNotPresent ) Valid values : Always , IfNotPresent , Never Impact : - Always : Slower pod starts, higher network usage, always gets latest image - IfNotPresent : Faster pod starts, uses cached images, may miss updates - Never : Fastest starts, requires images pre-loaded, fails if image not present Related variables : Affects all images pulled by MAS operator and workloads. custom_labels \u00a4 Comma-separated list of key=value labels to apply to all MAS resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to all resources created by the MAS operator. Labels are used for resource organization, selection, and filtering. They enable you to query, group, and manage MAS resources using standard Kubernetes tools. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,cost-center=12345 ) Impact : Labels are applied to all MAS resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect MAS functionality but are essential for resource management. Related variables : Works alongside mas_annotations for resource metadata. mas_manual_cert_mgmt \u00a4 Enables manual certificate management mode, disabling automatic certificate generation. Optional Environment Variable: MAS_MANUAL_CERT_MGMT Default: false Purpose : Controls whether MAS uses automatic certificate management via cert-manager or requires manually provided certificates. This is critical for environments with specific certificate requirements or where cert-manager cannot be used. When to use : - Set to true when you must use certificates from a specific Certificate Authority - Set to true in environments where cert-manager is not available or not permitted - Set to true when organizational policy requires manual certificate management - Leave as false (default) to use automatic certificate management with cert-manager Valid values : true , false Impact : When true , you must manually create and manage all certificates required by MAS. The suite_dns role will not configure automatic certificate issuers. You are responsible for certificate renewal before expiration. When false , cert-manager automatically generates and renews certificates. Related variables : - When false : Requires mas_cluster_issuer to be set for automatic certificate generation - Affects behavior of suite_dns role - Related to mas_certificate_duration and mas_certificate_renew_before Note : Manual certificate management requires significant operational overhead. Use automatic management unless you have specific requirements. mas_routing_mode \u00a4 Defines the URL routing strategy for MAS applications and services. Optional Environment Variable: MAS_ROUTING_MODE Default: subdomain Purpose : Controls how MAS constructs URLs for different applications and services. This affects the URL structure users see and how DNS must be configured. When to use : - Use subdomain (default) for cleaner URLs and better DNS organization (e.g., admin.mas.example.com , home.mas.example.com ) - Use path when subdomain routing is not possible due to DNS or certificate limitations (e.g., mas.example.com/admin , mas.example.com/home ) - Consider DNS provider capabilities and certificate management when choosing Valid values : subdomain , path Impact : - subdomain : Creates separate DNS entries for each service (requires wildcard DNS or multiple DNS entries) - path : Uses single DNS entry with path-based routing (simpler DNS, more complex URL structure) - Cannot be changed after installation without reinstalling Related variables : - Affects DNS configuration in suite_dns role - Impacts certificate requirements (wildcard vs single certificate) - Must align with mas_domain configuration Note : Choose carefully as this cannot be changed after installation. Subdomain routing is recommended for production environments. mas_trust_default_cas \u00a4 Controls whether default system Certificate Authorities are included in MAS trust stores. Optional Environment Variable: MAS_TRUST_DEFAULT_CAS Default: true Purpose : Determines if MAS will trust certificates signed by standard public Certificate Authorities (like Let's Encrypt, DigiCert, etc.) in addition to any custom CAs you configure. This affects MAS's ability to connect to external services using standard SSL certificates. When to use : - Leave as true (default) for most installations to enable connections to public services - Set to false only in highly restricted environments where you want to explicitly control all trusted CAs - Set to false in airgap environments where external connections are not permitted Valid values : true , false Impact : When true , MAS can connect to any service using certificates from well-known public CAs. When false , MAS will only trust explicitly configured custom CAs, which may break connections to external services. Related variables : Works with custom CA configuration in MAS. Note : Only available in MAS 8.11 and above. Has no effect in earlier versions. mas_pod_templates_dir \u00a4 Directory containing pod template customization files for MAS workloads. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Allows you to customize resource requests, limits, node selectors, tolerations, and other pod-level configurations for MAS workloads. This enables you to optimize MAS for your specific cluster configuration and resource availability. When to use : - Use to apply resource constraints in resource-limited environments - Use to configure node affinity for specific hardware (e.g., GPU nodes) - Use to apply tolerations for tainted nodes - Use to implement best-effort or guaranteed QoS classes - Leave unset to use default MAS pod configurations Valid values : Path to directory containing pod template YAML files: - ibm-mas-suite.yml - Core suite workloads - ibm-mas-coreidp.yml - Identity provider workloads - ibm-data-dictionary-assetdatadictionary.yml - Data dictionary workloads Impact : Pod templates directly affect resource allocation, scheduling, and performance of MAS workloads. Incorrect configurations can cause pods to fail scheduling or perform poorly. Related variables : Similar pod template configuration available in other roles (SLS, applications). Note : See MAS CLI pod templates for examples and product documentation for full details. Certificate Management \u00a4 mas_cluster_issuer \u00a4 Name of the cert-manager ClusterIssuer to use for automatic certificate generation. Optional Environment Variable: MAS_CLUSTER_ISSUER Default: None Purpose : Specifies which cert-manager ClusterIssuer will generate and manage SSL/TLS certificates for MAS. The ClusterIssuer defines the Certificate Authority and authentication method used for certificate issuance. When to use : - Required when mas_manual_cert_mgmt is false (automatic certificate management) - Set to the ClusterIssuer created by suite_dns role (e.g., {mas_instance_id}-cloudflare-le-prod ) - Set to a custom ClusterIssuer if you have specific certificate requirements - Leave unset only when using manual certificate management Valid values : Name of any valid ClusterIssuer resource in the cluster (e.g., prod-le-issuer , {mas_instance_id}-cloudflare-le-prod ) Impact : The specified ClusterIssuer will be used to generate all MAS certificates. If the ClusterIssuer is not properly configured or lacks necessary credentials, certificate generation will fail and MAS will not be accessible. Related variables : - Only used when mas_manual_cert_mgmt is false - Created by suite_dns role for Let's Encrypt integration - Works with mas_certificate_duration and mas_certificate_renew_before Note : Ensure the ClusterIssuer is created and functional before installing MAS. Test certificate generation with a test Certificate resource first. mas_certificate_duration \u00a4 Specifies the validity period for MAS certificates. Optional Environment Variable: MAS_CERTIFICATE_DURATION Default: 8760h0m0s (1 year) Purpose : Defines how long certificates will be valid before they expire. This affects how often certificates need to be renewed and the security posture of your installation. When to use : - Use default (8760h = 1 year) for most installations - Reduce for higher security environments requiring frequent rotation - Increase only if certificate renewal is problematic in your environment - Must be longer than mas_certificate_renew_before Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 8760h0m0s , 2160h0m0s , 17520h0m0s ) Impact : Shorter durations increase security but require more frequent renewals. Longer durations reduce renewal frequency but increase risk if certificates are compromised. Cert-manager will automatically renew certificates before expiration. Related variables : - Must be greater than mas_certificate_renew_before - Only applies when using automatic certificate management - Affects all MAS certificates mas_certificate_renew_before \u00a4 Specifies when to renew certificates before they expire. Optional Environment Variable: MAS_CERTIFICATE_RENEW_BEFORE Default: 720h0m0s (30 days) Purpose : Defines the renewal window - how far in advance cert-manager will renew certificates before they expire. This ensures certificates are renewed with sufficient time to handle any renewal issues. When to use : - Use default (720h = 30 days) for most installations - Increase in environments where certificate renewal may be slow or problematic - Decrease only if you need to minimize the number of certificate changes - Must be shorter than mas_certificate_duration Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 720h0m0s , 1440h0m0s , 168h0m0s ) Impact : Longer renewal windows provide more time to resolve renewal issues but result in more frequent certificate changes. Shorter windows reduce certificate churn but increase risk of expiration if renewal fails. Related variables : - Must be less than mas_certificate_duration - Only applies when using automatic certificate management - Affects all MAS certificates Note : Ensure this value provides adequate time to detect and resolve certificate renewal issues before expiration. Superuser Account \u00a4 The MAS Superuser account username and password can be customized during the install by setting both of these variables. mas_superuser_username \u00a4 Custom username for the MAS superuser administrator account. Optional Environment Variable: MAS_SUPERUSER_USERNAME Default: None (uses MAS default superuser) Purpose : Allows you to set a custom username for the MAS superuser account instead of using the default. The superuser has full administrative access to all MAS functions and applications. When to use : - Set both username and password to customize the superuser account - Use in environments with specific account naming requirements - Leave unset to use the default superuser account - Must be set together with mas_superuser_password Valid values : Any valid username string (alphanumeric, may include underscores and hyphens) Impact : When set (along with password), creates a superuser account with the specified username. If only one of username/password is set, both are ignored and default account is used. Related variables : Must be set together with mas_superuser_password to take effect. Note : Both username and password must be set for customization to take effect. Setting only one has no effect. mas_superuser_password \u00a4 Custom password for the MAS superuser administrator account. Optional Environment Variable: MAS_SUPERUSER_PASSWORD Default: None (uses auto-generated password) Purpose : Allows you to set a custom password for the MAS superuser account instead of using the auto-generated default. This enables you to control the initial superuser credentials. When to use : - Set both username and password to customize the superuser account - Use when you need to know the superuser password in advance - Use to comply with organizational password policies - Must be set together with mas_superuser_username Valid values : Any string meeting MAS password requirements (minimum length, complexity requirements) Impact : When set (along with username), creates a superuser account with the specified password. If only one of username/password is set, both are ignored and default account is used with auto-generated password. Related variables : Must be set together with mas_superuser_username to take effect. Note : Both username and password must be set for customization to take effect. Keep the password secure and do not commit to source control. Developer Mode \u00a4 artifactory_username \u00a4 Username for authenticating to Artifactory for development builds. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None Purpose : Provides authentication credentials for accessing development builds of MAS stored in Artifactory. This is only used for internal development and testing, not for production installations. When to use : - Required only when installing development builds from Artifactory - Not needed for production installations using IBM Container Registry - Use your IBM w3Id username for Artifactory access - Must be set together with artifactory_token Valid values : Your IBM w3Id username Impact : Used to create image pull secrets for accessing Artifactory registries. Without valid credentials, development image pulls will fail. Related variables : - Must be set with artifactory_token - Used with mas_icr_cp and mas_icr_cpopen when pointing to Artifactory - Related to mas_entitlement_username for registry authentication Note : Only for development use. Production installations should use IBM Container Registry with entitlement keys. artifactory_token \u00a4 API token for authenticating to Artifactory for development builds. Optional Environment Variable: ARTIFACTORY_TOKEN Default: None Purpose : Provides authentication token for accessing development builds of MAS stored in Artifactory. This is only used for internal development and testing, not for production installations. When to use : - Required only when installing development builds from Artifactory - Not needed for production installations using IBM Container Registry - Use your Artifactory API key - Must be set together with artifactory_username Valid values : Your Artifactory API key/token Impact : Used to create image pull secrets for accessing Artifactory registries. Without valid credentials, development image pulls will fail. Related variables : - Must be set with artifactory_username - Used with mas_icr_cp and mas_icr_cpopen when pointing to Artifactory - Can be used as mas_entitlement_key for development builds Note : Only for development use. Keep tokens secure. Production installations should use IBM Container Registry with entitlement keys. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_config_dir: \"/home/david/masconfig\" mas_entitlement_key: \"{{ lookup('env', 'IBM_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify License \u00a4 EPL-2.0","title":"suite_install"},{"location":"roles/suite_install/#suite_install","text":"This role installs Maximo Application Suite. It internally resolves the namespace based on the mas_instance_id as mas-{mas_instance_id}-core .","title":"suite_install"},{"location":"roles/suite_install/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_install/#basic-install","text":"","title":"Basic Install"},{"location":"roles/suite_install/#mas_catalog_source","text":"Specifies the OpenShift operator catalog source containing the MAS operator subscription. Optional Environment Variable: MAS_CATALOG_SOURCE Default: ibm-operator-catalog Purpose : Controls which operator catalog is used to locate and install the MAS operator. This determines where OpenShift looks for the operator images and metadata during installation. When to use : - Use default ibm-operator-catalog for production installations with official IBM releases - Use ibm-operator-catalog for development installations as well (supports both use cases) - Only change if directed to use a custom catalog for specific testing or airgap scenarios Valid values : Any valid CatalogSource name present in the openshift-marketplace namespace (typically ibm-operator-catalog ) Impact : Changing this value affects which operator versions are available for installation. An invalid catalog source will cause the subscription to fail. Related variables : Works with mas_channel to determine the specific operator version installed.","title":"mas_catalog_source"},{"location":"roles/suite_install/#mas_channel","text":"Specifies the MAS operator subscription channel, which determines the version stream you'll receive updates from. Required Environment Variable: MAS_CHANNEL Default: None Purpose : Controls which version of MAS will be installed and which updates will be automatically applied. The channel corresponds to major.minor version releases (e.g., 8.11.x , 9.0.x ) and determines the feature set and compatibility level of your MAS installation. When to use : - Set to the latest stable channel for new production deployments to receive the newest features - Use specific older channels when compatibility with existing applications or dependencies requires it - Consult the MAS compatibility matrix before selecting a channel to ensure compatibility with your applications - Change channels only during planned upgrade windows as this triggers version updates Valid values : 8.9.x , 8.10.x , 8.11.x , 9.0.x (check the IBM Operator Catalog for currently available channels) Impact : The channel determines which MAS version is installed and which automatic updates are applied. Changing channels after installation will trigger an upgrade to the latest version in that channel, which may require application reconfiguration and testing. Related variables : Works with mas_catalog_source to determine available channels. Note : Once installed, changing channels requires careful planning. Review the MAS upgrade documentation before changing this value.","title":"mas_channel"},{"location":"roles/suite_install/#basic-configuration","text":"","title":"Basic Configuration"},{"location":"roles/suite_install/#mas_domain","text":"Specifies the custom domain name for accessing MAS web interfaces and APIs. Optional Environment Variable: MAS_DOMAIN Default: None (uses cluster default subdomain) Purpose : Defines the base domain used to construct all MAS URLs (e.g., admin.{mas_domain}, home.{mas_domain}). This allows you to use a custom domain instead of the default OpenShift cluster subdomain, which is important for production environments with specific DNS requirements. When to use : - Set for production environments where you need branded or corporate domain names - Set when integrating with external DNS providers (Cloudflare, Route53, IBM CIS) - Set when using custom SSL certificates tied to specific domains - Leave unset for development/testing to use the default cluster subdomain automatically Valid values : Any valid DNS domain name (e.g., mas.mycompany.com , prod-mas.example.org ) Impact : When set, MAS will use this domain for all routes and certificates. You must ensure DNS is properly configured to resolve this domain to your cluster. When not set, MAS automatically uses the cluster's default ingress subdomain. Related variables : - Used by suite_dns role to configure DNS entries - Affects certificate generation when using mas_cluster_issuer - Must align with mas_routing_mode configuration","title":"mas_domain"},{"location":"roles/suite_install/#mas_instance_id","text":"Unique identifier for this MAS installation within the cluster. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Provides a unique identifier that distinguishes this MAS installation from others that may exist in the same cluster. This ID is used to generate namespace names, resource names, and configuration identifiers throughout the installation. When to use : - Always required for any MAS installation - Use short, descriptive names (e.g., prod , dev , test , inst1 ) - Must be unique within the cluster if running multiple MAS instances - Cannot be changed after installation without reinstalling Valid values : Lowercase alphanumeric string, 3-12 characters, starting with a letter (e.g., prod , dev01 , mastest ) Impact : This ID becomes part of the core namespace name ( mas-{mas_instance_id}-core ) and is embedded in many resource names. It cannot be changed after installation. All MAS configurations and applications will reference this instance ID. Related variables : - Used by all MAS configuration roles to target the correct instance - Referenced in application installation roles - Used in backup/restore operations to identify the instance Note : Choose carefully as this cannot be changed after installation. Use consistent naming across environments (e.g., dev , test , prod ).","title":"mas_instance_id"},{"location":"roles/suite_install/#mas_entitlement_key","text":"IBM entitlement key for authenticating access to IBM Container Registry. Required Environment Variable: MAS_ENTITLEMENT_KEY Default: Value of IBM_ENTITLEMENT_KEY if set Purpose : Provides authentication credentials to pull MAS container images from IBM's entitled registry. This key is tied to your IBM Cloud account and product entitlements, proving you have the right to use MAS software. When to use : - Required for all production installations using official IBM releases - Obtain from IBM Container Library at https://myibm.ibm.com/products-services/containerlibrary - For development builds on Artifactory, use your Artifactory API key instead - Key must be valid and have active MAS entitlements Valid values : - IBM entitlement key string (typically starts with \"eyJ...\") - Must be a valid, non-expired key with MAS product entitlements - For development: Artifactory API key Impact : Invalid or expired keys will cause image pull failures during installation. The key is stored in a Kubernetes secret and used to create image pull secrets for all MAS pods. Without a valid key, the installation cannot proceed. Related variables : - Falls back to IBM_ENTITLEMENT_KEY environment variable if not set - Used with mas_entitlement_username for registry authentication - Related to mas_icr_cp and mas_icr_cpopen registry settings Note : Keep this key secure. Do not commit it to source control. Use environment variables or secure secret management.","title":"mas_entitlement_key"},{"location":"roles/suite_install/#mas_config_dir","text":"Local directory path containing additional Kubernetes configuration files to apply during MAS installation. Optional Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Allows you to provide custom Kubernetes resources (YAML files) that will be automatically applied to the cluster during MAS installation. This enables advanced configuration scenarios and customization beyond the standard role variables. When to use : - Use to apply MAS configuration resources (MongoCfg, BASCfg, JdbcCfg, etc.) generated by other roles - Use to apply custom ConfigMaps, Secrets, or other Kubernetes resources - Use to pre-configure MAS settings before the suite becomes fully operational - Leave unset if you plan to apply configurations manually after installation Valid values : Any valid local filesystem path (e.g., /home/user/masconfig , ~/mas-configs , ./config ) Impact : All *.yaml and *.yml files in this directory will be applied to the cluster using oc apply . Files are processed in alphabetical order. Invalid YAML or resources will cause the role to fail. Related variables : - Used by mongodb , db2 , sls and other dependency roles to output configuration files - Used by suite_config role to apply configurations - Must be set if using generated configuration files from dependency roles Note : Ensure all files in this directory are valid Kubernetes resources. The role does not validate file contents before applying.","title":"mas_config_dir"},{"location":"roles/suite_install/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"roles/suite_install/#mas_annotations","text":"Comma-separated list of key=value annotations to apply to all MAS resources. Optional Environment Variable: MAS_ANNOTATIONS Default: None Purpose : Adds Kubernetes annotations to all resources created by the MAS operator. Annotations are used to attach metadata that can control operator behavior, enable specific features, or provide information to other tools and operators. When to use : - Set to mas.ibm.com/operationalMode=nonproduction for non-production environments (reduces resource requirements) - Use to add custom metadata for organizational tracking or automation - Use to enable specific MAS features controlled by annotations - Leave unset for default production configuration Valid values : Comma-separated list of key=value pairs (e.g., key1=value1,key2=value2 ) Impact : Annotations affect how the MAS operator configures resources. The operationalMode=nonproduction annotation significantly reduces CPU and memory requirements but is not suitable for production workloads. Related variables : Works alongside custom_labels for resource metadata. Note : The operationalMode=nonproduction annotation should only be used in development/test environments. It reduces resource allocations below production requirements.","title":"mas_annotations"},{"location":"roles/suite_install/#mas_img_pull_policy","text":"Controls the image pull policy for all MAS container images. Optional Environment Variable: MAS_IMG_PULL_POLICY Default: None (uses operator default) Purpose : Determines when Kubernetes will pull container images from the registry. This affects deployment speed, network usage, and whether you get the latest image updates. When to use : - Use Always in development to ensure latest images are pulled on every pod restart - Use IfNotPresent in production to reduce network traffic and improve pod startup time - Use Never in airgap environments where images are pre-loaded - Leave unset to use the operator's default policy (typically IfNotPresent ) Valid values : Always , IfNotPresent , Never Impact : - Always : Slower pod starts, higher network usage, always gets latest image - IfNotPresent : Faster pod starts, uses cached images, may miss updates - Never : Fastest starts, requires images pre-loaded, fails if image not present Related variables : Affects all images pulled by MAS operator and workloads.","title":"mas_img_pull_policy"},{"location":"roles/suite_install/#custom_labels","text":"Comma-separated list of key=value labels to apply to all MAS resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to all resources created by the MAS operator. Labels are used for resource organization, selection, and filtering. They enable you to query, group, and manage MAS resources using standard Kubernetes tools. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,cost-center=12345 ) Impact : Labels are applied to all MAS resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect MAS functionality but are essential for resource management. Related variables : Works alongside mas_annotations for resource metadata.","title":"custom_labels"},{"location":"roles/suite_install/#mas_manual_cert_mgmt","text":"Enables manual certificate management mode, disabling automatic certificate generation. Optional Environment Variable: MAS_MANUAL_CERT_MGMT Default: false Purpose : Controls whether MAS uses automatic certificate management via cert-manager or requires manually provided certificates. This is critical for environments with specific certificate requirements or where cert-manager cannot be used. When to use : - Set to true when you must use certificates from a specific Certificate Authority - Set to true in environments where cert-manager is not available or not permitted - Set to true when organizational policy requires manual certificate management - Leave as false (default) to use automatic certificate management with cert-manager Valid values : true , false Impact : When true , you must manually create and manage all certificates required by MAS. The suite_dns role will not configure automatic certificate issuers. You are responsible for certificate renewal before expiration. When false , cert-manager automatically generates and renews certificates. Related variables : - When false : Requires mas_cluster_issuer to be set for automatic certificate generation - Affects behavior of suite_dns role - Related to mas_certificate_duration and mas_certificate_renew_before Note : Manual certificate management requires significant operational overhead. Use automatic management unless you have specific requirements.","title":"mas_manual_cert_mgmt"},{"location":"roles/suite_install/#mas_routing_mode","text":"Defines the URL routing strategy for MAS applications and services. Optional Environment Variable: MAS_ROUTING_MODE Default: subdomain Purpose : Controls how MAS constructs URLs for different applications and services. This affects the URL structure users see and how DNS must be configured. When to use : - Use subdomain (default) for cleaner URLs and better DNS organization (e.g., admin.mas.example.com , home.mas.example.com ) - Use path when subdomain routing is not possible due to DNS or certificate limitations (e.g., mas.example.com/admin , mas.example.com/home ) - Consider DNS provider capabilities and certificate management when choosing Valid values : subdomain , path Impact : - subdomain : Creates separate DNS entries for each service (requires wildcard DNS or multiple DNS entries) - path : Uses single DNS entry with path-based routing (simpler DNS, more complex URL structure) - Cannot be changed after installation without reinstalling Related variables : - Affects DNS configuration in suite_dns role - Impacts certificate requirements (wildcard vs single certificate) - Must align with mas_domain configuration Note : Choose carefully as this cannot be changed after installation. Subdomain routing is recommended for production environments.","title":"mas_routing_mode"},{"location":"roles/suite_install/#mas_trust_default_cas","text":"Controls whether default system Certificate Authorities are included in MAS trust stores. Optional Environment Variable: MAS_TRUST_DEFAULT_CAS Default: true Purpose : Determines if MAS will trust certificates signed by standard public Certificate Authorities (like Let's Encrypt, DigiCert, etc.) in addition to any custom CAs you configure. This affects MAS's ability to connect to external services using standard SSL certificates. When to use : - Leave as true (default) for most installations to enable connections to public services - Set to false only in highly restricted environments where you want to explicitly control all trusted CAs - Set to false in airgap environments where external connections are not permitted Valid values : true , false Impact : When true , MAS can connect to any service using certificates from well-known public CAs. When false , MAS will only trust explicitly configured custom CAs, which may break connections to external services. Related variables : Works with custom CA configuration in MAS. Note : Only available in MAS 8.11 and above. Has no effect in earlier versions.","title":"mas_trust_default_cas"},{"location":"roles/suite_install/#mas_pod_templates_dir","text":"Directory containing pod template customization files for MAS workloads. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Allows you to customize resource requests, limits, node selectors, tolerations, and other pod-level configurations for MAS workloads. This enables you to optimize MAS for your specific cluster configuration and resource availability. When to use : - Use to apply resource constraints in resource-limited environments - Use to configure node affinity for specific hardware (e.g., GPU nodes) - Use to apply tolerations for tainted nodes - Use to implement best-effort or guaranteed QoS classes - Leave unset to use default MAS pod configurations Valid values : Path to directory containing pod template YAML files: - ibm-mas-suite.yml - Core suite workloads - ibm-mas-coreidp.yml - Identity provider workloads - ibm-data-dictionary-assetdatadictionary.yml - Data dictionary workloads Impact : Pod templates directly affect resource allocation, scheduling, and performance of MAS workloads. Incorrect configurations can cause pods to fail scheduling or perform poorly. Related variables : Similar pod template configuration available in other roles (SLS, applications). Note : See MAS CLI pod templates for examples and product documentation for full details.","title":"mas_pod_templates_dir"},{"location":"roles/suite_install/#certificate-management","text":"","title":"Certificate Management"},{"location":"roles/suite_install/#mas_cluster_issuer","text":"Name of the cert-manager ClusterIssuer to use for automatic certificate generation. Optional Environment Variable: MAS_CLUSTER_ISSUER Default: None Purpose : Specifies which cert-manager ClusterIssuer will generate and manage SSL/TLS certificates for MAS. The ClusterIssuer defines the Certificate Authority and authentication method used for certificate issuance. When to use : - Required when mas_manual_cert_mgmt is false (automatic certificate management) - Set to the ClusterIssuer created by suite_dns role (e.g., {mas_instance_id}-cloudflare-le-prod ) - Set to a custom ClusterIssuer if you have specific certificate requirements - Leave unset only when using manual certificate management Valid values : Name of any valid ClusterIssuer resource in the cluster (e.g., prod-le-issuer , {mas_instance_id}-cloudflare-le-prod ) Impact : The specified ClusterIssuer will be used to generate all MAS certificates. If the ClusterIssuer is not properly configured or lacks necessary credentials, certificate generation will fail and MAS will not be accessible. Related variables : - Only used when mas_manual_cert_mgmt is false - Created by suite_dns role for Let's Encrypt integration - Works with mas_certificate_duration and mas_certificate_renew_before Note : Ensure the ClusterIssuer is created and functional before installing MAS. Test certificate generation with a test Certificate resource first.","title":"mas_cluster_issuer"},{"location":"roles/suite_install/#mas_certificate_duration","text":"Specifies the validity period for MAS certificates. Optional Environment Variable: MAS_CERTIFICATE_DURATION Default: 8760h0m0s (1 year) Purpose : Defines how long certificates will be valid before they expire. This affects how often certificates need to be renewed and the security posture of your installation. When to use : - Use default (8760h = 1 year) for most installations - Reduce for higher security environments requiring frequent rotation - Increase only if certificate renewal is problematic in your environment - Must be longer than mas_certificate_renew_before Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 8760h0m0s , 2160h0m0s , 17520h0m0s ) Impact : Shorter durations increase security but require more frequent renewals. Longer durations reduce renewal frequency but increase risk if certificates are compromised. Cert-manager will automatically renew certificates before expiration. Related variables : - Must be greater than mas_certificate_renew_before - Only applies when using automatic certificate management - Affects all MAS certificates","title":"mas_certificate_duration"},{"location":"roles/suite_install/#mas_certificate_renew_before","text":"Specifies when to renew certificates before they expire. Optional Environment Variable: MAS_CERTIFICATE_RENEW_BEFORE Default: 720h0m0s (30 days) Purpose : Defines the renewal window - how far in advance cert-manager will renew certificates before they expire. This ensures certificates are renewed with sufficient time to handle any renewal issues. When to use : - Use default (720h = 30 days) for most installations - Increase in environments where certificate renewal may be slow or problematic - Decrease only if you need to minimize the number of certificate changes - Must be shorter than mas_certificate_duration Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 720h0m0s , 1440h0m0s , 168h0m0s ) Impact : Longer renewal windows provide more time to resolve renewal issues but result in more frequent certificate changes. Shorter windows reduce certificate churn but increase risk of expiration if renewal fails. Related variables : - Must be less than mas_certificate_duration - Only applies when using automatic certificate management - Affects all MAS certificates Note : Ensure this value provides adequate time to detect and resolve certificate renewal issues before expiration.","title":"mas_certificate_renew_before"},{"location":"roles/suite_install/#superuser-account","text":"The MAS Superuser account username and password can be customized during the install by setting both of these variables.","title":"Superuser Account"},{"location":"roles/suite_install/#mas_superuser_username","text":"Custom username for the MAS superuser administrator account. Optional Environment Variable: MAS_SUPERUSER_USERNAME Default: None (uses MAS default superuser) Purpose : Allows you to set a custom username for the MAS superuser account instead of using the default. The superuser has full administrative access to all MAS functions and applications. When to use : - Set both username and password to customize the superuser account - Use in environments with specific account naming requirements - Leave unset to use the default superuser account - Must be set together with mas_superuser_password Valid values : Any valid username string (alphanumeric, may include underscores and hyphens) Impact : When set (along with password), creates a superuser account with the specified username. If only one of username/password is set, both are ignored and default account is used. Related variables : Must be set together with mas_superuser_password to take effect. Note : Both username and password must be set for customization to take effect. Setting only one has no effect.","title":"mas_superuser_username"},{"location":"roles/suite_install/#mas_superuser_password","text":"Custom password for the MAS superuser administrator account. Optional Environment Variable: MAS_SUPERUSER_PASSWORD Default: None (uses auto-generated password) Purpose : Allows you to set a custom password for the MAS superuser account instead of using the auto-generated default. This enables you to control the initial superuser credentials. When to use : - Set both username and password to customize the superuser account - Use when you need to know the superuser password in advance - Use to comply with organizational password policies - Must be set together with mas_superuser_username Valid values : Any string meeting MAS password requirements (minimum length, complexity requirements) Impact : When set (along with username), creates a superuser account with the specified password. If only one of username/password is set, both are ignored and default account is used with auto-generated password. Related variables : Must be set together with mas_superuser_username to take effect. Note : Both username and password must be set for customization to take effect. Keep the password secure and do not commit to source control.","title":"mas_superuser_password"},{"location":"roles/suite_install/#developer-mode","text":"","title":"Developer Mode"},{"location":"roles/suite_install/#artifactory_username","text":"Username for authenticating to Artifactory for development builds. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None Purpose : Provides authentication credentials for accessing development builds of MAS stored in Artifactory. This is only used for internal development and testing, not for production installations. When to use : - Required only when installing development builds from Artifactory - Not needed for production installations using IBM Container Registry - Use your IBM w3Id username for Artifactory access - Must be set together with artifactory_token Valid values : Your IBM w3Id username Impact : Used to create image pull secrets for accessing Artifactory registries. Without valid credentials, development image pulls will fail. Related variables : - Must be set with artifactory_token - Used with mas_icr_cp and mas_icr_cpopen when pointing to Artifactory - Related to mas_entitlement_username for registry authentication Note : Only for development use. Production installations should use IBM Container Registry with entitlement keys.","title":"artifactory_username"},{"location":"roles/suite_install/#artifactory_token","text":"API token for authenticating to Artifactory for development builds. Optional Environment Variable: ARTIFACTORY_TOKEN Default: None Purpose : Provides authentication token for accessing development builds of MAS stored in Artifactory. This is only used for internal development and testing, not for production installations. When to use : - Required only when installing development builds from Artifactory - Not needed for production installations using IBM Container Registry - Use your Artifactory API key - Must be set together with artifactory_username Valid values : Your Artifactory API key/token Impact : Used to create image pull secrets for accessing Artifactory registries. Without valid credentials, development image pulls will fail. Related variables : - Must be set with artifactory_username - Used with mas_icr_cp and mas_icr_cpopen when pointing to Artifactory - Can be used as mas_entitlement_key for development builds Note : Only for development use. Keep tokens secure. Production installations should use IBM Container Registry with entitlement keys.","title":"artifactory_token"},{"location":"roles/suite_install/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_config_dir: \"/home/david/masconfig\" mas_entitlement_key: \"{{ lookup('env', 'IBM_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify","title":"Example Playbook"},{"location":"roles/suite_install/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_attachments_config/","text":"suite_manage_attachments_config \u00a4 This role configures storage for Maximo Manage application attachments, supporting multiple storage providers including IBM Cloud Object Storage, AWS S3, and persistent file storage. The role updates Manage configuration either through the ManageWorkspace custom resource or directly via database SQL updates. Prerequisites This role must be executed after Manage application is deployed and activated. Manage must be up and running before configuring attachment features. Storage Provider Options \u00a4 The role supports three storage providers: filestorage (default): Uses cluster's default file storage system (PVC) ibm : Uses IBM Cloud Object Storage buckets aws : Uses Amazon S3 buckets For cloud storage providers ( ibm or aws ), the cos_bucket role is automatically executed to set up the bucket configuration. Role Variables \u00a4 mas_manage_attachments_provider \u00a4 Storage provider type for Manage attachments. Optional Environment Variable: MAS_MANAGE_ATTACHMENTS_PROVIDER Default: filestorage Purpose : Determines which storage backend is used to store Manage application attachments (documents, images, files). When to use : - Use default filestorage for simple deployments with PVC storage - Use ibm when leveraging IBM Cloud Object Storage for scalability - Use aws when using AWS S3 for cloud-native storage Valid values : filestorage , ibm , aws Impact : - filestorage : Attachments stored in cluster file storage (PVC mount) - ibm : Attachments stored in IBM Cloud Object Storage (requires COS credentials) - aws : Attachments stored in AWS S3 (requires AWS credentials and CLI) Related variables : - mas_manage_attachment_configuration_mode : How configuration is applied - db2_instance_name : Required for database configuration mode Note : For ibm or aws providers, ensure you set COS/S3 credentials ( COS_APIKEY , COS_INSTANCE_NAME for IBM, or AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY for AWS). AWS provider requires AWS CLI to be installed. mas_manage_attachment_configuration_mode \u00a4 Configuration method for attachment properties. Optional Environment Variable: MAS_MANAGE_ATTACHMENT_CONFIGURATION_MODE Default: db Purpose : Determines how attachment configuration properties are applied to Manage - either through Kubernetes custom resource or direct database updates. When to use : - Use db (default) for direct database configuration (faster, simpler) - Use cr for GitOps workflows or when database access is restricted Valid values : db , cr Impact : - db : Updates attachment properties via SQL directly in Manage database (requires db2_instance_name , db2_namespace , db2_dbname ) - cr : Updates attachment properties in ManageWorkspace custom resource under bundleProperties (requires manage_workspace_cr_name ) Related variables : - db2_instance_name : Required when mode is db - manage_workspace_cr_name : Required when mode is cr Note : The db mode is recommended for most deployments as it's more direct. Use cr mode for declarative/GitOps approaches. mas_instance_id \u00a4 MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application to configure. When to use : - Always required for attachment configuration - Must match the instance ID from MAS installation Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Used to locate Manage application resources and construct resource names. Related variables : - mas_workspace_id : Workspace within this instance - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the instance ID used during Manage installation. mas_workspace_id \u00a4 Workspace identifier for Manage application. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application to configure. When to use : - Always required for attachment configuration - Must match the workspace ID where Manage is deployed Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , masdev ) Impact : Used to locate Manage application resources within the specified instance. Related variables : - mas_instance_id : Parent instance - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the workspace ID used during Manage installation. manage_workspace_cr_name \u00a4 ManageWorkspace custom resource name. Optional (Required when mas_manage_attachment_configuration_mode=cr ) Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: {mas_instance_id}-{mas_workspace_id} Purpose : Specifies the name of the ManageWorkspace custom resource to update when using CR configuration mode. When to use : - Required only when mas_manage_attachment_configuration_mode=cr - Use default unless you have a custom CR naming convention - Override if your ManageWorkspace CR has a non-standard name Valid values : Valid Kubernetes resource name Impact : Determines which ManageWorkspace CR is updated with attachment properties in bundleProperties section. Related variables : - mas_manage_attachment_configuration_mode : Must be cr for this to be used - mas_instance_id : Used in default name construction - mas_workspace_id : Used in default name construction Note : The default naming convention {instance}-{workspace} matches standard Manage deployments. Only override if you have custom CR names. db2_instance_name \u00a4 Db2 Warehouse instance name. Optional (Required when mas_manage_attachment_configuration_mode=db ) Environment Variable: DB2_INSTANCE_NAME Default: None Purpose : Identifies the Db2 Warehouse instance that stores Manage application data for direct database configuration updates. When to use : - Required when mas_manage_attachment_configuration_mode=db - Must match the Db2 instance name used by Manage - Used to connect to database for SQL updates Valid values : Valid Db2 instance name (e.g., db2w-manage , db2u-iot ) Impact : Determines which Db2 instance is accessed to update attachment configuration via SQL. Related variables : - mas_manage_attachment_configuration_mode : Must be db for this to be used - db2_namespace : Namespace containing this instance - db2_dbname : Database name within the instance Note : To find the instance name, go to the Db2 namespace and look for pods with label=engine . Describe the pod and find the app label value - that's your instance name. db2_namespace \u00a4 Db2 Warehouse namespace. Optional Environment Variable: DB2_NAMESPACE Default: db2u Purpose : Specifies the OpenShift namespace where the Db2 Warehouse instance is deployed. When to use : - Use default ( db2u ) for standard Db2 deployments - Override if Db2 is deployed in a custom namespace - Only relevant when mas_manage_attachment_configuration_mode=db Valid values : Valid Kubernetes namespace name Impact : Determines where to look for the Db2 instance when connecting for database updates. Related variables : - db2_instance_name : Instance to find in this namespace - mas_manage_attachment_configuration_mode : Must be db for this to be relevant Note : The default db2u namespace is used by most Db2 Warehouse deployments. Only change if you have a custom deployment. db2_dbname \u00a4 Database name within Db2 instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB Purpose : Specifies the database name within the Db2 instance where Manage tables are stored. When to use : - Use default ( BLUDB ) for standard Manage deployments - Override if Manage uses a custom database name - Only relevant when mas_manage_attachment_configuration_mode=db Valid values : Valid Db2 database name Impact : Determines which database within the Db2 instance is updated with attachment configuration. Related variables : - db2_instance_name : Instance containing this database - mas_manage_attachment_configuration_mode : Must be db for this to be relevant Note : BLUDB is the default database name for Manage deployments. Only change if you have a custom database configuration. Example Playbook \u00a4 Configure COS via ManageWorkspace CR \u00a4 The following sample can be used to configure COS for an existing Manage application instance's attachments via ManageWorkspace CR update (note cr as configuration mode + mas_instance_id and mas_workspace_id , which will be used to infer the CR name): - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: cr mas_instance_id: masinst1 mas_workspace_id: masdev cos_instance_name: cos-masinst1 cos_bucket_name: manage-attachments-bucket ibmcloud_apikey: xxxx mas_manage_attachments_provider: ibm roles: - ibm.mas_devops.suite_manage_attachments_config Provision COS and Configure via Database \u00a4 The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance's attachments via SQL updates in database (note db as configuration mode + db2_instance_name ): - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: db mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_instance_name: cos-masinst1 cos_bucket_name: manage-attachments-bucket ibmcloud_apikey: xxxx mas_manage_attachments_provider: ibm roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_attachments_config Configure File Storage for Attachments \u00a4 The following sample playbook can be used to deploy Manage with default persistent storage for Manage attachments (PVC mount path /DOCLINKS ), and configure Manage system properties with the corresponding attachments settings via SQL updates in database: - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: db mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_manage_attachments_provider: filestorage roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_attachments_config Configure AWS S3 Buckets \u00a4 The following sample can be used to configure AWS S3 buckets for an existing Manage application instance's attachments: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage aws_bucket_name: manage-attachments-bucket mas_manage_attachments_provider: aws roles: - ibm.mas_devops.suite_manage_attachments_config License \u00a4 EPL-2.0","title":"suite_manage_attachments_config"},{"location":"roles/suite_manage_attachments_config/#suite_manage_attachments_config","text":"This role configures storage for Maximo Manage application attachments, supporting multiple storage providers including IBM Cloud Object Storage, AWS S3, and persistent file storage. The role updates Manage configuration either through the ManageWorkspace custom resource or directly via database SQL updates. Prerequisites This role must be executed after Manage application is deployed and activated. Manage must be up and running before configuring attachment features.","title":"suite_manage_attachments_config"},{"location":"roles/suite_manage_attachments_config/#storage-provider-options","text":"The role supports three storage providers: filestorage (default): Uses cluster's default file storage system (PVC) ibm : Uses IBM Cloud Object Storage buckets aws : Uses Amazon S3 buckets For cloud storage providers ( ibm or aws ), the cos_bucket role is automatically executed to set up the bucket configuration.","title":"Storage Provider Options"},{"location":"roles/suite_manage_attachments_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_attachments_config/#mas_manage_attachments_provider","text":"Storage provider type for Manage attachments. Optional Environment Variable: MAS_MANAGE_ATTACHMENTS_PROVIDER Default: filestorage Purpose : Determines which storage backend is used to store Manage application attachments (documents, images, files). When to use : - Use default filestorage for simple deployments with PVC storage - Use ibm when leveraging IBM Cloud Object Storage for scalability - Use aws when using AWS S3 for cloud-native storage Valid values : filestorage , ibm , aws Impact : - filestorage : Attachments stored in cluster file storage (PVC mount) - ibm : Attachments stored in IBM Cloud Object Storage (requires COS credentials) - aws : Attachments stored in AWS S3 (requires AWS credentials and CLI) Related variables : - mas_manage_attachment_configuration_mode : How configuration is applied - db2_instance_name : Required for database configuration mode Note : For ibm or aws providers, ensure you set COS/S3 credentials ( COS_APIKEY , COS_INSTANCE_NAME for IBM, or AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY for AWS). AWS provider requires AWS CLI to be installed.","title":"mas_manage_attachments_provider"},{"location":"roles/suite_manage_attachments_config/#mas_manage_attachment_configuration_mode","text":"Configuration method for attachment properties. Optional Environment Variable: MAS_MANAGE_ATTACHMENT_CONFIGURATION_MODE Default: db Purpose : Determines how attachment configuration properties are applied to Manage - either through Kubernetes custom resource or direct database updates. When to use : - Use db (default) for direct database configuration (faster, simpler) - Use cr for GitOps workflows or when database access is restricted Valid values : db , cr Impact : - db : Updates attachment properties via SQL directly in Manage database (requires db2_instance_name , db2_namespace , db2_dbname ) - cr : Updates attachment properties in ManageWorkspace custom resource under bundleProperties (requires manage_workspace_cr_name ) Related variables : - db2_instance_name : Required when mode is db - manage_workspace_cr_name : Required when mode is cr Note : The db mode is recommended for most deployments as it's more direct. Use cr mode for declarative/GitOps approaches.","title":"mas_manage_attachment_configuration_mode"},{"location":"roles/suite_manage_attachments_config/#mas_instance_id","text":"MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application to configure. When to use : - Always required for attachment configuration - Must match the instance ID from MAS installation Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Used to locate Manage application resources and construct resource names. Related variables : - mas_workspace_id : Workspace within this instance - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the instance ID used during Manage installation.","title":"mas_instance_id"},{"location":"roles/suite_manage_attachments_config/#mas_workspace_id","text":"Workspace identifier for Manage application. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application to configure. When to use : - Always required for attachment configuration - Must match the workspace ID where Manage is deployed Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , masdev ) Impact : Used to locate Manage application resources within the specified instance. Related variables : - mas_instance_id : Parent instance - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the workspace ID used during Manage installation.","title":"mas_workspace_id"},{"location":"roles/suite_manage_attachments_config/#manage_workspace_cr_name","text":"ManageWorkspace custom resource name. Optional (Required when mas_manage_attachment_configuration_mode=cr ) Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: {mas_instance_id}-{mas_workspace_id} Purpose : Specifies the name of the ManageWorkspace custom resource to update when using CR configuration mode. When to use : - Required only when mas_manage_attachment_configuration_mode=cr - Use default unless you have a custom CR naming convention - Override if your ManageWorkspace CR has a non-standard name Valid values : Valid Kubernetes resource name Impact : Determines which ManageWorkspace CR is updated with attachment properties in bundleProperties section. Related variables : - mas_manage_attachment_configuration_mode : Must be cr for this to be used - mas_instance_id : Used in default name construction - mas_workspace_id : Used in default name construction Note : The default naming convention {instance}-{workspace} matches standard Manage deployments. Only override if you have custom CR names.","title":"manage_workspace_cr_name"},{"location":"roles/suite_manage_attachments_config/#db2_instance_name","text":"Db2 Warehouse instance name. Optional (Required when mas_manage_attachment_configuration_mode=db ) Environment Variable: DB2_INSTANCE_NAME Default: None Purpose : Identifies the Db2 Warehouse instance that stores Manage application data for direct database configuration updates. When to use : - Required when mas_manage_attachment_configuration_mode=db - Must match the Db2 instance name used by Manage - Used to connect to database for SQL updates Valid values : Valid Db2 instance name (e.g., db2w-manage , db2u-iot ) Impact : Determines which Db2 instance is accessed to update attachment configuration via SQL. Related variables : - mas_manage_attachment_configuration_mode : Must be db for this to be used - db2_namespace : Namespace containing this instance - db2_dbname : Database name within the instance Note : To find the instance name, go to the Db2 namespace and look for pods with label=engine . Describe the pod and find the app label value - that's your instance name.","title":"db2_instance_name"},{"location":"roles/suite_manage_attachments_config/#db2_namespace","text":"Db2 Warehouse namespace. Optional Environment Variable: DB2_NAMESPACE Default: db2u Purpose : Specifies the OpenShift namespace where the Db2 Warehouse instance is deployed. When to use : - Use default ( db2u ) for standard Db2 deployments - Override if Db2 is deployed in a custom namespace - Only relevant when mas_manage_attachment_configuration_mode=db Valid values : Valid Kubernetes namespace name Impact : Determines where to look for the Db2 instance when connecting for database updates. Related variables : - db2_instance_name : Instance to find in this namespace - mas_manage_attachment_configuration_mode : Must be db for this to be relevant Note : The default db2u namespace is used by most Db2 Warehouse deployments. Only change if you have a custom deployment.","title":"db2_namespace"},{"location":"roles/suite_manage_attachments_config/#db2_dbname","text":"Database name within Db2 instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB Purpose : Specifies the database name within the Db2 instance where Manage tables are stored. When to use : - Use default ( BLUDB ) for standard Manage deployments - Override if Manage uses a custom database name - Only relevant when mas_manage_attachment_configuration_mode=db Valid values : Valid Db2 database name Impact : Determines which database within the Db2 instance is updated with attachment configuration. Related variables : - db2_instance_name : Instance containing this database - mas_manage_attachment_configuration_mode : Must be db for this to be relevant Note : BLUDB is the default database name for Manage deployments. Only change if you have a custom database configuration.","title":"db2_dbname"},{"location":"roles/suite_manage_attachments_config/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_manage_attachments_config/#configure-cos-via-manageworkspace-cr","text":"The following sample can be used to configure COS for an existing Manage application instance's attachments via ManageWorkspace CR update (note cr as configuration mode + mas_instance_id and mas_workspace_id , which will be used to infer the CR name): - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: cr mas_instance_id: masinst1 mas_workspace_id: masdev cos_instance_name: cos-masinst1 cos_bucket_name: manage-attachments-bucket ibmcloud_apikey: xxxx mas_manage_attachments_provider: ibm roles: - ibm.mas_devops.suite_manage_attachments_config","title":"Configure COS via ManageWorkspace CR"},{"location":"roles/suite_manage_attachments_config/#provision-cos-and-configure-via-database","text":"The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance's attachments via SQL updates in database (note db as configuration mode + db2_instance_name ): - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: db mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_instance_name: cos-masinst1 cos_bucket_name: manage-attachments-bucket ibmcloud_apikey: xxxx mas_manage_attachments_provider: ibm roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_attachments_config","title":"Provision COS and Configure via Database"},{"location":"roles/suite_manage_attachments_config/#configure-file-storage-for-attachments","text":"The following sample playbook can be used to deploy Manage with default persistent storage for Manage attachments (PVC mount path /DOCLINKS ), and configure Manage system properties with the corresponding attachments settings via SQL updates in database: - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: db mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_manage_attachments_provider: filestorage roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_attachments_config","title":"Configure File Storage for Attachments"},{"location":"roles/suite_manage_attachments_config/#configure-aws-s3-buckets","text":"The following sample can be used to configure AWS S3 buckets for an existing Manage application instance's attachments: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage aws_bucket_name: manage-attachments-bucket mas_manage_attachments_provider: aws roles: - ibm.mas_devops.suite_manage_attachments_config","title":"Configure AWS S3 Buckets"},{"location":"roles/suite_manage_attachments_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_bim_config/","text":"suite_manage_bim_config \u00a4 This role configures Building Information Modeling (BIM) support in Maximo Manage application by setting up the persistent volume mount path and updating database system properties. BIM enables 3D visualization and management of building models within Manage. Prerequisites Manage application must be deployed with persistent volume storage configured A PVC with appropriate mount path must exist before running this role Use suite_app_config with mas_app_settings_persistent_volumes_flag: true to create default persistent storage For detailed information on persistent storage configuration, see Configuring persistent volume claims . What This Role Does \u00a4 Configures BIM folder paths in Manage system properties Updates Manage database with BIM mount path configuration Enables BIM functionality for 3D building model visualization Validates persistent volume mount path exists Role Variables \u00a4 mas_app_settings_bim_mount_path \u00a4 Persistent volume mount path for BIM files. Optional Environment Variable: MAS_APP_SETTINGS_BIM_MOUNT_PATH Default: /bim Purpose : Specifies the container mount path where BIM files (3D models, drawings, documents) are stored in persistent storage. When to use : - Use default /bim when using suite_app_config with persistent volumes - Override if you have a custom mount path configuration - Must match the actual PVC mount path in Manage pods Valid values : Valid Linux filesystem path (e.g., /bim , /data/bim , /mnt/bim ) Impact : Determines where Manage stores and retrieves BIM files. The path must exist as a mounted volume in Manage pods. Related variables : - mas_instance_id : Instance containing Manage - db2_instance_name : Database to update with BIM configuration Note : The mount path must correspond to an actual PVC mounted in the Manage deployment. If using suite_app_config with mas_app_settings_persistent_volumes_flag: true , the default /bim path is automatically configured. mas_instance_id \u00a4 MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application to configure for BIM. When to use : - Always required for BIM configuration - Must match the instance ID from MAS installation - Used to locate Manage resources Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance's Manage application is configured with BIM support. Related variables : - mas_app_settings_bim_mount_path : BIM storage path to configure - db2_instance_name : Database instance for this Manage deployment Note : This must match the instance ID used during Manage installation. db2_instance_name \u00a4 Db2 Warehouse instance name. Required Environment Variable: DB2_INSTANCE_NAME Default: None Purpose : Identifies the Db2 Warehouse instance that stores Manage application data, which will be updated with BIM system properties. When to use : - Always required for BIM configuration - Must match the Db2 instance name used by Manage - Used to connect to database for SQL updates Valid values : Valid Db2 instance name (e.g., db2w-manage , db2u-manage ) Impact : Determines which Db2 instance is accessed to update BIM configuration system properties via SQL. Related variables : - db2_namespace : Namespace containing this instance - db2_dbname : Database name within the instance - mas_instance_id : MAS instance using this database Note : To find the instance name, go to the Db2 namespace and look for pods with label=engine . Describe the pod and find the app label value - that's your instance name. db2_namespace \u00a4 Db2 Warehouse namespace. Optional Environment Variable: DB2_NAMESPACE Default: db2u Purpose : Specifies the OpenShift namespace where the Db2 Warehouse instance is deployed. When to use : - Use default ( db2u ) for standard Db2 deployments - Override if Db2 is deployed in a custom namespace - Required to locate the Db2 instance Valid values : Valid Kubernetes namespace name Impact : Determines where to look for the Db2 instance when connecting for BIM configuration updates. Related variables : - db2_instance_name : Instance to find in this namespace - db2_dbname : Database within the instance Note : The default db2u namespace is used by most Db2 Warehouse deployments. Only change if you have a custom deployment. db2_dbname \u00a4 Database name within Db2 instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB Purpose : Specifies the database name within the Db2 instance where Manage tables and BIM configuration are stored. When to use : - Use default ( BLUDB ) for standard Manage deployments - Override if Manage uses a custom database name - Required for database connection Valid values : Valid Db2 database name Impact : Determines which database within the Db2 instance is updated with BIM system properties. Related variables : - db2_instance_name : Instance containing this database - db2_namespace : Namespace of the instance Note : BLUDB is the default database name for Manage deployments. Only change if you have a custom database configuration. Example Playbook \u00a4 Configure BIM for Existing Manage Instance \u00a4 The following sample can be used to configure BIM for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 db2_instance_name: db2w-manage mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.suite_manage_bim_config Deploy Manage with BIM Configuration \u00a4 The following sample playbook can be used to deploy Manage with default persistent storage for BIM (PVC mount path /bim ), and configure Manage system properties with the corresponding BIM settings: - hosts: localhost any_errors_fatal: true vars: mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_bim_config License \u00a4 EPL-2.0","title":"suite_manage_bim_config"},{"location":"roles/suite_manage_bim_config/#suite_manage_bim_config","text":"This role configures Building Information Modeling (BIM) support in Maximo Manage application by setting up the persistent volume mount path and updating database system properties. BIM enables 3D visualization and management of building models within Manage. Prerequisites Manage application must be deployed with persistent volume storage configured A PVC with appropriate mount path must exist before running this role Use suite_app_config with mas_app_settings_persistent_volumes_flag: true to create default persistent storage For detailed information on persistent storage configuration, see Configuring persistent volume claims .","title":"suite_manage_bim_config"},{"location":"roles/suite_manage_bim_config/#what-this-role-does","text":"Configures BIM folder paths in Manage system properties Updates Manage database with BIM mount path configuration Enables BIM functionality for 3D building model visualization Validates persistent volume mount path exists","title":"What This Role Does"},{"location":"roles/suite_manage_bim_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_bim_config/#mas_app_settings_bim_mount_path","text":"Persistent volume mount path for BIM files. Optional Environment Variable: MAS_APP_SETTINGS_BIM_MOUNT_PATH Default: /bim Purpose : Specifies the container mount path where BIM files (3D models, drawings, documents) are stored in persistent storage. When to use : - Use default /bim when using suite_app_config with persistent volumes - Override if you have a custom mount path configuration - Must match the actual PVC mount path in Manage pods Valid values : Valid Linux filesystem path (e.g., /bim , /data/bim , /mnt/bim ) Impact : Determines where Manage stores and retrieves BIM files. The path must exist as a mounted volume in Manage pods. Related variables : - mas_instance_id : Instance containing Manage - db2_instance_name : Database to update with BIM configuration Note : The mount path must correspond to an actual PVC mounted in the Manage deployment. If using suite_app_config with mas_app_settings_persistent_volumes_flag: true , the default /bim path is automatically configured.","title":"mas_app_settings_bim_mount_path"},{"location":"roles/suite_manage_bim_config/#mas_instance_id","text":"MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application to configure for BIM. When to use : - Always required for BIM configuration - Must match the instance ID from MAS installation - Used to locate Manage resources Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance's Manage application is configured with BIM support. Related variables : - mas_app_settings_bim_mount_path : BIM storage path to configure - db2_instance_name : Database instance for this Manage deployment Note : This must match the instance ID used during Manage installation.","title":"mas_instance_id"},{"location":"roles/suite_manage_bim_config/#db2_instance_name","text":"Db2 Warehouse instance name. Required Environment Variable: DB2_INSTANCE_NAME Default: None Purpose : Identifies the Db2 Warehouse instance that stores Manage application data, which will be updated with BIM system properties. When to use : - Always required for BIM configuration - Must match the Db2 instance name used by Manage - Used to connect to database for SQL updates Valid values : Valid Db2 instance name (e.g., db2w-manage , db2u-manage ) Impact : Determines which Db2 instance is accessed to update BIM configuration system properties via SQL. Related variables : - db2_namespace : Namespace containing this instance - db2_dbname : Database name within the instance - mas_instance_id : MAS instance using this database Note : To find the instance name, go to the Db2 namespace and look for pods with label=engine . Describe the pod and find the app label value - that's your instance name.","title":"db2_instance_name"},{"location":"roles/suite_manage_bim_config/#db2_namespace","text":"Db2 Warehouse namespace. Optional Environment Variable: DB2_NAMESPACE Default: db2u Purpose : Specifies the OpenShift namespace where the Db2 Warehouse instance is deployed. When to use : - Use default ( db2u ) for standard Db2 deployments - Override if Db2 is deployed in a custom namespace - Required to locate the Db2 instance Valid values : Valid Kubernetes namespace name Impact : Determines where to look for the Db2 instance when connecting for BIM configuration updates. Related variables : - db2_instance_name : Instance to find in this namespace - db2_dbname : Database within the instance Note : The default db2u namespace is used by most Db2 Warehouse deployments. Only change if you have a custom deployment.","title":"db2_namespace"},{"location":"roles/suite_manage_bim_config/#db2_dbname","text":"Database name within Db2 instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB Purpose : Specifies the database name within the Db2 instance where Manage tables and BIM configuration are stored. When to use : - Use default ( BLUDB ) for standard Manage deployments - Override if Manage uses a custom database name - Required for database connection Valid values : Valid Db2 database name Impact : Determines which database within the Db2 instance is updated with BIM system properties. Related variables : - db2_instance_name : Instance containing this database - db2_namespace : Namespace of the instance Note : BLUDB is the default database name for Manage deployments. Only change if you have a custom database configuration.","title":"db2_dbname"},{"location":"roles/suite_manage_bim_config/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_manage_bim_config/#configure-bim-for-existing-manage-instance","text":"The following sample can be used to configure BIM for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 db2_instance_name: db2w-manage mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.suite_manage_bim_config","title":"Configure BIM for Existing Manage Instance"},{"location":"roles/suite_manage_bim_config/#deploy-manage-with-bim-configuration","text":"The following sample playbook can be used to deploy Manage with default persistent storage for BIM (PVC mount path /bim ), and configure Manage system properties with the corresponding BIM settings: - hosts: localhost any_errors_fatal: true vars: mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_bim_config","title":"Deploy Manage with BIM Configuration"},{"location":"roles/suite_manage_bim_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_birt_report_config/","text":"suite_manage_birt_report_config \u00a4 This role configures BIRT (Business Intelligence and Reporting Tools) reporting in Maximo Manage application by setting up a dedicated report bundle server workload. This enables scalable report generation by offloading report processing to dedicated pods. What This Role Does \u00a4 Configures dedicated report bundle server for BIRT report generation Sets up Manage report route endpoint for generated reports Updates Manage system properties for all bundles: mxe.report.birt.viewerurl : Points to dedicated report server route mxe.report.birt.disablequeuemanager : Enables queue manager only on report bundle (0 for report bundle, 1 for others) Forwards report workload to dedicated report-type bundle pods Performance Optimization Using a dedicated report bundle server improves Manage performance by isolating resource-intensive report generation from other Manage operations. Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application to configure for BIRT reporting. When to use : - Always required for BIRT report configuration - Must match the instance ID from MAS installation - Used to locate Manage resources Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance's Manage application is configured with BIRT report server. Related variables : - mas_workspace_id : Workspace within this instance - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the instance ID used during Manage installation. mas_workspace_id \u00a4 Workspace identifier for Manage application. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application to configure for BIRT reporting. When to use : - Always required for BIRT report configuration - Must match the workspace ID where Manage is deployed - Used in report route URL construction Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , main ) Impact : Used to construct the report server route URL: https://{workspace_id}-{report_bundle_name}.manage.{domain} Related variables : - mas_instance_id : Parent instance - manage_report_bundle_server_name : Report bundle name in route URL - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the workspace ID used during Manage installation. manage_workspace_cr_name \u00a4 ManageWorkspace custom resource name. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: {mas_instance_id}-{mas_workspace_id} Purpose : Specifies the name of the ManageWorkspace custom resource to update with BIRT report configuration. When to use : - Use default unless you have a custom CR naming convention - Override if your ManageWorkspace CR has a non-standard name - Required to update bundle properties Valid values : Valid Kubernetes resource name Impact : Determines which ManageWorkspace CR is updated with report bundle configuration and system properties. Related variables : - mas_instance_id : Used in default name construction - mas_workspace_id : Used in default name construction - manage_report_bundle_server_name : Report bundle to configure Note : The default naming convention {instance}-{workspace} matches standard Manage deployments. Only override if you have custom CR names. manage_report_bundle_server_name \u00a4 Report bundle server name. Optional Environment Variable: MANAGE_REPORT_BUNDLE_SERVER_NAME Default: rpt Purpose : Defines the name of the dedicated report bundle server and its corresponding route for BIRT report generation. When to use : - Use default ( rpt ) for standard deployments - Override for custom naming conventions - Not needed if report bundle server is already configured Valid values : Valid Kubernetes resource name (lowercase alphanumeric and hyphens) Impact : - Determines the report bundle server name in Manage configuration - Used in report route URL: https://{workspace_id}-{this_name}.manage.{domain} - Identifies which bundle pods handle report generation Related variables : - mas_workspace_id : Used in route URL construction - manage_workspace_cr_name : CR to update with this bundle configuration Note : The default rpt is a common abbreviation for \"report\". Choose a meaningful name that clearly identifies the report bundle server purpose. Example Playbook \u00a4 The following sample can be used to configure BIRT report for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: main roles: - ibm.mas_devops.suite_manage_birt_report_config Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export MAS_INSTANCE_ID=masinst1 export MAS_WORKSPACE_ID=main export MANAGE_REPORT_BUNDLE_SERVER_NAME=report ROLE_NAME='suite_manage_birt_report_config' ansible-playbook playbooks/run_role.yml License \u00a4 EPL-2.0","title":"suite_manage_birt_report_config"},{"location":"roles/suite_manage_birt_report_config/#suite_manage_birt_report_config","text":"This role configures BIRT (Business Intelligence and Reporting Tools) reporting in Maximo Manage application by setting up a dedicated report bundle server workload. This enables scalable report generation by offloading report processing to dedicated pods.","title":"suite_manage_birt_report_config"},{"location":"roles/suite_manage_birt_report_config/#what-this-role-does","text":"Configures dedicated report bundle server for BIRT report generation Sets up Manage report route endpoint for generated reports Updates Manage system properties for all bundles: mxe.report.birt.viewerurl : Points to dedicated report server route mxe.report.birt.disablequeuemanager : Enables queue manager only on report bundle (0 for report bundle, 1 for others) Forwards report workload to dedicated report-type bundle pods Performance Optimization Using a dedicated report bundle server improves Manage performance by isolating resource-intensive report generation from other Manage operations.","title":"What This Role Does"},{"location":"roles/suite_manage_birt_report_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_birt_report_config/#mas_instance_id","text":"MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application to configure for BIRT reporting. When to use : - Always required for BIRT report configuration - Must match the instance ID from MAS installation - Used to locate Manage resources Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance's Manage application is configured with BIRT report server. Related variables : - mas_workspace_id : Workspace within this instance - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the instance ID used during Manage installation.","title":"mas_instance_id"},{"location":"roles/suite_manage_birt_report_config/#mas_workspace_id","text":"Workspace identifier for Manage application. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application to configure for BIRT reporting. When to use : - Always required for BIRT report configuration - Must match the workspace ID where Manage is deployed - Used in report route URL construction Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , main ) Impact : Used to construct the report server route URL: https://{workspace_id}-{report_bundle_name}.manage.{domain} Related variables : - mas_instance_id : Parent instance - manage_report_bundle_server_name : Report bundle name in route URL - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the workspace ID used during Manage installation.","title":"mas_workspace_id"},{"location":"roles/suite_manage_birt_report_config/#manage_workspace_cr_name","text":"ManageWorkspace custom resource name. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: {mas_instance_id}-{mas_workspace_id} Purpose : Specifies the name of the ManageWorkspace custom resource to update with BIRT report configuration. When to use : - Use default unless you have a custom CR naming convention - Override if your ManageWorkspace CR has a non-standard name - Required to update bundle properties Valid values : Valid Kubernetes resource name Impact : Determines which ManageWorkspace CR is updated with report bundle configuration and system properties. Related variables : - mas_instance_id : Used in default name construction - mas_workspace_id : Used in default name construction - manage_report_bundle_server_name : Report bundle to configure Note : The default naming convention {instance}-{workspace} matches standard Manage deployments. Only override if you have custom CR names.","title":"manage_workspace_cr_name"},{"location":"roles/suite_manage_birt_report_config/#manage_report_bundle_server_name","text":"Report bundle server name. Optional Environment Variable: MANAGE_REPORT_BUNDLE_SERVER_NAME Default: rpt Purpose : Defines the name of the dedicated report bundle server and its corresponding route for BIRT report generation. When to use : - Use default ( rpt ) for standard deployments - Override for custom naming conventions - Not needed if report bundle server is already configured Valid values : Valid Kubernetes resource name (lowercase alphanumeric and hyphens) Impact : - Determines the report bundle server name in Manage configuration - Used in report route URL: https://{workspace_id}-{this_name}.manage.{domain} - Identifies which bundle pods handle report generation Related variables : - mas_workspace_id : Used in route URL construction - manage_workspace_cr_name : CR to update with this bundle configuration Note : The default rpt is a common abbreviation for \"report\". Choose a meaningful name that clearly identifies the report bundle server purpose.","title":"manage_report_bundle_server_name"},{"location":"roles/suite_manage_birt_report_config/#example-playbook","text":"The following sample can be used to configure BIRT report for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: main roles: - ibm.mas_devops.suite_manage_birt_report_config","title":"Example Playbook"},{"location":"roles/suite_manage_birt_report_config/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export MAS_INSTANCE_ID=masinst1 export MAS_WORKSPACE_ID=main export MANAGE_REPORT_BUNDLE_SERVER_NAME=report ROLE_NAME='suite_manage_birt_report_config' ansible-playbook playbooks/run_role.yml","title":"Run Role Playbook"},{"location":"roles/suite_manage_birt_report_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_customer_files_config/","text":"suite_manage_customer_files_config \u00a4 This role configures cloud object storage (IBM Cloud Object Storage or AWS S3) for storing Maximo Manage application customer files. Customer files include user-uploaded documents, images, and other files that need to be stored separately from the database. Prerequisites This role must be executed after Manage application is deployed and activated. Manage must be up and running before configuring customer files features. What This Role Does \u00a4 Creates three S3/COS buckets for customer files management: Main bucket : Stores active customer files Backup bucket : Stores backup copies of customer files Recovery bucket : Used for disaster recovery scenarios Configures Manage to use cloud storage for customer files Automatically executes cos_bucket role to set up buckets Updates ManageWorkspace CR with storage configuration Default Bucket Names \u00a4 The role creates three buckets with these default names: - {mas_instance_id}-{mas_workspace_id}-custfiles - {mas_instance_id}-{mas_workspace_id}-custfilesbackup - {mas_instance_id}-{mas_workspace_id}-custfilesrecovery Role Variables \u00a4 cos_type \u00a4 Cloud object storage provider type. Required Environment Variable: COS_TYPE Default: None Purpose : Specifies which cloud storage provider to use for Manage customer files storage. When to use : - Always required for customer files configuration - Choose based on your cloud infrastructure - Determines which credentials and CLI tools are needed Valid values : ibm , aws - ibm : IBM Cloud Object Storage - aws : Amazon S3 Impact : - ibm : Uses IBM Cloud Object Storage (requires ibmcloud_apikey , cos_instance_name ) - aws : Uses AWS S3 (requires AWS CLI installed and AWS credentials configured) Related variables : - custfiles_bucketname : Main bucket to create - mas_instance_id : Used in default bucket names Note : For AWS, you must install AWS CLI and configure credentials via aws configure or export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables. custfiles_bucketname \u00a4 Main customer files bucket name. Optional Environment Variable: MANAGE_CUSTFILES_BUCKET_NAME Default: {mas_instance_id}-{mas_workspace_id}-custfiles Purpose : Specifies the name of the primary S3/COS bucket where active customer files are stored. When to use : - Use default for standard naming convention - Override for custom bucket naming requirements - Must be unique within the storage provider Valid values : Valid S3/COS bucket name (lowercase, alphanumeric, hyphens) Impact : Determines where Manage stores active customer files. The bucket will be created if it doesn't exist. Related variables : - cos_type : Storage provider for this bucket - custfiles_bucketname_backup : Related backup bucket - custfiles_bucketname_recovery : Related recovery bucket Note : Bucket names must be globally unique in AWS S3. For IBM COS, they must be unique within your account. The default naming includes instance and workspace IDs to ensure uniqueness. custfiles_bucketname_backup \u00a4 Backup customer files bucket name. Optional Environment Variable: MANAGE_CUSTFILES_BACKUP_BUCKET_NAME Default: {mas_instance_id}-{mas_workspace_id}-custfilesbackup Purpose : Specifies the name of the S3/COS bucket used to store backup copies of customer files. When to use : - Use default for standard naming convention - Override for custom bucket naming requirements - Part of the backup and recovery strategy Valid values : Valid S3/COS bucket name (lowercase, alphanumeric, hyphens) Impact : Determines where backup copies of customer files are stored. Used during backup operations to preserve file versions. Related variables : - cos_type : Storage provider for this bucket - custfiles_bucketname : Main files bucket - custfiles_bucketname_recovery : Recovery bucket Note : Keep backup bucket separate from main bucket for data protection. The default naming appends \"backup\" to clearly identify the bucket purpose. custfiles_bucketname_recovery \u00a4 Recovery customer files bucket name. Optional Environment Variable: MANAGE_CUSTFILES_RECOVERY_BUCKET_NAME Default: {mas_instance_id}-{mas_workspace_id}-custfilesrecovery Purpose : Specifies the name of the S3/COS bucket used for disaster recovery of customer files. When to use : - Use default for standard naming convention - Override for custom bucket naming requirements - Part of the disaster recovery strategy Valid values : Valid S3/COS bucket name (lowercase, alphanumeric, hyphens) Impact : Determines where recovery copies of customer files are stored. Used during disaster recovery scenarios to restore files. Related variables : - cos_type : Storage provider for this bucket - custfiles_bucketname : Main files bucket - custfiles_bucketname_backup : Backup bucket Note : Keep recovery bucket separate from main and backup buckets for maximum data protection. The default naming appends \"recovery\" to clearly identify the bucket purpose. mas_instance_id \u00a4 MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application to configure for customer files storage. When to use : - Always required for customer files configuration - Must match the instance ID from MAS installation - Used in default bucket name construction Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Used to construct default bucket names and locate Manage application resources. Related variables : - mas_workspace_id : Workspace within this instance - custfiles_bucketname : Uses instance ID in default name - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the instance ID used during Manage installation. mas_workspace_id \u00a4 Workspace identifier for Manage application. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application to configure for customer files storage. When to use : - Always required for customer files configuration - Must match the workspace ID where Manage is deployed - Used in default bucket name construction Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , masdev ) Impact : Used to construct default bucket names and locate Manage application resources within the specified instance. Related variables : - mas_instance_id : Parent instance - custfiles_bucketname : Uses workspace ID in default name - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the workspace ID used during Manage installation. manage_workspace_cr_name \u00a4 ManageWorkspace custom resource name. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: {mas_instance_id}-{mas_workspace_id} Purpose : Specifies the name of the ManageWorkspace custom resource to update with customer files storage configuration. When to use : - Use default unless you have a custom CR naming convention - Override if your ManageWorkspace CR has a non-standard name - Required to update storage configuration Valid values : Valid Kubernetes resource name Impact : Determines which ManageWorkspace CR is updated with customer files bucket configuration. Related variables : - mas_instance_id : Used in default name construction - mas_workspace_id : Used in default name construction - cos_type : Storage provider being configured Note : The default naming convention {instance}-{workspace} matches standard Manage deployments. Only override if you have custom CR names. Example Playbook \u00a4 Configure COS for Existing Manage Instance \u00a4 The following sample can be used to configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx roles: - ibm.mas_devops.suite_manage_customer_files_config Provision and Configure IBM Cloud COS \u00a4 The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_customer_files_config Configure AWS S3 Buckets \u00a4 The following sample can be used to configure AWS S3 buckets for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: aws roles: - ibm.mas_devops.suite_manage_customer_files_config License \u00a4 EPL-2.0","title":"suite_manage_customer_files_config"},{"location":"roles/suite_manage_customer_files_config/#suite_manage_customer_files_config","text":"This role configures cloud object storage (IBM Cloud Object Storage or AWS S3) for storing Maximo Manage application customer files. Customer files include user-uploaded documents, images, and other files that need to be stored separately from the database. Prerequisites This role must be executed after Manage application is deployed and activated. Manage must be up and running before configuring customer files features.","title":"suite_manage_customer_files_config"},{"location":"roles/suite_manage_customer_files_config/#what-this-role-does","text":"Creates three S3/COS buckets for customer files management: Main bucket : Stores active customer files Backup bucket : Stores backup copies of customer files Recovery bucket : Used for disaster recovery scenarios Configures Manage to use cloud storage for customer files Automatically executes cos_bucket role to set up buckets Updates ManageWorkspace CR with storage configuration","title":"What This Role Does"},{"location":"roles/suite_manage_customer_files_config/#default-bucket-names","text":"The role creates three buckets with these default names: - {mas_instance_id}-{mas_workspace_id}-custfiles - {mas_instance_id}-{mas_workspace_id}-custfilesbackup - {mas_instance_id}-{mas_workspace_id}-custfilesrecovery","title":"Default Bucket Names"},{"location":"roles/suite_manage_customer_files_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_customer_files_config/#cos_type","text":"Cloud object storage provider type. Required Environment Variable: COS_TYPE Default: None Purpose : Specifies which cloud storage provider to use for Manage customer files storage. When to use : - Always required for customer files configuration - Choose based on your cloud infrastructure - Determines which credentials and CLI tools are needed Valid values : ibm , aws - ibm : IBM Cloud Object Storage - aws : Amazon S3 Impact : - ibm : Uses IBM Cloud Object Storage (requires ibmcloud_apikey , cos_instance_name ) - aws : Uses AWS S3 (requires AWS CLI installed and AWS credentials configured) Related variables : - custfiles_bucketname : Main bucket to create - mas_instance_id : Used in default bucket names Note : For AWS, you must install AWS CLI and configure credentials via aws configure or export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables.","title":"cos_type"},{"location":"roles/suite_manage_customer_files_config/#custfiles_bucketname","text":"Main customer files bucket name. Optional Environment Variable: MANAGE_CUSTFILES_BUCKET_NAME Default: {mas_instance_id}-{mas_workspace_id}-custfiles Purpose : Specifies the name of the primary S3/COS bucket where active customer files are stored. When to use : - Use default for standard naming convention - Override for custom bucket naming requirements - Must be unique within the storage provider Valid values : Valid S3/COS bucket name (lowercase, alphanumeric, hyphens) Impact : Determines where Manage stores active customer files. The bucket will be created if it doesn't exist. Related variables : - cos_type : Storage provider for this bucket - custfiles_bucketname_backup : Related backup bucket - custfiles_bucketname_recovery : Related recovery bucket Note : Bucket names must be globally unique in AWS S3. For IBM COS, they must be unique within your account. The default naming includes instance and workspace IDs to ensure uniqueness.","title":"custfiles_bucketname"},{"location":"roles/suite_manage_customer_files_config/#custfiles_bucketname_backup","text":"Backup customer files bucket name. Optional Environment Variable: MANAGE_CUSTFILES_BACKUP_BUCKET_NAME Default: {mas_instance_id}-{mas_workspace_id}-custfilesbackup Purpose : Specifies the name of the S3/COS bucket used to store backup copies of customer files. When to use : - Use default for standard naming convention - Override for custom bucket naming requirements - Part of the backup and recovery strategy Valid values : Valid S3/COS bucket name (lowercase, alphanumeric, hyphens) Impact : Determines where backup copies of customer files are stored. Used during backup operations to preserve file versions. Related variables : - cos_type : Storage provider for this bucket - custfiles_bucketname : Main files bucket - custfiles_bucketname_recovery : Recovery bucket Note : Keep backup bucket separate from main bucket for data protection. The default naming appends \"backup\" to clearly identify the bucket purpose.","title":"custfiles_bucketname_backup"},{"location":"roles/suite_manage_customer_files_config/#custfiles_bucketname_recovery","text":"Recovery customer files bucket name. Optional Environment Variable: MANAGE_CUSTFILES_RECOVERY_BUCKET_NAME Default: {mas_instance_id}-{mas_workspace_id}-custfilesrecovery Purpose : Specifies the name of the S3/COS bucket used for disaster recovery of customer files. When to use : - Use default for standard naming convention - Override for custom bucket naming requirements - Part of the disaster recovery strategy Valid values : Valid S3/COS bucket name (lowercase, alphanumeric, hyphens) Impact : Determines where recovery copies of customer files are stored. Used during disaster recovery scenarios to restore files. Related variables : - cos_type : Storage provider for this bucket - custfiles_bucketname : Main files bucket - custfiles_bucketname_backup : Backup bucket Note : Keep recovery bucket separate from main and backup buckets for maximum data protection. The default naming appends \"recovery\" to clearly identify the bucket purpose.","title":"custfiles_bucketname_recovery"},{"location":"roles/suite_manage_customer_files_config/#mas_instance_id","text":"MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application to configure for customer files storage. When to use : - Always required for customer files configuration - Must match the instance ID from MAS installation - Used in default bucket name construction Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Used to construct default bucket names and locate Manage application resources. Related variables : - mas_workspace_id : Workspace within this instance - custfiles_bucketname : Uses instance ID in default name - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the instance ID used during Manage installation.","title":"mas_instance_id"},{"location":"roles/suite_manage_customer_files_config/#mas_workspace_id","text":"Workspace identifier for Manage application. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application to configure for customer files storage. When to use : - Always required for customer files configuration - Must match the workspace ID where Manage is deployed - Used in default bucket name construction Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , masdev ) Impact : Used to construct default bucket names and locate Manage application resources within the specified instance. Related variables : - mas_instance_id : Parent instance - custfiles_bucketname : Uses workspace ID in default name - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the workspace ID used during Manage installation.","title":"mas_workspace_id"},{"location":"roles/suite_manage_customer_files_config/#manage_workspace_cr_name","text":"ManageWorkspace custom resource name. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: {mas_instance_id}-{mas_workspace_id} Purpose : Specifies the name of the ManageWorkspace custom resource to update with customer files storage configuration. When to use : - Use default unless you have a custom CR naming convention - Override if your ManageWorkspace CR has a non-standard name - Required to update storage configuration Valid values : Valid Kubernetes resource name Impact : Determines which ManageWorkspace CR is updated with customer files bucket configuration. Related variables : - mas_instance_id : Used in default name construction - mas_workspace_id : Used in default name construction - cos_type : Storage provider being configured Note : The default naming convention {instance}-{workspace} matches standard Manage deployments. Only override if you have custom CR names.","title":"manage_workspace_cr_name"},{"location":"roles/suite_manage_customer_files_config/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_manage_customer_files_config/#configure-cos-for-existing-manage-instance","text":"The following sample can be used to configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx roles: - ibm.mas_devops.suite_manage_customer_files_config","title":"Configure COS for Existing Manage Instance"},{"location":"roles/suite_manage_customer_files_config/#provision-and-configure-ibm-cloud-cos","text":"The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_customer_files_config","title":"Provision and Configure IBM Cloud COS"},{"location":"roles/suite_manage_customer_files_config/#configure-aws-s3-buckets","text":"The following sample can be used to configure AWS S3 buckets for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: aws roles: - ibm.mas_devops.suite_manage_customer_files_config","title":"Configure AWS S3 Buckets"},{"location":"roles/suite_manage_customer_files_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_imagestitching_config/","text":"suite_manage_imagestitching_config \u00a4 This role configures image stitching functionality in Maximo Manage for the Civil Infrastructure component. Image stitching enables combining multiple images into panoramic views for infrastructure inspection and analysis. The role creates persistent storage, updates the ManageWorkspace CR, and sets required system properties. Prerequisites Manage application must be deployed and activated Civil Infrastructure component must be installed Storage class must support ReadWriteMany (RWX) access mode What This Role Does \u00a4 Creates PVC for image stitching data storage Patches ManageWorkspace CR with PVC configuration Configures persistent volume specifications Sets system properties: mci.imagestitching.apiurl : API endpoint for image stitching service imagestitching.dataInputPath : Data input path for image processing Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application to configure for image stitching. When to use : - Always required for image stitching configuration - Must match the instance ID from MAS installation - Used in PVC name construction Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., civil , prod , masinst1 ) Impact : Used to construct PVC name ( {instance}-{workspace}-{pvcname} ) and locate Manage resources. Related variables : - mas_workspace_id : Workspace within this instance - stitching_pvcname : Combined to create full PVC name Note : This must match the instance ID used during Manage installation with Civil component. mas_workspace_id \u00a4 Workspace identifier for Manage application. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application with Civil component. When to use : - Always required for image stitching configuration - Must match the workspace ID where Manage is deployed - Used in PVC name construction Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , masdev ) Impact : Used to construct PVC name ( {instance}-{workspace}-{pvcname} ) and locate Manage resources. Related variables : - mas_instance_id : Parent instance - stitching_pvcname : Combined to create full PVC name Note : This must match the workspace ID used during Manage installation. mas_domain \u00a4 MAS cluster domain name. Optional Environment Variable: MAS_DOMAIN Default: Discovered from Suite CR Purpose : Specifies the domain name for the Manage cluster, used to construct the image stitching API URL. When to use : - Use default (auto-discovery) for standard deployments - Override if auto-discovery fails or for custom domains - Required for constructing service endpoints Valid values : Valid DNS domain name (e.g., civil.ibmmasdev.com , apps.cluster.example.com ) Impact : Used to construct the mci.imagestitching.apiurl system property for the image stitching service endpoint. Related variables : - mas_instance_id : Instance in this domain - mas_workspace_id : Workspace in this domain Note : The role attempts to discover the domain from the Suite CR. Only override if auto-discovery doesn't work for your environment. stitching_pvcname \u00a4 PVC name postfix for image stitching storage. Optional Environment Variable: IMAGESTITCHING_PVCNAME Default: manage-imagestitching Purpose : Defines the postfix for the PVC name. The full PVC name is constructed as {mas_instance_id}-{mas_workspace_id}-{stitching_pvcname} . When to use : - Use default for standard deployments - Override for custom naming conventions - Must be unique within the namespace Valid values : Valid Kubernetes resource name (lowercase alphanumeric and hyphens) Impact : Determines the PVC name used for image stitching data storage. The ManageWorkspace CR is updated with this PVC reference. Related variables : - mas_instance_id : Prepended to create full PVC name - mas_workspace_id : Prepended to create full PVC name - stitching_storage_class : Storage class for this PVC - stitching_storage_size : Size of this PVC Note : The default manage-imagestitching clearly identifies the PVC purpose. Full PVC name example: civil-masdev-manage-imagestitching . stitching_storage_class \u00a4 Storage class for image stitching PVC. Optional Environment Variable: IMAGESTITCHING_STORAGE_CLASS Default: Auto-discovered Purpose : Specifies the Kubernetes storage class to use for the image stitching PVC. Must support ReadWriteMany (RWX) access mode. When to use : - Use default (auto-discovery) if cluster has suitable RWX storage class - Override to specify a particular RWX-capable storage class - Required when multiple RWX storage classes exist Valid values : Valid Kubernetes storage class name that supports RWX (e.g., nfs-client , ocs-storagecluster-cephfs , ibmc-file-gold ) Impact : Determines the underlying storage technology for image stitching data. Must support ReadWriteMany for multiple pod access. Related variables : - stitching_pvcname : PVC using this storage class - stitching_storage_mode : Must be ReadWriteMany - stitching_storage_size : Size of storage to provision Note : CRITICAL - The storage class MUST support ReadWriteMany (RWX) access mode as image stitching requires shared access from multiple pods. Common RWX storage classes include NFS, CephFS, and IBM Cloud File Storage. stitching_storage_size \u00a4 PVC storage size for image stitching. Optional Environment Variable: IMAGESTITCHING_STORAGE_SIZE Default: 20Gi Purpose : Specifies the size of the persistent volume claim for storing image stitching data and processed images. When to use : - Use default ( 20Gi ) for small to medium deployments - Increase for large-scale infrastructure inspection projects - Consider image resolution and retention requirements Valid values : Kubernetes storage size format (e.g., 20Gi , 50Gi , 100Gi , 1Ti ) Impact : Determines how much image data can be stored. Insufficient storage will prevent new image processing. Related variables : - stitching_pvcname : PVC with this size - stitching_storage_class : Storage class providing this capacity Note : Size requirements depend on: - Number of images processed - Image resolution and format - Retention period for processed images - Number of concurrent stitching operations Start with 20Gi and monitor usage to adjust as needed. stitching_storage_mode \u00a4 PVC access mode for image stitching. Optional Environment Variable: IMAGESTITCHING_STORAGE_MODE Default: ReadWriteMany Purpose : Specifies the Kubernetes access mode for the image stitching PVC. Must be ReadWriteMany to allow multiple pods to access the storage simultaneously. When to use : - Always use default ReadWriteMany for image stitching - Do not change unless you understand the implications - Required for multi-pod access to shared data Valid values : ReadWriteMany (RWX) - other modes not supported Impact : Enables multiple Manage pods to read and write image stitching data concurrently. Essential for distributed image processing. Related variables : - stitching_storage_class : Must support this access mode - stitching_pvcname : PVC with this access mode Note : CRITICAL - ReadWriteMany (RWX) is mandatory for image stitching functionality. Do not change this value. Ensure your storage class supports RWX access mode. stitching_storage_mountpath \u00a4 Mount path for image stitching storage. Optional Environment Variable: IMAGESTITCHING_STORAGE_MOUNTPATH Default: imagestitching Purpose : Specifies the mount path where the image stitching PVC is mounted in Manage pods. When to use : - Use default ( imagestitching ) for standard deployments - Override only if you have specific path requirements - Must not conflict with other mount paths Valid values : Valid Linux filesystem path component (alphanumeric, no leading slash) Impact : Determines where image stitching data is accessible within Manage pods. The imagestitching.dataInputPath system property is set to this path. Related variables : - stitching_pvcname : PVC mounted at this path Note : The default imagestitching is relative to the Manage pod's base mount path. The full path is constructed by Manage. Do not include leading or trailing slashes. Example Playbook \u00a4 The following sample will configure image stitching for an existing Manage application instance via ManageWorkspace CR update: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: civil mas_workspace_id: masdev mas_domain: civil.ibmmasdev.com stitching_pvcname: manage-imagestitching stitching_storage_class: nfs-client stitching_storage_size: 20Gi stitching_storage_mode: ReadWriteMany stitching_storage_mountpath: imagestitching roles: - ibm.mas_devops.suite_manage_imagestitching_config License \u00a4 EPL-2.0","title":"suite_manage_imagestitching_config"},{"location":"roles/suite_manage_imagestitching_config/#suite_manage_imagestitching_config","text":"This role configures image stitching functionality in Maximo Manage for the Civil Infrastructure component. Image stitching enables combining multiple images into panoramic views for infrastructure inspection and analysis. The role creates persistent storage, updates the ManageWorkspace CR, and sets required system properties. Prerequisites Manage application must be deployed and activated Civil Infrastructure component must be installed Storage class must support ReadWriteMany (RWX) access mode","title":"suite_manage_imagestitching_config"},{"location":"roles/suite_manage_imagestitching_config/#what-this-role-does","text":"Creates PVC for image stitching data storage Patches ManageWorkspace CR with PVC configuration Configures persistent volume specifications Sets system properties: mci.imagestitching.apiurl : API endpoint for image stitching service imagestitching.dataInputPath : Data input path for image processing","title":"What This Role Does"},{"location":"roles/suite_manage_imagestitching_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_imagestitching_config/#mas_instance_id","text":"MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application to configure for image stitching. When to use : - Always required for image stitching configuration - Must match the instance ID from MAS installation - Used in PVC name construction Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., civil , prod , masinst1 ) Impact : Used to construct PVC name ( {instance}-{workspace}-{pvcname} ) and locate Manage resources. Related variables : - mas_workspace_id : Workspace within this instance - stitching_pvcname : Combined to create full PVC name Note : This must match the instance ID used during Manage installation with Civil component.","title":"mas_instance_id"},{"location":"roles/suite_manage_imagestitching_config/#mas_workspace_id","text":"Workspace identifier for Manage application. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application with Civil component. When to use : - Always required for image stitching configuration - Must match the workspace ID where Manage is deployed - Used in PVC name construction Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , masdev ) Impact : Used to construct PVC name ( {instance}-{workspace}-{pvcname} ) and locate Manage resources. Related variables : - mas_instance_id : Parent instance - stitching_pvcname : Combined to create full PVC name Note : This must match the workspace ID used during Manage installation.","title":"mas_workspace_id"},{"location":"roles/suite_manage_imagestitching_config/#mas_domain","text":"MAS cluster domain name. Optional Environment Variable: MAS_DOMAIN Default: Discovered from Suite CR Purpose : Specifies the domain name for the Manage cluster, used to construct the image stitching API URL. When to use : - Use default (auto-discovery) for standard deployments - Override if auto-discovery fails or for custom domains - Required for constructing service endpoints Valid values : Valid DNS domain name (e.g., civil.ibmmasdev.com , apps.cluster.example.com ) Impact : Used to construct the mci.imagestitching.apiurl system property for the image stitching service endpoint. Related variables : - mas_instance_id : Instance in this domain - mas_workspace_id : Workspace in this domain Note : The role attempts to discover the domain from the Suite CR. Only override if auto-discovery doesn't work for your environment.","title":"mas_domain"},{"location":"roles/suite_manage_imagestitching_config/#stitching_pvcname","text":"PVC name postfix for image stitching storage. Optional Environment Variable: IMAGESTITCHING_PVCNAME Default: manage-imagestitching Purpose : Defines the postfix for the PVC name. The full PVC name is constructed as {mas_instance_id}-{mas_workspace_id}-{stitching_pvcname} . When to use : - Use default for standard deployments - Override for custom naming conventions - Must be unique within the namespace Valid values : Valid Kubernetes resource name (lowercase alphanumeric and hyphens) Impact : Determines the PVC name used for image stitching data storage. The ManageWorkspace CR is updated with this PVC reference. Related variables : - mas_instance_id : Prepended to create full PVC name - mas_workspace_id : Prepended to create full PVC name - stitching_storage_class : Storage class for this PVC - stitching_storage_size : Size of this PVC Note : The default manage-imagestitching clearly identifies the PVC purpose. Full PVC name example: civil-masdev-manage-imagestitching .","title":"stitching_pvcname"},{"location":"roles/suite_manage_imagestitching_config/#stitching_storage_class","text":"Storage class for image stitching PVC. Optional Environment Variable: IMAGESTITCHING_STORAGE_CLASS Default: Auto-discovered Purpose : Specifies the Kubernetes storage class to use for the image stitching PVC. Must support ReadWriteMany (RWX) access mode. When to use : - Use default (auto-discovery) if cluster has suitable RWX storage class - Override to specify a particular RWX-capable storage class - Required when multiple RWX storage classes exist Valid values : Valid Kubernetes storage class name that supports RWX (e.g., nfs-client , ocs-storagecluster-cephfs , ibmc-file-gold ) Impact : Determines the underlying storage technology for image stitching data. Must support ReadWriteMany for multiple pod access. Related variables : - stitching_pvcname : PVC using this storage class - stitching_storage_mode : Must be ReadWriteMany - stitching_storage_size : Size of storage to provision Note : CRITICAL - The storage class MUST support ReadWriteMany (RWX) access mode as image stitching requires shared access from multiple pods. Common RWX storage classes include NFS, CephFS, and IBM Cloud File Storage.","title":"stitching_storage_class"},{"location":"roles/suite_manage_imagestitching_config/#stitching_storage_size","text":"PVC storage size for image stitching. Optional Environment Variable: IMAGESTITCHING_STORAGE_SIZE Default: 20Gi Purpose : Specifies the size of the persistent volume claim for storing image stitching data and processed images. When to use : - Use default ( 20Gi ) for small to medium deployments - Increase for large-scale infrastructure inspection projects - Consider image resolution and retention requirements Valid values : Kubernetes storage size format (e.g., 20Gi , 50Gi , 100Gi , 1Ti ) Impact : Determines how much image data can be stored. Insufficient storage will prevent new image processing. Related variables : - stitching_pvcname : PVC with this size - stitching_storage_class : Storage class providing this capacity Note : Size requirements depend on: - Number of images processed - Image resolution and format - Retention period for processed images - Number of concurrent stitching operations Start with 20Gi and monitor usage to adjust as needed.","title":"stitching_storage_size"},{"location":"roles/suite_manage_imagestitching_config/#stitching_storage_mode","text":"PVC access mode for image stitching. Optional Environment Variable: IMAGESTITCHING_STORAGE_MODE Default: ReadWriteMany Purpose : Specifies the Kubernetes access mode for the image stitching PVC. Must be ReadWriteMany to allow multiple pods to access the storage simultaneously. When to use : - Always use default ReadWriteMany for image stitching - Do not change unless you understand the implications - Required for multi-pod access to shared data Valid values : ReadWriteMany (RWX) - other modes not supported Impact : Enables multiple Manage pods to read and write image stitching data concurrently. Essential for distributed image processing. Related variables : - stitching_storage_class : Must support this access mode - stitching_pvcname : PVC with this access mode Note : CRITICAL - ReadWriteMany (RWX) is mandatory for image stitching functionality. Do not change this value. Ensure your storage class supports RWX access mode.","title":"stitching_storage_mode"},{"location":"roles/suite_manage_imagestitching_config/#stitching_storage_mountpath","text":"Mount path for image stitching storage. Optional Environment Variable: IMAGESTITCHING_STORAGE_MOUNTPATH Default: imagestitching Purpose : Specifies the mount path where the image stitching PVC is mounted in Manage pods. When to use : - Use default ( imagestitching ) for standard deployments - Override only if you have specific path requirements - Must not conflict with other mount paths Valid values : Valid Linux filesystem path component (alphanumeric, no leading slash) Impact : Determines where image stitching data is accessible within Manage pods. The imagestitching.dataInputPath system property is set to this path. Related variables : - stitching_pvcname : PVC mounted at this path Note : The default imagestitching is relative to the Manage pod's base mount path. The full path is constructed by Manage. Do not include leading or trailing slashes.","title":"stitching_storage_mountpath"},{"location":"roles/suite_manage_imagestitching_config/#example-playbook","text":"The following sample will configure image stitching for an existing Manage application instance via ManageWorkspace CR update: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: civil mas_workspace_id: masdev mas_domain: civil.ibmmasdev.com stitching_pvcname: manage-imagestitching stitching_storage_class: nfs-client stitching_storage_size: 20Gi stitching_storage_mode: ReadWriteMany stitching_storage_mountpath: imagestitching roles: - ibm.mas_devops.suite_manage_imagestitching_config","title":"Example Playbook"},{"location":"roles/suite_manage_imagestitching_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_import_certs_config/","text":"suite_manage_import_certs_config \u00a4 This role imports custom certificates into Maximo Manage application's workspace. This is useful for establishing trust with external systems, APIs, or services that Manage needs to communicate with securely. Prerequisites Manage application must be deployed and activated before importing certificates. Usage Modes \u00a4 The role supports two usage modes: Standalone Mode : Provide a local file path containing certificate definitions Programmatic Mode : Pass certificates as variables from another playbook/role Certificate Alias Naming \u00a4 When using programmatic mode, certificate aliases are auto-generated by concatenating the prefix with an incremented number: - Input: 3 certificates with prefix myaliasprefixpart - Output: myaliasprefixpart1 , myaliasprefixpart2 , myaliasprefixpart3 Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application where certificates will be imported. When to use : - Always required for certificate import - Must match the instance ID from MAS installation - Used to locate Manage resources Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance's Manage application receives the imported certificates. Related variables : - mas_workspace_id : Workspace within this instance - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the instance ID used during Manage installation. mas_workspace_id \u00a4 Workspace identifier for Manage application. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application where certificates will be imported. When to use : - Always required for certificate import - Must match the workspace ID where Manage is deployed - Used to locate Manage resources Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , masdev ) Impact : Determines which workspace's Manage application receives the imported certificates. Related variables : - mas_instance_id : Parent instance - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the workspace ID used during Manage installation. manage_workspace_cr_name \u00a4 ManageWorkspace custom resource name. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: {mas_instance_id}-{mas_workspace_id} Purpose : Specifies the name of the ManageWorkspace custom resource to update with imported certificates. When to use : - Use default unless you have a custom CR naming convention - Override if your ManageWorkspace CR has a non-standard name - Required to update certificate configuration Valid values : Valid Kubernetes resource name Impact : Determines which ManageWorkspace CR is updated with the imported certificate definitions. Related variables : - mas_instance_id : Used in default name construction - mas_workspace_id : Used in default name construction Note : The default naming convention {instance}-{workspace} matches standard Manage deployments. Only override if you have custom CR names. manage_certificates_file_path_local \u00a4 Local file path for certificate definitions. Required (when running as standalone role) Environment Variable: MANAGE_CERTIFICATES_FILE_PATH_LOCAL Default: None Purpose : Specifies the local filesystem path to a YAML file containing certificate definitions to import into Manage. When to use : - Required when running role standalone (not from another playbook) - Use when certificates are stored in a file - Alternative to passing certificates as variables - Mutually exclusive with manage_certificates variable Valid values : Valid local filesystem path (e.g., /path/to/manage-certs.yml , ~/certs/manage-certs.yml ) Impact : The role reads certificate definitions from this file and imports them into Manage. File must be in valid YAML format. Related variables : - manage_certificates : Alternative method (programmatic mode) - manage_certificates_alias_prefix : Not used with file mode Note : Sample file format available in files/manage-certs-sample.yml . The file should contain a list of certificate contents in PEM format. Use this mode for static certificate configurations. manage_certificates \u00a4 Certificate list for programmatic import. Optional (Required when not using file path) Environment Variable: MANAGE_CERTIFICATES Default: None Purpose : Provides certificate contents as a list variable for programmatic import from another playbook or role. When to use : - Use when invoking this role from another playbook/role - When certificates are generated or retrieved dynamically - Alternative to using manage_certificates_file_path_local - Requires manage_certificates_alias_prefix to be set Valid values : List of certificate strings in PEM format ['-----BEGIN CERTIFICATE-----\\n...\\n-----END CERTIFICATE-----'] Impact : Certificates in the list are imported into Manage with auto-generated alias names based on the prefix. Related variables : - manage_certificates_alias_prefix : Required with this variable - manage_certificates_file_path_local : Alternative method (file mode) Note : Use this mode for dynamic certificate management, such as when certificates are retrieved from a secret manager or generated during playbook execution. Each certificate must be a complete PEM-formatted string. manage_certificates_alias_prefix \u00a4 Certificate alias name prefix. Optional (Required when using manage_certificates variable) Environment Variable: MANAGE_CERTIFICATES_ALIAS_PREFIX Default: None Purpose : Defines the prefix for auto-generated certificate alias names. Aliases are created by appending an incremented number to this prefix. When to use : - Required when using manage_certificates variable - Use descriptive prefixes to identify certificate purpose - Not used when importing from file (file defines aliases) Valid values : String suitable for certificate alias (alphanumeric, no spaces) Impact : Determines how certificates are named in Manage's certificate store. Format: {prefix}1 , {prefix}2 , {prefix}3 , etc. Related variables : - manage_certificates : Certificates to name with this prefix Note : Choose meaningful prefixes that indicate the certificate purpose or source (e.g., coscertpart , s3certpart , apicertpart ). This helps identify certificates in Manage's certificate management interface. Example Playbook \u00a4 Using Local File Path \u00a4 The following sample can be used to import Manage certificates for an existing Manage instance, using a local path pointing the certificates definition from a custom file. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev manage_certificates_file_path_local: /my-path/manage-certs.yml roles: - ibm.mas_devops.suite_manage_import_certs_config Using Variables \u00a4 The following sample can be used to import Manage certificates for an existing Manage instance, passing the certificates and prefix from a variable. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev manage_certificates: ['-----BEGIN CERTIFICATE----- << your-cert-content >> -----END CERTIFICATE-----'] manage_certificates_alias_prefix: \"myaliasprefixpart\" roles: - ibm.mas_devops.suite_manage_import_certs_config Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export MAS_INSTANCE_ID=masinst1 export MAS_WORKSPACE_ID=masdev export MANAGE_CERTIFICATES_FILE_PATH_LOCAL=/my-path/manage-certs.yml ROLE_NAME='suite_manage_import_certs_config' ansible-playbook playbooks/run_role.yml License \u00a4 EPL-2.0","title":"suite_manage_import_certs_config"},{"location":"roles/suite_manage_import_certs_config/#suite_manage_import_certs_config","text":"This role imports custom certificates into Maximo Manage application's workspace. This is useful for establishing trust with external systems, APIs, or services that Manage needs to communicate with securely. Prerequisites Manage application must be deployed and activated before importing certificates.","title":"suite_manage_import_certs_config"},{"location":"roles/suite_manage_import_certs_config/#usage-modes","text":"The role supports two usage modes: Standalone Mode : Provide a local file path containing certificate definitions Programmatic Mode : Pass certificates as variables from another playbook/role","title":"Usage Modes"},{"location":"roles/suite_manage_import_certs_config/#certificate-alias-naming","text":"When using programmatic mode, certificate aliases are auto-generated by concatenating the prefix with an incremented number: - Input: 3 certificates with prefix myaliasprefixpart - Output: myaliasprefixpart1 , myaliasprefixpart2 , myaliasprefixpart3","title":"Certificate Alias Naming"},{"location":"roles/suite_manage_import_certs_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_import_certs_config/#mas_instance_id","text":"MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application where certificates will be imported. When to use : - Always required for certificate import - Must match the instance ID from MAS installation - Used to locate Manage resources Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance's Manage application receives the imported certificates. Related variables : - mas_workspace_id : Workspace within this instance - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the instance ID used during Manage installation.","title":"mas_instance_id"},{"location":"roles/suite_manage_import_certs_config/#mas_workspace_id","text":"Workspace identifier for Manage application. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application where certificates will be imported. When to use : - Always required for certificate import - Must match the workspace ID where Manage is deployed - Used to locate Manage resources Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , masdev ) Impact : Determines which workspace's Manage application receives the imported certificates. Related variables : - mas_instance_id : Parent instance - manage_workspace_cr_name : Constructed from instance and workspace IDs Note : This must match the workspace ID used during Manage installation.","title":"mas_workspace_id"},{"location":"roles/suite_manage_import_certs_config/#manage_workspace_cr_name","text":"ManageWorkspace custom resource name. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: {mas_instance_id}-{mas_workspace_id} Purpose : Specifies the name of the ManageWorkspace custom resource to update with imported certificates. When to use : - Use default unless you have a custom CR naming convention - Override if your ManageWorkspace CR has a non-standard name - Required to update certificate configuration Valid values : Valid Kubernetes resource name Impact : Determines which ManageWorkspace CR is updated with the imported certificate definitions. Related variables : - mas_instance_id : Used in default name construction - mas_workspace_id : Used in default name construction Note : The default naming convention {instance}-{workspace} matches standard Manage deployments. Only override if you have custom CR names.","title":"manage_workspace_cr_name"},{"location":"roles/suite_manage_import_certs_config/#manage_certificates_file_path_local","text":"Local file path for certificate definitions. Required (when running as standalone role) Environment Variable: MANAGE_CERTIFICATES_FILE_PATH_LOCAL Default: None Purpose : Specifies the local filesystem path to a YAML file containing certificate definitions to import into Manage. When to use : - Required when running role standalone (not from another playbook) - Use when certificates are stored in a file - Alternative to passing certificates as variables - Mutually exclusive with manage_certificates variable Valid values : Valid local filesystem path (e.g., /path/to/manage-certs.yml , ~/certs/manage-certs.yml ) Impact : The role reads certificate definitions from this file and imports them into Manage. File must be in valid YAML format. Related variables : - manage_certificates : Alternative method (programmatic mode) - manage_certificates_alias_prefix : Not used with file mode Note : Sample file format available in files/manage-certs-sample.yml . The file should contain a list of certificate contents in PEM format. Use this mode for static certificate configurations.","title":"manage_certificates_file_path_local"},{"location":"roles/suite_manage_import_certs_config/#manage_certificates","text":"Certificate list for programmatic import. Optional (Required when not using file path) Environment Variable: MANAGE_CERTIFICATES Default: None Purpose : Provides certificate contents as a list variable for programmatic import from another playbook or role. When to use : - Use when invoking this role from another playbook/role - When certificates are generated or retrieved dynamically - Alternative to using manage_certificates_file_path_local - Requires manage_certificates_alias_prefix to be set Valid values : List of certificate strings in PEM format ['-----BEGIN CERTIFICATE-----\\n...\\n-----END CERTIFICATE-----'] Impact : Certificates in the list are imported into Manage with auto-generated alias names based on the prefix. Related variables : - manage_certificates_alias_prefix : Required with this variable - manage_certificates_file_path_local : Alternative method (file mode) Note : Use this mode for dynamic certificate management, such as when certificates are retrieved from a secret manager or generated during playbook execution. Each certificate must be a complete PEM-formatted string.","title":"manage_certificates"},{"location":"roles/suite_manage_import_certs_config/#manage_certificates_alias_prefix","text":"Certificate alias name prefix. Optional (Required when using manage_certificates variable) Environment Variable: MANAGE_CERTIFICATES_ALIAS_PREFIX Default: None Purpose : Defines the prefix for auto-generated certificate alias names. Aliases are created by appending an incremented number to this prefix. When to use : - Required when using manage_certificates variable - Use descriptive prefixes to identify certificate purpose - Not used when importing from file (file defines aliases) Valid values : String suitable for certificate alias (alphanumeric, no spaces) Impact : Determines how certificates are named in Manage's certificate store. Format: {prefix}1 , {prefix}2 , {prefix}3 , etc. Related variables : - manage_certificates : Certificates to name with this prefix Note : Choose meaningful prefixes that indicate the certificate purpose or source (e.g., coscertpart , s3certpart , apicertpart ). This helps identify certificates in Manage's certificate management interface.","title":"manage_certificates_alias_prefix"},{"location":"roles/suite_manage_import_certs_config/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_manage_import_certs_config/#using-local-file-path","text":"The following sample can be used to import Manage certificates for an existing Manage instance, using a local path pointing the certificates definition from a custom file. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev manage_certificates_file_path_local: /my-path/manage-certs.yml roles: - ibm.mas_devops.suite_manage_import_certs_config","title":"Using Local File Path"},{"location":"roles/suite_manage_import_certs_config/#using-variables","text":"The following sample can be used to import Manage certificates for an existing Manage instance, passing the certificates and prefix from a variable. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev manage_certificates: ['-----BEGIN CERTIFICATE----- << your-cert-content >> -----END CERTIFICATE-----'] manage_certificates_alias_prefix: \"myaliasprefixpart\" roles: - ibm.mas_devops.suite_manage_import_certs_config","title":"Using Variables"},{"location":"roles/suite_manage_import_certs_config/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export MAS_INSTANCE_ID=masinst1 export MAS_WORKSPACE_ID=masdev export MANAGE_CERTIFICATES_FILE_PATH_LOCAL=/my-path/manage-certs.yml ROLE_NAME='suite_manage_import_certs_config' ansible-playbook playbooks/run_role.yml","title":"Run Role Playbook"},{"location":"roles/suite_manage_import_certs_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_load_dbc_scripts/","text":"suite_manage_load_dbc_scripts \u00a4 This role loads and executes ad-hoc DBC (Database Configuration) script files into Maximo Manage or Health server. DBC scripts are used to customize Manage/Health database configurations, add custom fields, modify system properties, or perform database maintenance tasks. Script Requirements Only .dbc format files are accepted Scripts must be valid Maximo DBC syntax Role validates successful execution and fails on errors What This Role Does \u00a4 Locates DBC script files from specified local directory Copies scripts to Manage/Health server pods Executes scripts using Maximo's DBC processor Validates each script execution for errors Fails if any script encounters errors Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage or Health application where DBC scripts will be executed. When to use : - Always required for DBC script execution - Must match the instance ID from MAS installation - Used to construct namespace for script execution Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance's Manage/Health application receives the DBC scripts. Namespace format: mas-{instance_id}-{app_id} . Related variables : - mas_app_id : Application within this instance (manage or health) - dbc_script_path_local : Location of scripts to execute Note : This must match the instance ID used during Manage/Health installation. mas_app_id \u00a4 MAS application identifier. Required Environment Variable: MAS_APP_ID Default: None Purpose : Specifies which MAS application (Manage or Health) will execute the DBC scripts. When to use : - Always required for DBC script execution - Must match the deployed application - Determines target namespace and server Valid values : manage , health Impact : Determines which application server executes the DBC scripts. Different applications have different database schemas and configurations. Related variables : - mas_instance_id : Instance containing this application - dbc_script_path_local : Scripts to execute on this application Note : DBC scripts are application-specific. Scripts written for Manage may not work in Health and vice versa. Ensure scripts are compatible with the target application. dbc_script_path_local \u00a4 Local directory path for DBC script files. Optional Environment Variable: DBC_SCRIPT_PATH_LOCAL Default: suite_manage_load_dbc_scripts/files Purpose : Specifies the local filesystem directory containing DBC script files to be loaded and executed on the Manage/Health server. When to use : - Use default for scripts in role's files directory - Override to specify custom script location - Directory must contain .dbc files Valid values : Valid local filesystem path (e.g., /path/to/dbc/scripts , ~/manage-scripts ) Impact : The role scans this directory for .dbc files, copies them to the server, and executes them in alphabetical order. Related variables : - mas_instance_id : Instance where scripts execute - mas_app_id : Application executing the scripts Note : - All .dbc files in the directory will be executed - Scripts execute in alphabetical order by filename - Use filename prefixes (e.g., 01- , 02- ) to control execution order - Ensure scripts are idempotent if role may be run multiple times - Test scripts in non-production environment first Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" mas_app_id: manage dbc_script_path_local: \"{{ lookup('env', 'DBC_SCRIPT_PATH_LOCAL') }}\" roles: - ibm.mas_devops.suite_manage_load_dbc_scripts License \u00a4 EPL-2.0","title":"suite_manage_load_dbc_scripts"},{"location":"roles/suite_manage_load_dbc_scripts/#suite_manage_load_dbc_scripts","text":"This role loads and executes ad-hoc DBC (Database Configuration) script files into Maximo Manage or Health server. DBC scripts are used to customize Manage/Health database configurations, add custom fields, modify system properties, or perform database maintenance tasks. Script Requirements Only .dbc format files are accepted Scripts must be valid Maximo DBC syntax Role validates successful execution and fails on errors","title":"suite_manage_load_dbc_scripts"},{"location":"roles/suite_manage_load_dbc_scripts/#what-this-role-does","text":"Locates DBC script files from specified local directory Copies scripts to Manage/Health server pods Executes scripts using Maximo's DBC processor Validates each script execution for errors Fails if any script encounters errors","title":"What This Role Does"},{"location":"roles/suite_manage_load_dbc_scripts/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_load_dbc_scripts/#mas_instance_id","text":"MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage or Health application where DBC scripts will be executed. When to use : - Always required for DBC script execution - Must match the instance ID from MAS installation - Used to construct namespace for script execution Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance's Manage/Health application receives the DBC scripts. Namespace format: mas-{instance_id}-{app_id} . Related variables : - mas_app_id : Application within this instance (manage or health) - dbc_script_path_local : Location of scripts to execute Note : This must match the instance ID used during Manage/Health installation.","title":"mas_instance_id"},{"location":"roles/suite_manage_load_dbc_scripts/#mas_app_id","text":"MAS application identifier. Required Environment Variable: MAS_APP_ID Default: None Purpose : Specifies which MAS application (Manage or Health) will execute the DBC scripts. When to use : - Always required for DBC script execution - Must match the deployed application - Determines target namespace and server Valid values : manage , health Impact : Determines which application server executes the DBC scripts. Different applications have different database schemas and configurations. Related variables : - mas_instance_id : Instance containing this application - dbc_script_path_local : Scripts to execute on this application Note : DBC scripts are application-specific. Scripts written for Manage may not work in Health and vice versa. Ensure scripts are compatible with the target application.","title":"mas_app_id"},{"location":"roles/suite_manage_load_dbc_scripts/#dbc_script_path_local","text":"Local directory path for DBC script files. Optional Environment Variable: DBC_SCRIPT_PATH_LOCAL Default: suite_manage_load_dbc_scripts/files Purpose : Specifies the local filesystem directory containing DBC script files to be loaded and executed on the Manage/Health server. When to use : - Use default for scripts in role's files directory - Override to specify custom script location - Directory must contain .dbc files Valid values : Valid local filesystem path (e.g., /path/to/dbc/scripts , ~/manage-scripts ) Impact : The role scans this directory for .dbc files, copies them to the server, and executes them in alphabetical order. Related variables : - mas_instance_id : Instance where scripts execute - mas_app_id : Application executing the scripts Note : - All .dbc files in the directory will be executed - Scripts execute in alphabetical order by filename - Use filename prefixes (e.g., 01- , 02- ) to control execution order - Ensure scripts are idempotent if role may be run multiple times - Test scripts in non-production environment first","title":"dbc_script_path_local"},{"location":"roles/suite_manage_load_dbc_scripts/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" mas_app_id: manage dbc_script_path_local: \"{{ lookup('env', 'DBC_SCRIPT_PATH_LOCAL') }}\" roles: - ibm.mas_devops.suite_manage_load_dbc_scripts","title":"Example Playbook"},{"location":"roles/suite_manage_load_dbc_scripts/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_logging_config/","text":"suite_manage_logging_config \u00a4 This role configures cloud object storage (IBM Cloud Object Storage or AWS S3) for storing Maximo Manage application server logs. Storing logs in object storage enables centralized log management, long-term retention, and analysis without consuming cluster storage. Prerequisites Manage application must be deployed and activated before configuring logging storage. What This Role Does \u00a4 Configures cloud storage bucket for Manage server logs Updates Manage database with logging storage configuration Automatically executes cos_bucket role to set up bucket Enables log shipping from Manage pods to object storage Supports both IBM Cloud Object Storage and AWS S3 Role Variables \u00a4 cos_type \u00a4 Cloud object storage provider type. Required Environment Variable: COS_TYPE Default: None Purpose : Specifies which cloud storage provider to use for Manage application server logs. When to use : - Always required for logging configuration - Choose based on your cloud infrastructure - Determines which credentials and configuration are needed Valid values : ibm , aws - ibm : IBM Cloud Object Storage - aws : Amazon S3 Impact : - ibm : Uses IBM Cloud Object Storage (requires ibmcloud_apikey , cos_instance_name , cos_bucket_name ) - aws : Uses AWS S3 (requires AWS credentials and bucket configuration) Related variables : - mas_instance_id : Instance whose logs are stored - db2_instance_name : Database to update with logging config Note : The cos_bucket role is automatically executed to set up the bucket. Ensure you provide the required variables for your chosen provider (e.g., cos_instance_name and ibmcloud_apikey for IBM, or AWS credentials for S3). mas_instance_id \u00a4 MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application whose logs will be stored in object storage. When to use : - Always required for logging configuration - Must match the instance ID from MAS installation - Used to locate Manage resources Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance's Manage application is configured for log storage in object storage. Related variables : - mas_workspace_id : Workspace within this instance - db2_instance_name : Database for this Manage instance Note : This must match the instance ID used during Manage installation. mas_workspace_id \u00a4 Workspace identifier for Manage application. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application whose logs will be stored. When to use : - Always required for logging configuration - Must match the workspace ID where Manage is deployed - Used to locate Manage resources Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , masdev ) Impact : Determines which workspace's Manage application is configured for log storage. Related variables : - mas_instance_id : Parent instance - db2_instance_name : Database for this workspace's Manage Note : This must match the workspace ID used during Manage installation. db2_instance_name \u00a4 Db2 Warehouse instance name. Required Environment Variable: DB2_INSTANCE_NAME Default: None Purpose : Identifies the Db2 Warehouse instance that stores Manage application data, which will be updated with logging storage configuration. When to use : - Always required for logging configuration - Must match the Db2 instance name used by Manage - Used to connect to database for configuration updates Valid values : Valid Db2 instance name (e.g., db2w-manage , db2u-manage ) Impact : Determines which Db2 instance is accessed to update logging configuration via SQL. Related variables : - db2_namespace : Namespace containing this instance - db2_dbname : Database name within the instance - mas_instance_id : MAS instance using this database Note : To find the instance name, go to the Db2 namespace and look for pods with label=engine . Describe the pod and find the app label value - that's your instance name. db2_namespace \u00a4 Db2 Warehouse namespace. Optional Environment Variable: DB2_NAMESPACE Default: db2u Purpose : Specifies the OpenShift namespace where the Db2 Warehouse instance is deployed. When to use : - Use default ( db2u ) for standard Db2 deployments - Override if Db2 is deployed in a custom namespace - Required to locate the Db2 instance Valid values : Valid Kubernetes namespace name Impact : Determines where to look for the Db2 instance when connecting for logging configuration updates. Related variables : - db2_instance_name : Instance to find in this namespace - db2_dbname : Database within the instance Note : The default db2u namespace is used by most Db2 Warehouse deployments. Only change if you have a custom deployment. db2_dbname \u00a4 Database name within Db2 instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB Purpose : Specifies the database name within the Db2 instance where Manage tables and logging configuration are stored. When to use : - Use default ( BLUDB ) for standard Manage deployments - Override if Manage uses a custom database name - Required for database connection Valid values : Valid Db2 database name Impact : Determines which database within the Db2 instance is updated with logging storage configuration. Related variables : - db2_instance_name : Instance containing this database - db2_namespace : Namespace of the instance Note : BLUDB is the default database name for Manage deployments. Only change if you have a custom database configuration. Example Playbook \u00a4 Configure IBM Cloud COS for Existing Manage Instance \u00a4 The following sample can be used to configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm db2_instance_name: db2w-manage cos_instance_name: cos-masinst1 cos_bucket_name: manage-logs-bucket ibmcloud_apikey: xxxx roles: - ibm.mas_devops.suite_manage_logging_config Configure AWS S3 for Existing Manage Instance \u00a4 The following sample can be used to configure AWS S3 for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: aws cos_bucket_action: create aws_bucket_name: manage-logs-bucket aws_region: us-east-2 aws_bucket_versioning_flag: True aws_bucket_encryption: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}' db2_instance_name: db2w-manage roles: - ibm.mas_devops.suite_manage_logging_config Provision and Configure IBM Cloud COS \u00a4 The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_type: ibm cos_instance_name: cos-masinst1 cos_bucket_name: manage-logs-bucket ibmcloud_apikey: xxxx roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_logging_config License \u00a4 EPL-2.0","title":"suite_manage_logging_config"},{"location":"roles/suite_manage_logging_config/#suite_manage_logging_config","text":"This role configures cloud object storage (IBM Cloud Object Storage or AWS S3) for storing Maximo Manage application server logs. Storing logs in object storage enables centralized log management, long-term retention, and analysis without consuming cluster storage. Prerequisites Manage application must be deployed and activated before configuring logging storage.","title":"suite_manage_logging_config"},{"location":"roles/suite_manage_logging_config/#what-this-role-does","text":"Configures cloud storage bucket for Manage server logs Updates Manage database with logging storage configuration Automatically executes cos_bucket role to set up bucket Enables log shipping from Manage pods to object storage Supports both IBM Cloud Object Storage and AWS S3","title":"What This Role Does"},{"location":"roles/suite_manage_logging_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_logging_config/#cos_type","text":"Cloud object storage provider type. Required Environment Variable: COS_TYPE Default: None Purpose : Specifies which cloud storage provider to use for Manage application server logs. When to use : - Always required for logging configuration - Choose based on your cloud infrastructure - Determines which credentials and configuration are needed Valid values : ibm , aws - ibm : IBM Cloud Object Storage - aws : Amazon S3 Impact : - ibm : Uses IBM Cloud Object Storage (requires ibmcloud_apikey , cos_instance_name , cos_bucket_name ) - aws : Uses AWS S3 (requires AWS credentials and bucket configuration) Related variables : - mas_instance_id : Instance whose logs are stored - db2_instance_name : Database to update with logging config Note : The cos_bucket role is automatically executed to set up the bucket. Ensure you provide the required variables for your chosen provider (e.g., cos_instance_name and ibmcloud_apikey for IBM, or AWS credentials for S3).","title":"cos_type"},{"location":"roles/suite_manage_logging_config/#mas_instance_id","text":"MAS instance identifier. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application whose logs will be stored in object storage. When to use : - Always required for logging configuration - Must match the instance ID from MAS installation - Used to locate Manage resources Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance's Manage application is configured for log storage in object storage. Related variables : - mas_workspace_id : Workspace within this instance - db2_instance_name : Database for this Manage instance Note : This must match the instance ID used during Manage installation.","title":"mas_instance_id"},{"location":"roles/suite_manage_logging_config/#mas_workspace_id","text":"Workspace identifier for Manage application. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application whose logs will be stored. When to use : - Always required for logging configuration - Must match the workspace ID where Manage is deployed - Used to locate Manage resources Valid values : Lowercase alphanumeric string, typically 3-12 characters (e.g., prod , dev , masdev ) Impact : Determines which workspace's Manage application is configured for log storage. Related variables : - mas_instance_id : Parent instance - db2_instance_name : Database for this workspace's Manage Note : This must match the workspace ID used during Manage installation.","title":"mas_workspace_id"},{"location":"roles/suite_manage_logging_config/#db2_instance_name","text":"Db2 Warehouse instance name. Required Environment Variable: DB2_INSTANCE_NAME Default: None Purpose : Identifies the Db2 Warehouse instance that stores Manage application data, which will be updated with logging storage configuration. When to use : - Always required for logging configuration - Must match the Db2 instance name used by Manage - Used to connect to database for configuration updates Valid values : Valid Db2 instance name (e.g., db2w-manage , db2u-manage ) Impact : Determines which Db2 instance is accessed to update logging configuration via SQL. Related variables : - db2_namespace : Namespace containing this instance - db2_dbname : Database name within the instance - mas_instance_id : MAS instance using this database Note : To find the instance name, go to the Db2 namespace and look for pods with label=engine . Describe the pod and find the app label value - that's your instance name.","title":"db2_instance_name"},{"location":"roles/suite_manage_logging_config/#db2_namespace","text":"Db2 Warehouse namespace. Optional Environment Variable: DB2_NAMESPACE Default: db2u Purpose : Specifies the OpenShift namespace where the Db2 Warehouse instance is deployed. When to use : - Use default ( db2u ) for standard Db2 deployments - Override if Db2 is deployed in a custom namespace - Required to locate the Db2 instance Valid values : Valid Kubernetes namespace name Impact : Determines where to look for the Db2 instance when connecting for logging configuration updates. Related variables : - db2_instance_name : Instance to find in this namespace - db2_dbname : Database within the instance Note : The default db2u namespace is used by most Db2 Warehouse deployments. Only change if you have a custom deployment.","title":"db2_namespace"},{"location":"roles/suite_manage_logging_config/#db2_dbname","text":"Database name within Db2 instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB Purpose : Specifies the database name within the Db2 instance where Manage tables and logging configuration are stored. When to use : - Use default ( BLUDB ) for standard Manage deployments - Override if Manage uses a custom database name - Required for database connection Valid values : Valid Db2 database name Impact : Determines which database within the Db2 instance is updated with logging storage configuration. Related variables : - db2_instance_name : Instance containing this database - db2_namespace : Namespace of the instance Note : BLUDB is the default database name for Manage deployments. Only change if you have a custom database configuration.","title":"db2_dbname"},{"location":"roles/suite_manage_logging_config/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_manage_logging_config/#configure-ibm-cloud-cos-for-existing-manage-instance","text":"The following sample can be used to configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm db2_instance_name: db2w-manage cos_instance_name: cos-masinst1 cos_bucket_name: manage-logs-bucket ibmcloud_apikey: xxxx roles: - ibm.mas_devops.suite_manage_logging_config","title":"Configure IBM Cloud COS for Existing Manage Instance"},{"location":"roles/suite_manage_logging_config/#configure-aws-s3-for-existing-manage-instance","text":"The following sample can be used to configure AWS S3 for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: aws cos_bucket_action: create aws_bucket_name: manage-logs-bucket aws_region: us-east-2 aws_bucket_versioning_flag: True aws_bucket_encryption: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}' db2_instance_name: db2w-manage roles: - ibm.mas_devops.suite_manage_logging_config","title":"Configure AWS S3 for Existing Manage Instance"},{"location":"roles/suite_manage_logging_config/#provision-and-configure-ibm-cloud-cos","text":"The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_type: ibm cos_instance_name: cos-masinst1 cos_bucket_name: manage-logs-bucket ibmcloud_apikey: xxxx roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_logging_config","title":"Provision and Configure IBM Cloud COS"},{"location":"roles/suite_manage_logging_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_pvc_config/","text":"suite_manage_pvc_config \u00a4 This role extends support for configuring persistent volume claims for Manage application. Note This role should be executed after Manage application is deployed and activated because it needs Manage up and running prior to configuring the additional persistent volume claims. There are two options to setup new Manage PVCs: Exporting Manage PVCs variables Loading Manage PVCs variables from a file Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier for locating Manage resources. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application to configure. Used to locate the correct ManageWorkspace resource. When to use : - Always required when configuring Manage PVCs - Must match the instance ID from MAS installation - Used to construct resource names and locate Manage workspace Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance's Manage workspace will be configured with additional PVCs. Incorrect instance ID will cause configuration to fail. Related variables : - mas_workspace_id : Workspace within this instance - manage_workspace_cr_name : Defaults to {instance_id}-{workspace_id} Note : Manage application must already be deployed and activated in this instance before configuring PVCs. mas_workspace_id \u00a4 Workspace identifier for locating Manage resources. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application to configure. Used to locate the correct ManageWorkspace resource. When to use : - Always required when configuring Manage PVCs - Must match the workspace ID from Manage installation - Used to construct resource names and locate Manage workspace Valid values : Lowercase alphanumeric string (e.g., masdev , prod , test ) Impact : Determines which Manage workspace will be configured with additional PVCs. Incorrect workspace ID will cause configuration to fail. Related variables : - mas_instance_id : Instance containing this workspace - manage_workspace_cr_name : Defaults to {instance_id}-{workspace_id} Note : Manage application must already be deployed and activated in this workspace before configuring PVCs. manage_workspace_cr_name \u00a4 ManageWorkspace custom resource name to configure. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: {mas_instance_id}-{mas_workspace_id} Purpose : Specifies the exact name of the ManageWorkspace custom resource to modify with new PVC definitions. When to use : - Leave as default for standard deployments - Set explicitly if ManageWorkspace CR has a custom name - Useful when workspace CR name doesn't follow default pattern Valid values : Valid Kubernetes resource name matching an existing ManageWorkspace CR Impact : Determines which ManageWorkspace CR will be updated with PVC configuration. Incorrect name will cause configuration to fail. Related variables : - mas_instance_id : Used in default name - mas_workspace_id : Used in default name Note : The default value follows the standard naming pattern {instance_id}-{workspace_id} . Only override if your ManageWorkspace CR uses a different name. mas_app_settings_custom_persistent_volume_pvc_name \u00a4 Name for the new Persistent Volume Claim. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PVC_NAME Default: {mas_instance_id}-{mas_workspace_id}-cust-files-pvc Purpose : Specifies the name of the PVC to create for additional Manage storage. This PVC will be mounted in Manage pods. When to use : - Use default for standard custom files storage - Set custom name for specific storage purposes - Must be unique within the namespace Valid values : Valid Kubernetes PVC name (lowercase alphanumeric with hyphens) Impact : Creates a PVC with this name that will be mounted in Manage pods at the specified mount path. Related variables : - mas_app_settings_custom_persistent_volume_mount_path : Where this PVC is mounted - mas_app_settings_custom_persistent_volume_pvc_size : Size of this PVC - mas_app_settings_custom_persistent_volume_sc_name : Storage class for this PVC Note : The default name follows the pattern {instance}-{workspace}-cust-files-pvc . Choose descriptive names for multiple PVCs. mas_app_settings_custom_persistent_volume_pv_name \u00a4 Name for the Persistent Volume (storage provider dependent). Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_NAME Default: None (random name generated) Purpose : Specifies the name of the underlying Persistent Volume. Some storage providers allow or require specific PV names. When to use : - Leave unset (recommended) for automatic random name generation - Set only if your storage provider requires specific PV naming - Verify your storage class supports custom PV names before setting Valid values : Valid PV name if supported by storage provider, or leave unset Impact : If set and storage provider doesn't support custom PV names, provisioning may fail. When unset, a random name is generated automatically. Related variables : - mas_app_settings_custom_persistent_volume_sc_name : Storage class that provisions this PV - mas_app_settings_custom_persistent_volume_pvc_name : PVC that binds to this PV Note : Most storage classes use dynamic provisioning with auto-generated PV names. Only set this if you have specific requirements or your storage provider requires it. mas_app_settings_custom_persistent_volume_pvc_size \u00a4 Size of the Persistent Volume Claim. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PVC_SIZE Default: 100Gi Purpose : Specifies the storage capacity for the PVC. This determines how much data can be stored in the mounted volume. When to use : - Use default ( 100Gi ) for standard custom files storage - Increase for larger storage requirements - Consider your data volume and growth projections - Ensure sufficient space for your use case Valid values : Kubernetes storage size format (e.g., 50Gi , 100Gi , 500Gi , 1Ti ) Impact : Determines the storage capacity available to Manage. Insufficient size will cause storage full errors. Some storage classes may not support resizing after creation. Related variables : - mas_app_settings_custom_persistent_volume_pvc_name : PVC with this size - mas_app_settings_custom_persistent_volume_sc_name : Storage class providing this capacity Note : The default 100Gi is suitable for moderate custom files storage. Plan capacity based on expected data volume. Check if your storage class supports volume expansion for future growth. mas_app_settings_custom_persistent_volume_mount_path \u00a4 Mount path for the PVC in Manage containers. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_MOUNT_PATH Default: /MeaGlobalDirs Purpose : Specifies where the PVC will be mounted inside Manage server containers. This is the directory path where the persistent storage will be accessible. When to use : - Use default ( /MeaGlobalDirs ) for standard Manage global directories - Set custom path for specific storage purposes - Ensure path doesn't conflict with existing Manage directories - Path must be absolute (start with / ) Valid values : Absolute Linux filesystem path (e.g., /MeaGlobalDirs , /MyCustomDir , /opt/data ) Impact : Determines where Manage can access the persistent storage. Applications and configurations must reference this path to use the storage. Related variables : - mas_app_settings_custom_persistent_volume_pvc_name : PVC mounted at this path - mas_app_settings_custom_persistent_volume_pvc_size : Size available at this path Note : The default /MeaGlobalDirs is the standard location for Manage global directories. Ensure the path doesn't conflict with existing Manage directories or mount points. mas_app_settings_custom_persistent_volume_sc_name \u00a4 Storage class for the PVC (supports RWX or RWO). Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_STORAGE_CLASS Default: Auto-detected from available storage classes Purpose : Specifies which storage class to use for provisioning the PVC. The storage class determines the underlying storage technology and performance characteristics. When to use : - Leave unset for automatic detection (recommended) - Set explicitly when you need a specific storage class - Both ReadWriteMany (RWX) and ReadWriteOnce (RWO) access modes are supported Valid values : Any storage class name available in your cluster Impact : Determines the storage technology, performance, and access mode for the PVC. Incorrect storage class will cause provisioning to fail. Related variables : - mas_app_settings_custom_persistent_volume_pvc_size : Size to provision from this storage class - mas_app_settings_custom_persistent_volume_pv_name : PV name (if storage class supports it) Note : Both RWX (ReadWriteMany) and RWO (ReadWriteOnce) storage classes are supported. RWX allows multiple pods to access the volume simultaneously, while RWO restricts to a single pod. Choose based on your access requirements. mas_app_settings_custom_persistent_volume_file_path \u00a4 Path to custom PVC definition file (alternative to individual variables). Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_FILE_PATH Default: None Purpose : Provides an alternative method to configure PVCs using a complete YAML definition file instead of individual variables. Useful for complex or multiple PVC configurations. When to use : - Use when you have complex PVC requirements - Use when configuring multiple PVCs at once - Use when you need full control over PVC specification - Alternative to setting individual PVC variables Valid values : Absolute path to YAML file containing PVC definitions Impact : When set, the role uses this file instead of individual PVC variables. The file must contain valid PVC definitions matching the expected format. Related variables : Overrides all individual mas_app_settings_custom_persistent_volume_* variables when set Note : See files/manage-persistent-volumes-sample.yml for file format example. This approach is more flexible but requires understanding of PVC YAML structure. For simple single-PVC scenarios, use individual variables instead. Example Playbook \u00a4 Using Variables \u00a4 The following sample can be used to configure new PVCs for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev mas_app_settings_custom_persistent_volume_sc_name: \"ibmc-file-gold-gid\" mas_app_settings_custom_persistent_volume_pvc_name: \"my-manage-pvc\" mas_app_settings_custom_persistent_volume_pvc_size: \"20Gi\" mas_app_settings_custom_persistent_volume_mount_path: \"/MyOwnFolder\" roles: - ibm.mas_devops.suite_manage_pvc_config Using File Definition \u00a4 The following sample can be used to configure new PVCs for an existing Manage application instance from a custom file definition. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev mas_app_settings_custom_persistent_volume_file_path: \"/my-path/manage-pv.yml\" roles: - ibm.mas_devops.suite_manage_pvc_config Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export MAS_APP_SETTINGS_CUSTOM_PV_STORAGE_CLASS=ibmc-file-silver-gid export MAS_APP_SETTINGS_CUSTOM_PVC_NAME=my-manage-pvc export MAS_APP_SETTINGS_CUSTOM_PVC_SIZE=20Gi export MAS_APP_SETTINGS_CUSTOM_MOUNT_PATH=/MyOwnDir export MAS_APP_SETTINGS_CUSTOM_PV_FILE_PATH=/my-path/manage-pv.yml ROLE_NAME='suite_manage_pvc_config' ansible-playbook playbooks/run_role.yml License \u00a4 EPL-2.0","title":"suite_manage_pvc_config"},{"location":"roles/suite_manage_pvc_config/#suite_manage_pvc_config","text":"This role extends support for configuring persistent volume claims for Manage application. Note This role should be executed after Manage application is deployed and activated because it needs Manage up and running prior to configuring the additional persistent volume claims. There are two options to setup new Manage PVCs: Exporting Manage PVCs variables Loading Manage PVCs variables from a file","title":"suite_manage_pvc_config"},{"location":"roles/suite_manage_pvc_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_pvc_config/#mas_instance_id","text":"MAS instance identifier for locating Manage resources. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application to configure. Used to locate the correct ManageWorkspace resource. When to use : - Always required when configuring Manage PVCs - Must match the instance ID from MAS installation - Used to construct resource names and locate Manage workspace Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance's Manage workspace will be configured with additional PVCs. Incorrect instance ID will cause configuration to fail. Related variables : - mas_workspace_id : Workspace within this instance - manage_workspace_cr_name : Defaults to {instance_id}-{workspace_id} Note : Manage application must already be deployed and activated in this instance before configuring PVCs.","title":"mas_instance_id"},{"location":"roles/suite_manage_pvc_config/#mas_workspace_id","text":"Workspace identifier for locating Manage resources. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application to configure. Used to locate the correct ManageWorkspace resource. When to use : - Always required when configuring Manage PVCs - Must match the workspace ID from Manage installation - Used to construct resource names and locate Manage workspace Valid values : Lowercase alphanumeric string (e.g., masdev , prod , test ) Impact : Determines which Manage workspace will be configured with additional PVCs. Incorrect workspace ID will cause configuration to fail. Related variables : - mas_instance_id : Instance containing this workspace - manage_workspace_cr_name : Defaults to {instance_id}-{workspace_id} Note : Manage application must already be deployed and activated in this workspace before configuring PVCs.","title":"mas_workspace_id"},{"location":"roles/suite_manage_pvc_config/#manage_workspace_cr_name","text":"ManageWorkspace custom resource name to configure. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: {mas_instance_id}-{mas_workspace_id} Purpose : Specifies the exact name of the ManageWorkspace custom resource to modify with new PVC definitions. When to use : - Leave as default for standard deployments - Set explicitly if ManageWorkspace CR has a custom name - Useful when workspace CR name doesn't follow default pattern Valid values : Valid Kubernetes resource name matching an existing ManageWorkspace CR Impact : Determines which ManageWorkspace CR will be updated with PVC configuration. Incorrect name will cause configuration to fail. Related variables : - mas_instance_id : Used in default name - mas_workspace_id : Used in default name Note : The default value follows the standard naming pattern {instance_id}-{workspace_id} . Only override if your ManageWorkspace CR uses a different name.","title":"manage_workspace_cr_name"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_pvc_name","text":"Name for the new Persistent Volume Claim. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PVC_NAME Default: {mas_instance_id}-{mas_workspace_id}-cust-files-pvc Purpose : Specifies the name of the PVC to create for additional Manage storage. This PVC will be mounted in Manage pods. When to use : - Use default for standard custom files storage - Set custom name for specific storage purposes - Must be unique within the namespace Valid values : Valid Kubernetes PVC name (lowercase alphanumeric with hyphens) Impact : Creates a PVC with this name that will be mounted in Manage pods at the specified mount path. Related variables : - mas_app_settings_custom_persistent_volume_mount_path : Where this PVC is mounted - mas_app_settings_custom_persistent_volume_pvc_size : Size of this PVC - mas_app_settings_custom_persistent_volume_sc_name : Storage class for this PVC Note : The default name follows the pattern {instance}-{workspace}-cust-files-pvc . Choose descriptive names for multiple PVCs.","title":"mas_app_settings_custom_persistent_volume_pvc_name"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_pv_name","text":"Name for the Persistent Volume (storage provider dependent). Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_NAME Default: None (random name generated) Purpose : Specifies the name of the underlying Persistent Volume. Some storage providers allow or require specific PV names. When to use : - Leave unset (recommended) for automatic random name generation - Set only if your storage provider requires specific PV naming - Verify your storage class supports custom PV names before setting Valid values : Valid PV name if supported by storage provider, or leave unset Impact : If set and storage provider doesn't support custom PV names, provisioning may fail. When unset, a random name is generated automatically. Related variables : - mas_app_settings_custom_persistent_volume_sc_name : Storage class that provisions this PV - mas_app_settings_custom_persistent_volume_pvc_name : PVC that binds to this PV Note : Most storage classes use dynamic provisioning with auto-generated PV names. Only set this if you have specific requirements or your storage provider requires it.","title":"mas_app_settings_custom_persistent_volume_pv_name"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_pvc_size","text":"Size of the Persistent Volume Claim. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PVC_SIZE Default: 100Gi Purpose : Specifies the storage capacity for the PVC. This determines how much data can be stored in the mounted volume. When to use : - Use default ( 100Gi ) for standard custom files storage - Increase for larger storage requirements - Consider your data volume and growth projections - Ensure sufficient space for your use case Valid values : Kubernetes storage size format (e.g., 50Gi , 100Gi , 500Gi , 1Ti ) Impact : Determines the storage capacity available to Manage. Insufficient size will cause storage full errors. Some storage classes may not support resizing after creation. Related variables : - mas_app_settings_custom_persistent_volume_pvc_name : PVC with this size - mas_app_settings_custom_persistent_volume_sc_name : Storage class providing this capacity Note : The default 100Gi is suitable for moderate custom files storage. Plan capacity based on expected data volume. Check if your storage class supports volume expansion for future growth.","title":"mas_app_settings_custom_persistent_volume_pvc_size"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_mount_path","text":"Mount path for the PVC in Manage containers. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_MOUNT_PATH Default: /MeaGlobalDirs Purpose : Specifies where the PVC will be mounted inside Manage server containers. This is the directory path where the persistent storage will be accessible. When to use : - Use default ( /MeaGlobalDirs ) for standard Manage global directories - Set custom path for specific storage purposes - Ensure path doesn't conflict with existing Manage directories - Path must be absolute (start with / ) Valid values : Absolute Linux filesystem path (e.g., /MeaGlobalDirs , /MyCustomDir , /opt/data ) Impact : Determines where Manage can access the persistent storage. Applications and configurations must reference this path to use the storage. Related variables : - mas_app_settings_custom_persistent_volume_pvc_name : PVC mounted at this path - mas_app_settings_custom_persistent_volume_pvc_size : Size available at this path Note : The default /MeaGlobalDirs is the standard location for Manage global directories. Ensure the path doesn't conflict with existing Manage directories or mount points.","title":"mas_app_settings_custom_persistent_volume_mount_path"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_sc_name","text":"Storage class for the PVC (supports RWX or RWO). Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_STORAGE_CLASS Default: Auto-detected from available storage classes Purpose : Specifies which storage class to use for provisioning the PVC. The storage class determines the underlying storage technology and performance characteristics. When to use : - Leave unset for automatic detection (recommended) - Set explicitly when you need a specific storage class - Both ReadWriteMany (RWX) and ReadWriteOnce (RWO) access modes are supported Valid values : Any storage class name available in your cluster Impact : Determines the storage technology, performance, and access mode for the PVC. Incorrect storage class will cause provisioning to fail. Related variables : - mas_app_settings_custom_persistent_volume_pvc_size : Size to provision from this storage class - mas_app_settings_custom_persistent_volume_pv_name : PV name (if storage class supports it) Note : Both RWX (ReadWriteMany) and RWO (ReadWriteOnce) storage classes are supported. RWX allows multiple pods to access the volume simultaneously, while RWO restricts to a single pod. Choose based on your access requirements.","title":"mas_app_settings_custom_persistent_volume_sc_name"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_file_path","text":"Path to custom PVC definition file (alternative to individual variables). Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_FILE_PATH Default: None Purpose : Provides an alternative method to configure PVCs using a complete YAML definition file instead of individual variables. Useful for complex or multiple PVC configurations. When to use : - Use when you have complex PVC requirements - Use when configuring multiple PVCs at once - Use when you need full control over PVC specification - Alternative to setting individual PVC variables Valid values : Absolute path to YAML file containing PVC definitions Impact : When set, the role uses this file instead of individual PVC variables. The file must contain valid PVC definitions matching the expected format. Related variables : Overrides all individual mas_app_settings_custom_persistent_volume_* variables when set Note : See files/manage-persistent-volumes-sample.yml for file format example. This approach is more flexible but requires understanding of PVC YAML structure. For simple single-PVC scenarios, use individual variables instead.","title":"mas_app_settings_custom_persistent_volume_file_path"},{"location":"roles/suite_manage_pvc_config/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_manage_pvc_config/#using-variables","text":"The following sample can be used to configure new PVCs for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev mas_app_settings_custom_persistent_volume_sc_name: \"ibmc-file-gold-gid\" mas_app_settings_custom_persistent_volume_pvc_name: \"my-manage-pvc\" mas_app_settings_custom_persistent_volume_pvc_size: \"20Gi\" mas_app_settings_custom_persistent_volume_mount_path: \"/MyOwnFolder\" roles: - ibm.mas_devops.suite_manage_pvc_config","title":"Using Variables"},{"location":"roles/suite_manage_pvc_config/#using-file-definition","text":"The following sample can be used to configure new PVCs for an existing Manage application instance from a custom file definition. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev mas_app_settings_custom_persistent_volume_file_path: \"/my-path/manage-pv.yml\" roles: - ibm.mas_devops.suite_manage_pvc_config","title":"Using File Definition"},{"location":"roles/suite_manage_pvc_config/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export MAS_APP_SETTINGS_CUSTOM_PV_STORAGE_CLASS=ibmc-file-silver-gid export MAS_APP_SETTINGS_CUSTOM_PVC_NAME=my-manage-pvc export MAS_APP_SETTINGS_CUSTOM_PVC_SIZE=20Gi export MAS_APP_SETTINGS_CUSTOM_MOUNT_PATH=/MyOwnDir export MAS_APP_SETTINGS_CUSTOM_PV_FILE_PATH=/my-path/manage-pv.yml ROLE_NAME='suite_manage_pvc_config' ansible-playbook playbooks/run_role.yml","title":"Run Role Playbook"},{"location":"roles/suite_manage_pvc_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_rollback/","text":"suite_rollback \u00a4 This role rolls back Maximo Application Suite core platform to an earlier version. Rollback capability is available in MAS 8.11 and later versions. Each MAS version includes a list of supported rollback target versions. For example, you can roll back from MAS 8.11.x to 8.11.0. Warning Rollback is a significant operation that should be carefully planned and tested. Always perform a dry run first and ensure you have backups before proceeding with actual rollback. What This Role Does \u00a4 Validates the specified target version is compatible for rollback from the current version Verifies the core platform is not already at the target version Executes the rollback to the desired version (unless dry run mode is enabled) Validates the core platform successfully reconciles at the rolled back version Note : Does not validate that all core services successfully deploy after reconcile (future enhancement) Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier to rollback. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to rollback. The role validates and executes rollback operations on this specific instance. When to use : - Always required for rollback operations - Must match the instance ID from MAS installation - Used to target specific instance for version rollback Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance undergoes rollback. All validation and rollback operations target this instance. Related variables : - mas_core_version : Target version to rollback to - rollback_mas_core : Controls whether rollback is executed Note : Ensure the instance is in a stable state before initiating rollback. Applications should be stopped or in maintenance mode during rollback. mas_core_version \u00a4 Target MAS core version for rollback or verification. Required (when rollback_mas_core=true or verify_core_version=true ) Environment Variable: MAS_CORE_VERSION Default: None Purpose : Specifies the MAS core version to rollback to or to verify against the current version. Must be a supported rollback target for the current version. When to use : - Required when performing rollback ( rollback_mas_core=true ) - Required when verifying version ( verify_core_version=true ) - Must be a version listed in the current version's supported rollback targets Valid values : Valid MAS version string (e.g., 8.11.0 , 8.11.1 , 8.12.0 ) Impact : Determines the target version for rollback operations. The role validates this version is compatible before proceeding. Related variables : - mas_instance_id : Instance to rollback - rollback_mas_core : Enables rollback execution - skip_compatibility_check : Bypasses version compatibility validation Note : Check the MAS documentation for supported rollback paths from your current version. Not all versions support rollback to all earlier versions. rollback_mas_core \u00a4 Enable rollback execution. Optional Environment Variable: ROLLBACK_MAS_CORE Default: true Purpose : Controls whether the role actually performs the rollback operation or only validates/verifies version information. When to use : - Leave as true (default) to execute rollback - Set to false when only verifying current version - Use with mas_rollback_dryrun=true for validation without changes Valid values : true , false Impact : - true : Executes rollback to the specified version (default behavior) - false : Skips rollback execution (use with verify_core_version=true for version checks) Related variables : - mas_core_version : Target version for rollback - verify_core_version : Alternative mode for version verification - mas_rollback_dryrun : Validation-only mode Note : When false , typically used with verify_core_version=true to check current version without making changes. verify_core_version \u00a4 Enable version verification mode. Optional Environment Variable: VERIFY_CORE_VERSION Default: false Purpose : When enabled, verifies that the current MAS core version matches the specified mas_core_version without performing any rollback operations. When to use : - Set to true to verify current version matches expected version - Use after rollback to confirm successful version change - Helpful for validation in automation pipelines - Typically used with rollback_mas_core=false Valid values : true , false Impact : - true : Checks current version matches mas_core_version , fails if mismatch - false : Skips version verification (default) Related variables : - mas_core_version : Expected version to verify against - rollback_mas_core : Should be false when this is true Note : Useful for post-rollback validation or confirming version before proceeding with other operations. mas_rollback_dryrun \u00a4 Enable dry run mode (validation only). Optional Environment Variable: MAS_ROLLBACK_DRYRUN Default: false Purpose : When enabled, performs all validation checks for rollback compatibility without making any actual changes to the MAS installation. When to use : - Set to true before actual rollback to validate compatibility - Use to test rollback feasibility without risk - Recommended first step before any rollback operation - Helpful for planning and validation Valid values : true , false Impact : - true : Validates rollback compatibility but makes no changes (safe) - false : Executes actual rollback after validation (default) Related variables : - rollback_mas_core : Rollback must be enabled for dry run to be meaningful - mas_core_version : Target version to validate Note : BEST PRACTICE - Always run with mas_rollback_dryrun=true first to validate the rollback is possible before executing the actual rollback with mas_rollback_dryrun=false . skip_compatibility_check \u00a4 Skip version compatibility validation. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: false Purpose : Bypasses the compatibility check that validates the target version is in the list of supported rollback versions. Intended for development and testing scenarios only. When to use : - Development/testing only - for pre-release version testing - Allows rollback between pre-built versions on same base (e.g., 8.11.0-pre.dev to 8.11.0-pre.stable ) - NEVER use in production environments Valid values : true , false Impact : - true : Skips compatibility validation, allows unsupported rollback paths (dangerous) - false : Enforces compatibility checks (safe, default) Related variables : - mas_core_version : Target version (may be unsupported if check skipped) - rollback_mas_core : Rollback execution control Note : WARNING - This option is for development purposes only. Skipping compatibility checks can result in failed rollbacks, data corruption, or unstable installations. Never use in production. The compatibility check exists to prevent unsupported rollback scenarios. Example Playbook \u00a4 Rollback to Specified Version \u00a4 Running this playbook will rollback MAS core to the specified version. If you run this playbook when you are already on the same version it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_core_version: 8.11.0 mas_rollback_dryrun: False roles: - ibm.mas_devops.suite_rollback Verify MAS Core Version \u00a4 Running this playbook will attempt to verify the current version of MAS core matches with the specified version. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_core_version: 8.11.0 mas_upgrade_dryrun: False rollback_mas_core: False verify_core_version: True roles: - ibm.mas_devops.suite_rollback License \u00a4 EPL-2.0","title":"suite_rollback"},{"location":"roles/suite_rollback/#suite_rollback","text":"This role rolls back Maximo Application Suite core platform to an earlier version. Rollback capability is available in MAS 8.11 and later versions. Each MAS version includes a list of supported rollback target versions. For example, you can roll back from MAS 8.11.x to 8.11.0. Warning Rollback is a significant operation that should be carefully planned and tested. Always perform a dry run first and ensure you have backups before proceeding with actual rollback.","title":"suite_rollback"},{"location":"roles/suite_rollback/#what-this-role-does","text":"Validates the specified target version is compatible for rollback from the current version Verifies the core platform is not already at the target version Executes the rollback to the desired version (unless dry run mode is enabled) Validates the core platform successfully reconciles at the rolled back version Note : Does not validate that all core services successfully deploy after reconcile (future enhancement)","title":"What This Role Does"},{"location":"roles/suite_rollback/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_rollback/#mas_instance_id","text":"MAS instance identifier to rollback. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to rollback. The role validates and executes rollback operations on this specific instance. When to use : - Always required for rollback operations - Must match the instance ID from MAS installation - Used to target specific instance for version rollback Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance undergoes rollback. All validation and rollback operations target this instance. Related variables : - mas_core_version : Target version to rollback to - rollback_mas_core : Controls whether rollback is executed Note : Ensure the instance is in a stable state before initiating rollback. Applications should be stopped or in maintenance mode during rollback.","title":"mas_instance_id"},{"location":"roles/suite_rollback/#mas_core_version","text":"Target MAS core version for rollback or verification. Required (when rollback_mas_core=true or verify_core_version=true ) Environment Variable: MAS_CORE_VERSION Default: None Purpose : Specifies the MAS core version to rollback to or to verify against the current version. Must be a supported rollback target for the current version. When to use : - Required when performing rollback ( rollback_mas_core=true ) - Required when verifying version ( verify_core_version=true ) - Must be a version listed in the current version's supported rollback targets Valid values : Valid MAS version string (e.g., 8.11.0 , 8.11.1 , 8.12.0 ) Impact : Determines the target version for rollback operations. The role validates this version is compatible before proceeding. Related variables : - mas_instance_id : Instance to rollback - rollback_mas_core : Enables rollback execution - skip_compatibility_check : Bypasses version compatibility validation Note : Check the MAS documentation for supported rollback paths from your current version. Not all versions support rollback to all earlier versions.","title":"mas_core_version"},{"location":"roles/suite_rollback/#rollback_mas_core","text":"Enable rollback execution. Optional Environment Variable: ROLLBACK_MAS_CORE Default: true Purpose : Controls whether the role actually performs the rollback operation or only validates/verifies version information. When to use : - Leave as true (default) to execute rollback - Set to false when only verifying current version - Use with mas_rollback_dryrun=true for validation without changes Valid values : true , false Impact : - true : Executes rollback to the specified version (default behavior) - false : Skips rollback execution (use with verify_core_version=true for version checks) Related variables : - mas_core_version : Target version for rollback - verify_core_version : Alternative mode for version verification - mas_rollback_dryrun : Validation-only mode Note : When false , typically used with verify_core_version=true to check current version without making changes.","title":"rollback_mas_core"},{"location":"roles/suite_rollback/#verify_core_version","text":"Enable version verification mode. Optional Environment Variable: VERIFY_CORE_VERSION Default: false Purpose : When enabled, verifies that the current MAS core version matches the specified mas_core_version without performing any rollback operations. When to use : - Set to true to verify current version matches expected version - Use after rollback to confirm successful version change - Helpful for validation in automation pipelines - Typically used with rollback_mas_core=false Valid values : true , false Impact : - true : Checks current version matches mas_core_version , fails if mismatch - false : Skips version verification (default) Related variables : - mas_core_version : Expected version to verify against - rollback_mas_core : Should be false when this is true Note : Useful for post-rollback validation or confirming version before proceeding with other operations.","title":"verify_core_version"},{"location":"roles/suite_rollback/#mas_rollback_dryrun","text":"Enable dry run mode (validation only). Optional Environment Variable: MAS_ROLLBACK_DRYRUN Default: false Purpose : When enabled, performs all validation checks for rollback compatibility without making any actual changes to the MAS installation. When to use : - Set to true before actual rollback to validate compatibility - Use to test rollback feasibility without risk - Recommended first step before any rollback operation - Helpful for planning and validation Valid values : true , false Impact : - true : Validates rollback compatibility but makes no changes (safe) - false : Executes actual rollback after validation (default) Related variables : - rollback_mas_core : Rollback must be enabled for dry run to be meaningful - mas_core_version : Target version to validate Note : BEST PRACTICE - Always run with mas_rollback_dryrun=true first to validate the rollback is possible before executing the actual rollback with mas_rollback_dryrun=false .","title":"mas_rollback_dryrun"},{"location":"roles/suite_rollback/#skip_compatibility_check","text":"Skip version compatibility validation. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: false Purpose : Bypasses the compatibility check that validates the target version is in the list of supported rollback versions. Intended for development and testing scenarios only. When to use : - Development/testing only - for pre-release version testing - Allows rollback between pre-built versions on same base (e.g., 8.11.0-pre.dev to 8.11.0-pre.stable ) - NEVER use in production environments Valid values : true , false Impact : - true : Skips compatibility validation, allows unsupported rollback paths (dangerous) - false : Enforces compatibility checks (safe, default) Related variables : - mas_core_version : Target version (may be unsupported if check skipped) - rollback_mas_core : Rollback execution control Note : WARNING - This option is for development purposes only. Skipping compatibility checks can result in failed rollbacks, data corruption, or unstable installations. Never use in production. The compatibility check exists to prevent unsupported rollback scenarios.","title":"skip_compatibility_check"},{"location":"roles/suite_rollback/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_rollback/#rollback-to-specified-version","text":"Running this playbook will rollback MAS core to the specified version. If you run this playbook when you are already on the same version it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_core_version: 8.11.0 mas_rollback_dryrun: False roles: - ibm.mas_devops.suite_rollback","title":"Rollback to Specified Version"},{"location":"roles/suite_rollback/#verify-mas-core-version","text":"Running this playbook will attempt to verify the current version of MAS core matches with the specified version. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_core_version: 8.11.0 mas_upgrade_dryrun: False rollback_mas_core: False verify_core_version: True roles: - ibm.mas_devops.suite_rollback","title":"Verify MAS Core Version"},{"location":"roles/suite_rollback/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_uninstall/","text":"suite_uninstall \u00a4 This role removes Maximo Application Suite Core Platform. Note that it does not remove any data from MongoDB by default, and does not remove any applications from the MAS install. Generally it should be used after suite_app_uninstall to remove all installed Maximo Application Suite applications. Warning This role performs destructive operations. Ensure you have backups before proceeding. Use mas_wipe_mongo_data carefully as it permanently deletes all MAS data. Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier to uninstall. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to remove from the cluster. The role deletes the Suite custom resource and associated core platform resources. When to use : - Always required for uninstall operations - Must match the instance ID from MAS installation - Used to target specific instance for removal Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance is uninstalled. All core platform resources for this instance will be removed, but MongoDB data is preserved by default unless mas_wipe_mongo_data is enabled. Related variables : - mas_wipe_mongo_data : Controls whether MongoDB data is deleted Note : This role only removes the MAS core platform. Applications must be uninstalled separately using suite_app_uninstall before running this role. MongoDB data is preserved by default for safety. mas_wipe_mongo_data \u00a4 Delete MongoDB databases during uninstall. Optional Environment Variable: MAS_WIPE_MONGO_DATA Default: false Purpose : Controls whether MongoDB databases containing MAS data are permanently deleted during uninstall. When disabled (default), only the MAS platform is removed while preserving all data. When to use : - Leave as false (default) to preserve data (recommended) - Set to true only for complete removal including all data - Use true for test/dev environments or when data is no longer needed - NEVER use true in production without verified backups Valid values : true , false Impact : - false : Removes MAS platform but preserves all MongoDB data (safe, allows reinstall) - true : Removes MAS platform AND permanently deletes all MongoDB databases (destructive) Related variables : - mas_instance_id : Instance whose data may be deleted Note : CRITICAL - Setting to true permanently deletes all MAS data including configurations, workspaces, and application data. This operation cannot be undone. Always ensure you have verified backups before enabling this option. The default false is strongly recommended for production environments. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" roles: - ibm.mas_devops.suite_uninstall License \u00a4 EPL-2.0","title":"suite_uninstall"},{"location":"roles/suite_uninstall/#suite_uninstall","text":"This role removes Maximo Application Suite Core Platform. Note that it does not remove any data from MongoDB by default, and does not remove any applications from the MAS install. Generally it should be used after suite_app_uninstall to remove all installed Maximo Application Suite applications. Warning This role performs destructive operations. Ensure you have backups before proceeding. Use mas_wipe_mongo_data carefully as it permanently deletes all MAS data.","title":"suite_uninstall"},{"location":"roles/suite_uninstall/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_uninstall/#mas_instance_id","text":"MAS instance identifier to uninstall. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to remove from the cluster. The role deletes the Suite custom resource and associated core platform resources. When to use : - Always required for uninstall operations - Must match the instance ID from MAS installation - Used to target specific instance for removal Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance is uninstalled. All core platform resources for this instance will be removed, but MongoDB data is preserved by default unless mas_wipe_mongo_data is enabled. Related variables : - mas_wipe_mongo_data : Controls whether MongoDB data is deleted Note : This role only removes the MAS core platform. Applications must be uninstalled separately using suite_app_uninstall before running this role. MongoDB data is preserved by default for safety.","title":"mas_instance_id"},{"location":"roles/suite_uninstall/#mas_wipe_mongo_data","text":"Delete MongoDB databases during uninstall. Optional Environment Variable: MAS_WIPE_MONGO_DATA Default: false Purpose : Controls whether MongoDB databases containing MAS data are permanently deleted during uninstall. When disabled (default), only the MAS platform is removed while preserving all data. When to use : - Leave as false (default) to preserve data (recommended) - Set to true only for complete removal including all data - Use true for test/dev environments or when data is no longer needed - NEVER use true in production without verified backups Valid values : true , false Impact : - false : Removes MAS platform but preserves all MongoDB data (safe, allows reinstall) - true : Removes MAS platform AND permanently deletes all MongoDB databases (destructive) Related variables : - mas_instance_id : Instance whose data may be deleted Note : CRITICAL - Setting to true permanently deletes all MAS data including configurations, workspaces, and application data. This operation cannot be undone. Always ensure you have verified backups before enabling this option. The default false is strongly recommended for production environments.","title":"mas_wipe_mongo_data"},{"location":"roles/suite_uninstall/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" roles: - ibm.mas_devops.suite_uninstall","title":"Example Playbook"},{"location":"roles/suite_uninstall/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_upgrade/","text":"suite_upgrade \u00a4 This role validates if a given MAS installation is ready for the core platform to be upgraded to a specific subscription channel, and (as long as dry run mode is not enabled) will execute the upgrade. It will validate that the current subscription channel is able to be upgraded to the target channel. It will validate that all installed applications have already been upgraded to versions compatible with the new version of the Core Platform. It will upgrade the MAS core platform to the desired channel (as long as dry run is not enabled). It will validate that the core platform has been successfully reconciled at the upgraded version. It will not validate that all core services successfully deploy after the reconcile (but we will be working on this limitation). Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier for core platform upgrade. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance's core platform to upgrade. Used to locate and validate the MAS installation. When to use : - Always required for MAS core platform upgrades - Must match the instance ID from MAS installation - Used to validate upgrade readiness and application compatibility Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance will be upgraded. Incorrect instance ID will cause upgrade to fail. Related variables : - mas_channel : Target upgrade channel for this instance Note : The role validates that all installed applications are compatible with the target MAS version before proceeding with the upgrade. mas_channel \u00a4 Target subscription channel for MAS core platform upgrade. Optional Environment Variable: MAS_CHANNEL Default: Auto-selected based on current version Purpose : Specifies the target subscription channel for MAS core platform upgrade. If not provided, the role automatically selects the next appropriate version. When to use : - Leave unset for automatic upgrade to next release - Set explicitly when you need a specific target version - Must be a valid upgrade path from current version Valid values : Valid MAS subscription channel (e.g., 8.8.x , 8.9.x , 8.10.x , 8.11.x ) Impact : Determines the target version for MAS core platform upgrade. The role validates: - Upgrade path compatibility (can upgrade from current to target) - Application compatibility (all apps support target MAS version) - If validation fails, no upgrade is performed Related variables : - mas_instance_id : Instance being upgraded - skip_compatibility_check : Whether to skip validation (not recommended) Note : When unset, the role automatically selects the next release. If already on the latest release, no action is taken. The role validates that all installed applications are compatible with the target MAS version before upgrading. mas_upgrade_dryrun \u00a4 Dry-run mode for upgrade validation only. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: false Purpose : Enables dry-run mode where the role performs all validation checks without making any changes to the MAS installation. Useful for testing upgrade paths. When to use : - Set to true to validate upgrade without executing it - Use for testing and planning upgrade paths - Recommended before production upgrades - Leave as false (default) to perform actual upgrade Valid values : true , false Impact : - true : Performs validation only (upgrade path check, application compatibility check) without modifying the installation - false : Performs validation and executes the upgrade if validation passes Related variables : - skip_compatibility_check : Controls whether compatibility validation is performed Note : Dry-run mode is highly recommended before production upgrades to identify potential issues. All validation checks are performed, including application compatibility, but no changes are made to the subscription channel or MAS core platform. skip_compatibility_check \u00a4 Skip compatibility validation before upgrade. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: false Purpose : Controls whether compatibility validation is performed before MAS core platform upgrade. Validation checks if the target channel is compatible with current MAS version and all installed applications. When to use : - Leave as false (default) for safe upgrades with validation - Set to true only in exceptional cases (not recommended) - Use only when you have verified compatibility manually Valid values : true , false Impact : - false (default): Performs comprehensive compatibility validation before upgrade (recommended) - true : Skips compatibility validation, allowing potentially incompatible upgrades Related variables : - mas_upgrade_dryrun : Controls whether upgrade is executed or only validated - mas_channel : Target channel being validated Note : WARNING - Skipping compatibility checks can lead to failed upgrades, application incompatibilities, or unstable installations. Only skip validation if you have manually verified that: 1. The upgrade path from current to target MAS version is supported 2. All installed applications are compatible with the target MAS version The default validation protects against incompatible upgrades and application version mismatches. Example Playbook \u00a4 Automatic Target Selection \u00a4 Running this playbook will upgrade MAS to the next release. If you run this playbook when you are already on the latest release then it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade Explicit Upgrade Target \u00a4 Running this playbook will attempt to upgrade MAS to the specified release. If the specified release cannot be upgraded to from the installed version of MAS then no action will be taken. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_channel: 8.8.x mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade License \u00a4 EPL-2.0","title":"suite_upgrade"},{"location":"roles/suite_upgrade/#suite_upgrade","text":"This role validates if a given MAS installation is ready for the core platform to be upgraded to a specific subscription channel, and (as long as dry run mode is not enabled) will execute the upgrade. It will validate that the current subscription channel is able to be upgraded to the target channel. It will validate that all installed applications have already been upgraded to versions compatible with the new version of the Core Platform. It will upgrade the MAS core platform to the desired channel (as long as dry run is not enabled). It will validate that the core platform has been successfully reconciled at the upgraded version. It will not validate that all core services successfully deploy after the reconcile (but we will be working on this limitation).","title":"suite_upgrade"},{"location":"roles/suite_upgrade/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_upgrade/#mas_instance_id","text":"MAS instance identifier for core platform upgrade. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance's core platform to upgrade. Used to locate and validate the MAS installation. When to use : - Always required for MAS core platform upgrades - Must match the instance ID from MAS installation - Used to validate upgrade readiness and application compatibility Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance will be upgraded. Incorrect instance ID will cause upgrade to fail. Related variables : - mas_channel : Target upgrade channel for this instance Note : The role validates that all installed applications are compatible with the target MAS version before proceeding with the upgrade.","title":"mas_instance_id"},{"location":"roles/suite_upgrade/#mas_channel","text":"Target subscription channel for MAS core platform upgrade. Optional Environment Variable: MAS_CHANNEL Default: Auto-selected based on current version Purpose : Specifies the target subscription channel for MAS core platform upgrade. If not provided, the role automatically selects the next appropriate version. When to use : - Leave unset for automatic upgrade to next release - Set explicitly when you need a specific target version - Must be a valid upgrade path from current version Valid values : Valid MAS subscription channel (e.g., 8.8.x , 8.9.x , 8.10.x , 8.11.x ) Impact : Determines the target version for MAS core platform upgrade. The role validates: - Upgrade path compatibility (can upgrade from current to target) - Application compatibility (all apps support target MAS version) - If validation fails, no upgrade is performed Related variables : - mas_instance_id : Instance being upgraded - skip_compatibility_check : Whether to skip validation (not recommended) Note : When unset, the role automatically selects the next release. If already on the latest release, no action is taken. The role validates that all installed applications are compatible with the target MAS version before upgrading.","title":"mas_channel"},{"location":"roles/suite_upgrade/#mas_upgrade_dryrun","text":"Dry-run mode for upgrade validation only. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: false Purpose : Enables dry-run mode where the role performs all validation checks without making any changes to the MAS installation. Useful for testing upgrade paths. When to use : - Set to true to validate upgrade without executing it - Use for testing and planning upgrade paths - Recommended before production upgrades - Leave as false (default) to perform actual upgrade Valid values : true , false Impact : - true : Performs validation only (upgrade path check, application compatibility check) without modifying the installation - false : Performs validation and executes the upgrade if validation passes Related variables : - skip_compatibility_check : Controls whether compatibility validation is performed Note : Dry-run mode is highly recommended before production upgrades to identify potential issues. All validation checks are performed, including application compatibility, but no changes are made to the subscription channel or MAS core platform.","title":"mas_upgrade_dryrun"},{"location":"roles/suite_upgrade/#skip_compatibility_check","text":"Skip compatibility validation before upgrade. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: false Purpose : Controls whether compatibility validation is performed before MAS core platform upgrade. Validation checks if the target channel is compatible with current MAS version and all installed applications. When to use : - Leave as false (default) for safe upgrades with validation - Set to true only in exceptional cases (not recommended) - Use only when you have verified compatibility manually Valid values : true , false Impact : - false (default): Performs comprehensive compatibility validation before upgrade (recommended) - true : Skips compatibility validation, allowing potentially incompatible upgrades Related variables : - mas_upgrade_dryrun : Controls whether upgrade is executed or only validated - mas_channel : Target channel being validated Note : WARNING - Skipping compatibility checks can lead to failed upgrades, application incompatibilities, or unstable installations. Only skip validation if you have manually verified that: 1. The upgrade path from current to target MAS version is supported 2. All installed applications are compatible with the target MAS version The default validation protects against incompatible upgrades and application version mismatches.","title":"skip_compatibility_check"},{"location":"roles/suite_upgrade/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_upgrade/#automatic-target-selection","text":"Running this playbook will upgrade MAS to the next release. If you run this playbook when you are already on the latest release then it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade","title":"Automatic Target Selection"},{"location":"roles/suite_upgrade/#explicit-upgrade-target","text":"Running this playbook will attempt to upgrade MAS to the specified release. If the specified release cannot be upgraded to from the installed version of MAS then no action will be taken. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_channel: 8.8.x mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade","title":"Explicit Upgrade Target"},{"location":"roles/suite_upgrade/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_verify/","text":"suite_verify \u00a4 Verify a MAS installation is ready to use. This role will also print out the Admin Dashboard URL and the username and password of the superuser. If you want to disable these credentials being written to the output set the mas_hide_superuser_credentials to True . Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier to verify. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to verify. The role checks that the instance is ready to use and retrieves access credentials. When to use : - Always required for verification operations - Must match the instance ID from MAS installation - Used after installation to confirm readiness Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance is verified. The role checks instance status and retrieves the Admin Dashboard URL and superuser credentials. Related variables : - mas_hide_superuser_credentials : Controls credential display in output Note : This role verifies that the MAS instance is ready to use and provides access information. Run after installation or upgrade to confirm successful deployment. mas_hide_superuser_credentials \u00a4 Hide superuser credentials in output. Optional Environment Variable: MAS_HIDE_SUPERUSER_CREDENTIALS Default: true Purpose : Controls whether superuser credentials are displayed in the verification output. When enabled, only the secret name is shown instead of actual credentials. When to use : - Leave as true (default) for security (recommended) - Set to false only when you need to see credentials in output - Use true in CI/CD pipelines and shared environments Valid values : true , false Impact : - true : Displays only the secret name containing credentials (secure) - false : Displays actual username and password in output (insecure) Related variables : - mas_instance_id : Instance whose credentials are being verified Note : SECURITY - The default true is recommended for security. Only set to false in secure, private environments where you need immediate access to credentials. Credentials can always be retrieved from the Kubernetes secret. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_hide_superuser_credentials: True roles: - ibm.mas_devops.suite_verify License \u00a4 EPL-2.0","title":"suite_verify"},{"location":"roles/suite_verify/#suite_verify","text":"Verify a MAS installation is ready to use. This role will also print out the Admin Dashboard URL and the username and password of the superuser. If you want to disable these credentials being written to the output set the mas_hide_superuser_credentials to True .","title":"suite_verify"},{"location":"roles/suite_verify/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_verify/#mas_instance_id","text":"MAS instance identifier to verify. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to verify. The role checks that the instance is ready to use and retrieves access credentials. When to use : - Always required for verification operations - Must match the instance ID from MAS installation - Used after installation to confirm readiness Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance is verified. The role checks instance status and retrieves the Admin Dashboard URL and superuser credentials. Related variables : - mas_hide_superuser_credentials : Controls credential display in output Note : This role verifies that the MAS instance is ready to use and provides access information. Run after installation or upgrade to confirm successful deployment.","title":"mas_instance_id"},{"location":"roles/suite_verify/#mas_hide_superuser_credentials","text":"Hide superuser credentials in output. Optional Environment Variable: MAS_HIDE_SUPERUSER_CREDENTIALS Default: true Purpose : Controls whether superuser credentials are displayed in the verification output. When enabled, only the secret name is shown instead of actual credentials. When to use : - Leave as true (default) for security (recommended) - Set to false only when you need to see credentials in output - Use true in CI/CD pipelines and shared environments Valid values : true , false Impact : - true : Displays only the secret name containing credentials (secure) - false : Displays actual username and password in output (insecure) Related variables : - mas_instance_id : Instance whose credentials are being verified Note : SECURITY - The default true is recommended for security. Only set to false in secure, private environments where you need immediate access to credentials. Credentials can always be retrieved from the Kubernetes secret.","title":"mas_hide_superuser_credentials"},{"location":"roles/suite_verify/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_hide_superuser_credentials: True roles: - ibm.mas_devops.suite_verify","title":"Example Playbook"},{"location":"roles/suite_verify/#license","text":"EPL-2.0","title":"License"},{"location":"roles/turbonomic/","text":"turbonomic \u00a4 Installs and configures kubeturbo to connect your OpenShift cluster to a Turbonomic Application Resource Management (ARM) server. Kubeturbo is an agent that collects cluster resource utilization data and sends it to Turbonomic for analysis and optimization recommendations. Disconnected Installation Not Supported The Turbonomic Kubernetes Operator does not support disconnected/air-gapped installation. The kubeturbo deployment uses image tags rather than digests, preventing the use of ImageContentSourcePolicy to configure a mirror registry. What This Role Does \u00a4 Installs the Turbonomic Kubernetes Operator from available CatalogSource Deploys kubeturbo agent in the specified namespace Configures kubeturbo to connect to your Turbonomic server Sets up authentication credentials for server communication Role Variables - KubeTurbo Configuration \u00a4 Role Variables - General \u00a4 kubeturbo_namespace \u00a4 Namespace for KubeTurbo operator installation. Optional Environment Variable: KUBETURBO_NAMESPACE Default: kubeturbo Purpose : Specifies the OpenShift namespace where the KubeTurbo operator and agent will be installed. When to use : - Use default ( kubeturbo ) for standard installations - Override for custom namespace requirements - Useful for multi-tenant or organized deployments Valid values : Valid Kubernetes namespace name (lowercase alphanumeric and hyphens) Impact : Determines where kubeturbo resources are deployed. All operator and agent components will be created in this namespace. Related variables : - turbonomic_target_name : Cluster identifier in Turbonomic Note : The namespace will be created if it doesn't exist. Using the default kubeturbo namespace is recommended for consistency. Role Variables - Turbonomic Server Configuration \u00a4 turbonomic_target_name \u00a4 Cluster name in Turbonomic server. Required Environment Variable: TURBONOMIC_TARGET_NAME Default: None Purpose : Defines the name by which this cluster will be identified in the Turbonomic ARM server. The kubeturbo agent uses this name when registering and sending data to Turbonomic. When to use : - Always required for kubeturbo installation - Should be unique across all clusters managed by Turbonomic - Use descriptive names for easy identification Valid values : String, typically cluster name or identifier (e.g., prod-ocp-cluster , dev-cluster-01 ) Impact : Determines how the cluster appears in Turbonomic dashboards and reports. This name is used for all resource tracking and optimization recommendations. Related variables : - turbonomic_server_url : Server to register with - kubeturbo_namespace : Where agent is deployed Note : Choose a meaningful name that clearly identifies the cluster in your Turbonomic environment. This name should match your cluster naming conventions. turbonomic_server_url \u00a4 Turbonomic server URL. Required Environment Variable: TURBONOMIC_SERVER_URL Default: None Purpose : Specifies the URL of the Turbonomic ARM server that kubeturbo will connect to for sending cluster data and receiving optimization actions. When to use : - Always required for kubeturbo installation - Must be accessible from the OpenShift cluster - Should include protocol (https://) Valid values : Full URL to Turbonomic server (e.g., https://turbonomic.example.com , https://turbo-server.company.com:8080 ) Impact : Determines which Turbonomic server receives cluster data. Kubeturbo establishes connection to this endpoint for all communication. Related variables : - turbonomic_username / turbonomic_password : Authentication credentials - turbonomic_server_version : Optional version specification - turbonomic_target_name : Cluster identifier on this server Note : Ensure the URL is accessible from the cluster network. The server must be reachable for kubeturbo to function properly. turbonomic_server_version \u00a4 Turbonomic server version. Optional Environment Variable: TURBONOMIC_SERVER_VERSION Default: None Purpose : Specifies the version of the Turbonomic server being connected to. Used for version-specific compatibility and feature enablement. When to use : - Optional for most installations - Specify when connecting to specific Turbonomic versions - May be required for version-specific features or compatibility Valid values : Turbonomic version string (e.g., 8.9.4 , 8.10.0 , 8.11.2 ) Impact : May affect feature availability or compatibility checks between kubeturbo agent and Turbonomic server. Related variables : - turbonomic_server_url : Server being connected to Note : If not specified, kubeturbo will attempt to detect server version automatically. Specify only if you need to enforce specific version compatibility. turbonomic_username \u00a4 Turbonomic server username. Required Environment Variable: TURBONOMIC_USERNAME Default: None Purpose : Provides the username for authenticating kubeturbo agent with the Turbonomic server. When to use : - Always required for kubeturbo installation - Should be a service account or dedicated user - Requires appropriate permissions in Turbonomic Valid values : Valid Turbonomic username string Impact : Used for authentication when kubeturbo connects to Turbonomic server. The user must have permissions to register targets and send data. Related variables : - turbonomic_password : Password for this username - turbonomic_server_url : Server to authenticate with - turbonomic_target_name : Cluster this user will register Note : SECURITY - Use a dedicated service account rather than a personal user account. Ensure the account has appropriate permissions in Turbonomic for target registration and data submission. turbonomic_password \u00a4 Turbonomic server password. Required Environment Variable: TURBONOMIC_PASSWORD Default: None Purpose : Provides the password for authenticating kubeturbo agent with the Turbonomic server. When to use : - Always required for kubeturbo installation - Must match the password for turbonomic_username - Used for secure authentication Valid values : Valid password string for the specified username Impact : Used for authentication when kubeturbo connects to Turbonomic server. Authentication must succeed for data transmission. Related variables : - turbonomic_username : Username for this password - turbonomic_server_url : Server to authenticate with Note : SECURITY - Password should be kept secure and not committed to version control. Store in secure secret management systems. Use strong passwords following your organization's security policies. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: turbonomic_server_url: https://myturbonomicserver.com turbonomic_server_version: \"8.9.4\" turbonomic_username: user turbonomic_password: passw0rd turbonomic_target_name: myocp roles: - ibm.mas_devops.turbonomic License \u00a4 EPL-2.0","title":"turbonomic"},{"location":"roles/turbonomic/#turbonomic","text":"Installs and configures kubeturbo to connect your OpenShift cluster to a Turbonomic Application Resource Management (ARM) server. Kubeturbo is an agent that collects cluster resource utilization data and sends it to Turbonomic for analysis and optimization recommendations. Disconnected Installation Not Supported The Turbonomic Kubernetes Operator does not support disconnected/air-gapped installation. The kubeturbo deployment uses image tags rather than digests, preventing the use of ImageContentSourcePolicy to configure a mirror registry.","title":"turbonomic"},{"location":"roles/turbonomic/#what-this-role-does","text":"Installs the Turbonomic Kubernetes Operator from available CatalogSource Deploys kubeturbo agent in the specified namespace Configures kubeturbo to connect to your Turbonomic server Sets up authentication credentials for server communication","title":"What This Role Does"},{"location":"roles/turbonomic/#role-variables-kubeturbo-configuration","text":"","title":"Role Variables - KubeTurbo Configuration"},{"location":"roles/turbonomic/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/turbonomic/#kubeturbo_namespace","text":"Namespace for KubeTurbo operator installation. Optional Environment Variable: KUBETURBO_NAMESPACE Default: kubeturbo Purpose : Specifies the OpenShift namespace where the KubeTurbo operator and agent will be installed. When to use : - Use default ( kubeturbo ) for standard installations - Override for custom namespace requirements - Useful for multi-tenant or organized deployments Valid values : Valid Kubernetes namespace name (lowercase alphanumeric and hyphens) Impact : Determines where kubeturbo resources are deployed. All operator and agent components will be created in this namespace. Related variables : - turbonomic_target_name : Cluster identifier in Turbonomic Note : The namespace will be created if it doesn't exist. Using the default kubeturbo namespace is recommended for consistency.","title":"kubeturbo_namespace"},{"location":"roles/turbonomic/#role-variables-turbonomic-server-configuration","text":"","title":"Role Variables - Turbonomic Server Configuration"},{"location":"roles/turbonomic/#turbonomic_target_name","text":"Cluster name in Turbonomic server. Required Environment Variable: TURBONOMIC_TARGET_NAME Default: None Purpose : Defines the name by which this cluster will be identified in the Turbonomic ARM server. The kubeturbo agent uses this name when registering and sending data to Turbonomic. When to use : - Always required for kubeturbo installation - Should be unique across all clusters managed by Turbonomic - Use descriptive names for easy identification Valid values : String, typically cluster name or identifier (e.g., prod-ocp-cluster , dev-cluster-01 ) Impact : Determines how the cluster appears in Turbonomic dashboards and reports. This name is used for all resource tracking and optimization recommendations. Related variables : - turbonomic_server_url : Server to register with - kubeturbo_namespace : Where agent is deployed Note : Choose a meaningful name that clearly identifies the cluster in your Turbonomic environment. This name should match your cluster naming conventions.","title":"turbonomic_target_name"},{"location":"roles/turbonomic/#turbonomic_server_url","text":"Turbonomic server URL. Required Environment Variable: TURBONOMIC_SERVER_URL Default: None Purpose : Specifies the URL of the Turbonomic ARM server that kubeturbo will connect to for sending cluster data and receiving optimization actions. When to use : - Always required for kubeturbo installation - Must be accessible from the OpenShift cluster - Should include protocol (https://) Valid values : Full URL to Turbonomic server (e.g., https://turbonomic.example.com , https://turbo-server.company.com:8080 ) Impact : Determines which Turbonomic server receives cluster data. Kubeturbo establishes connection to this endpoint for all communication. Related variables : - turbonomic_username / turbonomic_password : Authentication credentials - turbonomic_server_version : Optional version specification - turbonomic_target_name : Cluster identifier on this server Note : Ensure the URL is accessible from the cluster network. The server must be reachable for kubeturbo to function properly.","title":"turbonomic_server_url"},{"location":"roles/turbonomic/#turbonomic_server_version","text":"Turbonomic server version. Optional Environment Variable: TURBONOMIC_SERVER_VERSION Default: None Purpose : Specifies the version of the Turbonomic server being connected to. Used for version-specific compatibility and feature enablement. When to use : - Optional for most installations - Specify when connecting to specific Turbonomic versions - May be required for version-specific features or compatibility Valid values : Turbonomic version string (e.g., 8.9.4 , 8.10.0 , 8.11.2 ) Impact : May affect feature availability or compatibility checks between kubeturbo agent and Turbonomic server. Related variables : - turbonomic_server_url : Server being connected to Note : If not specified, kubeturbo will attempt to detect server version automatically. Specify only if you need to enforce specific version compatibility.","title":"turbonomic_server_version"},{"location":"roles/turbonomic/#turbonomic_username","text":"Turbonomic server username. Required Environment Variable: TURBONOMIC_USERNAME Default: None Purpose : Provides the username for authenticating kubeturbo agent with the Turbonomic server. When to use : - Always required for kubeturbo installation - Should be a service account or dedicated user - Requires appropriate permissions in Turbonomic Valid values : Valid Turbonomic username string Impact : Used for authentication when kubeturbo connects to Turbonomic server. The user must have permissions to register targets and send data. Related variables : - turbonomic_password : Password for this username - turbonomic_server_url : Server to authenticate with - turbonomic_target_name : Cluster this user will register Note : SECURITY - Use a dedicated service account rather than a personal user account. Ensure the account has appropriate permissions in Turbonomic for target registration and data submission.","title":"turbonomic_username"},{"location":"roles/turbonomic/#turbonomic_password","text":"Turbonomic server password. Required Environment Variable: TURBONOMIC_PASSWORD Default: None Purpose : Provides the password for authenticating kubeturbo agent with the Turbonomic server. When to use : - Always required for kubeturbo installation - Must match the password for turbonomic_username - Used for secure authentication Valid values : Valid password string for the specified username Impact : Used for authentication when kubeturbo connects to Turbonomic server. Authentication must succeed for data transmission. Related variables : - turbonomic_username : Username for this password - turbonomic_server_url : Server to authenticate with Note : SECURITY - Password should be kept secure and not committed to version control. Store in secure secret management systems. Use strong passwords following your organization's security policies.","title":"turbonomic_password"},{"location":"roles/turbonomic/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: turbonomic_server_url: https://myturbonomicserver.com turbonomic_server_version: \"8.9.4\" turbonomic_username: user turbonomic_password: passw0rd turbonomic_target_name: myocp roles: - ibm.mas_devops.turbonomic","title":"Example Playbook"},{"location":"roles/turbonomic/#license","text":"EPL-2.0","title":"License"}]}