{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MAS DevOps Ansible Collection \u00a4 The ibm.mas_devops Ansible Collection is published on Ansible Galaxy and works with all supported releases of IBM Maximo Application Suite. Release information for the collection can be found in GitHub . Overview \u00a4 Many users ask what is the difference between the MAS Ansible collection and the MAS CLI , the best way we have come up with so far to explain the difference is as below: The ansible collection is a toolbox The cli is a solution built using that toolbox Both are viable ways to install, but anyone using the ansible collection needs to understand what they are using; it is a means to create a solution, it's not a solution in it's own right. The MAS CLI is the reference solution that we (IBM) offer, based on the tools provided in the ansible collection. Using the CLI is the right answer for 95% of users; if you are unsure what is right for you, start here . Usage \u00a4 Run a Playbook \u00a4 The collection includes a number of playbooks that string together multiple roles, you can directly invoke them after installing the collection: ansible-playbook ibm.mas_devops.mas_install_core Run a Role \u00a4 If you only want to perform a single action, you can directly invoke one of our roles from the command line without the need to build a playbook: ansible localhost -m include_role -a name=ibm.mas_devops.ocp_verify You can also use the run_role playbook: ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role Running in a Container Image \u00a4 The easiest way to use this collection is to take advantage of the ibmmas/cli container image, this negates the need to install anything on your local machine (other than docker - or podman if you prefer). # Run with docker docker run -ti --rm --pull always quay.io/ibmmas/cli # Run with podman podman run -ti --rm --pull always quay.io/ibmmas/cli Local Install \u00a4 Install the collection direct from Ansible Galaxy , you must also install the mas-devops Python package. Python 3.11 is recommended as it is the most widely used version of Python within our development team, but any in-support version of Python should work. ansible-galaxy collection install ibm.mas_devops python3 -m pip install mas-devops Optionally, you can also pin the version of the collection that you install, allowing you to control exactly what version of the collection is in use in your solution: ansible-galaxy collection install ibm.mas_devops:18.10.4 python3 -m pip install mas-devops The ansible collection makes use of many dependencies, you can find install scripts showing how we install these dependencies in our own container image in the ibm-mas/cli-base repository, the dependencies you need will be determined by the roles that you intend to use, refer to the roles documentation for dependency infomation. Tip Many systems contain more than one installation of Python, when you install the mas-devops package you must install it to the Python that Ansible is configured to use. You can check the version being used by Ansible by reviewing the output of ansible --version . If you see the error message ERROR! Unexpected Exception, this is probably a bug: No module named 'mas' it almost certainly means that you have not installed the mas-devops package, or have added it to the wrong instance of Python. Ansible Automation Platform \u00a4 If you wish to use Red Hat Ansible Automation Platform then a Automation Execution Environment image is available at quay.io/ibmmas/ansible-devops-ee that contains the ibm.mas_devops collection at the same release level, plus required client packages and access to the automation content collections supported by Red Hat. More details on how to use the ansible-devops execution environment can be found here Action Groups \u00a4 The collection provide a new action group ibm.mas_devops.k8s which can be used to set the default Kubernetes target cluster as an alternative to authenticating with the cluster prior to running our ansible playbooks/roles/actions, see the example below which would return the default storage classes that would be used in this collection for the specified cluster: --- - hosts: localhost any_errors_fatal: true collections: - ibm.mas_devops module_defaults: group/ibm.mas_devops.k8s: host: \"<your host url>\" api_key: \"<your api key>\" tasks: - name: \"Lookup default storage classes\" ibm.mas_devops.get_default_storage_classes: register: classes - debug: msg: \"{{classes}}\" Support \u00a4 This Ansible collection is developed by the IBM Maximo Application Suite development team, customers may raise support tickets via the same routes they would an issue with the product itself, or raise an issue directly in the GitHub repository .","title":"Home"},{"location":"#mas-devops-ansible-collection","text":"The ibm.mas_devops Ansible Collection is published on Ansible Galaxy and works with all supported releases of IBM Maximo Application Suite. Release information for the collection can be found in GitHub .","title":"MAS DevOps Ansible Collection"},{"location":"#overview","text":"Many users ask what is the difference between the MAS Ansible collection and the MAS CLI , the best way we have come up with so far to explain the difference is as below: The ansible collection is a toolbox The cli is a solution built using that toolbox Both are viable ways to install, but anyone using the ansible collection needs to understand what they are using; it is a means to create a solution, it's not a solution in it's own right. The MAS CLI is the reference solution that we (IBM) offer, based on the tools provided in the ansible collection. Using the CLI is the right answer for 95% of users; if you are unsure what is right for you, start here .","title":"Overview"},{"location":"#usage","text":"","title":"Usage"},{"location":"#run-a-playbook","text":"The collection includes a number of playbooks that string together multiple roles, you can directly invoke them after installing the collection: ansible-playbook ibm.mas_devops.mas_install_core","title":"Run a Playbook"},{"location":"#run-a-role","text":"If you only want to perform a single action, you can directly invoke one of our roles from the command line without the need to build a playbook: ansible localhost -m include_role -a name=ibm.mas_devops.ocp_verify You can also use the run_role playbook: ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role","title":"Run a Role"},{"location":"#running-in-a-container-image","text":"The easiest way to use this collection is to take advantage of the ibmmas/cli container image, this negates the need to install anything on your local machine (other than docker - or podman if you prefer). # Run with docker docker run -ti --rm --pull always quay.io/ibmmas/cli # Run with podman podman run -ti --rm --pull always quay.io/ibmmas/cli","title":"Running in a Container Image"},{"location":"#local-install","text":"Install the collection direct from Ansible Galaxy , you must also install the mas-devops Python package. Python 3.11 is recommended as it is the most widely used version of Python within our development team, but any in-support version of Python should work. ansible-galaxy collection install ibm.mas_devops python3 -m pip install mas-devops Optionally, you can also pin the version of the collection that you install, allowing you to control exactly what version of the collection is in use in your solution: ansible-galaxy collection install ibm.mas_devops:18.10.4 python3 -m pip install mas-devops The ansible collection makes use of many dependencies, you can find install scripts showing how we install these dependencies in our own container image in the ibm-mas/cli-base repository, the dependencies you need will be determined by the roles that you intend to use, refer to the roles documentation for dependency infomation. Tip Many systems contain more than one installation of Python, when you install the mas-devops package you must install it to the Python that Ansible is configured to use. You can check the version being used by Ansible by reviewing the output of ansible --version . If you see the error message ERROR! Unexpected Exception, this is probably a bug: No module named 'mas' it almost certainly means that you have not installed the mas-devops package, or have added it to the wrong instance of Python.","title":"Local Install"},{"location":"#ansible-automation-platform","text":"If you wish to use Red Hat Ansible Automation Platform then a Automation Execution Environment image is available at quay.io/ibmmas/ansible-devops-ee that contains the ibm.mas_devops collection at the same release level, plus required client packages and access to the automation content collections supported by Red Hat. More details on how to use the ansible-devops execution environment can be found here","title":"Ansible Automation Platform"},{"location":"#action-groups","text":"The collection provide a new action group ibm.mas_devops.k8s which can be used to set the default Kubernetes target cluster as an alternative to authenticating with the cluster prior to running our ansible playbooks/roles/actions, see the example below which would return the default storage classes that would be used in this collection for the specified cluster: --- - hosts: localhost any_errors_fatal: true collections: - ibm.mas_devops module_defaults: group/ibm.mas_devops.k8s: host: \"<your host url>\" api_key: \"<your api key>\" tasks: - name: \"Lookup default storage classes\" ibm.mas_devops.get_default_storage_classes: register: classes - debug: msg: \"{{classes}}\"","title":"Action Groups"},{"location":"#support","text":"This Ansible collection is developed by the IBM Maximo Application Suite development team, customers may raise support tickets via the same routes they would an issue with the product itself, or raise an issue directly in the GitHub repository .","title":"Support"},{"location":"execution-environment/","text":"Execution Environment \u00a4 Details on the Red Hat Ansible Automation Platform Execution Environment for the ibm.mas_devops Ansible Collection. Execution Environment Image \u00a4 The execution environment image for ansible-devops builds on the latest ansible-automation-platform-24/ee-supported-rhel9 image from Red Hat that provides the ansible-core and Red Hat supported collections. The ansible-devops-ee image includes the ibm.mas_devops collection and all required client libraries to function. The image is uploaded to quay.io at quay.io/ibmmas/ansible-devops-ee How to setup Anisble Automation Platform \u00a4 Organization \u00a4 An Organization is a logical collection of Users, Teams, Projects, and Inventories, and is the highest level in the automation controller object hierarchy. Create an organization if you don't already have one: Inventory \u00a4 An Inventory is a collection of hosts against which jobs may be launched, the same as an Ansible inventory file. The ibm.mas_devops collection runs against localhost so an Inventory of hosts just requires the one host of localhost to be added but this might be important for any other roles you might want to execute outside of this collection but within this organization. Create an initial inventory if one doesn't exist yet: Ensure that the host entry has the following variables set to ensure a local connection: Execution Environment \u00a4 In Ansible Automation Platform (AAP) you can setup a new Execution Environment by specifying the image and tag. You can use either a versioned tag or latest such as quay.io/ibmmas/ansible-devops-ee:latest or quay.io/ibmmas/ansible-devops-ee:24.0.0 Credentials \u00a4 To use your own playbooks that are in source control management (SCM) you will need to setup credentials in AAP to fetch these (unless the repo is public). An example of this is providing a GitHub fine-grained access token which will just allow the token to read the contents of the repo. \u00a1 creds-2 Depending on if you have an exiting cluster or not, then you will need to setup the OpenShift or Kubernetes API Bearer Token credentials to access the Openshift cluster, which will be required on any Jobs that interect with the cluster. Project \u00a4 Once you have the execution environment setup you can now create a Project that will point to the source of your playbooks. It is recommended that you use your own source of playbooks rather than the playbooks contained in this repo, to give you full control over what is run to suit your needs. Create the Project and set the Execution Environment to be the one created eariler, and set the source control details and credentials needed for AAP to read your playbooks. Job Templates \u00a4 The Job Templates are what each Job is launched from and contains the details of what Playbook to execute. The Job is the executed instance of a Template that contains any specific values required. To create the Template add the Inventory, Project and Execution Environment setup previously. The playbook dropdown should contain all the playbooks that are found in your SCM (if no playbooks are shown then check the Troubleshooting section). At this point you can choose any other settings that you might want to use, and it is recommended to check the AAP documentation on this. Once the Job template is created then you can click on the created template and navigate to Survey . This is where you can provide any secure variables for your playbook that you don't want to keep in source control. You can choose the questions to ask and what type the variable should be. The variable name should make the variable name that the corresponding role is expecting. Finally, ensure that the Survey is Enabled: Workflow Templates \u00a4 The Workflow Templates allow you to configure a number of job templates (or other workflow templates) together. This can be useful when you want to use other roles from other collections (see the AAP Certified Content here ) as well as the ansible-devops collection. For example, you might want to run some AWS or VMWare roles to configure resources after or before the ansible-devops collection roles. How to run a Job \u00a4 Now you have the Job Template setup, you can launch the Job from that Template. Navigate to the Templates view and click the launch icon: Enter any survey questions you have configured: The Job now launches and you can see the output. The list of executing and executed jobs can be seen from the Jobs seciton Examples of playbooks \u00a4 You can use the ansbile-devops playbooks as a reference on how to setup your own playbooks. Defining your own playbooks gives you full control over what roles can be run and also include your own roles or roles from the supported Red Hat collection. If you are using the playbooks as a starting point then please note the following changes that would be required: Remove any pre_tasks related to environment variables Remove any lookups of environment variables from the playbook mas_install_core.yml \u00a4 An example of this is taking the mas_install_core.yml playbook: --- - hosts: localhost any_errors_fatal: true vars: # Install SLS # Note: We need to create some intermediate variables to construct sls_mongodb_cfg_file, # This is the only reason they feature here, all the roles that use these variables would also # load them directly from the same environment variables if they were not defined here. mas_config_dir: \"{{ lookup('env', 'MAS_CONFIG_DIR') }}\" mongodb_namespace: \"{{ lookup('env', 'MONGODB_NAMESPACE') | default('mongoce', True) }}\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" # Core Services Configuration mas_channel: \"{{ lookup('env', 'MAS_CHANNEL') | default('9.1.x', true) }}\" # Workspace Configuration mas_workspace_name: \"{{ lookup('env', 'MAS_WORKSPACE_NAME') | default('MAS Development', true) }}\" mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') | default('masdev', true) }}\" pre_tasks: # For the full set of supported environment variables refer to the playbook documentation - name: Check for required environment variables assert: that: # IBM - lookup('env', 'IBM_ENTITLEMENT_KEY') != \"\" # MAS - lookup('env', 'MAS_INSTANCE_ID') != \"\" - lookup('env', 'MAS_CONFIG_DIR') != \"\" # SLS - (lookup('env', 'SLS_LICENSE_ID') != \"\" and lookup('env', 'SLS_LICENSE_FILE') != \"\") or (lookup('env', 'SLS_ENTITLEMENT_FILE') != \"\") # DRO - lookup('env', 'DRO_CONTACT_EMAIL') != \"\" - lookup('env', 'DRO_CONTACT_FIRSTNAME') != \"\" - lookup('env', 'DRO_CONTACT_LASTNAME') != \"\" fail_msg: \"One or more required environment variables are not defined\" roles: # 1. Install cluster-scoped dependencies (e.g. Cert-Manager, Operator Catalogs) & Grafana - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.cert_manager - ibm.mas_devops.grafana # 2. Install MongoDb - ibm.mas_devops.mongodb # 3. Install SLS # Set sls_url, sls_tls_crt_local_file_path, sls_registration_key variables to skip install and set up SLSCfg for # an existing installation of SLS - ibm.mas_devops.sls # 4 Install DRO - ibm.mas_devops.dro # 5. Generate a Workspace - ibm.mas_devops.gencfg_workspace # 6. Install & configure MAS - ibm.mas_devops.suite_dns - ibm.mas_devops.suite_certs - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify and changing it to look like the following: --- - name: \"mas-core\" hosts: localhost any_errors_fatal: true vars: mas_config_dir: \"/tmp\" mongodb_namespace: \"mongoce\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" # Core Services Configuration mas_channel: \"9.1.x\" mas_instance_id: \"aap1\" # Workspace Configuration mas_workspace_name: \"MAS Development\" mas_workspace_id: \"masdev\" entitlement_file: \"license_file/entitlement.lic\" dro_contact: email: \"whitfiea@uk.ibm.com\" first_name: \"Andrew\" last_name: \"Whitfield\" roles: # 1. Install cluster-scoped dependencies (e.g. Cert-Manager, Operator Catalogs) & Grafana - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.cert_manager # 2. Install MongoDb - ibm.mas_devops.mongodb # 3. Install SLS - ibm.mas_devops.sls # 4 Install DRO - ibm.mas_devops.dro # 5. Generate a Workspace - ibm.mas_devops.gencfg_workspace # 6. Install & configure MAS - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify The above has removed any environment variable lookups in the playbook itself, and also modified the roles so it doesn't run the grafana role. ocp-provision \u00a4 If you need to set an environment variable then you can do this in the playbook as well, this would be needed in certain cases such as setting the AWS cli details. The aws_access_key_id and aws_secret_access_key are variables set by AAP as a result of the Survey questions: --- - name: \"ocp-provision\" hosts: localhost vars: cluster_name: testcluster cluster_type: rosa ocp_version: 4.19.14 rosa_compute_nodes: 3 environment: AWS_DEFAULT_REGION: us-east-1 AWS_ACCESS_KEY_ID: \"{{ aws_access_key_id }}\" AWS_SECRET_ACCESS_KEY: \"{{ aws_secret_access_key }}\" roles: # 1. Provision the ROSA cluster - ibm.mas_devops.ocp_provision # 2. Login and verify the cluster is ready - ibm.mas_devops.ocp_login - ibm.mas_devops.ocp_verify # 3. Set up storage classes - ibm.mas_devops.ocp_efs Troubleshooting \u00a4 Helpful Links \u00a4 Red Hat AAP documentation Installing AAP in OCP Other install methods of AAP AAP Certified Content Can't see all output in log file \u00a4 The output for the job doesn't output all the data in the standard view. To see the expanded data you must click on the entry to see the details and then select JSON: My playbook or updated playbook can't be found \u00a4 If you have updated your Playbook in your SCM but it is not reflected in AAP then you can sync the Project. Navigate to the Project view and click the sync icon so the Project gets the latest revision. Job failed can I re-launch it? \u00a4 If the job fails and you want to relaunch with the same variables (from both hosts and survey), then you can re-launch the job from the Job page or the Jobs list page How to provide supporting files? \u00a4 If you have a file that is not a playbook but you want to reference it, then you can include this in the same repo as your playbooks as AAP will be syncing the whole repo. You can then reference these files from your playbook or roles, using a pth relative to the playbook being run. An exmaple of this is the entitlement_file which can be contained in your repo: and then referenced in your playbook in the path \"license_file/entitlement.lic\" : --- - hosts: localhost any_errors_fatal: true vars: mas_config_dir: \"/tmp\" mongodb_namespace: \"mongoce\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" entitlement_file: \"license_file/entitlement.lic\" roles: - ibm.mas_devops.mongodb - ibm.mas_devops.sls How do I set environment variables? \u00a4 Running in AAP means you can't set environment variables on the ansible controller before a playbook is executed. If a role in the ibm.mas_devops collection has an environment variable to be set there is normally a corresponding ansible variable. For exmaple, in the documentation for (kafka_action)[https://ibm-mas.github.io/ansible-devops/roles/kafka/#kafka_action] in the kafka role it says to set the environment variable KAFKA_VERSION , this is just setting the ansible role varible called kafka_version which you can set in your playbook. With ansible predence in place, the play vars or role vars will override the default vars that use the environment variable. Example setting play vars: --- - hosts: localhost any_errors_fatal: true vars: kafka_version: 3.7.0 roles: - ibm.mas_devops.kafka Example setting role vars: --- - hosts: localhost any_errors_fatal: true roles: - role: ibm.mas_devops.kafka vars: kafka_version: 3.7.0","title":"Ansible Automation Platform"},{"location":"execution-environment/#execution-environment","text":"Details on the Red Hat Ansible Automation Platform Execution Environment for the ibm.mas_devops Ansible Collection.","title":"Execution Environment"},{"location":"execution-environment/#execution-environment-image","text":"The execution environment image for ansible-devops builds on the latest ansible-automation-platform-24/ee-supported-rhel9 image from Red Hat that provides the ansible-core and Red Hat supported collections. The ansible-devops-ee image includes the ibm.mas_devops collection and all required client libraries to function. The image is uploaded to quay.io at quay.io/ibmmas/ansible-devops-ee","title":"Execution Environment Image"},{"location":"execution-environment/#how-to-setup-anisble-automation-platform","text":"","title":"How to setup Anisble Automation Platform"},{"location":"execution-environment/#organization","text":"An Organization is a logical collection of Users, Teams, Projects, and Inventories, and is the highest level in the automation controller object hierarchy. Create an organization if you don't already have one:","title":"Organization"},{"location":"execution-environment/#inventory","text":"An Inventory is a collection of hosts against which jobs may be launched, the same as an Ansible inventory file. The ibm.mas_devops collection runs against localhost so an Inventory of hosts just requires the one host of localhost to be added but this might be important for any other roles you might want to execute outside of this collection but within this organization. Create an initial inventory if one doesn't exist yet: Ensure that the host entry has the following variables set to ensure a local connection:","title":"Inventory"},{"location":"execution-environment/#execution-environment_1","text":"In Ansible Automation Platform (AAP) you can setup a new Execution Environment by specifying the image and tag. You can use either a versioned tag or latest such as quay.io/ibmmas/ansible-devops-ee:latest or quay.io/ibmmas/ansible-devops-ee:24.0.0","title":"Execution Environment"},{"location":"execution-environment/#credentials","text":"To use your own playbooks that are in source control management (SCM) you will need to setup credentials in AAP to fetch these (unless the repo is public). An example of this is providing a GitHub fine-grained access token which will just allow the token to read the contents of the repo. \u00a1 creds-2 Depending on if you have an exiting cluster or not, then you will need to setup the OpenShift or Kubernetes API Bearer Token credentials to access the Openshift cluster, which will be required on any Jobs that interect with the cluster.","title":"Credentials"},{"location":"execution-environment/#project","text":"Once you have the execution environment setup you can now create a Project that will point to the source of your playbooks. It is recommended that you use your own source of playbooks rather than the playbooks contained in this repo, to give you full control over what is run to suit your needs. Create the Project and set the Execution Environment to be the one created eariler, and set the source control details and credentials needed for AAP to read your playbooks.","title":"Project"},{"location":"execution-environment/#job-templates","text":"The Job Templates are what each Job is launched from and contains the details of what Playbook to execute. The Job is the executed instance of a Template that contains any specific values required. To create the Template add the Inventory, Project and Execution Environment setup previously. The playbook dropdown should contain all the playbooks that are found in your SCM (if no playbooks are shown then check the Troubleshooting section). At this point you can choose any other settings that you might want to use, and it is recommended to check the AAP documentation on this. Once the Job template is created then you can click on the created template and navigate to Survey . This is where you can provide any secure variables for your playbook that you don't want to keep in source control. You can choose the questions to ask and what type the variable should be. The variable name should make the variable name that the corresponding role is expecting. Finally, ensure that the Survey is Enabled:","title":"Job Templates"},{"location":"execution-environment/#workflow-templates","text":"The Workflow Templates allow you to configure a number of job templates (or other workflow templates) together. This can be useful when you want to use other roles from other collections (see the AAP Certified Content here ) as well as the ansible-devops collection. For example, you might want to run some AWS or VMWare roles to configure resources after or before the ansible-devops collection roles.","title":"Workflow Templates"},{"location":"execution-environment/#how-to-run-a-job","text":"Now you have the Job Template setup, you can launch the Job from that Template. Navigate to the Templates view and click the launch icon: Enter any survey questions you have configured: The Job now launches and you can see the output. The list of executing and executed jobs can be seen from the Jobs seciton","title":"How to run a Job"},{"location":"execution-environment/#examples-of-playbooks","text":"You can use the ansbile-devops playbooks as a reference on how to setup your own playbooks. Defining your own playbooks gives you full control over what roles can be run and also include your own roles or roles from the supported Red Hat collection. If you are using the playbooks as a starting point then please note the following changes that would be required: Remove any pre_tasks related to environment variables Remove any lookups of environment variables from the playbook","title":"Examples of playbooks"},{"location":"execution-environment/#mas_install_coreyml","text":"An example of this is taking the mas_install_core.yml playbook: --- - hosts: localhost any_errors_fatal: true vars: # Install SLS # Note: We need to create some intermediate variables to construct sls_mongodb_cfg_file, # This is the only reason they feature here, all the roles that use these variables would also # load them directly from the same environment variables if they were not defined here. mas_config_dir: \"{{ lookup('env', 'MAS_CONFIG_DIR') }}\" mongodb_namespace: \"{{ lookup('env', 'MONGODB_NAMESPACE') | default('mongoce', True) }}\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" # Core Services Configuration mas_channel: \"{{ lookup('env', 'MAS_CHANNEL') | default('9.1.x', true) }}\" # Workspace Configuration mas_workspace_name: \"{{ lookup('env', 'MAS_WORKSPACE_NAME') | default('MAS Development', true) }}\" mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') | default('masdev', true) }}\" pre_tasks: # For the full set of supported environment variables refer to the playbook documentation - name: Check for required environment variables assert: that: # IBM - lookup('env', 'IBM_ENTITLEMENT_KEY') != \"\" # MAS - lookup('env', 'MAS_INSTANCE_ID') != \"\" - lookup('env', 'MAS_CONFIG_DIR') != \"\" # SLS - (lookup('env', 'SLS_LICENSE_ID') != \"\" and lookup('env', 'SLS_LICENSE_FILE') != \"\") or (lookup('env', 'SLS_ENTITLEMENT_FILE') != \"\") # DRO - lookup('env', 'DRO_CONTACT_EMAIL') != \"\" - lookup('env', 'DRO_CONTACT_FIRSTNAME') != \"\" - lookup('env', 'DRO_CONTACT_LASTNAME') != \"\" fail_msg: \"One or more required environment variables are not defined\" roles: # 1. Install cluster-scoped dependencies (e.g. Cert-Manager, Operator Catalogs) & Grafana - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.cert_manager - ibm.mas_devops.grafana # 2. Install MongoDb - ibm.mas_devops.mongodb # 3. Install SLS # Set sls_url, sls_tls_crt_local_file_path, sls_registration_key variables to skip install and set up SLSCfg for # an existing installation of SLS - ibm.mas_devops.sls # 4 Install DRO - ibm.mas_devops.dro # 5. Generate a Workspace - ibm.mas_devops.gencfg_workspace # 6. Install & configure MAS - ibm.mas_devops.suite_dns - ibm.mas_devops.suite_certs - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify and changing it to look like the following: --- - name: \"mas-core\" hosts: localhost any_errors_fatal: true vars: mas_config_dir: \"/tmp\" mongodb_namespace: \"mongoce\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" # Core Services Configuration mas_channel: \"9.1.x\" mas_instance_id: \"aap1\" # Workspace Configuration mas_workspace_name: \"MAS Development\" mas_workspace_id: \"masdev\" entitlement_file: \"license_file/entitlement.lic\" dro_contact: email: \"whitfiea@uk.ibm.com\" first_name: \"Andrew\" last_name: \"Whitfield\" roles: # 1. Install cluster-scoped dependencies (e.g. Cert-Manager, Operator Catalogs) & Grafana - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.cert_manager # 2. Install MongoDb - ibm.mas_devops.mongodb # 3. Install SLS - ibm.mas_devops.sls # 4 Install DRO - ibm.mas_devops.dro # 5. Generate a Workspace - ibm.mas_devops.gencfg_workspace # 6. Install & configure MAS - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify The above has removed any environment variable lookups in the playbook itself, and also modified the roles so it doesn't run the grafana role.","title":"mas_install_core.yml"},{"location":"execution-environment/#ocp-provision","text":"If you need to set an environment variable then you can do this in the playbook as well, this would be needed in certain cases such as setting the AWS cli details. The aws_access_key_id and aws_secret_access_key are variables set by AAP as a result of the Survey questions: --- - name: \"ocp-provision\" hosts: localhost vars: cluster_name: testcluster cluster_type: rosa ocp_version: 4.19.14 rosa_compute_nodes: 3 environment: AWS_DEFAULT_REGION: us-east-1 AWS_ACCESS_KEY_ID: \"{{ aws_access_key_id }}\" AWS_SECRET_ACCESS_KEY: \"{{ aws_secret_access_key }}\" roles: # 1. Provision the ROSA cluster - ibm.mas_devops.ocp_provision # 2. Login and verify the cluster is ready - ibm.mas_devops.ocp_login - ibm.mas_devops.ocp_verify # 3. Set up storage classes - ibm.mas_devops.ocp_efs","title":"ocp-provision"},{"location":"execution-environment/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"execution-environment/#helpful-links","text":"Red Hat AAP documentation Installing AAP in OCP Other install methods of AAP AAP Certified Content","title":"Helpful Links"},{"location":"execution-environment/#cant-see-all-output-in-log-file","text":"The output for the job doesn't output all the data in the standard view. To see the expanded data you must click on the entry to see the details and then select JSON:","title":"Can't see all output in log file"},{"location":"execution-environment/#my-playbook-or-updated-playbook-cant-be-found","text":"If you have updated your Playbook in your SCM but it is not reflected in AAP then you can sync the Project. Navigate to the Project view and click the sync icon so the Project gets the latest revision.","title":"My playbook or updated playbook can't be found"},{"location":"execution-environment/#job-failed-can-i-re-launch-it","text":"If the job fails and you want to relaunch with the same variables (from both hosts and survey), then you can re-launch the job from the Job page or the Jobs list page","title":"Job failed can I re-launch it?"},{"location":"execution-environment/#how-to-provide-supporting-files","text":"If you have a file that is not a playbook but you want to reference it, then you can include this in the same repo as your playbooks as AAP will be syncing the whole repo. You can then reference these files from your playbook or roles, using a pth relative to the playbook being run. An exmaple of this is the entitlement_file which can be contained in your repo: and then referenced in your playbook in the path \"license_file/entitlement.lic\" : --- - hosts: localhost any_errors_fatal: true vars: mas_config_dir: \"/tmp\" mongodb_namespace: \"mongoce\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" entitlement_file: \"license_file/entitlement.lic\" roles: - ibm.mas_devops.mongodb - ibm.mas_devops.sls","title":"How to provide supporting files?"},{"location":"execution-environment/#how-do-i-set-environment-variables","text":"Running in AAP means you can't set environment variables on the ansible controller before a playbook is executed. If a role in the ibm.mas_devops collection has an environment variable to be set there is normally a corresponding ansible variable. For exmaple, in the documentation for (kafka_action)[https://ibm-mas.github.io/ansible-devops/roles/kafka/#kafka_action] in the kafka role it says to set the environment variable KAFKA_VERSION , this is just setting the ansible role varible called kafka_version which you can set in your playbook. With ansible predence in place, the play vars or role vars will override the default vars that use the environment variable. Example setting play vars: --- - hosts: localhost any_errors_fatal: true vars: kafka_version: 3.7.0 roles: - ibm.mas_devops.kafka Example setting role vars: --- - hosts: localhost any_errors_fatal: true roles: - role: ibm.mas_devops.kafka vars: kafka_version: 3.7.0","title":"How do I set environment variables?"},{"location":"playbooks/aiservice/","text":"Install AI Service \u00a4 Important These playbooks are samples to demonstrate how to use the roles in this collection. They are note intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install MAS is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance. Dependencies \u00a4 IBM Suite License Service installed on OCP cluster or external instance or details from external instance IBM Data Reporter Operator installed on OCP cluster or external instance or details from external instance Object Storage Minio (installed on the same cluster what aiservice) or external instance or details from external instance AWS S3 (if customer use AWS S3 bucket bucket) buckets needs to have unique names Overview \u00a4 This playbook will add AI Service v9.1.x to OCP cluster. This playbook can be ran against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: IBM Maximo Operator Catalog optional RedHat Certificate Manager optional MongoDb optional IBM Suite License Service (~10 Minutes) optional IBM Data Reporter Operator (~10 Minutes) optional IBM Db2 optional Minio (~5 minutes) optional Install ODH: Install Red Hat OpenShift Serverless Operator Install Red Hat OpenShift Service Mesh Operator Install Authorino Operator Install Open Data Hub Operator Create DSCInitialization instance Create Data Science Cluster Create Create Data Science Pipelines Application Install AI Service (using playbook): Install application (~20 Minutes) Configure AI Service (kmodels, tenant, etc) (~20 Minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing application, just customize the application install and configure stages at the end of the playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the AI service install MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry MAS_ENTITLEMENT_USERNAME Your IBM Entitlement user to access the IBM Container Registry MAS_APP_CHANNEL Aiservice application channel AISERVICE_S3_ACCESSKEY Your strage provider access key AISERVICE_S3_SECRETKEY Your storage provider secret key AISERVICE_S3_HOST Your storage provider host AISERVICE_S3_REGION Your storage provider region - only when use AWS S3 instance AISERVICE_S3_SSL Your storage ssl (true/false) AISERVICE_S3_TENANTS_BUCKET Your tenants bucket AISERVICE_S3_TEMPLATES_BUCKET Your templates bucket AISERVICE_WATSONXAI_APIKEY You WatsonX AI api key AISERVICE_WATSONXAI_URL You WatsonX AI url AISERVICE_WATSONXAI_PROJECT_ID You WatsonX projedt Id Tip AI service supports AWS and Minio storage providers. Required environment variables (SaaS) \u00a4 AISERVICE_SAAS specify if saas deployment (default value is: false) MAS_CONFIG_DIR specify config location, mandatory when AISERVICE_SAAS=true AISERVICE_DOMAIN specify cluster domain, mandatory when AISERVICE_SAAS=true AISERVICE_SLS_URL specify SLS url, mandatory when AISERVICE_SAAS=true AISERVICE_SLS_REGISTRATION_KEY specify sls registration key, mandatory when AISERVICE_SAAS=true , to get value: look in ibm-sls namespace, pod sls-api-licensing-xxx and in Environment tab check REGISTRATION_KEY value AISERVICE_DRO_URL specify DRO url, mandatory when AISERVICE_SAAS=true AISERVICE_DRO_TOKEN specify DRO token, mandatory when AISERVICE_SAAS=true to get value: go to mas-{{ instance_id }}-core and look in secret dro-apikey DB2_INSTANCE_NAME specify DB2 instance name (default value is: aiservice), mandatory when AISERVICE_SAAS=true IBM_ENTITLEMENT_KEY specify IBM Entitlement key, mandatory when AISERVICE_SAAS=true Optional environment variables \u00a4 MAS_ICR_CP Provide custom registry for AI service applications MAS_ICR_CPOPEN Provide custom registry for AI service operator MAS_CATALOG_VERSION Your custom AI service catalog version ARTIFACTORY_USERNAME Your artifactory user name to access - this is needed if user deploy from custom registry for example docker-na-public.artifactory.swg-devops.com ARTIFACTORY_TOKEN Your artifactory token for user to access - this is needed if user deploy from custom registry for example docker-na-public.artifactory.swg-devops.com AISERVICE_TENANT_ACTION Whether to install or remove tenant (default value is: install) AISERVICE_APIKEY_ACTION Whether to install or remove or update apikey (default value is: install) AISERVICE_WATSONX_ACTION Whether to install or remove watsonx secret (default value is: install) AISERVICE_S3_ACTION Whether to install or remove s3 (default value is: install) INSTALL_DB2 Whether to install DB2 (default value is: false) INSTALL_MINIO Whether to install minio (default value is: false) INSTALL_SLS Whether to install IBM Suite License Service (default value is: false) INSTALL_DRO Whether to install IBM Data Reporter Operator (default value is: false) AISERVICE_DB2_USERNAME The username to use for authentication with the database AISERVICE_DB2_PASSWORD The password to use for authentication with the database AISERVICE_DB2_JDBC_URL The JDBC URL specifying the host and port of the database, typically in the format jdbc:db2://host:port/ AISERVICE_DB2_SSL_ENABLED A flag indicating whether to enable SSL encryption for the database connection (default value is: true) USE_AWS_DB2 A flag indicating whether to use an AWS-hosted DB2 instance (default value is: false) AISERVICE_DOMAIN Provide custom domain (default value is: empty) AISERVICE_WATSONXAI_CA_CRT provide WatsonX AI CA certificate AISERVICE_WATSONXAI_FULL optional on prem to define if WatsonX AI engine is full or light (true/false) AISERVICE_WATSONXAI_DEPLOYMENT_ID optional on prem define deployment Id AISERVICE_WATSONXAI_SPACE_ID optional on prem define space Id AISERVICE_WATSONXAI_INSTANCE_ID optional on prem define instance id (default: openshift) AISERVICE_WATSONXAI_USERNAME optional on prem define user name AISERVICE_WATSONXAI_VERSION optional on prem define version of CPD Usage \u00a4 AI service deployment steps \u00a4 Tip For S3 manage please make sure you have deployed dependencies Install boto3 python module (use python environment): python3 -m venv /tmp/venv source /tmp/venv/bin/activate python3 -m pip install boto3 Run playbooks for deploy AI service: AISERVICE_SLS_REGISTRATION_KEY - value can be found in ibm-sls namespace, in pod sls-api-licensing-85699fb57-9lmrq please look in environments tab, then value REGISTRATION_KEY AISERVICE_DRO_TOKEN - go to mas-instance_id-core namespace and in secrets find dro-apikey In AWS for AISERVICE_S3_TENANTS_BUCKET , AISERVICE_S3_TEMPLATES_BUCKET user need to create S3 buckets with unique name export ARTIFACTORY_USERNAME=\"\" export ARTIFACTORY_TOKEN=\"\" export MAS_ICR_CP=\"\" export MAS_ICR_CPOPEN=\"\" export MAS_ENTITLEMENT_USERNAME=\"\" export MAS_ENTITLEMENT_KEY=\"\" export MAS_INSTANCE_ID=\"\" export MAS_APP_CHANNEL=\"\" export MAS_CATALOG_VERSION=\"\" export IBM_ENTITLEMENT_KEY=${MAS_ENTITLEMENT_KEY} export MAS_CONFIG_DIR=\"\" export DRO_CONTACT_EMAIL=\"\" export DRO_CONTACT_FIRSTNAME=\"\" export DRO_CONTACT_LASTNAME=\"\" export SLS_MONGODB_CFG_FILE=${MAS_CONFIG_DIR}/mongo-mongoce.yml export SLS_LICENSE_ID=\"\" export SLS_LICENSE_FILE=\"\" export INSTALL_DB2=\"\" export INSTALL_MINIO=\"\" export INSTALL_MONGO=\"\" export INSTALL_SLS=\"\" export INSTALL_DRO=\"\" export AISERVICE_S3_BUCKET_PREFIX=\"\" export AISERVICE_S3_REGION=\"\" export AISERVICE_S3_ENDPOINT_URL=\"\" export AISERVICE_TENANT_S3_REGION=\"\" export AISERVICE_TENANT_S3_ENDPOINT_URL=\"\" export AISERVICE_TENANT_S3_BUCKET_PREFIX=\"\" export AISERVICE_TENANT_S3_ACCESS_KEY=\"\" export AISERVICE_TENANT_S3_SECRET_KEY=\"\" export RSL_URL=\"\" export RSL_ORG_ID=\"\" export RSL_TOKEN=\"\" export MINIO_ROOT_PASSWORD=\"\" export AISERVICE_S3_ACCESSKEY=\"\" export AISERVICE_S3_SECRETKEY=${MINIO_ROOT_PASSWORD} export AISERVICE_S3_HOST=\"\" export AISERVICE_S3_SSL=\"\" export AISERVICE_S3_PROVIDER=\"\" export AISERVICE_S3_PORT=\"\" export AISERVICE_S3_REGION=\"\" export AISERVICE_S3_TENANTS_BUCKET=\"\" export AISERVICE_S3_TEMPLATES_BUCKET=\"\" export AISERVICE_WATSONXAI_APIKEY=\"\" export AISERVICE_WATSONXAI_URL=\"\" export AISERVICE_WATSONXAI_PROJECT_ID=\"\" export AISERVICE_SUBSCRIPTION_ID=\"\" export AISERVICE_DRO_TENANT_ID=\"\" export AISERVICE_TENANT_ENTITLEMENT_START_DATE=\"YYYY-MM-DD\" export AISERVICE_TENANT_ENTITLEMENT_END_DATE=\"YYYY-MM-DD\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/aiservice.yml Create S3 \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_S3_ACCESSKEY=\"<storage provider access key>\" export AISERVICE_S3_SECRETKEY=\"<storage provider secret key>\" export AISERVICE_S3_HOST=\"<storage provider host>\" export AISERVICE_S3_REGION=\"<storage provider region>\" export AISERVICE_S3_ACTION=\"install\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Delete S3 \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_S3_ACCESSKEY=\"<storage provider access key>\" export AISERVICE_S3_SECRETKEY=\"<storage provider secret key>\" export AISERVICE_S3_HOST=\"<storage provider host>\" export AISERVICE_S3_REGION=\"<storage provider region>\" export AISERVICE_S3_ACTION=\"remove\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Create API Key \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_APIKEY_ACTION=\"install\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Delete API Key \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_APIKEY_ACTION=\"remove\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Create WatsonX API Key \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_WATSONX_ACTION=\"install\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Delete WatsonX API Key \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_WATSONX_ACTION=\"remove\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Create Tenant \u00a4 The AISERVICE_SLS_REGISTRATION_KEY value can be found in ibm-sls namespace, in pod sls-api-licensing-85699fb57-9lmrq please look in environments tab, then value REGISTRATION_KEY . To obtain the AISERVICE_DRO_TOKEN go to mas-instance_id-core namespace and in secrets find dro-apikey export AISERVICE_TENANT_NAME=\"user7\" export AISERVICE_SLS_SUBSCRIPTION_ID=\"007\" export TENANT_ACTION=\"install\" export ROLE_NAME=\"aiservice_tenant\" export AISERVICE_SAAS=\"true\" export AISERVICE_DOMAIN=\"\" export AISERVICE_SLS_URL=\"https://sls.ibm-sls.ibm-sls.\"${AISERVICE_DOMAIN} export AISERVICE_SLS_REGISTRATION_KEY=\"\" export AISERVICE_DRO_URL=\"https://ibm-data-reporter-redhat-marketplace.\"${AISERVICE_DOMAIN} export AISERVICE_DRO_TOKEN=\"\" export AISERVICE_SLS_CACERT=\"\" export AISERVICE_DRO_CACERT=\"\" export AISERVICE_WATSONXAI_APIKEY=\"\" export AISERVICE_WATSONXAI_URL=\"\" export AISERVICE_WATSONXAI_PROJECT_ID=\"\" export AISERVICE_S3_ACCESSKEY=\"\" export AISERVICE_S3_SECRETKEY=\"\" export AISERVICE_S3_HOST=\"\" export AISERVICE_S3_SSL=\"\" export AISERVICE_S3_PROVIDER=\"\" export AISERVICE_S3_PORT=\"\" export AISERVICE_S3_REGION=\"\" export AISERVICE_S3_TENANTS_BUCKET=\"\" export AISERVICE_S3_TEMPLATES_BUCKET=\"\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Tip To create addidional tenants we don't need to specify buckets","title":"Install AI Service"},{"location":"playbooks/aiservice/#install-ai-service","text":"Important These playbooks are samples to demonstrate how to use the roles in this collection. They are note intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install MAS is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance.","title":"Install AI Service"},{"location":"playbooks/aiservice/#dependencies","text":"IBM Suite License Service installed on OCP cluster or external instance or details from external instance IBM Data Reporter Operator installed on OCP cluster or external instance or details from external instance Object Storage Minio (installed on the same cluster what aiservice) or external instance or details from external instance AWS S3 (if customer use AWS S3 bucket bucket) buckets needs to have unique names","title":"Dependencies"},{"location":"playbooks/aiservice/#overview","text":"This playbook will add AI Service v9.1.x to OCP cluster. This playbook can be ran against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: IBM Maximo Operator Catalog optional RedHat Certificate Manager optional MongoDb optional IBM Suite License Service (~10 Minutes) optional IBM Data Reporter Operator (~10 Minutes) optional IBM Db2 optional Minio (~5 minutes) optional Install ODH: Install Red Hat OpenShift Serverless Operator Install Red Hat OpenShift Service Mesh Operator Install Authorino Operator Install Open Data Hub Operator Create DSCInitialization instance Create Data Science Cluster Create Create Data Science Pipelines Application Install AI Service (using playbook): Install application (~20 Minutes) Configure AI Service (kmodels, tenant, etc) (~20 Minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/aiservice/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the AI service install MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry MAS_ENTITLEMENT_USERNAME Your IBM Entitlement user to access the IBM Container Registry MAS_APP_CHANNEL Aiservice application channel AISERVICE_S3_ACCESSKEY Your strage provider access key AISERVICE_S3_SECRETKEY Your storage provider secret key AISERVICE_S3_HOST Your storage provider host AISERVICE_S3_REGION Your storage provider region - only when use AWS S3 instance AISERVICE_S3_SSL Your storage ssl (true/false) AISERVICE_S3_TENANTS_BUCKET Your tenants bucket AISERVICE_S3_TEMPLATES_BUCKET Your templates bucket AISERVICE_WATSONXAI_APIKEY You WatsonX AI api key AISERVICE_WATSONXAI_URL You WatsonX AI url AISERVICE_WATSONXAI_PROJECT_ID You WatsonX projedt Id Tip AI service supports AWS and Minio storage providers.","title":"Required environment variables"},{"location":"playbooks/aiservice/#required-environment-variables-saas","text":"AISERVICE_SAAS specify if saas deployment (default value is: false) MAS_CONFIG_DIR specify config location, mandatory when AISERVICE_SAAS=true AISERVICE_DOMAIN specify cluster domain, mandatory when AISERVICE_SAAS=true AISERVICE_SLS_URL specify SLS url, mandatory when AISERVICE_SAAS=true AISERVICE_SLS_REGISTRATION_KEY specify sls registration key, mandatory when AISERVICE_SAAS=true , to get value: look in ibm-sls namespace, pod sls-api-licensing-xxx and in Environment tab check REGISTRATION_KEY value AISERVICE_DRO_URL specify DRO url, mandatory when AISERVICE_SAAS=true AISERVICE_DRO_TOKEN specify DRO token, mandatory when AISERVICE_SAAS=true to get value: go to mas-{{ instance_id }}-core and look in secret dro-apikey DB2_INSTANCE_NAME specify DB2 instance name (default value is: aiservice), mandatory when AISERVICE_SAAS=true IBM_ENTITLEMENT_KEY specify IBM Entitlement key, mandatory when AISERVICE_SAAS=true","title":"Required environment variables (SaaS)"},{"location":"playbooks/aiservice/#optional-environment-variables","text":"MAS_ICR_CP Provide custom registry for AI service applications MAS_ICR_CPOPEN Provide custom registry for AI service operator MAS_CATALOG_VERSION Your custom AI service catalog version ARTIFACTORY_USERNAME Your artifactory user name to access - this is needed if user deploy from custom registry for example docker-na-public.artifactory.swg-devops.com ARTIFACTORY_TOKEN Your artifactory token for user to access - this is needed if user deploy from custom registry for example docker-na-public.artifactory.swg-devops.com AISERVICE_TENANT_ACTION Whether to install or remove tenant (default value is: install) AISERVICE_APIKEY_ACTION Whether to install or remove or update apikey (default value is: install) AISERVICE_WATSONX_ACTION Whether to install or remove watsonx secret (default value is: install) AISERVICE_S3_ACTION Whether to install or remove s3 (default value is: install) INSTALL_DB2 Whether to install DB2 (default value is: false) INSTALL_MINIO Whether to install minio (default value is: false) INSTALL_SLS Whether to install IBM Suite License Service (default value is: false) INSTALL_DRO Whether to install IBM Data Reporter Operator (default value is: false) AISERVICE_DB2_USERNAME The username to use for authentication with the database AISERVICE_DB2_PASSWORD The password to use for authentication with the database AISERVICE_DB2_JDBC_URL The JDBC URL specifying the host and port of the database, typically in the format jdbc:db2://host:port/ AISERVICE_DB2_SSL_ENABLED A flag indicating whether to enable SSL encryption for the database connection (default value is: true) USE_AWS_DB2 A flag indicating whether to use an AWS-hosted DB2 instance (default value is: false) AISERVICE_DOMAIN Provide custom domain (default value is: empty) AISERVICE_WATSONXAI_CA_CRT provide WatsonX AI CA certificate AISERVICE_WATSONXAI_FULL optional on prem to define if WatsonX AI engine is full or light (true/false) AISERVICE_WATSONXAI_DEPLOYMENT_ID optional on prem define deployment Id AISERVICE_WATSONXAI_SPACE_ID optional on prem define space Id AISERVICE_WATSONXAI_INSTANCE_ID optional on prem define instance id (default: openshift) AISERVICE_WATSONXAI_USERNAME optional on prem define user name AISERVICE_WATSONXAI_VERSION optional on prem define version of CPD","title":"Optional environment variables"},{"location":"playbooks/aiservice/#usage","text":"","title":"Usage"},{"location":"playbooks/aiservice/#ai-service-deployment-steps","text":"Tip For S3 manage please make sure you have deployed dependencies Install boto3 python module (use python environment): python3 -m venv /tmp/venv source /tmp/venv/bin/activate python3 -m pip install boto3 Run playbooks for deploy AI service: AISERVICE_SLS_REGISTRATION_KEY - value can be found in ibm-sls namespace, in pod sls-api-licensing-85699fb57-9lmrq please look in environments tab, then value REGISTRATION_KEY AISERVICE_DRO_TOKEN - go to mas-instance_id-core namespace and in secrets find dro-apikey In AWS for AISERVICE_S3_TENANTS_BUCKET , AISERVICE_S3_TEMPLATES_BUCKET user need to create S3 buckets with unique name export ARTIFACTORY_USERNAME=\"\" export ARTIFACTORY_TOKEN=\"\" export MAS_ICR_CP=\"\" export MAS_ICR_CPOPEN=\"\" export MAS_ENTITLEMENT_USERNAME=\"\" export MAS_ENTITLEMENT_KEY=\"\" export MAS_INSTANCE_ID=\"\" export MAS_APP_CHANNEL=\"\" export MAS_CATALOG_VERSION=\"\" export IBM_ENTITLEMENT_KEY=${MAS_ENTITLEMENT_KEY} export MAS_CONFIG_DIR=\"\" export DRO_CONTACT_EMAIL=\"\" export DRO_CONTACT_FIRSTNAME=\"\" export DRO_CONTACT_LASTNAME=\"\" export SLS_MONGODB_CFG_FILE=${MAS_CONFIG_DIR}/mongo-mongoce.yml export SLS_LICENSE_ID=\"\" export SLS_LICENSE_FILE=\"\" export INSTALL_DB2=\"\" export INSTALL_MINIO=\"\" export INSTALL_MONGO=\"\" export INSTALL_SLS=\"\" export INSTALL_DRO=\"\" export AISERVICE_S3_BUCKET_PREFIX=\"\" export AISERVICE_S3_REGION=\"\" export AISERVICE_S3_ENDPOINT_URL=\"\" export AISERVICE_TENANT_S3_REGION=\"\" export AISERVICE_TENANT_S3_ENDPOINT_URL=\"\" export AISERVICE_TENANT_S3_BUCKET_PREFIX=\"\" export AISERVICE_TENANT_S3_ACCESS_KEY=\"\" export AISERVICE_TENANT_S3_SECRET_KEY=\"\" export RSL_URL=\"\" export RSL_ORG_ID=\"\" export RSL_TOKEN=\"\" export MINIO_ROOT_PASSWORD=\"\" export AISERVICE_S3_ACCESSKEY=\"\" export AISERVICE_S3_SECRETKEY=${MINIO_ROOT_PASSWORD} export AISERVICE_S3_HOST=\"\" export AISERVICE_S3_SSL=\"\" export AISERVICE_S3_PROVIDER=\"\" export AISERVICE_S3_PORT=\"\" export AISERVICE_S3_REGION=\"\" export AISERVICE_S3_TENANTS_BUCKET=\"\" export AISERVICE_S3_TEMPLATES_BUCKET=\"\" export AISERVICE_WATSONXAI_APIKEY=\"\" export AISERVICE_WATSONXAI_URL=\"\" export AISERVICE_WATSONXAI_PROJECT_ID=\"\" export AISERVICE_SUBSCRIPTION_ID=\"\" export AISERVICE_DRO_TENANT_ID=\"\" export AISERVICE_TENANT_ENTITLEMENT_START_DATE=\"YYYY-MM-DD\" export AISERVICE_TENANT_ENTITLEMENT_END_DATE=\"YYYY-MM-DD\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/aiservice.yml","title":"AI service deployment steps"},{"location":"playbooks/aiservice/#create-s3","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_S3_ACCESSKEY=\"<storage provider access key>\" export AISERVICE_S3_SECRETKEY=\"<storage provider secret key>\" export AISERVICE_S3_HOST=\"<storage provider host>\" export AISERVICE_S3_REGION=\"<storage provider region>\" export AISERVICE_S3_ACTION=\"install\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Create S3"},{"location":"playbooks/aiservice/#delete-s3","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_S3_ACCESSKEY=\"<storage provider access key>\" export AISERVICE_S3_SECRETKEY=\"<storage provider secret key>\" export AISERVICE_S3_HOST=\"<storage provider host>\" export AISERVICE_S3_REGION=\"<storage provider region>\" export AISERVICE_S3_ACTION=\"remove\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Delete S3"},{"location":"playbooks/aiservice/#create-api-key","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_APIKEY_ACTION=\"install\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Create API Key"},{"location":"playbooks/aiservice/#delete-api-key","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_APIKEY_ACTION=\"remove\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Delete API Key"},{"location":"playbooks/aiservice/#create-watsonx-api-key","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_WATSONX_ACTION=\"install\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Create WatsonX API Key"},{"location":"playbooks/aiservice/#delete-watsonx-api-key","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export AISERVICE_WATSONX_ACTION=\"remove\" export ROLE_NAME=\"aiservice\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Delete WatsonX API Key"},{"location":"playbooks/aiservice/#create-tenant","text":"The AISERVICE_SLS_REGISTRATION_KEY value can be found in ibm-sls namespace, in pod sls-api-licensing-85699fb57-9lmrq please look in environments tab, then value REGISTRATION_KEY . To obtain the AISERVICE_DRO_TOKEN go to mas-instance_id-core namespace and in secrets find dro-apikey export AISERVICE_TENANT_NAME=\"user7\" export AISERVICE_SLS_SUBSCRIPTION_ID=\"007\" export TENANT_ACTION=\"install\" export ROLE_NAME=\"aiservice_tenant\" export AISERVICE_SAAS=\"true\" export AISERVICE_DOMAIN=\"\" export AISERVICE_SLS_URL=\"https://sls.ibm-sls.ibm-sls.\"${AISERVICE_DOMAIN} export AISERVICE_SLS_REGISTRATION_KEY=\"\" export AISERVICE_DRO_URL=\"https://ibm-data-reporter-redhat-marketplace.\"${AISERVICE_DOMAIN} export AISERVICE_DRO_TOKEN=\"\" export AISERVICE_SLS_CACERT=\"\" export AISERVICE_DRO_CACERT=\"\" export AISERVICE_WATSONXAI_APIKEY=\"\" export AISERVICE_WATSONXAI_URL=\"\" export AISERVICE_WATSONXAI_PROJECT_ID=\"\" export AISERVICE_S3_ACCESSKEY=\"\" export AISERVICE_S3_SECRETKEY=\"\" export AISERVICE_S3_HOST=\"\" export AISERVICE_S3_SSL=\"\" export AISERVICE_S3_PROVIDER=\"\" export AISERVICE_S3_PORT=\"\" export AISERVICE_S3_REGION=\"\" export AISERVICE_S3_TENANTS_BUCKET=\"\" export AISERVICE_S3_TEMPLATES_BUCKET=\"\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml Tip To create addidional tenants we don't need to specify buckets","title":"Create Tenant"},{"location":"playbooks/backup-restore/","text":"Backup and Restore \u00a4 Overview \u00a4 MAS Devops Collection includes playbooks for backing up and restoring of the following MAS components and their dependencies: MongoDB Db2 MAS Core Manage IoT Monitor Health Optimizer Visual Inspection Creation of both full and incremental backups are supported. The backup and restore Ansible roles can also be used individually, allowing you to build your own customized backup and restore playbook covering exactly what you need. For example, you can only backup/restore Manage attachments . Important The backup and restore playbooks in this collection are still work in progress, they are not suitable for production use at this time. You may track development progress using the Backup & Restore label in the Github repository. Production-ready backup and restore options are detailed in the Backup and restore topic in the product documentation. Configuration - Storage \u00a4 You can save the backup files to a folder on your local file system by setting the following environment variables: Envrionment variable Required (Default Value) Description MASBR_STORAGE_LOCAL_FOLDER Yes The local path to save the backup files MASBR_LOCAL_TEMP_FOLDER No ( /tmp/masbr ) Local folder for saving the temporary backup/restore data, the data in this folder will be deleted after the backup/restore job completed. Configuration - Backup \u00a4 Envrionment variable Required (Default Value) Description MASBR_ACTION Yes Whether to run the playbook to perform a backup or a restore MASBR_BACKUP_TYPE No ( full ) Set full or incr to indicate the playbook to create a full backup or incremental backup. MASBR_BACKUP_FROM_VERSION No Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). The playbooks are switched to backup mode by setting MASBR_ACTION to backup . Full Backups \u00a4 If you set environment variable MASBR_BACKUP_TYPE=full or do not specify a value for this variable, the playbook will take a full backup. Incremental Backups \u00a4 You can set environment variable MASBR_BACKUP_TYPE=incr to indicate the playbook to take an incremental backup. Important Only supports creating incremental backup for MonogDB, Db2 and persistent volume data. The playbook will always create a full backup for other type of data regardless of whether this variable be set to incr . The environment variable MASBR_BACKUP_FROM_VERSION is only valid if MASBR_BACKUP_TYPE=incr . It indicates which backup version that the incremental backup to based on. If you do not set a value for this variable, the playbook will try to find the latest Completed Full backup from the specified storage location, and then take an incremental backup based on it. Important The backup files you specified by MASBR_BACKUP_FROM_VERSION must be a Full backup. And the component name and data types in the specified Full backup file must be same as the current incremental backup job. Configuration - Restore \u00a4 Envrionment variable Required (Default Value) Description MASBR_ACTION Yes Whether to run the playbook to perform a backup or a restore MASBR_RESTORE_FROM_VERSION Yes Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) MASBR_RESTORE_OVERWRITE Yes Set whether the restore should overwrite any existing data or if we should stop and FAIL if there is data detected in the directory. WARNING: This will overwrite all data when restoring! The playbooks are switched to restore mode by setting MASBR_ACTION to restore . You must specify the MASBR_RESTORE_FROM_VERSION environment variable to indicate which version of the backup files to use. In the case of restoring from an incremental backup, the corresponding full backup will be restored first before continuing to restore the incremental backup. Backup/Restore for MongoDB \u00a4 This playbook ibm.mas_devops.br_mongodb will invoke the role mongodb to backup/restore the MongoDB databases. This playbook supports backing up and restoring databases for an in-cluster MongoDB CE instance. If you are using other MongoDB venders, such as IBM Cloud Databases for MongoDB, Amazon DocumentDB or MongoDB Altas Database, please refer to the corresponding vender's documentation for more information about their provided backup/restore service. Environment Variables \u00a4 MONGODB_NAMESPACE : By default the backup and restore processes will use a namespace of mongoce , if you have customized the install of MongoDb CE you must set this environment variable to the appropriate namespace you wish to backup from/restore to. MAS_INSTANCE_ID : Required . This playbook supports backup/restore MongoDB databases that belong to a specific MAS instance, call the playbook multiple times with different values for MAS_INSTANCE_ID if you wish to back up multiple MAS instances that use the same MongoDB CE instance. MAS_APP_ID : Optional . By default, this playbook will backup all databases belonging to the specified MAS instance. You can backup the databases only belong to a specific MAS application by setting this environment variable to a supported MAS application id core , manage , iot , monitor , health , optimizer or visualinspection . Examples \u00a4 # Full backup all MongoDB data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_mongodb # Incremental backup all MongoDB data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_mongodb # Restore all MongoDB data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_mongodb # Backup just the IoT MongoDB data for the dev2 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev2 export MAS_APP_ID=iot ansible-playbook ibm.mas_devops.br_mongodb Backup/Restore for Db2 \u00a4 This playbook ibm.mas_devops.br_db2 will invoke the role db2 to backup/restore a single Db2 instance. Environment Variables \u00a4 DB2_INSTANCE_NAME : Required This playbook only supports backing up specific Db2 instance at a time. If you want to backup all Db2 instances in the Db2 cluster, you need to run this playbook multiple times with different value of this environment variable. MAS_INSTANCE_ID : Required Set the instance ID for the MAS install. MASBR_ACTION : Required Set the action to be performed, backup or restore . MASBR_STORAGE_LOCAL_FOLDER : Required Set the local path to the directory to be used for backup and restore. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u Examples \u00a4 # Full backup for the db2w-shared Db2 instance export MAS_INSTANCE_ID=dev export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 # Incremental backup for the db2w-shared Db2 instance export MAS_INSTANCE_ID=dev export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 # Restore for the db2w-shared Db2 instance export MAS_INSTANCE_ID=dev export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 Backup/Restore for MAS Core \u00a4 This playbook ibm.mas_devops.br_core will backup the following components that MAS Core depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core core suite_backup_restore MAS Core namespace resources Environment Variables \u00a4 MAS_INSTANCE_ID Required . The MAS instance ID to perform a backup for. Examples \u00a4 # Full backup all core data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_core # Incremental backup all core data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_core # Restore all core data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_core Backup/Restore for Manage \u00a4 This playbook ibm.mas_devops.br_manage will backup the following components that Manage depends on in order: Component Role Data included mongodb mongodb MongoDB databases used by MAS Core db2 db2 Db2 instance used by Manage core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Optional . When defined, this playbook will backup the Db2 instance used by Manage. DB2 role is skipped when environment variable is not defined.. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u Examples \u00a4 # Full backup all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage # Incremental backup all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage # Restore all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage Backup/Restore for IoT \u00a4 This playbook ibm.mas_devops.br_iot will backup the following components that IoT depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and IoT db2 db2 Db2 instance used by IoT core suite_backup_restore MAS Core namespace resources iot suite_app_backup_restore IoT namespace resources Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by IoT, you need to set the correct Db2 instance name for this environment variable. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u Examples \u00a4 # Full backup all iot data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot # Incremental backup all iot data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot # Restore all iot data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot Backup/Restore for Monitor \u00a4 This playbook ibm.mas_devops.br_monitor will backup the following components that Monitor depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core, IoT and Monitor db2 db2 Db2 instance used by IoT and Monitor core suite_backup_restore MAS Core namespace resources iot suite_app_backup_restore IoT namespace resources monitor suite_app_backup_restore Monitor namespace resources Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by IoT and Monitor, you need to set the correct Db2 instance name for this environment variable. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u Examples \u00a4 # Full backup all monitor data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor # Incremental backup all monitor data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor # Restore all monitor data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor Backup/Restore for Health \u00a4 This playbook ibm.mas_devops.br_health will backup the following components that Health depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core db2 db2 Db2 instance used by Manage and Health core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments health suite_backup_restore Health namespace resources Watson Studio project assets Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by Manage and Health, you need to set the correct Db2 instance name for this environment variable. Examples \u00a4 # Full backup all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health # Incremental backup all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health # Restore all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health Backup/Restore for Optimizer \u00a4 This playbook ibm.mas_devops.br_optimizer will backup the following components that Optimizer depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and Optimizer db2 db2 Db2 instance used by Manage core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments optimizer suite_backup_restore Optimizer namespace resources Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by Manage, you need to set the correct Db2 instance name for this environment variable. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u Examples \u00a4 # Full backup all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer # Incremental backup all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer # Restore all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer Backup/Restore for Visual Inspection \u00a4 This playbook ibm.mas_devops.br_visualinspection will backup the following components that Visual Inspection depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and Visual Inspection core suite_backup_restore MAS Core namespace resources visualinspection suite_app_backup_restore Visual Inspection namespace resources Persistent volume data, such as images and models Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. Examples \u00a4 # Full backup all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection # Incremental backup all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection # Restore all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection Reference \u00a4 Directory Structure \u00a4 No matter what kind of storage systems you choose, the folder structure created in the storage system is same. Below is the sample folder structure for saving backup jobs: <root_folder>/backups/mongodb-main-full-20240621122530 \u251c\u2500\u2500 backup.yml \u251c\u2500\u2500 database \u2502 \u251c\u2500\u2500 mongodb-main-full-20240621122530.tar.gz \u2502 \u2514\u2500\u2500 query.json \u2514\u2500\u2500 log \u251c\u2500\u2500 mongodb-main-full-20240621122530-backup-log.tar.gz \u2514\u2500\u2500 mongodb-main-full-20240621122530-ansible-log.tar.gz <root_folder>/backups/core-main-full-20240621122530 \u251c\u2500\u2500 backup.yml \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 core-main-full-20240621122530-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 core-main-full-20240621122530-namespace-log.tar.gz \u2514\u2500\u2500 namespace \u2514\u2500\u2500 core-main-full-20240621122530-namespace.tar.gz <root_folder> : the root folder is specified by MASBR_STORAGE_LOCAL_FOLDER or MASBR_STORAGE_CLOUD_BUCKET The backup playbooks will create a seperated backup job folder under the backups folder for each component. The backup job folder is named by following this format: {BACKUP COMPONENT}-{INSTANCE ID}-{BACKUP TYPE}-{BACKUP VERSION} . When using playbook to backup multiple components at once, all backup job folders will be assigned to the same backup version. In above example, the same backup version 20240621122530 for backing up mongodb and core components. backup.yml : keep the backup job information database : data type for database. This folder save the backup files of MongoDB database, Db2 database. namespace : data type for namespace resources. This folder save the exported namespace resources. pv : data type for persistent volume. This folder save the persistent volume data, e.g. the Manage attachments, VI images and models. log : this folder save all job running log files In addition to the backup jobs, we also save restore jobs in the specified storage location. For example: <root_folder>/restores/mongodb-main-incr-20240622040201-20240622075501 \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 mongodb-main-incr-20240622040201-20240622075501-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 mongodb-main-incr-20240622040201-20240622075501-restore-log.tar.gz \u2514\u2500\u2500 restore.yml <root_folder>/restores/core-main-incr-20240622040201-20240622075501 \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 core-main-incr-20240622040201-20240622075501-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 core-main-incr-20240622040201-20240622075501-namespace-log.tar.gz \u2514\u2500\u2500 restore.yml The restore playbooks will create a seperated restore job folder under the restores folder for each component. The restore job folder is named by following this format: {BACKUP JOB NAME}-{RESTORE VERSION} . restore.yml : keep the restore job information log : this folder save all job running log files Data Model \u00a4 backup.yml \u00a4 kind: Backup name: \"core-main-incr-20240622040201\" version: \"20240622040201\" type: \"incr\" from: \"core-main-full-20240621122530\" source: domain: \"source-cluster.mydomain.com\" suite: \"8.11.11\" instance: \"main\" workspace: \"\" component: name: \"core\" instance: \"main\" namespace: \"mas-main-core\" data: - seq: \"1\" type: \"namespace\" phase: \"Completed\" status: phase: \"Completed\" startTimestamp: \"2024-06-22T04:05:22\" completionTimestamp: \"2024-06-22T04:06:04\" sentNotifications: - type: \"Slack\" channel: \"#ansible-slack-dev\" timestamp: \"2024-06-22T04:05:34\" phase: \"InProgress\" - type: \"Slack\" channel: \"#ansible-slack-dev\" timestamp: \"2024-06-22T04:06:10\" phase: \"Completed\" restore.yml \u00a4 kind: Restore name: \"core-main-incr-20240622040201-20240622075501\" version: \"20240622075501\" from: \"core-main-incr-20240622040201\" target: domain: \"target-cluster.mydomain.com\" component: name: \"core\" instance: \"main\" namespace: \"mas-main-core\" data: - seq: 1 type: \"namespace\" phase: \"Completed\" status: phase: \"Completed\" startTimestamp: \"2024-06-22T08:04:19\" completionTimestamp: \"2024-06-22T08:04:33\"","title":"Backup & Restore"},{"location":"playbooks/backup-restore/#backup-and-restore","text":"","title":"Backup and Restore"},{"location":"playbooks/backup-restore/#overview","text":"MAS Devops Collection includes playbooks for backing up and restoring of the following MAS components and their dependencies: MongoDB Db2 MAS Core Manage IoT Monitor Health Optimizer Visual Inspection Creation of both full and incremental backups are supported. The backup and restore Ansible roles can also be used individually, allowing you to build your own customized backup and restore playbook covering exactly what you need. For example, you can only backup/restore Manage attachments . Important The backup and restore playbooks in this collection are still work in progress, they are not suitable for production use at this time. You may track development progress using the Backup & Restore label in the Github repository. Production-ready backup and restore options are detailed in the Backup and restore topic in the product documentation.","title":"Overview"},{"location":"playbooks/backup-restore/#configuration-storage","text":"You can save the backup files to a folder on your local file system by setting the following environment variables: Envrionment variable Required (Default Value) Description MASBR_STORAGE_LOCAL_FOLDER Yes The local path to save the backup files MASBR_LOCAL_TEMP_FOLDER No ( /tmp/masbr ) Local folder for saving the temporary backup/restore data, the data in this folder will be deleted after the backup/restore job completed.","title":"Configuration - Storage"},{"location":"playbooks/backup-restore/#configuration-backup","text":"Envrionment variable Required (Default Value) Description MASBR_ACTION Yes Whether to run the playbook to perform a backup or a restore MASBR_BACKUP_TYPE No ( full ) Set full or incr to indicate the playbook to create a full backup or incremental backup. MASBR_BACKUP_FROM_VERSION No Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). The playbooks are switched to backup mode by setting MASBR_ACTION to backup .","title":"Configuration - Backup"},{"location":"playbooks/backup-restore/#full-backups","text":"If you set environment variable MASBR_BACKUP_TYPE=full or do not specify a value for this variable, the playbook will take a full backup.","title":"Full Backups"},{"location":"playbooks/backup-restore/#incremental-backups","text":"You can set environment variable MASBR_BACKUP_TYPE=incr to indicate the playbook to take an incremental backup. Important Only supports creating incremental backup for MonogDB, Db2 and persistent volume data. The playbook will always create a full backup for other type of data regardless of whether this variable be set to incr . The environment variable MASBR_BACKUP_FROM_VERSION is only valid if MASBR_BACKUP_TYPE=incr . It indicates which backup version that the incremental backup to based on. If you do not set a value for this variable, the playbook will try to find the latest Completed Full backup from the specified storage location, and then take an incremental backup based on it. Important The backup files you specified by MASBR_BACKUP_FROM_VERSION must be a Full backup. And the component name and data types in the specified Full backup file must be same as the current incremental backup job.","title":"Incremental Backups"},{"location":"playbooks/backup-restore/#configuration-restore","text":"Envrionment variable Required (Default Value) Description MASBR_ACTION Yes Whether to run the playbook to perform a backup or a restore MASBR_RESTORE_FROM_VERSION Yes Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) MASBR_RESTORE_OVERWRITE Yes Set whether the restore should overwrite any existing data or if we should stop and FAIL if there is data detected in the directory. WARNING: This will overwrite all data when restoring! The playbooks are switched to restore mode by setting MASBR_ACTION to restore . You must specify the MASBR_RESTORE_FROM_VERSION environment variable to indicate which version of the backup files to use. In the case of restoring from an incremental backup, the corresponding full backup will be restored first before continuing to restore the incremental backup.","title":"Configuration - Restore"},{"location":"playbooks/backup-restore/#backuprestore-for-mongodb","text":"This playbook ibm.mas_devops.br_mongodb will invoke the role mongodb to backup/restore the MongoDB databases. This playbook supports backing up and restoring databases for an in-cluster MongoDB CE instance. If you are using other MongoDB venders, such as IBM Cloud Databases for MongoDB, Amazon DocumentDB or MongoDB Altas Database, please refer to the corresponding vender's documentation for more information about their provided backup/restore service.","title":"Backup/Restore for MongoDB"},{"location":"playbooks/backup-restore/#environment-variables","text":"MONGODB_NAMESPACE : By default the backup and restore processes will use a namespace of mongoce , if you have customized the install of MongoDb CE you must set this environment variable to the appropriate namespace you wish to backup from/restore to. MAS_INSTANCE_ID : Required . This playbook supports backup/restore MongoDB databases that belong to a specific MAS instance, call the playbook multiple times with different values for MAS_INSTANCE_ID if you wish to back up multiple MAS instances that use the same MongoDB CE instance. MAS_APP_ID : Optional . By default, this playbook will backup all databases belonging to the specified MAS instance. You can backup the databases only belong to a specific MAS application by setting this environment variable to a supported MAS application id core , manage , iot , monitor , health , optimizer or visualinspection .","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples","text":"# Full backup all MongoDB data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_mongodb # Incremental backup all MongoDB data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_mongodb # Restore all MongoDB data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_mongodb # Backup just the IoT MongoDB data for the dev2 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev2 export MAS_APP_ID=iot ansible-playbook ibm.mas_devops.br_mongodb","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-db2","text":"This playbook ibm.mas_devops.br_db2 will invoke the role db2 to backup/restore a single Db2 instance.","title":"Backup/Restore for Db2"},{"location":"playbooks/backup-restore/#environment-variables_1","text":"DB2_INSTANCE_NAME : Required This playbook only supports backing up specific Db2 instance at a time. If you want to backup all Db2 instances in the Db2 cluster, you need to run this playbook multiple times with different value of this environment variable. MAS_INSTANCE_ID : Required Set the instance ID for the MAS install. MASBR_ACTION : Required Set the action to be performed, backup or restore . MASBR_STORAGE_LOCAL_FOLDER : Required Set the local path to the directory to be used for backup and restore. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_1","text":"# Full backup for the db2w-shared Db2 instance export MAS_INSTANCE_ID=dev export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 # Incremental backup for the db2w-shared Db2 instance export MAS_INSTANCE_ID=dev export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 # Restore for the db2w-shared Db2 instance export MAS_INSTANCE_ID=dev export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-mas-core","text":"This playbook ibm.mas_devops.br_core will backup the following components that MAS Core depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core core suite_backup_restore MAS Core namespace resources","title":"Backup/Restore for MAS Core"},{"location":"playbooks/backup-restore/#environment-variables_2","text":"MAS_INSTANCE_ID Required . The MAS instance ID to perform a backup for.","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_2","text":"# Full backup all core data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_core # Incremental backup all core data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_core # Restore all core data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev ansible-playbook ibm.mas_devops.br_core","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-manage","text":"This playbook ibm.mas_devops.br_manage will backup the following components that Manage depends on in order: Component Role Data included mongodb mongodb MongoDB databases used by MAS Core db2 db2 Db2 instance used by Manage core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments","title":"Backup/Restore for Manage"},{"location":"playbooks/backup-restore/#environment-variables_3","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Optional . When defined, this playbook will backup the Db2 instance used by Manage. DB2 role is skipped when environment variable is not defined.. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_3","text":"# Full backup all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage # Incremental backup all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage # Restore all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-iot","text":"This playbook ibm.mas_devops.br_iot will backup the following components that IoT depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and IoT db2 db2 Db2 instance used by IoT core suite_backup_restore MAS Core namespace resources iot suite_app_backup_restore IoT namespace resources","title":"Backup/Restore for IoT"},{"location":"playbooks/backup-restore/#environment-variables_4","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by IoT, you need to set the correct Db2 instance name for this environment variable. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_4","text":"# Full backup all iot data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot # Incremental backup all iot data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot # Restore all iot data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-monitor","text":"This playbook ibm.mas_devops.br_monitor will backup the following components that Monitor depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core, IoT and Monitor db2 db2 Db2 instance used by IoT and Monitor core suite_backup_restore MAS Core namespace resources iot suite_app_backup_restore IoT namespace resources monitor suite_app_backup_restore Monitor namespace resources","title":"Backup/Restore for Monitor"},{"location":"playbooks/backup-restore/#environment-variables_5","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by IoT and Monitor, you need to set the correct Db2 instance name for this environment variable. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_5","text":"# Full backup all monitor data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor # Incremental backup all monitor data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor # Restore all monitor data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-health","text":"This playbook ibm.mas_devops.br_health will backup the following components that Health depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core db2 db2 Db2 instance used by Manage and Health core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments health suite_backup_restore Health namespace resources Watson Studio project assets","title":"Backup/Restore for Health"},{"location":"playbooks/backup-restore/#environment-variables_6","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by Manage and Health, you need to set the correct Db2 instance name for this environment variable.","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_6","text":"# Full backup all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health # Incremental backup all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health # Restore all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-optimizer","text":"This playbook ibm.mas_devops.br_optimizer will backup the following components that Optimizer depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and Optimizer db2 db2 Db2 instance used by Manage core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments optimizer suite_backup_restore Optimizer namespace resources","title":"Backup/Restore for Optimizer"},{"location":"playbooks/backup-restore/#environment-variables_7","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by Manage, you need to set the correct Db2 instance name for this environment variable. DB2_NAMESPACE : Optional Set the DB2 namespace, defaults to db2u","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_7","text":"# Full backup all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer # Incremental backup all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer # Restore all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-visual-inspection","text":"This playbook ibm.mas_devops.br_visualinspection will backup the following components that Visual Inspection depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and Visual Inspection core suite_backup_restore MAS Core namespace resources visualinspection suite_app_backup_restore Visual Inspection namespace resources Persistent volume data, such as images and models","title":"Backup/Restore for Visual Inspection"},{"location":"playbooks/backup-restore/#environment-variables_8","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable.","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_8","text":"# Full backup all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection # Incremental backup all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection # Restore all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection","title":"Examples"},{"location":"playbooks/backup-restore/#reference","text":"","title":"Reference"},{"location":"playbooks/backup-restore/#directory-structure","text":"No matter what kind of storage systems you choose, the folder structure created in the storage system is same. Below is the sample folder structure for saving backup jobs: <root_folder>/backups/mongodb-main-full-20240621122530 \u251c\u2500\u2500 backup.yml \u251c\u2500\u2500 database \u2502 \u251c\u2500\u2500 mongodb-main-full-20240621122530.tar.gz \u2502 \u2514\u2500\u2500 query.json \u2514\u2500\u2500 log \u251c\u2500\u2500 mongodb-main-full-20240621122530-backup-log.tar.gz \u2514\u2500\u2500 mongodb-main-full-20240621122530-ansible-log.tar.gz <root_folder>/backups/core-main-full-20240621122530 \u251c\u2500\u2500 backup.yml \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 core-main-full-20240621122530-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 core-main-full-20240621122530-namespace-log.tar.gz \u2514\u2500\u2500 namespace \u2514\u2500\u2500 core-main-full-20240621122530-namespace.tar.gz <root_folder> : the root folder is specified by MASBR_STORAGE_LOCAL_FOLDER or MASBR_STORAGE_CLOUD_BUCKET The backup playbooks will create a seperated backup job folder under the backups folder for each component. The backup job folder is named by following this format: {BACKUP COMPONENT}-{INSTANCE ID}-{BACKUP TYPE}-{BACKUP VERSION} . When using playbook to backup multiple components at once, all backup job folders will be assigned to the same backup version. In above example, the same backup version 20240621122530 for backing up mongodb and core components. backup.yml : keep the backup job information database : data type for database. This folder save the backup files of MongoDB database, Db2 database. namespace : data type for namespace resources. This folder save the exported namespace resources. pv : data type for persistent volume. This folder save the persistent volume data, e.g. the Manage attachments, VI images and models. log : this folder save all job running log files In addition to the backup jobs, we also save restore jobs in the specified storage location. For example: <root_folder>/restores/mongodb-main-incr-20240622040201-20240622075501 \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 mongodb-main-incr-20240622040201-20240622075501-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 mongodb-main-incr-20240622040201-20240622075501-restore-log.tar.gz \u2514\u2500\u2500 restore.yml <root_folder>/restores/core-main-incr-20240622040201-20240622075501 \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 core-main-incr-20240622040201-20240622075501-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 core-main-incr-20240622040201-20240622075501-namespace-log.tar.gz \u2514\u2500\u2500 restore.yml The restore playbooks will create a seperated restore job folder under the restores folder for each component. The restore job folder is named by following this format: {BACKUP JOB NAME}-{RESTORE VERSION} . restore.yml : keep the restore job information log : this folder save all job running log files","title":"Directory Structure"},{"location":"playbooks/backup-restore/#data-model","text":"","title":"Data Model"},{"location":"playbooks/backup-restore/#backupyml","text":"kind: Backup name: \"core-main-incr-20240622040201\" version: \"20240622040201\" type: \"incr\" from: \"core-main-full-20240621122530\" source: domain: \"source-cluster.mydomain.com\" suite: \"8.11.11\" instance: \"main\" workspace: \"\" component: name: \"core\" instance: \"main\" namespace: \"mas-main-core\" data: - seq: \"1\" type: \"namespace\" phase: \"Completed\" status: phase: \"Completed\" startTimestamp: \"2024-06-22T04:05:22\" completionTimestamp: \"2024-06-22T04:06:04\" sentNotifications: - type: \"Slack\" channel: \"#ansible-slack-dev\" timestamp: \"2024-06-22T04:05:34\" phase: \"InProgress\" - type: \"Slack\" channel: \"#ansible-slack-dev\" timestamp: \"2024-06-22T04:06:10\" phase: \"Completed\"","title":"backup.yml"},{"location":"playbooks/backup-restore/#restoreyml","text":"kind: Restore name: \"core-main-incr-20240622040201-20240622075501\" version: \"20240622075501\" from: \"core-main-incr-20240622040201\" target: domain: \"target-cluster.mydomain.com\" component: name: \"core\" instance: \"main\" namespace: \"mas-main-core\" data: - seq: 1 type: \"namespace\" phase: \"Completed\" status: phase: \"Completed\" startTimestamp: \"2024-06-22T08:04:19\" completionTimestamp: \"2024-06-22T08:04:33\"","title":"restore.yml"},{"location":"playbooks/cp4d/","text":"Install Cloud Pak for Data \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift v4.10 cluster with IBM Maximo Application Suite Core v8.10 already installed, the mas_install_core playbook can be used to set this up. Overview \u00a4 This playbook will add Cloud Pak for Data 4.x to an existing cluster installation. It could also install additional CP4D services. This playbook can be run against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install CP4D ControlPlane (~1 hour) Install CP4D Services (~30 Minutes - 1 hour for each service) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \u00a4 MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) CPD_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry CPD_ENTITLEMENT_USERNAME Your user name to access the IBM Container Registr CPD_INSTALL_PLATFORM True/False - If you HAVE CP4D already installed in your cluster, then set it to \"false\" CPD_PRODUCT_VERSION (Required) Cloud Pak for Data version installed in the cluster in 4.X format. These variables are required only if you set CP4D_INSTALL_WSL to false in optional varibles, otherwise don't set it. \u00a4 CPD_ADMIN_USERNAME CP4D Username CPD_ADMIN_PASSWORD CP4D Password CPD_ADMIN_URL CP4D Base URL Optional environment variables \u00a4 CPD_INSTALL_COGNOS True/False - Set to true to install Cognos Analytics CPD_INSTALL_WSL True/False - Set to true to install Watson Studio CPD_INSTALL_WML True/False - Set to true to install Watson Machine Learning CPD_INSTALL_SPARK True/False - Set to true to install Analytics Engine \"Spark\" CPD_INSTALL_OPENSCALE True/False - Set to true to install AI Openscale CPD_INSTALL_DISCOVERY True/False - Set to true to install Watson Discovery CPD_INSTALL_SPSS True/False - Set to true to install SPSS Modeler Usage when you already HAVE CP4D installed \u00a4 export MAS_CONFIG_DIR=~/masconfig export CPD_INSTALL_COGNOS=\"true\" export CPD_ENTITLEMENT_KEY=xxx export CPD_ENTITLEMENT_USERNAME=xxx export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_URL=\"https://mycp4durl\" export CPD_PRODUCT_VERSION=\"4.6.6\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.cp4d Usage when you DON'T HAVE CP4D installed \u00a4 export MAS_CONFIG_DIR=~/masconfig export CPD_ENTITLEMENT_KEY=xxx export CPD_ENTITLEMENT_USERNAME=xxx export CPD_INSTALL_PLATFORM=\"true\" export CPD_PRODUCT_VERSION=\"4.6.6\" ## To install additional CP4D services, add one or many of these environment variables: export CPD_INSTALL_COGNOS=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_INSTALL_WML=\"true\" export CPD_INSTALL_SPARK=\"true\" export CPD_INSTALL_OPENSCALE=\"true\" export CPD_INSTALL_DISCOVERY=\"true\" export CPD_INSTALL_SPSS=\"true\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.cp4d Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Install Cloud Pak For Data"},{"location":"playbooks/cp4d/#install-cloud-pak-for-data","text":"","title":"Install Cloud Pak for Data"},{"location":"playbooks/cp4d/#prerequisites","text":"You will need a RedHat OpenShift v4.10 cluster with IBM Maximo Application Suite Core v8.10 already installed, the mas_install_core playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/cp4d/#overview","text":"This playbook will add Cloud Pak for Data 4.x to an existing cluster installation. It could also install additional CP4D services. This playbook can be run against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install CP4D ControlPlane (~1 hour) Install CP4D Services (~30 Minutes - 1 hour for each service) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/cp4d/#required-environment-variables","text":"MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) CPD_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry CPD_ENTITLEMENT_USERNAME Your user name to access the IBM Container Registr CPD_INSTALL_PLATFORM True/False - If you HAVE CP4D already installed in your cluster, then set it to \"false\" CPD_PRODUCT_VERSION (Required) Cloud Pak for Data version installed in the cluster in 4.X format.","title":"Required environment variables"},{"location":"playbooks/cp4d/#these-variables-are-required-only-if-you-set-cp4d_install_wsl-to-false-in-optional-varibles-otherwise-dont-set-it","text":"CPD_ADMIN_USERNAME CP4D Username CPD_ADMIN_PASSWORD CP4D Password CPD_ADMIN_URL CP4D Base URL","title":"These variables are required only if you set CP4D_INSTALL_WSL to false in optional varibles, otherwise don't set it."},{"location":"playbooks/cp4d/#optional-environment-variables","text":"CPD_INSTALL_COGNOS True/False - Set to true to install Cognos Analytics CPD_INSTALL_WSL True/False - Set to true to install Watson Studio CPD_INSTALL_WML True/False - Set to true to install Watson Machine Learning CPD_INSTALL_SPARK True/False - Set to true to install Analytics Engine \"Spark\" CPD_INSTALL_OPENSCALE True/False - Set to true to install AI Openscale CPD_INSTALL_DISCOVERY True/False - Set to true to install Watson Discovery CPD_INSTALL_SPSS True/False - Set to true to install SPSS Modeler","title":"Optional environment variables"},{"location":"playbooks/cp4d/#usage-when-you-already-have-cp4d-installed","text":"export MAS_CONFIG_DIR=~/masconfig export CPD_INSTALL_COGNOS=\"true\" export CPD_ENTITLEMENT_KEY=xxx export CPD_ENTITLEMENT_USERNAME=xxx export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_URL=\"https://mycp4durl\" export CPD_PRODUCT_VERSION=\"4.6.6\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.cp4d","title":"Usage when you already HAVE CP4D installed"},{"location":"playbooks/cp4d/#usage-when-you-dont-have-cp4d-installed","text":"export MAS_CONFIG_DIR=~/masconfig export CPD_ENTITLEMENT_KEY=xxx export CPD_ENTITLEMENT_USERNAME=xxx export CPD_INSTALL_PLATFORM=\"true\" export CPD_PRODUCT_VERSION=\"4.6.6\" ## To install additional CP4D services, add one or many of these environment variables: export CPD_INSTALL_COGNOS=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_INSTALL_WML=\"true\" export CPD_INSTALL_SPARK=\"true\" export CPD_INSTALL_OPENSCALE=\"true\" export CPD_INSTALL_DISCOVERY=\"true\" export CPD_INSTALL_SPSS=\"true\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.cp4d Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage when you DON'T HAVE CP4D installed"},{"location":"playbooks/mas-core/","text":"OneClick Install for MAS Core \u00a4 Important These playbooks are samples to demonstrate how to use the roles in this collection. They are note intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install MAS is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance. Overview \u00a4 This playbook will install and configure IBM Maximo Application Suite Core along with all necessary dependencies. This can be ran against any OCP cluster regardless of its type, whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. It will take approximately 90 minutes to set up MAS core services and all of its dependencies, at the end of the process you will be able to login to the MAS admin dashboard to install any applications that you wish to use, or you can use our other playbooks to automate the installation of those applications (including any additional dependencies) Playbook Content \u00a4 Install IBM Operator Catalogs (1 minute) Install Certificate Manager Operator (3 minutes) Install Mongodb Operator and Create a Cluster (10 minutes, skipped if SKIP_MONGO set to TRUE) Install and bootstrap IBM Suite License Service (10 minutes) Generate a MAS Workspace Configuration (1 minute) Configure Cloud Internet Services Integration for Maximo Application Suite (Optional, 1 minute) Install Maximo Application Suite Core Services (1 minute) Configure Maximo Application Suite (1 minute) Verify the Install and Configuration of Maximo Application Suite (25 minutes) All timings are estimates, see the individual pages for each of these roles for more information and full details of all configuration options available in this playbook. Preparation \u00a4 1. IBM Entitlement key \u00a4 Access Container Software Library using your IBMId to access your entitlement key 2. MAS License File \u00a4 Access IBM License Key Center , on the Get Keys menu select IBM AppPoint Suites . Select IBM MAXIMO APPLICATION SUITE AppPOINT LIC and on the next page fill in the information as below: Field Content Number of Keys How many AppPoints to assign to the license file Host ID Type Set to Ethernet Address Host ID Enter any 12 digit hexadecimal string Hostname Set to the hostname of your OCP instance Port Set to 27000 The other values can be left at their defaults. Finally, click Generate and download the license file to your home directory as entitlement.lic , set SLS_LICENSE_FILE to point to this location. Usage \u00a4 Required environment variables \u00a4 IBM_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in SLS_LICENSE_FILE SLS_LICENSE_FILE The path to the location of the license file. DRO_CONTACT_EMAIL Primary contact e-mail address DRO_CONTACT_FIRSTNAME Primary contact first name DRO_CONTACT_LASTNAME Primary contact last name Optional environment variables (for this playbook only) \u00a4 SKIP_MONGO Controls whether to use an custom MongoDB configuration instead of deploying a new MongoDB cluster. If set to \"true\" , the playbook will skip the mongo role and expect that MongoDB connection details are provided via a YAML configuration file in the directory specified by MAS_CONFIG_DIR . If set to \"false\" (or left unset), the playbook will deploy MongoDB using the ibm.mas_devops.mongodb role. Storage Class Configuraton \u00a4 Storage class configuration is built into the collection and the playbook will auto-select the appropriate storage classes when it detects the presence of certain storage classes in your cluster (IBM Cloud Storage or OpenShift Container Storage). If you are running the install on a cluster that does not have these storage classes then you will also must configure the following environment variables: ReadWriteMany Access Mode \u00a4 Usually fulfilled by file storage classes: PROMETHEUS_ALERTMGR_STORAGE_CLASS ReadWriteOnce Access Mode \u00a4 Usually fulfilled by block storage classes: PROMETHEUS_STORAGE_CLASS PROMETHEUS_USERWORKLOAD_STORAGE_CLASS GRAFANA_INSTANCE_STORAGE_CLASS MONGODB_STORAGE_CLASS DRO_STORAGE_CLASS Examples \u00a4 Release build \u00a4 The simplest configuration to deploy a release build of IBM Maximo Application Suite (core only) with dependencies is: export IBM_ENTITLEMENT_KEY=xxx export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_install_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli Pre-release build \u00a4 To deploy a pre-release build of IBM Maximo Application Suite (core only) with dependencies a number of additional parameters are required, note that pre-release builds are only available to IBM employees: export IBM_ENTITLEMENT_KEY=xxx export ARTIFACTORY_USERNAME=$W3_USERNAME_LOWERCASE export ARTIFACTORY_TOKEN=xxx export MAS_ICR_CP=docker-na-public.artifactory.swg-devops.com/wiotp-docker-local export MAS_ICR_CPOPEN=docker-na-public.artifactory.swg-devops.com/wiotp-docker-local export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_TOKEN export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export MAS_CATALOG_SOURCE=ibm-operator-catalog export MAS_CHANNEL=rp1dev88 export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export SKIP_MONGO=TRUE oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_install_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli Tip Enable login for Maximo Application Suite self-signed certificates. If you are using self-signed certificates in a development or test environment, you must manually enable login by using either of the following methods: Download the certificates from the cluster and add them to your local certificate manager. In your browser, go to the Maximo Application Suite API URL: \"https://api.mas_domain/\" and then accept the certificate security risks. After you accept the risks, an AIUC01999E error is displayed. This message is expected. You can now continue with the setup process.","title":"Install Core"},{"location":"playbooks/mas-core/#oneclick-install-for-mas-core","text":"Important These playbooks are samples to demonstrate how to use the roles in this collection. They are note intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install MAS is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance.","title":"OneClick Install for MAS Core"},{"location":"playbooks/mas-core/#overview","text":"This playbook will install and configure IBM Maximo Application Suite Core along with all necessary dependencies. This can be ran against any OCP cluster regardless of its type, whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. It will take approximately 90 minutes to set up MAS core services and all of its dependencies, at the end of the process you will be able to login to the MAS admin dashboard to install any applications that you wish to use, or you can use our other playbooks to automate the installation of those applications (including any additional dependencies)","title":"Overview"},{"location":"playbooks/mas-core/#playbook-content","text":"Install IBM Operator Catalogs (1 minute) Install Certificate Manager Operator (3 minutes) Install Mongodb Operator and Create a Cluster (10 minutes, skipped if SKIP_MONGO set to TRUE) Install and bootstrap IBM Suite License Service (10 minutes) Generate a MAS Workspace Configuration (1 minute) Configure Cloud Internet Services Integration for Maximo Application Suite (Optional, 1 minute) Install Maximo Application Suite Core Services (1 minute) Configure Maximo Application Suite (1 minute) Verify the Install and Configuration of Maximo Application Suite (25 minutes) All timings are estimates, see the individual pages for each of these roles for more information and full details of all configuration options available in this playbook.","title":"Playbook Content"},{"location":"playbooks/mas-core/#preparation","text":"","title":"Preparation"},{"location":"playbooks/mas-core/#1-ibm-entitlement-key","text":"Access Container Software Library using your IBMId to access your entitlement key","title":"1. IBM Entitlement key"},{"location":"playbooks/mas-core/#2-mas-license-file","text":"Access IBM License Key Center , on the Get Keys menu select IBM AppPoint Suites . Select IBM MAXIMO APPLICATION SUITE AppPOINT LIC and on the next page fill in the information as below: Field Content Number of Keys How many AppPoints to assign to the license file Host ID Type Set to Ethernet Address Host ID Enter any 12 digit hexadecimal string Hostname Set to the hostname of your OCP instance Port Set to 27000 The other values can be left at their defaults. Finally, click Generate and download the license file to your home directory as entitlement.lic , set SLS_LICENSE_FILE to point to this location.","title":"2. MAS License File"},{"location":"playbooks/mas-core/#usage","text":"","title":"Usage"},{"location":"playbooks/mas-core/#required-environment-variables","text":"IBM_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in SLS_LICENSE_FILE SLS_LICENSE_FILE The path to the location of the license file. DRO_CONTACT_EMAIL Primary contact e-mail address DRO_CONTACT_FIRSTNAME Primary contact first name DRO_CONTACT_LASTNAME Primary contact last name","title":"Required environment variables"},{"location":"playbooks/mas-core/#optional-environment-variables-for-this-playbook-only","text":"SKIP_MONGO Controls whether to use an custom MongoDB configuration instead of deploying a new MongoDB cluster. If set to \"true\" , the playbook will skip the mongo role and expect that MongoDB connection details are provided via a YAML configuration file in the directory specified by MAS_CONFIG_DIR . If set to \"false\" (or left unset), the playbook will deploy MongoDB using the ibm.mas_devops.mongodb role.","title":"Optional environment variables (for this playbook only)"},{"location":"playbooks/mas-core/#storage-class-configuraton","text":"Storage class configuration is built into the collection and the playbook will auto-select the appropriate storage classes when it detects the presence of certain storage classes in your cluster (IBM Cloud Storage or OpenShift Container Storage). If you are running the install on a cluster that does not have these storage classes then you will also must configure the following environment variables:","title":"Storage Class Configuraton"},{"location":"playbooks/mas-core/#readwritemany-access-mode","text":"Usually fulfilled by file storage classes: PROMETHEUS_ALERTMGR_STORAGE_CLASS","title":"ReadWriteMany Access Mode"},{"location":"playbooks/mas-core/#readwriteonce-access-mode","text":"Usually fulfilled by block storage classes: PROMETHEUS_STORAGE_CLASS PROMETHEUS_USERWORKLOAD_STORAGE_CLASS GRAFANA_INSTANCE_STORAGE_CLASS MONGODB_STORAGE_CLASS DRO_STORAGE_CLASS","title":"ReadWriteOnce Access Mode"},{"location":"playbooks/mas-core/#examples","text":"","title":"Examples"},{"location":"playbooks/mas-core/#release-build","text":"The simplest configuration to deploy a release build of IBM Maximo Application Suite (core only) with dependencies is: export IBM_ENTITLEMENT_KEY=xxx export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_install_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Release build"},{"location":"playbooks/mas-core/#pre-release-build","text":"To deploy a pre-release build of IBM Maximo Application Suite (core only) with dependencies a number of additional parameters are required, note that pre-release builds are only available to IBM employees: export IBM_ENTITLEMENT_KEY=xxx export ARTIFACTORY_USERNAME=$W3_USERNAME_LOWERCASE export ARTIFACTORY_TOKEN=xxx export MAS_ICR_CP=docker-na-public.artifactory.swg-devops.com/wiotp-docker-local export MAS_ICR_CPOPEN=docker-na-public.artifactory.swg-devops.com/wiotp-docker-local export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_TOKEN export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export MAS_CATALOG_SOURCE=ibm-operator-catalog export MAS_CHANNEL=rp1dev88 export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export SKIP_MONGO=TRUE oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_install_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli Tip Enable login for Maximo Application Suite self-signed certificates. If you are using self-signed certificates in a development or test environment, you must manually enable login by using either of the following methods: Download the certificates from the cluster and add them to your local certificate manager. In your browser, go to the Maximo Application Suite API URL: \"https://api.mas_domain/\" and then accept the certificate security risks. After you accept the risks, an AIUC01999E error is displayed. This message is expected. You can now continue with the setup process.","title":"Pre-release build"},{"location":"playbooks/mas-facilities/","text":"Install Real Estate and Facilities Application \u00a4 Important These playbooks are samples to demonstrate how to use the roles in this collection. They are note intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install MAS is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance. Overview \u00a4 This playbook will add Maximo Real Estate and Facilities to an existing IBM Maximo Application Suite Instance. Refer to the mas_install_core playbook to set up the MAS Core Platform before running this playbook. This playbook will create an in-cluster Db2 instance using the IBM Db2 Universal Operator, which will be automatically set up as the workspace-application level JDBC configuration in MAS. Playbook Content \u00a4 Create and initialize Db2 Instance for Maximo Real Estate and Facilities Configure MAS to use BYO database (optional, set CONFIGURE_EXTERNAL_DB ) Configure MAS to use the new Db2 Instance Install Maximo Real Estate and Facilities Application Configure Maximo Real Estate and Facilities Workspace See the individual pages for each of these roles for more information and full details of all configuration options available in this playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry In-Cluster Db2 \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_facilities Bring Your Own Database \u00a4 If you do not want to use the Db2 Universal Operator to provide the database for MREF then you can configure the playbook as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CONFIGURE_EXTERNAL_DB=true export DB_INSTANCE_ID=maxdbxx export MAS_JDBC_USER=user1 export MAS_JDBC_PASSWORD=xxx export MAS_JDBC_URL=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_facilities For full details of configuration options available refer to the gencfg_jdbc role documentation.","title":"Add Real Estate and Facilities"},{"location":"playbooks/mas-facilities/#install-real-estate-and-facilities-application","text":"Important These playbooks are samples to demonstrate how to use the roles in this collection. They are note intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install MAS is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance.","title":"Install Real Estate and Facilities Application"},{"location":"playbooks/mas-facilities/#overview","text":"This playbook will add Maximo Real Estate and Facilities to an existing IBM Maximo Application Suite Instance. Refer to the mas_install_core playbook to set up the MAS Core Platform before running this playbook. This playbook will create an in-cluster Db2 instance using the IBM Db2 Universal Operator, which will be automatically set up as the workspace-application level JDBC configuration in MAS.","title":"Overview"},{"location":"playbooks/mas-facilities/#playbook-content","text":"Create and initialize Db2 Instance for Maximo Real Estate and Facilities Configure MAS to use BYO database (optional, set CONFIGURE_EXTERNAL_DB ) Configure MAS to use the new Db2 Instance Install Maximo Real Estate and Facilities Application Configure Maximo Real Estate and Facilities Workspace See the individual pages for each of these roles for more information and full details of all configuration options available in this playbook.","title":"Playbook Content"},{"location":"playbooks/mas-facilities/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/mas-facilities/#in-cluster-db2","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_facilities","title":"In-Cluster Db2"},{"location":"playbooks/mas-facilities/#bring-your-own-database","text":"If you do not want to use the Db2 Universal Operator to provide the database for MREF then you can configure the playbook as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CONFIGURE_EXTERNAL_DB=true export DB_INSTANCE_ID=maxdbxx export MAS_JDBC_USER=user1 export MAS_JDBC_PASSWORD=xxx export MAS_JDBC_URL=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_facilities For full details of configuration options available refer to the gencfg_jdbc role documentation.","title":"Bring Your Own Database"},{"location":"playbooks/mas-iot/","text":"Install IoT Application \u00a4 Important These playbooks are samples to demonstrate how to use the roles in this collection. They are note intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install MAS is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance. Prerequisites \u00a4 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 already be installed, the mas_install_core playbook can be used to set this up. Overview \u00a4 This playbook will add Maximo IoT v8.4 to an existing IBM Maximo Application Suite Core installation. It will also creatie an in-cluster Db2 instance and Kafka cluster, both of which will be automatically set up as system-level configurations in MAS. IoT will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install IBM Db2 Universal Operator (2 minutes) Create Db2 Warehouse Instance (45 minutes) Install RedHat AMQ Streams Operator (2 minutes) Create Apache Kafka Cluster (15 minutes) Configure Maximo Application Suite: Set up Db2 instance as the system-level JDBC datasource Set up Kafka cluster as the system-level Kafka Install Maximo IoT application: Install application (90 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Storage Class Configuraton \u00a4 A persistent volume storage class is required for the FPL component. Storage class configuration is built into the collection and the playbook will auto-select the appropriate storage classes when it detects the presence of certain storage classes in your cluster (IBM Cloud Storage or OpenShift Container Storage). If you are running the install on a cluster that does not have these storage classes then you will also must configure the following environment variables: ReadWriteOnce Access Mode \u00a4 Usually fulfilled by block storage classes: MAS_APP_SETTINGS_IOT_FPL_PVC_STORAGE_CLASS Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Optional environment variables \u00a4 MAS_APP_SETTINGS_IOT_DEPLOYMENT_SIZE Define the IoT deployment size, one of dev , small or large . Defaults to small . Usage \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_iot Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Add IoT"},{"location":"playbooks/mas-iot/#install-iot-application","text":"Important These playbooks are samples to demonstrate how to use the roles in this collection. They are note intended for production use as-is, they are a starting point for power users to aid in the development of their own Ansible playbooks using the roles in this collection. The recommended way to install MAS is to use the MAS CLI , which uses this Ansible Collection to deliver a complete managed lifecycle for your MAS instance.","title":"Install IoT Application"},{"location":"playbooks/mas-iot/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 already be installed, the mas_install_core playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/mas-iot/#overview","text":"This playbook will add Maximo IoT v8.4 to an existing IBM Maximo Application Suite Core installation. It will also creatie an in-cluster Db2 instance and Kafka cluster, both of which will be automatically set up as system-level configurations in MAS. IoT will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install IBM Db2 Universal Operator (2 minutes) Create Db2 Warehouse Instance (45 minutes) Install RedHat AMQ Streams Operator (2 minutes) Create Apache Kafka Cluster (15 minutes) Configure Maximo Application Suite: Set up Db2 instance as the system-level JDBC datasource Set up Kafka cluster as the system-level Kafka Install Maximo IoT application: Install application (90 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/mas-iot/#storage-class-configuraton","text":"A persistent volume storage class is required for the FPL component. Storage class configuration is built into the collection and the playbook will auto-select the appropriate storage classes when it detects the presence of certain storage classes in your cluster (IBM Cloud Storage or OpenShift Container Storage). If you are running the install on a cluster that does not have these storage classes then you will also must configure the following environment variables:","title":"Storage Class Configuraton"},{"location":"playbooks/mas-iot/#readwriteonce-access-mode","text":"Usually fulfilled by block storage classes: MAS_APP_SETTINGS_IOT_FPL_PVC_STORAGE_CLASS","title":"ReadWriteOnce Access Mode"},{"location":"playbooks/mas-iot/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/mas-iot/#optional-environment-variables","text":"MAS_APP_SETTINGS_IOT_DEPLOYMENT_SIZE Define the IoT deployment size, one of dev , small or large . Defaults to small .","title":"Optional environment variables"},{"location":"playbooks/mas-iot/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_iot Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/mas-manage/","text":"Install Manage Application \u00a4 This playbook will add Maximo Manage to an existing IBM Maximo Application Suite Instance. Refer to the mas_install_core playbook to set up the MAS Core Platform before running this playbook. The playbook will also create an in-cluster Db2 instance using the IBM Db2 Universal Operator, which will be automatically set up as the system-level JDBC configuration in MAS. Playbook Content \u00a4 Install Cloud Pak for Data (optional, set CPD_INSTALL_PLATFORM ) Add Cognos to CP4D (optional, set CPD_INSTALL_COGNOS ) Add Watson Studio Local to CP4D (optional, set CPD_INSTALL_WSL ) Create Db2 Instance using IBM Db2 Universal Operator Initialize Db2 Instance for Maximo Manage Configure MAS to use the new Db2 Instance Configure MAS to use BYO database (optional, set CONFIGURE_EXTERNAL_DB ) Install Maximo Manage Application Configure Maximo Manage Workspace Configure Manage Attachments (optional, set CONFIGURE_MANAGE_ATTACHMENTS ) Configure Manage Building Information Models (optional, set CONFIGURE_MANAGE_BIM ) See the individual pages for each of these roles for more information and full details of all configuration options available in this playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Tip Manage requires the user to select one or more application components to enable in the workspace. By default the base component at the latest version will be installed if no MAS_APPWS_COMPONENTS is set. To customise the components that are enabled use the MAS_APPWS_COMPONENTS environment variable, for example to enable Manage(base) and Health set it to the following: export MAS_APPWS_COMPONENTS=\"base=latest,health=latest\" To disable Asset Investment Optimizer, optional feature of health, set MAS_APP_SETTINGS_AIO_FLAG to false . By default this flag is set to true . This feature is only avalaible on Manage with health as a addon or on Health as a Standalone install. This feature is disabled on MAS Core 9.1 and later. export MAS_APP_SETTINGS_AIO_FLAG=false Note : To install Manage Foundation only that is available on MAS Core 9.1 or later, export the following environment variable: MAS_APPWS_COMPONENTS environment variable must be empty: export MAS_APPWS_COMPONENTS=\"\" Optional Cloud Pak for Data Installation \u00a4 Optional integration with Cloud Pak for Data is supported in Maximo Manage. This can be enabled in the playbook as below: export CPD_INSTALL_PLATFORM=true export CPD_INSTALL_COGNOS=true export CPD_INSTALL_WSL=true export CPD_PRODUCT_VERSION=x.y.z Usage \u00a4 Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli In-Cluster Db2 \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage Bring Your Own Database \u00a4 If you do not want to use the Db2 Universal Operator to provide the datbase for Maximo Manage then you can configure the playbook as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CONFIGURE_EXTERNAL_DB=true export DB_INSTANCE_ID=maxdbxx export MAS_JDBC_USER=maximo export MAS_JDBC_PASSWORD=xxx export MAS_JDBC_URL=xxx export MAS_APP_SETTINGS_DB_SCHEMA=maximo export MAS_APP_SETTINGS_TABLESPACE=maxdata export MAS_APP_SETTINGS_INDEXSPACE=maxindex oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage For full details of configuration options available refer to the gencfg_jdbc role documentation. Cloud Pak For Data Integration \u00a4 To install CP4D with Cognos and/or Watson Studio Local optional dependencies: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CPD_INSTALL_PLATFORM=\"true\" export CPD_INSTALL_COGNOS=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_PRODUCT_VERSION=\"4.6.6\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage For full details of configuration options available refer to the cp4d & cp4d_service role documentation. Health Standalone Install \u00a4 To install Health as a Standalone application, set MAS_APP_ID and MAS_APPWS_COMPONENTS as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_ID=health export MAS_APPWS_COMPONENTS=\"health=latest\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage Warning Note that installing Health standalone will prevent the use of all other Manage components. It is recommended to install Manage normally and just enable the Health component in the Manage application.","title":"Add Manage"},{"location":"playbooks/mas-manage/#install-manage-application","text":"This playbook will add Maximo Manage to an existing IBM Maximo Application Suite Instance. Refer to the mas_install_core playbook to set up the MAS Core Platform before running this playbook. The playbook will also create an in-cluster Db2 instance using the IBM Db2 Universal Operator, which will be automatically set up as the system-level JDBC configuration in MAS.","title":"Install Manage Application"},{"location":"playbooks/mas-manage/#playbook-content","text":"Install Cloud Pak for Data (optional, set CPD_INSTALL_PLATFORM ) Add Cognos to CP4D (optional, set CPD_INSTALL_COGNOS ) Add Watson Studio Local to CP4D (optional, set CPD_INSTALL_WSL ) Create Db2 Instance using IBM Db2 Universal Operator Initialize Db2 Instance for Maximo Manage Configure MAS to use the new Db2 Instance Configure MAS to use BYO database (optional, set CONFIGURE_EXTERNAL_DB ) Install Maximo Manage Application Configure Maximo Manage Workspace Configure Manage Attachments (optional, set CONFIGURE_MANAGE_ATTACHMENTS ) Configure Manage Building Information Models (optional, set CONFIGURE_MANAGE_BIM ) See the individual pages for each of these roles for more information and full details of all configuration options available in this playbook.","title":"Playbook Content"},{"location":"playbooks/mas-manage/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Tip Manage requires the user to select one or more application components to enable in the workspace. By default the base component at the latest version will be installed if no MAS_APPWS_COMPONENTS is set. To customise the components that are enabled use the MAS_APPWS_COMPONENTS environment variable, for example to enable Manage(base) and Health set it to the following: export MAS_APPWS_COMPONENTS=\"base=latest,health=latest\" To disable Asset Investment Optimizer, optional feature of health, set MAS_APP_SETTINGS_AIO_FLAG to false . By default this flag is set to true . This feature is only avalaible on Manage with health as a addon or on Health as a Standalone install. This feature is disabled on MAS Core 9.1 and later. export MAS_APP_SETTINGS_AIO_FLAG=false Note : To install Manage Foundation only that is available on MAS Core 9.1 or later, export the following environment variable: MAS_APPWS_COMPONENTS environment variable must be empty: export MAS_APPWS_COMPONENTS=\"\"","title":"Required environment variables"},{"location":"playbooks/mas-manage/#optional-cloud-pak-for-data-installation","text":"Optional integration with Cloud Pak for Data is supported in Maximo Manage. This can be enabled in the playbook as below: export CPD_INSTALL_PLATFORM=true export CPD_INSTALL_COGNOS=true export CPD_INSTALL_WSL=true export CPD_PRODUCT_VERSION=x.y.z","title":"Optional Cloud Pak for Data Installation"},{"location":"playbooks/mas-manage/#usage","text":"Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/mas-manage/#in-cluster-db2","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage","title":"In-Cluster Db2"},{"location":"playbooks/mas-manage/#bring-your-own-database","text":"If you do not want to use the Db2 Universal Operator to provide the datbase for Maximo Manage then you can configure the playbook as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CONFIGURE_EXTERNAL_DB=true export DB_INSTANCE_ID=maxdbxx export MAS_JDBC_USER=maximo export MAS_JDBC_PASSWORD=xxx export MAS_JDBC_URL=xxx export MAS_APP_SETTINGS_DB_SCHEMA=maximo export MAS_APP_SETTINGS_TABLESPACE=maxdata export MAS_APP_SETTINGS_INDEXSPACE=maxindex oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage For full details of configuration options available refer to the gencfg_jdbc role documentation.","title":"Bring Your Own Database"},{"location":"playbooks/mas-manage/#cloud-pak-for-data-integration","text":"To install CP4D with Cognos and/or Watson Studio Local optional dependencies: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CPD_INSTALL_PLATFORM=\"true\" export CPD_INSTALL_COGNOS=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_PRODUCT_VERSION=\"4.6.6\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage For full details of configuration options available refer to the cp4d & cp4d_service role documentation.","title":"Cloud Pak For Data Integration"},{"location":"playbooks/mas-manage/#health-standalone-install","text":"To install Health as a Standalone application, set MAS_APP_ID and MAS_APPWS_COMPONENTS as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_ID=health export MAS_APPWS_COMPONENTS=\"health=latest\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_manage Warning Note that installing Health standalone will prevent the use of all other Manage components. It is recommended to install Manage normally and just enable the Health component in the Manage application.","title":"Health Standalone Install"},{"location":"playbooks/mas-monitor/","text":"Install Monitor Application \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 and Maximo IoT v8.4 already be installed, the mas_install_core and mas-iot playbooks can be used to set this up. Overview \u00a4 This playbook will add Maximo Monitor v8.7 to an existing IBM Maximo Application Suite Core installation. Monitor will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Monitor application: Install application (60 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Optional environment variables \u00a4 MAS_APP_SETTINGS_MONITOR_DEPLOYMENT_SIZE Define the Monitor deployment size, one of dev , small or large . Defaults to dev . Usage \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_monitor Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Add Monitor"},{"location":"playbooks/mas-monitor/#install-monitor-application","text":"","title":"Install Monitor Application"},{"location":"playbooks/mas-monitor/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 and Maximo IoT v8.4 already be installed, the mas_install_core and mas-iot playbooks can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/mas-monitor/#overview","text":"This playbook will add Maximo Monitor v8.7 to an existing IBM Maximo Application Suite Core installation. Monitor will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Monitor application: Install application (60 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/mas-monitor/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/mas-monitor/#optional-environment-variables","text":"MAS_APP_SETTINGS_MONITOR_DEPLOYMENT_SIZE Define the Monitor deployment size, one of dev , small or large . Defaults to dev .","title":"Optional environment variables"},{"location":"playbooks/mas-monitor/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_monitor Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/mas-optimizer/","text":"Install Optimizer Application \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.9. The [mas_install_core] (mas-core.md) playbook can be used to set this up. Overview \u00a4 This playbook will add Maximo Optimizer v8.3 to an existing IBM Maximo Application Suite Core installation. Optimizer will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Optimizer application: Install application (10 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Usage \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_optimizer Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Add Optimizer"},{"location":"playbooks/mas-optimizer/#install-optimizer-application","text":"","title":"Install Optimizer Application"},{"location":"playbooks/mas-optimizer/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.9. The [mas_install_core] (mas-core.md) playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/mas-optimizer/#overview","text":"This playbook will add Maximo Optimizer v8.3 to an existing IBM Maximo Application Suite Core installation. Optimizer will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Optimizer application: Install application (10 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/mas-optimizer/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/mas-optimizer/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_optimizer Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/mas-predict/","text":"Install Predict Application \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift cluster with IBM Maximo Application Suite Core v8.11 already be installed, the mas_install_core playbook can be used to set this up. Overview \u00a4 This playbook will add Predict v8.9 to an existing IBM Maximo Application Suite Core installation. It will also install CloudPak for Data + CP4D services. This playbook can be ran against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install CP4D (~1 1/2 hours) Install Watson Studio (~3 hours) Install Watson Machine Learning (~2 1/2 hours) Install Spark (~30 minutes) Install Openscale (~1 hour) Install SPSS Install Predict application: Install application (~15 Minutes) Configure workspace (~30 Minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. As of MAS 8.10, predict 8.8.0 will start to support SPSS Modeler, to install SPSS as part of CP4D set CPD_INSTALL_SPSS=true in your environment variables before running the playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry CPD_WSL_PROJECT_ID - Ensure a Project ID Text box has a valid Watson Studio project ID. To obtain the project ID, Navigate to Cp4d/Watson Studio and create/Reuse a project. Open the project and look into the Browser URL, obtain the project ID from the URL and update Project ID settings. CPD_WML_INSTANCE_ID Set Default value to \"openshift\" CPD_WML_URL Set Default value to \"https://internal-nginx-svc.ibm-cpd.svc:12443\" . ibm-cpd in the URL corresponds to the project name (namespace) of cp4d installation CPD_PRODUCT_VERSION (Required if WML_VERSION is not informed) Cloud Pak for Data version installed in the cluster in 4.X format, it will be used to obtain the correct WML version to be installed WML_VERSION (Required if CPD_PRODUCT_VERSION is not informed) The wml_version for cp4d 4.0.x will be 4.0, if cp4d is 4.5.x , wml_version should change to 4.5, if cp4d is 4.6.x , wml_version should change to 4.6 These variables are required only if you set CP4D_INSTALL_WSL to false in optional varibles: CPD_ADMIN_USERNAME CP4D Username CPD_ADMIN_PASSWORD CP4D Password CPD_ADMIN_URL CP4D Base URL Warning When not using this playbook to install Cloud Pak for Data it is important to ensure that your existing instance already has all the required services enabled. Optional environment variables \u00a4 CPD_INSTALL_PLATFORM True/False - If you HAVE CP4D already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_WSL True/False - If you HAVE Watson Studio already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_WML True/False - If you HAVE Watson Machine Learning already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_SPARK True/False - If you HAVE Spark already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_OPENSCALE True/False - If you HAVE Openscale already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_SPSS True/False - If you HAVE SPSS Modeler already installed in your cluster you can skip this variable as False is set default Usage \u00a4 Tip If you do not want to set up all the dependencies on your local system, you can run the playbook from inside the CLI container image: docker run -ti --pull always quay.io/ibmmas/cli Cloud Pak for Data is already installed \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_CHANNEL=\"8.9.x\" export CPD_PRODUCT_VERSION=\"4.6.4\" export CPD_WSL_PROJECT_ID=\"xxxx\" export CPD_WML_INSTANCE_ID=\"openshift\" export CPD_WML_URL=\"https://internal-nginx-svc.ibm-cpd.svc:12443\" export WML_VERSION=\"4.6\" export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_ADMIN_URL=\"https://mycp4durl\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_predict Cloud Pak for Data is not installed \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_CHANNEL=\"8.9.x\" export CPD_PRODUCT_VERSION=\"4.6.4\" export CPD_INSTALL_PLATFORM=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_INSTALL_WML=\"true\" export CPD_INSTALL_SPARK=\"true\" export CPD_INSTALL_OPENSCALE=\"true\" export CPD_INSTALL_DISCOVERY=\"true\" export CPD_INSTALL_SPSS=\"true\" export CPD_WSL_PROJECT_ID=\"xxxx\" export CPD_WML_INSTANCE_ID=\"openshift\" export CPD_WML_URL=\"https://internal-nginx-svc.ibm-cpd.svc:12443\" export WML_VERSION=\"4.6\" export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_ADMIN_URL=\"https://mycp4durl\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_predict","title":"Add Predict"},{"location":"playbooks/mas-predict/#install-predict-application","text":"","title":"Install Predict Application"},{"location":"playbooks/mas-predict/#prerequisites","text":"You will need a RedHat OpenShift cluster with IBM Maximo Application Suite Core v8.11 already be installed, the mas_install_core playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/mas-predict/#overview","text":"This playbook will add Predict v8.9 to an existing IBM Maximo Application Suite Core installation. It will also install CloudPak for Data + CP4D services. This playbook can be ran against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install CP4D (~1 1/2 hours) Install Watson Studio (~3 hours) Install Watson Machine Learning (~2 1/2 hours) Install Spark (~30 minutes) Install Openscale (~1 hour) Install SPSS Install Predict application: Install application (~15 Minutes) Configure workspace (~30 Minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. As of MAS 8.10, predict 8.8.0 will start to support SPSS Modeler, to install SPSS as part of CP4D set CPD_INSTALL_SPSS=true in your environment variables before running the playbook.","title":"Overview"},{"location":"playbooks/mas-predict/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry CPD_WSL_PROJECT_ID - Ensure a Project ID Text box has a valid Watson Studio project ID. To obtain the project ID, Navigate to Cp4d/Watson Studio and create/Reuse a project. Open the project and look into the Browser URL, obtain the project ID from the URL and update Project ID settings. CPD_WML_INSTANCE_ID Set Default value to \"openshift\" CPD_WML_URL Set Default value to \"https://internal-nginx-svc.ibm-cpd.svc:12443\" . ibm-cpd in the URL corresponds to the project name (namespace) of cp4d installation CPD_PRODUCT_VERSION (Required if WML_VERSION is not informed) Cloud Pak for Data version installed in the cluster in 4.X format, it will be used to obtain the correct WML version to be installed WML_VERSION (Required if CPD_PRODUCT_VERSION is not informed) The wml_version for cp4d 4.0.x will be 4.0, if cp4d is 4.5.x , wml_version should change to 4.5, if cp4d is 4.6.x , wml_version should change to 4.6 These variables are required only if you set CP4D_INSTALL_WSL to false in optional varibles: CPD_ADMIN_USERNAME CP4D Username CPD_ADMIN_PASSWORD CP4D Password CPD_ADMIN_URL CP4D Base URL Warning When not using this playbook to install Cloud Pak for Data it is important to ensure that your existing instance already has all the required services enabled.","title":"Required environment variables"},{"location":"playbooks/mas-predict/#optional-environment-variables","text":"CPD_INSTALL_PLATFORM True/False - If you HAVE CP4D already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_WSL True/False - If you HAVE Watson Studio already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_WML True/False - If you HAVE Watson Machine Learning already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_SPARK True/False - If you HAVE Spark already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_OPENSCALE True/False - If you HAVE Openscale already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_SPSS True/False - If you HAVE SPSS Modeler already installed in your cluster you can skip this variable as False is set default","title":"Optional environment variables"},{"location":"playbooks/mas-predict/#usage","text":"Tip If you do not want to set up all the dependencies on your local system, you can run the playbook from inside the CLI container image: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/mas-predict/#cloud-pak-for-data-is-already-installed","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_CHANNEL=\"8.9.x\" export CPD_PRODUCT_VERSION=\"4.6.4\" export CPD_WSL_PROJECT_ID=\"xxxx\" export CPD_WML_INSTANCE_ID=\"openshift\" export CPD_WML_URL=\"https://internal-nginx-svc.ibm-cpd.svc:12443\" export WML_VERSION=\"4.6\" export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_ADMIN_URL=\"https://mycp4durl\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_predict","title":"Cloud Pak for Data is already installed"},{"location":"playbooks/mas-predict/#cloud-pak-for-data-is-not-installed","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_CHANNEL=\"8.9.x\" export CPD_PRODUCT_VERSION=\"4.6.4\" export CPD_INSTALL_PLATFORM=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_INSTALL_WML=\"true\" export CPD_INSTALL_SPARK=\"true\" export CPD_INSTALL_OPENSCALE=\"true\" export CPD_INSTALL_DISCOVERY=\"true\" export CPD_INSTALL_SPSS=\"true\" export CPD_WSL_PROJECT_ID=\"xxxx\" export CPD_WML_INSTANCE_ID=\"openshift\" export CPD_WML_URL=\"https://internal-nginx-svc.ibm-cpd.svc:12443\" export WML_VERSION=\"4.6\" export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_ADMIN_URL=\"https://mycp4durl\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_predict","title":"Cloud Pak for Data is not installed"},{"location":"playbooks/mas-update/","text":"OneClick Update \u00a4 This playbook will update the IBM Maximo Operator Catalog on your OpenShift cluster. This will make available new operator updates, which will be automatically applied across the cluster. These updates will not change the functionality of the software in your cluster, they will only carry fixes for security vulnerabilities and bugs. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) this playbook can be ignored, as you will recieve catalog updates in your cluster as soon as they are released. This playbook is specifically for customers who choose to use static catalogs to control the consumption of updates in their cluster. This is distinct from an upgrade , which will modify the operator subscriptions on your cluster to deliver new features. Performing an updating may make new upgrades available in the cluster, but it will never initiate the upgrade, you must choose when to upgrade. Playbook Content \u00a4 Install IBM Operator Catalog (1 minute) Preparation \u00a4 You will need to determine the version of the IBM Maximo Operator Catalog that you wish to update to. Generally speaking, you should update the most recent catalog available. Important If you are using a private/mirror registry it is critical that you mirror the images from the updated catalog before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the catalog has been updated. You do not need to worry about translating the image tags to digests to make these catalogs compatible with image mirroring on OpenShift, the role will automatically usse the image digest when it installs any static operator catalog. Usage \u00a4 Required environment variables \u00a4 MAS_CATALOG_VERSION Example \u00a4 Only one parameter is required, the new tag of the IBM Maximo Operator Catalog that you wish to use: export MAS_CATALOG_VERSION=@@MAS_LATEST_CATALOG@@ oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_update Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm --pull always quay.io/ibmmas/cli","title":"Update"},{"location":"playbooks/mas-update/#oneclick-update","text":"This playbook will update the IBM Maximo Operator Catalog on your OpenShift cluster. This will make available new operator updates, which will be automatically applied across the cluster. These updates will not change the functionality of the software in your cluster, they will only carry fixes for security vulnerabilities and bugs. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) this playbook can be ignored, as you will recieve catalog updates in your cluster as soon as they are released. This playbook is specifically for customers who choose to use static catalogs to control the consumption of updates in their cluster. This is distinct from an upgrade , which will modify the operator subscriptions on your cluster to deliver new features. Performing an updating may make new upgrades available in the cluster, but it will never initiate the upgrade, you must choose when to upgrade.","title":"OneClick Update"},{"location":"playbooks/mas-update/#playbook-content","text":"Install IBM Operator Catalog (1 minute)","title":"Playbook Content"},{"location":"playbooks/mas-update/#preparation","text":"You will need to determine the version of the IBM Maximo Operator Catalog that you wish to update to. Generally speaking, you should update the most recent catalog available. Important If you are using a private/mirror registry it is critical that you mirror the images from the updated catalog before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the catalog has been updated. You do not need to worry about translating the image tags to digests to make these catalogs compatible with image mirroring on OpenShift, the role will automatically usse the image digest when it installs any static operator catalog.","title":"Preparation"},{"location":"playbooks/mas-update/#usage","text":"","title":"Usage"},{"location":"playbooks/mas-update/#required-environment-variables","text":"MAS_CATALOG_VERSION","title":"Required environment variables"},{"location":"playbooks/mas-update/#example","text":"Only one parameter is required, the new tag of the IBM Maximo Operator Catalog that you wish to use: export MAS_CATALOG_VERSION=@@MAS_LATEST_CATALOG@@ oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_update Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm --pull always quay.io/ibmmas/cli","title":"Example"},{"location":"playbooks/mas-upgrade/","text":"OneClick Upgrade \u00a4 This playbook will upgrade the channel subscriptions for IBM Maximo Application Suite on your OpenShift cluster. Upgrades can only be performed to releases available in the version of the IBM Maximo Pperator Catalog that is installed in your cluster. To update to a newer version of the operator catalog refer to the mas-update playbook documentation. The playbook will attempt to upgrade MAS Core and all installed applications. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) you will always have access to the latest MAS releases, as you will recieve catalog updates in your cluster as soon as they are released. Customers using the static catalogs to control the consumption of updates in their cluster will need to ensure that the version of the catalog they have installed supports the version of MAS that they wish to upgrade to. Playbook Content \u00a4 Upgrade MAS Core Verify MAS Core Upgrade MAS Application (Assist) Upgrade MAS Application (IoT) Upgrade MAS Application (Manage) Upgrade MAS Application (Monitor) Upgrade MAS Application (Optimizer) Upgrade MAS Application (Predict) Upgrade MAS Application (Visual Inspection) Preparation \u00a4 If you are using a private/mirror registry it is critical that you mirror the images for the new release before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the subscription has been changed, if you have not mirrored the new images the subscription change itself may fail if the operator bundle is not on your private registry. Usage \u00a4 Required Parameters \u00a4 MAS_INSTANCE_ID Set the instance ID of the MAS installation to upgrade Optional Parameters \u00a4 If you provide no values for MAS Core or the individual applications, the roles will attempt to upgrade to the next level of MAS and upgrade applications to the latest version supported by the installed version of MAS Core (after upgrading MAS Core). MAS_CHANNEL Set the target subscription channel for MAS Core MAS_APP_CHANNEL_ASSIST Set the target subscription channel for Assist MAS_APP_CHANNEL_IOT Set the target subscription channel for IoT MAS_APP_CHANNEL_MONITOR Set the target subscription channel for Monitor MAS_APP_CHANNEL_OPTIMIZER Set the target subscription channel for Optimizer MAS_APP_CHANNEL_PREDICT Set the target subscription channel for Predict MAS_APP_CHANNEL_VISUALINSPECTION Set the target subscription channel for Visual Inspection Example \u00a4 The simplest way to upgrade MAS is to provide only the instance ID that you wish to upgrade, allowing the roles to determine the correct target version each application. export MAS_INSTANCE_ID=instance1 oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_upgrade You can also explicitly specify the target upgrade: export MAS_INSTANCE_ID=instance1 export MAS_CHANNEL=8.8.x export MAS_APP_CHANNEL_IOT=8.5.x oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_upgrade Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm --pull always quay.io/ibmmas/cli","title":"Upgrade"},{"location":"playbooks/mas-upgrade/#oneclick-upgrade","text":"This playbook will upgrade the channel subscriptions for IBM Maximo Application Suite on your OpenShift cluster. Upgrades can only be performed to releases available in the version of the IBM Maximo Pperator Catalog that is installed in your cluster. To update to a newer version of the operator catalog refer to the mas-update playbook documentation. The playbook will attempt to upgrade MAS Core and all installed applications. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) you will always have access to the latest MAS releases, as you will recieve catalog updates in your cluster as soon as they are released. Customers using the static catalogs to control the consumption of updates in their cluster will need to ensure that the version of the catalog they have installed supports the version of MAS that they wish to upgrade to.","title":"OneClick Upgrade"},{"location":"playbooks/mas-upgrade/#playbook-content","text":"Upgrade MAS Core Verify MAS Core Upgrade MAS Application (Assist) Upgrade MAS Application (IoT) Upgrade MAS Application (Manage) Upgrade MAS Application (Monitor) Upgrade MAS Application (Optimizer) Upgrade MAS Application (Predict) Upgrade MAS Application (Visual Inspection)","title":"Playbook Content"},{"location":"playbooks/mas-upgrade/#preparation","text":"If you are using a private/mirror registry it is critical that you mirror the images for the new release before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the subscription has been changed, if you have not mirrored the new images the subscription change itself may fail if the operator bundle is not on your private registry.","title":"Preparation"},{"location":"playbooks/mas-upgrade/#usage","text":"","title":"Usage"},{"location":"playbooks/mas-upgrade/#required-parameters","text":"MAS_INSTANCE_ID Set the instance ID of the MAS installation to upgrade","title":"Required Parameters"},{"location":"playbooks/mas-upgrade/#optional-parameters","text":"If you provide no values for MAS Core or the individual applications, the roles will attempt to upgrade to the next level of MAS and upgrade applications to the latest version supported by the installed version of MAS Core (after upgrading MAS Core). MAS_CHANNEL Set the target subscription channel for MAS Core MAS_APP_CHANNEL_ASSIST Set the target subscription channel for Assist MAS_APP_CHANNEL_IOT Set the target subscription channel for IoT MAS_APP_CHANNEL_MONITOR Set the target subscription channel for Monitor MAS_APP_CHANNEL_OPTIMIZER Set the target subscription channel for Optimizer MAS_APP_CHANNEL_PREDICT Set the target subscription channel for Predict MAS_APP_CHANNEL_VISUALINSPECTION Set the target subscription channel for Visual Inspection","title":"Optional Parameters"},{"location":"playbooks/mas-upgrade/#example","text":"The simplest way to upgrade MAS is to provide only the instance ID that you wish to upgrade, allowing the roles to determine the correct target version each application. export MAS_INSTANCE_ID=instance1 oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_upgrade You can also explicitly specify the target upgrade: export MAS_INSTANCE_ID=instance1 export MAS_CHANNEL=8.8.x export MAS_APP_CHANNEL_IOT=8.5.x oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_upgrade Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm --pull always quay.io/ibmmas/cli","title":"Example"},{"location":"playbooks/mas-visualinspection/","text":"Install Visual Inspection Application \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.9. The [mas_install_core] (mas-core.md) playbook can be used to set this up. Overview \u00a4 This playbook will add Maximo Visual Inspection v8.7 to an existing IBM Maximo Application Suite Core installation. MVI will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install NVIDIA Graphical Processing Unit (GPU) (10 minutes) Install Maximo Visual Inspection application: Install application (15 minutes) Configure workspace (10 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Optional environment variables \u00a4 MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_CLASS Defines a custom file storage class for Visual Inspection application. If none provided, then a default storage class will be auto defined accordingly to your cluster's availability i.e ibmc-file-gold for IBM Cloud or azurefiles-premium for Azure clusters. MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_SIZE Defines persistent storage size for Visual Inspection application. If not provided, default is 100Gi . MAS_APP_SETTINGS_VISUALINSPECTION_OBJECT_STORAGE_ENABLED If set to true , enables Object Storage integration with Visual Inspection . MAS_APP_SETTINGS_VISUALINSPECTION_OBJECT_STORAGE_WORKSPACE Defines the Object Storage bucket name to be used for Visual Inspection integration. CONFIGURE_COS If set to true , an Object Storage instance will be configured as MAS system scope configuration which will be used for Visual Inspection integration. See cos role documentation for detailed information. CONFIGURE_COS_BUCKET If set to true , an Object Storage bucket will be configured to be used for Visual Inspection application. See cos_bucket role documentation for detailed information. Usage \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_visualinspection Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Add Visual Inspection"},{"location":"playbooks/mas-visualinspection/#install-visual-inspection-application","text":"","title":"Install Visual Inspection Application"},{"location":"playbooks/mas-visualinspection/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.9. The [mas_install_core] (mas-core.md) playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/mas-visualinspection/#overview","text":"This playbook will add Maximo Visual Inspection v8.7 to an existing IBM Maximo Application Suite Core installation. MVI will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install NVIDIA Graphical Processing Unit (GPU) (10 minutes) Install Maximo Visual Inspection application: Install application (15 minutes) Configure workspace (10 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/mas-visualinspection/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/mas-visualinspection/#optional-environment-variables","text":"MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_CLASS Defines a custom file storage class for Visual Inspection application. If none provided, then a default storage class will be auto defined accordingly to your cluster's availability i.e ibmc-file-gold for IBM Cloud or azurefiles-premium for Azure clusters. MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_SIZE Defines persistent storage size for Visual Inspection application. If not provided, default is 100Gi . MAS_APP_SETTINGS_VISUALINSPECTION_OBJECT_STORAGE_ENABLED If set to true , enables Object Storage integration with Visual Inspection . MAS_APP_SETTINGS_VISUALINSPECTION_OBJECT_STORAGE_WORKSPACE Defines the Object Storage bucket name to be used for Visual Inspection integration. CONFIGURE_COS If set to true , an Object Storage instance will be configured as MAS system scope configuration which will be used for Visual Inspection integration. See cos role documentation for detailed information. CONFIGURE_COS_BUCKET If set to true , an Object Storage bucket will be configured to be used for Visual Inspection application. See cos_bucket role documentation for detailed information.","title":"Optional environment variables"},{"location":"playbooks/mas-visualinspection/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.mas_add_visualinspection Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/ocp/","text":"OCP Playbooks \u00a4 Provision \u00a4 Refer to the ocp_provision role documentation for more information. Provision on AWS ROSA \u00a4 This playbook uses your ROSA API Token to provision a brand new OCP cluster, provision an instance of EFS and set up the cluster with a ReadWriteMany storage class named efs utilizing that instance. To obtain your API token login to the OpenShift cluster manager . export AWS_ACCESS_KEY_ID=xxx export AWS_SECRET_ACCESS_KEY=xxx export ROSA_TOKEN=xxx export CLUSTER_NAME=masonrosa export OCP_VERSION=4.19 export ROSA_COMPUTE_NODES=5 export ROSA_CLUSTER_ADMIN_PASSWORD=xxx ansible-playbook ibm.mas_devops.ocp_rosa_provision Provision on IBMCloud ROKS \u00a4 This playbook uses your IBMCloud API key to provision a brand new OCP cluster. The playbook supports installing an IBM entitlement key as a cluster-wide image pull secret and reboot all worker nodes, which is required for IBM Cloud Pak for Data v4; this can be enabled by setting REBOOT_WORKER_NODES to true and providing the entitlement key with CPD_ENTITLEMENT_KEY . This also supports upgrading the storage volume used for the cluster's internal image registry from 100Gb to 400Gb, this must be enabled by setting UPGRADE_IMAGE_REGISTRY_STORAGE to true . This option is stringly recommended if you intend to install the Watson services from Cloud Pak for Data as the default volume size is too small. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.19_openshift export IBMCLOUD_APIKEY=xxx export REBOOT_WORKER_NODES=true export CPD_ENTITLEMENT_KEY=xxx export UPGRADE_IMAGE_REGISTRY_STORAGE=true ansible-playbook ibm.mas_devops.ocp_roks_provision Provision on IBM DevIT Fyre \u00a4 This playbook will provision a QuickBurn OCP cluster in IBM DevIT Fyre service, QuickBurn clusters will be automatically deprovisioned after 36 hours and are only suitable for small scale deployments for local development and demostration systems. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.19 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx export FYRE_PRODUCT_ID=xxx ansible-playbook ibm.mas_devops.ocp_fyre_provision Deprovision \u00a4 Refer to the ocp_deprovision role documentation for more information. Deprovision on IBMCloud ROKS \u00a4 export CLUSTER_NAME=masinst1 export IBMCLOUD_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_roks_deprovision Deprovision on IBM DevIT Fyre \u00a4 export CLUSTER_NAME=masinst1 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_fyre_deprovision","title":"OCP"},{"location":"playbooks/ocp/#ocp-playbooks","text":"","title":"OCP Playbooks"},{"location":"playbooks/ocp/#provision","text":"Refer to the ocp_provision role documentation for more information.","title":"Provision"},{"location":"playbooks/ocp/#provision-on-aws-rosa","text":"This playbook uses your ROSA API Token to provision a brand new OCP cluster, provision an instance of EFS and set up the cluster with a ReadWriteMany storage class named efs utilizing that instance. To obtain your API token login to the OpenShift cluster manager . export AWS_ACCESS_KEY_ID=xxx export AWS_SECRET_ACCESS_KEY=xxx export ROSA_TOKEN=xxx export CLUSTER_NAME=masonrosa export OCP_VERSION=4.19 export ROSA_COMPUTE_NODES=5 export ROSA_CLUSTER_ADMIN_PASSWORD=xxx ansible-playbook ibm.mas_devops.ocp_rosa_provision","title":"Provision on AWS ROSA"},{"location":"playbooks/ocp/#provision-on-ibmcloud-roks","text":"This playbook uses your IBMCloud API key to provision a brand new OCP cluster. The playbook supports installing an IBM entitlement key as a cluster-wide image pull secret and reboot all worker nodes, which is required for IBM Cloud Pak for Data v4; this can be enabled by setting REBOOT_WORKER_NODES to true and providing the entitlement key with CPD_ENTITLEMENT_KEY . This also supports upgrading the storage volume used for the cluster's internal image registry from 100Gb to 400Gb, this must be enabled by setting UPGRADE_IMAGE_REGISTRY_STORAGE to true . This option is stringly recommended if you intend to install the Watson services from Cloud Pak for Data as the default volume size is too small. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.19_openshift export IBMCLOUD_APIKEY=xxx export REBOOT_WORKER_NODES=true export CPD_ENTITLEMENT_KEY=xxx export UPGRADE_IMAGE_REGISTRY_STORAGE=true ansible-playbook ibm.mas_devops.ocp_roks_provision","title":"Provision on IBMCloud ROKS"},{"location":"playbooks/ocp/#provision-on-ibm-devit-fyre","text":"This playbook will provision a QuickBurn OCP cluster in IBM DevIT Fyre service, QuickBurn clusters will be automatically deprovisioned after 36 hours and are only suitable for small scale deployments for local development and demostration systems. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.19 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx export FYRE_PRODUCT_ID=xxx ansible-playbook ibm.mas_devops.ocp_fyre_provision","title":"Provision on IBM DevIT Fyre"},{"location":"playbooks/ocp/#deprovision","text":"Refer to the ocp_deprovision role documentation for more information.","title":"Deprovision"},{"location":"playbooks/ocp/#deprovision-on-ibmcloud-roks","text":"export CLUSTER_NAME=masinst1 export IBMCLOUD_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_roks_deprovision","title":"Deprovision on IBMCloud ROKS"},{"location":"playbooks/ocp/#deprovision-on-ibm-devit-fyre","text":"export CLUSTER_NAME=masinst1 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_fyre_deprovision","title":"Deprovision on IBM DevIT Fyre"},{"location":"roles/aiservice/","text":"aiservice \u00a4 This role provides support to install and configure AI Service for IBM Maximo Application Suite. AI Service enables AI-powered capabilities within MAS applications, particularly for Maximo Manage. The role supports the following operations: - Install AI Service API application - Create and delete AI Service tenants - Manage AI Service API keys - Configure AWS S3 storage integration - Configure WatsonX AI integration Role Variables \u00a4 General Variables \u00a4 tenant_action \u00a4 Action to perform on AI Service tenant. Optional Environment Variable: TENANT_ACTION Default: install Purpose : Specifies whether to install or remove an AI Service tenant. Controls tenant lifecycle management. When to use : - Use install (default) to create a new AI Service tenant - Use remove to delete an existing AI Service tenant - Required for tenant management operations Valid values : install , remove Impact : - install : Creates tenant with specified configuration - remove : Deletes tenant and associated resources Related variables : - tenantName : Name of tenant to install/remove - All other variables apply only when action is install Note : WARNING - remove action permanently deletes the tenant and all associated data. Ensure you have backups before removing a tenant. tenantName \u00a4 AI Service tenant identifier. Optional Environment Variable: AISERVICE_TENANT_NAME Default: user Purpose : Specifies the name/identifier for the AI Service tenant. Tenants provide isolation between different users or environments. When to use : - Use default ( user ) for single-tenant deployments - Set custom name for multi-tenant environments - Use descriptive names (e.g., production , development , team-a ) Valid values : Valid tenant name string (alphanumeric, lowercase recommended) Impact : Tenant name is used in resource names, API keys, and configuration. Must be unique within the AI Service instance. Related variables : - tenant_action : Whether to install or remove this tenant - Tenant name is used in generated API key secrets Note : Choose meaningful tenant names for multi-tenant scenarios. The default user is suitable for single-tenant deployments. app_domain \u00a4 Application domain for AI Service routes. Optional Environment Variable: APP_DOMAIN Default: Auto-detected from cluster Purpose : Specifies the application domain for AI Service routes and endpoints. Used to construct the full URL for AI Service API access. When to use : - Leave unset for automatic detection from cluster configuration - Set explicitly when cluster domain cannot be auto-detected - Required for custom domain configurations Valid values : Domain string in format apps.domain (e.g., apps.mycluster.example.com ) Impact : Determines the URL where AI Service API is accessible. Incorrect domain will prevent API access. Related variables : - aiservice_domain : Custom domain override (takes precedence if set) Note : The role automatically detects the cluster's application domain. Only set this if auto-detection fails or you need a custom domain. Format must be apps.<domain> . aiservice_domain \u00a4 Custom domain override for AI Service. Optional Environment Variable: AISERVICE_DOMAIN Default: None (uses app_domain or cluster default) Purpose : Provides a custom domain specifically for AI Service, overriding the general application domain. Useful for custom DNS configurations. When to use : - Leave unset to use app_domain or cluster default - Set when AI Service needs a different domain than other applications - Required for custom DNS or external domain configurations Valid values : Valid domain string (e.g., aiservice.example.com ) Impact : When set, this domain is used instead of app_domain for AI Service routes. Takes precedence over app_domain . Related variables : - app_domain : General application domain (used if this is not set) Note : This is an advanced configuration option. Most deployments should use app_domain or cluster auto-detection. Only set this if AI Service requires a separate domain. S3 Storage Configuration Variables \u00a4 aiservice_s3_host \u00a4 S3-compatible storage host endpoint. Optional Environment Variable: AISERVICE_S3_HOST Default: None Purpose : Specifies the endpoint URL for S3-compatible object storage used by AI Service for storing models, data, and artifacts. When to use : - Required when configuring S3 storage integration - Set to AWS S3 endpoint (e.g., s3.amazonaws.com ) or compatible service - Must be accessible from the cluster Valid values : Valid S3 endpoint URL (e.g., s3.amazonaws.com , s3.us-east-1.amazonaws.com , MinIO endpoint) Impact : AI Service uses this storage for persistent data. Without proper S3 configuration, AI Service functionality will be limited. Related variables : - aiservice_s3_accesskey : Access credentials for this host - aiservice_s3_secretkey : Secret credentials for this host - aiservice_s3_region : Region for this host Note : All S3 variables ( aiservice_s3_* ) must be configured together for S3 integration. Supports AWS S3 and S3-compatible services like MinIO, IBM Cloud Object Storage. aiservice_s3_accesskey \u00a4 S3 storage access key ID. Optional Environment Variable: AISERVICE_S3_ACCESSKEY Default: None Purpose : Provides the access key ID for authenticating to S3-compatible object storage. Part of the credential pair for S3 access. When to use : - Required when configuring S3 storage integration - Obtain from your S3 provider (AWS IAM, MinIO, etc.) - Must have permissions to create/read/write buckets and objects Valid values : Valid S3 access key ID string Impact : Without valid credentials, AI Service cannot access S3 storage, limiting functionality. Related variables : - aiservice_s3_secretkey : Secret key paired with this access key - aiservice_s3_host : S3 endpoint to authenticate against - aiservice_s3_region : Region for the S3 service Note : SECURITY - Keep access keys secure. Do not commit to source control. Use environment variables or secure secret management. Ensure the access key has appropriate S3 permissions for AI Service operations. aiservice_s3_secretkey \u00a4 S3 storage secret access key. Optional Environment Variable: AISERVICE_S3_SECRETKEY Default: None Purpose : Provides the secret access key for authenticating to S3-compatible object storage. Part of the credential pair for S3 access. When to use : - Required when configuring S3 storage integration - Obtain from your S3 provider (AWS IAM, MinIO, etc.) - Must be paired with corresponding aiservice_s3_accesskey Valid values : Valid S3 secret access key string Impact : Without valid credentials, AI Service cannot access S3 storage, limiting functionality. Related variables : - aiservice_s3_accesskey : Access key ID paired with this secret key - aiservice_s3_host : S3 endpoint to authenticate against - aiservice_s3_region : Region for the S3 service Note : SECURITY - Keep secret keys secure. Never commit to source control or expose in logs. Use environment variables or secure secret management. The secret key must match the access key ID. aiservice_s3_region \u00a4 S3 storage region. Optional Environment Variable: AISERVICE_S3_REGION Default: None Purpose : Specifies the AWS region or region identifier for S3-compatible object storage. Required for proper S3 API operations. When to use : - Required when configuring S3 storage integration - Set to AWS region (e.g., us-east-1 , eu-west-1 ) or compatible service region - Must match the region where your S3 buckets are located Valid values : Valid AWS region code or S3-compatible service region identifier Impact : Incorrect region will cause S3 API calls to fail. Must match the actual bucket location. Related variables : - aiservice_s3_host : S3 endpoint (may include region in URL) - aiservice_s3_accesskey : Access credentials for this region - aiservice_s3_secretkey : Secret credentials for this region Note : For AWS S3, use standard region codes (e.g., us-east-1 ). For S3-compatible services, use the region identifier provided by your service. Some services may not require a region. WatsonX AI Configuration Variables \u00a4 aiservice_watsonx_action \u00a4 Action to perform on WatsonX AI integration. Optional Environment Variable: AISERVICE_WATSONX_ACTION Default: install Purpose : Specifies whether to install or remove WatsonX AI integration with AI Service. Controls WatsonX integration lifecycle. When to use : - Use install (default) to configure WatsonX AI integration - Use remove to delete WatsonX AI integration - Required for WatsonX integration management Valid values : install , remove Impact : - install : Configures AI Service to use WatsonX AI for AI/ML capabilities - remove : Removes WatsonX AI integration configuration Related variables : - aiservice_watsonxai_apikey : API key for WatsonX (required for install) - aiservice_watsonxai_url : WatsonX endpoint (required for install) - aiservice_watsonxai_project_id : WatsonX project (required for install) Note : WatsonX AI integration enables advanced AI capabilities in AI Service. All WatsonX variables must be configured together for successful integration. aiservice_watsonxai_apikey \u00a4 WatsonX AI API key for authentication. Optional Environment Variable: AISERVICE_WATSONXAI_APIKEY Default: None Purpose : Provides the API key for authenticating AI Service with IBM WatsonX AI platform. Required for WatsonX AI integration. When to use : - Required when aiservice_watsonx_action is install - Obtain from IBM Cloud WatsonX AI service - Must have appropriate WatsonX AI permissions Valid values : Valid IBM WatsonX AI API key string Impact : Without valid API key, AI Service cannot access WatsonX AI capabilities. Integration will fail. Related variables : - aiservice_watsonxai_url : WatsonX AI endpoint to authenticate against - aiservice_watsonxai_project_id : WatsonX project to access - aiservice_watsonx_action : Whether to install or remove integration Note : SECURITY - Keep API keys secure. Do not commit to source control. Use environment variables or secure secret management. Obtain from IBM Cloud IAM or WatsonX AI service credentials. aiservice_watsonxai_url \u00a4 WatsonX AI service endpoint URL. Optional Environment Variable: AISERVICE_WATSONXAI_URL Default: None Purpose : Specifies the endpoint URL for IBM WatsonX AI service. Required for AI Service to connect to WatsonX AI platform. When to use : - Required when aiservice_watsonx_action is install - Set to your WatsonX AI region endpoint - Must be accessible from the cluster Valid values : Valid WatsonX AI endpoint URL (e.g., https://us-south.ml.cloud.ibm.com , https://eu-de.ml.cloud.ibm.com ) Impact : AI Service uses this URL to access WatsonX AI APIs. Incorrect URL will prevent WatsonX integration. Related variables : - aiservice_watsonxai_apikey : API key for authenticating to this endpoint - aiservice_watsonxai_project_id : Project to access at this endpoint - aiservice_watsonx_action : Whether to install or remove integration Note : Use the WatsonX AI endpoint for your IBM Cloud region. Common endpoints: https://us-south.ml.cloud.ibm.com (Dallas), https://eu-de.ml.cloud.ibm.com (Frankfurt), https://jp-tok.ml.cloud.ibm.com (Tokyo). aiservice_watsonxai_project_id \u00a4 WatsonX AI project identifier. Optional Environment Variable: AISERVICE_WATSONXAI_PROJECT_ID Default: None Purpose : Specifies the WatsonX AI project ID that AI Service will use for AI/ML operations. Projects organize resources and control access in WatsonX AI. When to use : - Required when aiservice_watsonx_action is install - Obtain from your WatsonX AI project in IBM Cloud - Project must have appropriate models and resources configured Valid values : Valid WatsonX AI project ID (UUID format) Impact : AI Service uses this project for accessing WatsonX AI models and resources. Incorrect project ID will prevent access to AI capabilities. Related variables : - aiservice_watsonxai_apikey : API key must have access to this project - aiservice_watsonxai_url : WatsonX endpoint where this project exists - aiservice_watsonx_action : Whether to install or remove integration Note : The project ID is found in your WatsonX AI project settings in IBM Cloud. Ensure the API key has appropriate permissions for the project. The project should have the required AI models and resources configured. Certificate Management \u00a4 aiservice_certificate_issuer \u00a4 Name of the cert-manager Issuer to use for automatic certificate generation. Optional Environment Variable: AISERVICE_CERTIFICATE_ISSUER Default: None Purpose : Specifies which cert-manager Issuer will generate and manage SSL/TLS certificates for AI Service. The Issuer defines the Certificate Authority and authentication method used for certificate issuance. When to use : - Set to the Issuer created by suite_dns role (e.g., {aiservice_instance_id}-cloudflare-le-prod ) - Set to a custom Issuer if you have specific certificate requirements Valid values : Name of any valid Issuer resource in the cluster (e.g., prod-le-issuer , {mas_instance_id}-cloudflare-le-prod ) Impact : The specified Issuer will be used to generate all certificates for AI Service. If the Issuer is not properly configured or lacks necessary credentials, certificate generation will fail and AI Service will not be accessible. Related variables : - Created by suite_dns role for Let's Encrypt integration - Works with aiservice_certificate_duration and aiservice_certificate_renew_before Note : Ensure the Issuer is created and functional before installing AI Service. Test certificate generation with a test Certificate resource first. aiservice_certificate_duration \u00a4 Specifies the validity period for AI Service certificates. Optional Environment Variable: AISERVICE_CERTIFICATE_DURATION Default: 8760h0m0s (1 year) Purpose : Defines how long certificates will be valid before they expire. This affects how often certificates need to be renewed and the security posture of your installation. When to use : - Use default (8760h = 1 year) for most installations - Reduce for higher security environments requiring frequent rotation - Increase only if certificate renewal is problematic in your environment - Must be longer than aiservice_certificate_renew_before Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 8760h0m0s , 2160h0m0s , 17520h0m0s ) Impact : Shorter durations increase security but require more frequent renewals. Longer durations reduce renewal frequency but increase risk if certificates are compromised. Cert-manager will automatically renew certificates before expiration. Related variables : - Must be greater than aiservice_certificate_renew_before - Only applies when using automatic certificate management - Affects all AI Service certificates aiservice_certificate_renew_before \u00a4 Specifies when to renew certificates before they expire. Optional Environment Variable: AISERVICE_CERTIFICATE_RENEW_BEFORE Default: 720h0m0s (30 days) Purpose : Defines the renewal window - how far in advance cert-manager will renew certificates before they expire. This ensures certificates are renewed with sufficient time to handle any renewal issues. When to use : - Use default (720h = 30 days) for most installations - Increase in environments where certificate renewal may be slow or problematic - Decrease only if you need to minimize the number of certificate changes - Must be shorter than aiservice_certificate_duration Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 720h0m0s , 1440h0m0s , 168h0m0s ) Impact : Longer renewal windows provide more time to resolve renewal issues but result in more frequent certificate changes. Shorter windows reduce certificate churn but increase risk of expiration if renewal fails. Related variables : - Must be less than aiservice_certificate_duration - Only applies when using automatic certificate management - Affects all AI Service certificates Note : Ensure this value provides adequate time to detect and resolve certificate renewal issues before expiration. Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: tenant_action: install tenantName: production app_domain: apps.mycluster.example.com aiservice_s3_host: s3.amazonaws.com aiservice_s3_accesskey: \"{{ lookup('env', 'AWS_ACCESS_KEY') }}\" aiservice_s3_secretkey: \"{{ lookup('env', 'AWS_SECRET_KEY') }}\" aiservice_s3_region: us-east-1 aiservice_watsonxai_apikey: \"{{ lookup('env', 'WATSONX_API_KEY') }}\" aiservice_watsonxai_url: https://us-south.ml.cloud.ibm.com aiservice_watsonxai_project_id: my-project-id roles: - ibm.mas_devops.aiservice Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export TENANT_ACTION=install export AISERVICE_TENANT_NAME=production export APP_DOMAIN=apps.mycluster.example.com export AISERVICE_S3_HOST=s3.amazonaws.com export AISERVICE_S3_ACCESSKEY=your_access_key export AISERVICE_S3_SECRETKEY=your_secret_key export AISERVICE_S3_REGION=us-east-1 export AISERVICE_WATSONXAI_APIKEY=your_watsonx_api_key export AISERVICE_WATSONXAI_URL=https://us-south.ml.cloud.ibm.com export AISERVICE_WATSONXAI_PROJECT_ID=my-project-id ROLE_NAME=aiservice ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aiservice"},{"location":"roles/aiservice/#aiservice","text":"This role provides support to install and configure AI Service for IBM Maximo Application Suite. AI Service enables AI-powered capabilities within MAS applications, particularly for Maximo Manage. The role supports the following operations: - Install AI Service API application - Create and delete AI Service tenants - Manage AI Service API keys - Configure AWS S3 storage integration - Configure WatsonX AI integration","title":"aiservice"},{"location":"roles/aiservice/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aiservice/#general-variables","text":"","title":"General Variables"},{"location":"roles/aiservice/#tenant_action","text":"Action to perform on AI Service tenant. Optional Environment Variable: TENANT_ACTION Default: install Purpose : Specifies whether to install or remove an AI Service tenant. Controls tenant lifecycle management. When to use : - Use install (default) to create a new AI Service tenant - Use remove to delete an existing AI Service tenant - Required for tenant management operations Valid values : install , remove Impact : - install : Creates tenant with specified configuration - remove : Deletes tenant and associated resources Related variables : - tenantName : Name of tenant to install/remove - All other variables apply only when action is install Note : WARNING - remove action permanently deletes the tenant and all associated data. Ensure you have backups before removing a tenant.","title":"tenant_action"},{"location":"roles/aiservice/#tenantname","text":"AI Service tenant identifier. Optional Environment Variable: AISERVICE_TENANT_NAME Default: user Purpose : Specifies the name/identifier for the AI Service tenant. Tenants provide isolation between different users or environments. When to use : - Use default ( user ) for single-tenant deployments - Set custom name for multi-tenant environments - Use descriptive names (e.g., production , development , team-a ) Valid values : Valid tenant name string (alphanumeric, lowercase recommended) Impact : Tenant name is used in resource names, API keys, and configuration. Must be unique within the AI Service instance. Related variables : - tenant_action : Whether to install or remove this tenant - Tenant name is used in generated API key secrets Note : Choose meaningful tenant names for multi-tenant scenarios. The default user is suitable for single-tenant deployments.","title":"tenantName"},{"location":"roles/aiservice/#app_domain","text":"Application domain for AI Service routes. Optional Environment Variable: APP_DOMAIN Default: Auto-detected from cluster Purpose : Specifies the application domain for AI Service routes and endpoints. Used to construct the full URL for AI Service API access. When to use : - Leave unset for automatic detection from cluster configuration - Set explicitly when cluster domain cannot be auto-detected - Required for custom domain configurations Valid values : Domain string in format apps.domain (e.g., apps.mycluster.example.com ) Impact : Determines the URL where AI Service API is accessible. Incorrect domain will prevent API access. Related variables : - aiservice_domain : Custom domain override (takes precedence if set) Note : The role automatically detects the cluster's application domain. Only set this if auto-detection fails or you need a custom domain. Format must be apps.<domain> .","title":"app_domain"},{"location":"roles/aiservice/#aiservice_domain","text":"Custom domain override for AI Service. Optional Environment Variable: AISERVICE_DOMAIN Default: None (uses app_domain or cluster default) Purpose : Provides a custom domain specifically for AI Service, overriding the general application domain. Useful for custom DNS configurations. When to use : - Leave unset to use app_domain or cluster default - Set when AI Service needs a different domain than other applications - Required for custom DNS or external domain configurations Valid values : Valid domain string (e.g., aiservice.example.com ) Impact : When set, this domain is used instead of app_domain for AI Service routes. Takes precedence over app_domain . Related variables : - app_domain : General application domain (used if this is not set) Note : This is an advanced configuration option. Most deployments should use app_domain or cluster auto-detection. Only set this if AI Service requires a separate domain.","title":"aiservice_domain"},{"location":"roles/aiservice/#s3-storage-configuration-variables","text":"","title":"S3 Storage Configuration Variables"},{"location":"roles/aiservice/#aiservice_s3_host","text":"S3-compatible storage host endpoint. Optional Environment Variable: AISERVICE_S3_HOST Default: None Purpose : Specifies the endpoint URL for S3-compatible object storage used by AI Service for storing models, data, and artifacts. When to use : - Required when configuring S3 storage integration - Set to AWS S3 endpoint (e.g., s3.amazonaws.com ) or compatible service - Must be accessible from the cluster Valid values : Valid S3 endpoint URL (e.g., s3.amazonaws.com , s3.us-east-1.amazonaws.com , MinIO endpoint) Impact : AI Service uses this storage for persistent data. Without proper S3 configuration, AI Service functionality will be limited. Related variables : - aiservice_s3_accesskey : Access credentials for this host - aiservice_s3_secretkey : Secret credentials for this host - aiservice_s3_region : Region for this host Note : All S3 variables ( aiservice_s3_* ) must be configured together for S3 integration. Supports AWS S3 and S3-compatible services like MinIO, IBM Cloud Object Storage.","title":"aiservice_s3_host"},{"location":"roles/aiservice/#aiservice_s3_accesskey","text":"S3 storage access key ID. Optional Environment Variable: AISERVICE_S3_ACCESSKEY Default: None Purpose : Provides the access key ID for authenticating to S3-compatible object storage. Part of the credential pair for S3 access. When to use : - Required when configuring S3 storage integration - Obtain from your S3 provider (AWS IAM, MinIO, etc.) - Must have permissions to create/read/write buckets and objects Valid values : Valid S3 access key ID string Impact : Without valid credentials, AI Service cannot access S3 storage, limiting functionality. Related variables : - aiservice_s3_secretkey : Secret key paired with this access key - aiservice_s3_host : S3 endpoint to authenticate against - aiservice_s3_region : Region for the S3 service Note : SECURITY - Keep access keys secure. Do not commit to source control. Use environment variables or secure secret management. Ensure the access key has appropriate S3 permissions for AI Service operations.","title":"aiservice_s3_accesskey"},{"location":"roles/aiservice/#aiservice_s3_secretkey","text":"S3 storage secret access key. Optional Environment Variable: AISERVICE_S3_SECRETKEY Default: None Purpose : Provides the secret access key for authenticating to S3-compatible object storage. Part of the credential pair for S3 access. When to use : - Required when configuring S3 storage integration - Obtain from your S3 provider (AWS IAM, MinIO, etc.) - Must be paired with corresponding aiservice_s3_accesskey Valid values : Valid S3 secret access key string Impact : Without valid credentials, AI Service cannot access S3 storage, limiting functionality. Related variables : - aiservice_s3_accesskey : Access key ID paired with this secret key - aiservice_s3_host : S3 endpoint to authenticate against - aiservice_s3_region : Region for the S3 service Note : SECURITY - Keep secret keys secure. Never commit to source control or expose in logs. Use environment variables or secure secret management. The secret key must match the access key ID.","title":"aiservice_s3_secretkey"},{"location":"roles/aiservice/#aiservice_s3_region","text":"S3 storage region. Optional Environment Variable: AISERVICE_S3_REGION Default: None Purpose : Specifies the AWS region or region identifier for S3-compatible object storage. Required for proper S3 API operations. When to use : - Required when configuring S3 storage integration - Set to AWS region (e.g., us-east-1 , eu-west-1 ) or compatible service region - Must match the region where your S3 buckets are located Valid values : Valid AWS region code or S3-compatible service region identifier Impact : Incorrect region will cause S3 API calls to fail. Must match the actual bucket location. Related variables : - aiservice_s3_host : S3 endpoint (may include region in URL) - aiservice_s3_accesskey : Access credentials for this region - aiservice_s3_secretkey : Secret credentials for this region Note : For AWS S3, use standard region codes (e.g., us-east-1 ). For S3-compatible services, use the region identifier provided by your service. Some services may not require a region.","title":"aiservice_s3_region"},{"location":"roles/aiservice/#watsonx-ai-configuration-variables","text":"","title":"WatsonX AI Configuration Variables"},{"location":"roles/aiservice/#aiservice_watsonx_action","text":"Action to perform on WatsonX AI integration. Optional Environment Variable: AISERVICE_WATSONX_ACTION Default: install Purpose : Specifies whether to install or remove WatsonX AI integration with AI Service. Controls WatsonX integration lifecycle. When to use : - Use install (default) to configure WatsonX AI integration - Use remove to delete WatsonX AI integration - Required for WatsonX integration management Valid values : install , remove Impact : - install : Configures AI Service to use WatsonX AI for AI/ML capabilities - remove : Removes WatsonX AI integration configuration Related variables : - aiservice_watsonxai_apikey : API key for WatsonX (required for install) - aiservice_watsonxai_url : WatsonX endpoint (required for install) - aiservice_watsonxai_project_id : WatsonX project (required for install) Note : WatsonX AI integration enables advanced AI capabilities in AI Service. All WatsonX variables must be configured together for successful integration.","title":"aiservice_watsonx_action"},{"location":"roles/aiservice/#aiservice_watsonxai_apikey","text":"WatsonX AI API key for authentication. Optional Environment Variable: AISERVICE_WATSONXAI_APIKEY Default: None Purpose : Provides the API key for authenticating AI Service with IBM WatsonX AI platform. Required for WatsonX AI integration. When to use : - Required when aiservice_watsonx_action is install - Obtain from IBM Cloud WatsonX AI service - Must have appropriate WatsonX AI permissions Valid values : Valid IBM WatsonX AI API key string Impact : Without valid API key, AI Service cannot access WatsonX AI capabilities. Integration will fail. Related variables : - aiservice_watsonxai_url : WatsonX AI endpoint to authenticate against - aiservice_watsonxai_project_id : WatsonX project to access - aiservice_watsonx_action : Whether to install or remove integration Note : SECURITY - Keep API keys secure. Do not commit to source control. Use environment variables or secure secret management. Obtain from IBM Cloud IAM or WatsonX AI service credentials.","title":"aiservice_watsonxai_apikey"},{"location":"roles/aiservice/#aiservice_watsonxai_url","text":"WatsonX AI service endpoint URL. Optional Environment Variable: AISERVICE_WATSONXAI_URL Default: None Purpose : Specifies the endpoint URL for IBM WatsonX AI service. Required for AI Service to connect to WatsonX AI platform. When to use : - Required when aiservice_watsonx_action is install - Set to your WatsonX AI region endpoint - Must be accessible from the cluster Valid values : Valid WatsonX AI endpoint URL (e.g., https://us-south.ml.cloud.ibm.com , https://eu-de.ml.cloud.ibm.com ) Impact : AI Service uses this URL to access WatsonX AI APIs. Incorrect URL will prevent WatsonX integration. Related variables : - aiservice_watsonxai_apikey : API key for authenticating to this endpoint - aiservice_watsonxai_project_id : Project to access at this endpoint - aiservice_watsonx_action : Whether to install or remove integration Note : Use the WatsonX AI endpoint for your IBM Cloud region. Common endpoints: https://us-south.ml.cloud.ibm.com (Dallas), https://eu-de.ml.cloud.ibm.com (Frankfurt), https://jp-tok.ml.cloud.ibm.com (Tokyo).","title":"aiservice_watsonxai_url"},{"location":"roles/aiservice/#aiservice_watsonxai_project_id","text":"WatsonX AI project identifier. Optional Environment Variable: AISERVICE_WATSONXAI_PROJECT_ID Default: None Purpose : Specifies the WatsonX AI project ID that AI Service will use for AI/ML operations. Projects organize resources and control access in WatsonX AI. When to use : - Required when aiservice_watsonx_action is install - Obtain from your WatsonX AI project in IBM Cloud - Project must have appropriate models and resources configured Valid values : Valid WatsonX AI project ID (UUID format) Impact : AI Service uses this project for accessing WatsonX AI models and resources. Incorrect project ID will prevent access to AI capabilities. Related variables : - aiservice_watsonxai_apikey : API key must have access to this project - aiservice_watsonxai_url : WatsonX endpoint where this project exists - aiservice_watsonx_action : Whether to install or remove integration Note : The project ID is found in your WatsonX AI project settings in IBM Cloud. Ensure the API key has appropriate permissions for the project. The project should have the required AI models and resources configured.","title":"aiservice_watsonxai_project_id"},{"location":"roles/aiservice/#certificate-management","text":"","title":"Certificate Management"},{"location":"roles/aiservice/#aiservice_certificate_issuer","text":"Name of the cert-manager Issuer to use for automatic certificate generation. Optional Environment Variable: AISERVICE_CERTIFICATE_ISSUER Default: None Purpose : Specifies which cert-manager Issuer will generate and manage SSL/TLS certificates for AI Service. The Issuer defines the Certificate Authority and authentication method used for certificate issuance. When to use : - Set to the Issuer created by suite_dns role (e.g., {aiservice_instance_id}-cloudflare-le-prod ) - Set to a custom Issuer if you have specific certificate requirements Valid values : Name of any valid Issuer resource in the cluster (e.g., prod-le-issuer , {mas_instance_id}-cloudflare-le-prod ) Impact : The specified Issuer will be used to generate all certificates for AI Service. If the Issuer is not properly configured or lacks necessary credentials, certificate generation will fail and AI Service will not be accessible. Related variables : - Created by suite_dns role for Let's Encrypt integration - Works with aiservice_certificate_duration and aiservice_certificate_renew_before Note : Ensure the Issuer is created and functional before installing AI Service. Test certificate generation with a test Certificate resource first.","title":"aiservice_certificate_issuer"},{"location":"roles/aiservice/#aiservice_certificate_duration","text":"Specifies the validity period for AI Service certificates. Optional Environment Variable: AISERVICE_CERTIFICATE_DURATION Default: 8760h0m0s (1 year) Purpose : Defines how long certificates will be valid before they expire. This affects how often certificates need to be renewed and the security posture of your installation. When to use : - Use default (8760h = 1 year) for most installations - Reduce for higher security environments requiring frequent rotation - Increase only if certificate renewal is problematic in your environment - Must be longer than aiservice_certificate_renew_before Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 8760h0m0s , 2160h0m0s , 17520h0m0s ) Impact : Shorter durations increase security but require more frequent renewals. Longer durations reduce renewal frequency but increase risk if certificates are compromised. Cert-manager will automatically renew certificates before expiration. Related variables : - Must be greater than aiservice_certificate_renew_before - Only applies when using automatic certificate management - Affects all AI Service certificates","title":"aiservice_certificate_duration"},{"location":"roles/aiservice/#aiservice_certificate_renew_before","text":"Specifies when to renew certificates before they expire. Optional Environment Variable: AISERVICE_CERTIFICATE_RENEW_BEFORE Default: 720h0m0s (30 days) Purpose : Defines the renewal window - how far in advance cert-manager will renew certificates before they expire. This ensures certificates are renewed with sufficient time to handle any renewal issues. When to use : - Use default (720h = 30 days) for most installations - Increase in environments where certificate renewal may be slow or problematic - Decrease only if you need to minimize the number of certificate changes - Must be shorter than aiservice_certificate_duration Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 720h0m0s , 1440h0m0s , 168h0m0s ) Impact : Longer renewal windows provide more time to resolve renewal issues but result in more frequent certificate changes. Shorter windows reduce certificate churn but increase risk of expiration if renewal fails. Related variables : - Must be less than aiservice_certificate_duration - Only applies when using automatic certificate management - Affects all AI Service certificates Note : Ensure this value provides adequate time to detect and resolve certificate renewal issues before expiration.","title":"aiservice_certificate_renew_before"},{"location":"roles/aiservice/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: tenant_action: install tenantName: production app_domain: apps.mycluster.example.com aiservice_s3_host: s3.amazonaws.com aiservice_s3_accesskey: \"{{ lookup('env', 'AWS_ACCESS_KEY') }}\" aiservice_s3_secretkey: \"{{ lookup('env', 'AWS_SECRET_KEY') }}\" aiservice_s3_region: us-east-1 aiservice_watsonxai_apikey: \"{{ lookup('env', 'WATSONX_API_KEY') }}\" aiservice_watsonxai_url: https://us-south.ml.cloud.ibm.com aiservice_watsonxai_project_id: my-project-id roles: - ibm.mas_devops.aiservice","title":"Example Playbook"},{"location":"roles/aiservice/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export TENANT_ACTION=install export AISERVICE_TENANT_NAME=production export APP_DOMAIN=apps.mycluster.example.com export AISERVICE_S3_HOST=s3.amazonaws.com export AISERVICE_S3_ACCESSKEY=your_access_key export AISERVICE_S3_SECRETKEY=your_secret_key export AISERVICE_S3_REGION=us-east-1 export AISERVICE_WATSONXAI_APIKEY=your_watsonx_api_key export AISERVICE_WATSONXAI_URL=https://us-south.ml.cloud.ibm.com export AISERVICE_WATSONXAI_PROJECT_ID=my-project-id ROLE_NAME=aiservice ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aiservice/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aiservice_odh/","text":"AI Service ODH \u00a4 This role provides support to deploy odh components for AI Broker Application: Install Red Hat OpenShift Serverless Operator Install Red Hat OpenShift Service Mesh Operator Install Authorino Operator Install Open Data Hub Operator Create DSCInitialization instance Create Data Science Cluster Create Create Data Science Pipelines Application Role Variables \u00a4 tenantName \u00a4 The tenant name for odh role. Environment Variable: AISERVICE_TENANT_NAME Default Value: user serverless_catalog_source \u00a4 The serverless catalog source for odh role. Environment Variable: SERVERLESS_CATALOG_SOURCE Default Value: redhat-operators serverless_channel \u00a4 The serverless_channel for odh role. Environment Variable: SERVERLESS_CHANNEL Default Value: stable service_mesh_channel \u00a4 The service mesh channel for odh role. Environment Variable: SERVICEMESH_CHANNEL Default Value: stable service_mesh_catalog_source \u00a4 The service mesh catalog source for odh role. Environment Variable: SERVICEMESH_CATALOG_SOURCE Default Value: redhat-operators authorino_catalog_source \u00a4 The authorino catalog source for odh role. Environment Variable: AUTHORINO_CATALOG_SOURCE Default Value: community-operators odh_channel \u00a4 The odh channel for odh role. Environment Variable: ODH_CHANNEL Default Value: fast odh_catalog_source \u00a4 The odh catalog source for odh role. Environment Variable: ODH_CATALOG_SOURCE Default Value: community-operators License \u00a4 EPL-2.0","title":"AI Service ODH"},{"location":"roles/aiservice_odh/#ai-service-odh","text":"This role provides support to deploy odh components for AI Broker Application: Install Red Hat OpenShift Serverless Operator Install Red Hat OpenShift Service Mesh Operator Install Authorino Operator Install Open Data Hub Operator Create DSCInitialization instance Create Data Science Cluster Create Create Data Science Pipelines Application","title":"AI Service ODH"},{"location":"roles/aiservice_odh/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aiservice_odh/#tenantname","text":"The tenant name for odh role. Environment Variable: AISERVICE_TENANT_NAME Default Value: user","title":"tenantName"},{"location":"roles/aiservice_odh/#serverless_catalog_source","text":"The serverless catalog source for odh role. Environment Variable: SERVERLESS_CATALOG_SOURCE Default Value: redhat-operators","title":"serverless_catalog_source"},{"location":"roles/aiservice_odh/#serverless_channel","text":"The serverless_channel for odh role. Environment Variable: SERVERLESS_CHANNEL Default Value: stable","title":"serverless_channel"},{"location":"roles/aiservice_odh/#service_mesh_channel","text":"The service mesh channel for odh role. Environment Variable: SERVICEMESH_CHANNEL Default Value: stable","title":"service_mesh_channel"},{"location":"roles/aiservice_odh/#service_mesh_catalog_source","text":"The service mesh catalog source for odh role. Environment Variable: SERVICEMESH_CATALOG_SOURCE Default Value: redhat-operators","title":"service_mesh_catalog_source"},{"location":"roles/aiservice_odh/#authorino_catalog_source","text":"The authorino catalog source for odh role. Environment Variable: AUTHORINO_CATALOG_SOURCE Default Value: community-operators","title":"authorino_catalog_source"},{"location":"roles/aiservice_odh/#odh_channel","text":"The odh channel for odh role. Environment Variable: ODH_CHANNEL Default Value: fast","title":"odh_channel"},{"location":"roles/aiservice_odh/#odh_catalog_source","text":"The odh catalog source for odh role. Environment Variable: ODH_CATALOG_SOURCE Default Value: community-operators","title":"odh_catalog_source"},{"location":"roles/aiservice_odh/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aiservice_tenant/","text":"AI Broker \u00a4 ===== This role provides support to install and configure AI Broker: Install AI Broker api application Create, delete AI Broker tenant Create, delete AI Broker API Key Create, delete AWS S3 API Key Create, delete WatsonX AI API Key Role Variables \u00a4 tenant_action \u00a4 Action to be performed by AI Broker role. Valid values are install or remove . Environment Variable: TENANT_ACTION Default Value: install tenantID \u00a4 The tenant ID for AI Broker role. Environment Variable: AISERVICE_TENANT_ID Default Value: user app_domain \u00a4 The application domain for AI Broker role. Valid values is domain string apps.domain Environment Variable: APP_DOMAIN Default Value: `` aiservice_watsonx_action \u00a4 Action to be performed by AI Broker role. Valid values are install or remove Environment Variable: AISERVICE_WATSONX_ACTION Default Value: install aiservice_watsonxai_apikey \u00a4 The watsonxai apikey for AI Broker role. Environment Variable: AISERVICE_WATSONXAI_APIKEY Default Value: `` aiservice_watsonxai_url \u00a4 The watsonxai url for AI Broker role. Environment Variable: AISERVICE_WATSONXAI_URL Default Value: `` aiservice_watsonxai_project_id \u00a4 The watsonxai project id for AI Broker role. Environment Variable: AISERVICE_WATSONXAI_PROJECT_ID Default Value: `` Entitlement \u00a4 Configure the tenant's entitlement to the AI Service. tenant_entitlement_type \u00a4 Set the tenant's entitlement type. Environment Variable: AISERVICE_TENANT_ENTITLEMENT_TYPE tenant_entitlement_start_date \u00a4 Set the tenant's entitlement start date in format YYYY-MM-DD . Environment Variable: AISERVICE_TENANT_ENTITLEMENT_START_DATE tenant_entitlement_end_date \u00a4 Set the tenant's entitlement end date in format YYYY-MM-DD . Environment Variable: AISERVICE_TENANT_ENTITLEMENT_END_DATE License \u00a4 EPL-2.0","title":"AI Broker"},{"location":"roles/aiservice_tenant/#ai-broker","text":"===== This role provides support to install and configure AI Broker: Install AI Broker api application Create, delete AI Broker tenant Create, delete AI Broker API Key Create, delete AWS S3 API Key Create, delete WatsonX AI API Key","title":"AI Broker"},{"location":"roles/aiservice_tenant/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aiservice_tenant/#tenant_action","text":"Action to be performed by AI Broker role. Valid values are install or remove . Environment Variable: TENANT_ACTION Default Value: install","title":"tenant_action"},{"location":"roles/aiservice_tenant/#tenantid","text":"The tenant ID for AI Broker role. Environment Variable: AISERVICE_TENANT_ID Default Value: user","title":"tenantID"},{"location":"roles/aiservice_tenant/#app_domain","text":"The application domain for AI Broker role. Valid values is domain string apps.domain Environment Variable: APP_DOMAIN Default Value: ``","title":"app_domain"},{"location":"roles/aiservice_tenant/#aiservice_watsonx_action","text":"Action to be performed by AI Broker role. Valid values are install or remove Environment Variable: AISERVICE_WATSONX_ACTION Default Value: install","title":"aiservice_watsonx_action"},{"location":"roles/aiservice_tenant/#aiservice_watsonxai_apikey","text":"The watsonxai apikey for AI Broker role. Environment Variable: AISERVICE_WATSONXAI_APIKEY Default Value: ``","title":"aiservice_watsonxai_apikey"},{"location":"roles/aiservice_tenant/#aiservice_watsonxai_url","text":"The watsonxai url for AI Broker role. Environment Variable: AISERVICE_WATSONXAI_URL Default Value: ``","title":"aiservice_watsonxai_url"},{"location":"roles/aiservice_tenant/#aiservice_watsonxai_project_id","text":"The watsonxai project id for AI Broker role. Environment Variable: AISERVICE_WATSONXAI_PROJECT_ID Default Value: ``","title":"aiservice_watsonxai_project_id"},{"location":"roles/aiservice_tenant/#entitlement","text":"Configure the tenant's entitlement to the AI Service.","title":"Entitlement"},{"location":"roles/aiservice_tenant/#tenant_entitlement_type","text":"Set the tenant's entitlement type. Environment Variable: AISERVICE_TENANT_ENTITLEMENT_TYPE","title":"tenant_entitlement_type"},{"location":"roles/aiservice_tenant/#tenant_entitlement_start_date","text":"Set the tenant's entitlement start date in format YYYY-MM-DD . Environment Variable: AISERVICE_TENANT_ENTITLEMENT_START_DATE","title":"tenant_entitlement_start_date"},{"location":"roles/aiservice_tenant/#tenant_entitlement_end_date","text":"Set the tenant's entitlement end date in format YYYY-MM-DD . Environment Variable: AISERVICE_TENANT_ENTITLEMENT_END_DATE","title":"tenant_entitlement_end_date"},{"location":"roles/aiservice_tenant/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ansible_version_check/","text":"ansible_version_check \u00a4 Internal-use role that all other roles in the collection declare a dependency upon to ensure that the minimum supported level of Ansible is used. License \u00a4 EPL-2.0","title":"ansible_version_check"},{"location":"roles/ansible_version_check/#ansible_version_check","text":"Internal-use role that all other roles in the collection declare a dependency upon to ensure that the minimum supported level of Ansible is used.","title":"ansible_version_check"},{"location":"roles/ansible_version_check/#license","text":"EPL-2.0","title":"License"},{"location":"roles/arcgis/","text":"arcgis \u00a4 Installs IBM Maximo Location Services for Esri . This dependency is an alternative solution if you are planning to leverage geospatial and map features with Maximo Spatial. The biggest benefit of using it is that you could have both IBM Maximo Location Services for Esri and Maximo Spatial deployed and running into the same cluster, which improves significantly your overall networking performance. Note: IBM Maximo Location Services for Esri will make use of MAS cluster issuers while managing internal and public certificates thus, you while using suite_dns to setup cluster issuer and public certificates for your MAS instances, these are automatically reused for your instance of IBM Maximo Location Services for Esri . Deployment details \u00a4 Here are the full deployment details for a default installation, considering number of running pods, cpu/memory and storage utilization: oc get deployments -n mas-$MAS_INSTANCE_ID-arcgis NAME READY UP-TO-DATE AVAILABLE AGE arcgis-enterprise-apps 1/1 1 1 101m arcgis-enterprise-manager 1/1 1 1 114m arcgis-enterprise-portal 1/1 1 1 101m arcgis-enterprise-web-style-app 1/1 1 1 101m arcgis-featureserver-webhook-processor 1/1 1 1 74m arcgis-gpserver-webhook-processor 1/1 1 1 74m arcgis-help 1/1 1 1 114m arcgis-ingress-controller 1/1 1 1 115m arcgis-javascript-api 1/1 1 1 101m arcgis-ki7vnxghb8526ejnxiqcf-mapserver 1/1 1 1 82m arcgis-kphc76hvhto7lzv74xxts-featureserver 1/1 1 1 73m arcgis-kvrl4t01w78hbbyl1fsof-mapserver 1/1 1 1 77m arcgis-private-ingress-controller 2/2 2 2 106m arcgis-rest-administrator-api 1/1 1 1 114m arcgis-rest-services-api 1/1 1 1 97m arcgis-service-lifecycle-manager 1/1 1 1 97m arcgis-system-cachingcontrollers-gpserver 1/1 1 1 89m arcgis-system-cachingcontrollers-gpsyncserver 1/1 1 1 89m arcgis-system-cachingtools-gpserver 1/1 1 1 89m arcgis-system-cachingtools-gpsyncserver 1/1 1 1 89m arcgis-system-featureservicetools-gpserver 1/1 1 1 84m arcgis-system-featureservicetools-gpsyncserver 1/1 1 1 84m arcgis-system-publishingtools-gpserver 1/1 1 1 89m arcgis-system-publishingtools-gpsyncserver 3/3 3 3 89m arcgis-system-reportingtools-gpserver 1/1 1 1 89m arcgis-system-spatialanalysistools-gpserver 1/1 1 1 82m arcgis-system-spatialanalysistools-gpsyncserver 1/1 1 1 82m arcgis-system-synctools-gpserver 1/1 1 1 84m arcgis-system-synctools-gpsyncserver 1/1 1 1 84m arcgis-utilities-geocodingtools-gpserver 1/1 1 1 80m arcgis-utilities-geocodingtools-gpsyncserver 1/1 1 1 80m arcgis-utilities-geometry-geometryserver 1/1 1 1 81m arcgis-utilities-offlinepackaging-gpserver 1/1 1 1 79m arcgis-utilities-offlinepackaging-gpsyncserver 1/1 1 1 79m arcgis-utilities-printingtools-gpserver 1/1 1 1 80m arcgis-utilities-symbols-symbolserver- 1/1 1 1 79m ibm-mas-arcgis-entitymgr-ws 1/1 1 1 118m ibm-mas-arcgis-operator 1/1 1 1 121m Total of 49 running pods. oc adm top pods -n mas-$MAS_INSTANCE_ID-arcgis NAME CPU(cores) MEMORY(bytes) arcgis-enterprise-apps 1m 105Mi arcgis-enterprise-manager 0m 132Mi arcgis-enterprise-portal 1m 144Mi arcgis-enterprise-web-style-app 1m 80Mi arcgis-featureserver-webhook-processor 2m 426Mi arcgis-gpserver-webhook-processor 5m 438Mi arcgis-help 1m 96Mi arcgis-in-memory-store 5m 378Mi arcgis-ingress-controller 2m 122Mi arcgis-javascript-api 0m 12Mi arcgis-ki7vnxghb8526ejnxiqcf-mapserver 8m 1102Mi arcgis-kphc76hvhto7lzv74xxts-featureserver 4m 2250Mi arcgis-kvrl4t01w78hbbyl1fsof-mapserver 8m 934Mi arcgis-object-store 29m 3355Mi arcgis-private-ingress-controller 4m 114Mi arcgis-private-ingress-controller 2m 113Mi arcgis-queue-store-cgatl-0 16m 186Mi arcgis-relational-store-pfxpx-mcap-0 14m 894Mi arcgis-relational-store-pfxpx-yjnr-0 4m 687Mi arcgis-rest-administrator-api 26m 706Mi arcgis-rest-metrics-api-nmbtw-0 4m 62Mi arcgis-rest-portal-api-rpcnv-0 4m 678Mi arcgis-rest-services-api 24m 876Mi arcgis-service-lifecycle-manager 6m 744Mi arcgis-spatiotemporal-index-store-dejcm-coordinator-0 4m 3612Mi arcgis-system-cachingcontrollers-gpserver 12m 919Mi arcgis-system-cachingcontrollers-gpsyncserver 2m 1179Mi arcgis-system-cachingtools-gpserver 7m 906Mi arcgis-system-cachingtools-gpsyncserver 1m 1273Mi arcgis-system-featureservicetools-gpserver 7m 985Mi arcgis-system-featureservicetools-gpsyncserver 6m 977Mi arcgis-system-publishingtools-gpserver 13m 1043Mi arcgis-system-publishingtools-gpsyncserver 8m 939Mi arcgis-system-publishingtools-gpsyncserver 10m 1189Mi arcgis-system-publishingtools-gpsyncserver 16m 891Mi arcgis-system-reportingtools-gpserver 20m 636Mi arcgis-system-spatialanalysistools-gpserver 8m 982Mi arcgis-system-spatialanalysistools-gpsyncserver 14m 927Mi arcgis-system-synctools-gpserver 11m 1093Mi arcgis-system-synctools-gpsyncserver 23m 960Mi arcgis-utilities-geocodingtools-gpserver 24m 959Mi arcgis-utilities-geocodingtools-gpsyncserver 10m 1189Mi arcgis-utilities-geometry-geometryserver 6m 1053Mi arcgis-utilities-offlinepackaging-gpserver 19m 983Mi arcgis-utilities-offlinepackaging-gpsyncserver 6m 914Mi arcgis-utilities-printingtools-gpserver 21m 1323Mi arcgis-utilities-symbols-symbolserver 9m 580Mi ibm-mas-arcgis-entitymgr-ws 1222m 201Mi ibm-mas-arcgis-operator 0m 48Mi Average of 1650 milicores (1.65 vCPUs) and 40 gigabytes of memory RAM. oc get pvc -n mas-$MAS_INSTANCE_ID-arcgis NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE arcgis-in-memory-store-feiz3-0-data-volume Bound pvc-432e190c-dbe4-44d5-828b-bf8126a326fe 20Gi RWO ibmc-block-gold 108m arcgis-rest-portal-api-rpcnv-0-portal-sharing-volume Bound pvc-a1201747-6b7e-42b8-beed-c842c21cfa01 20Gi RWO ibmc-block-gold 62m data-volume-arcgis-object-store-o0vq5-awsrx-0 Bound pvc-6c97716f-20d4-46b2-a110-706546dda95d 32Gi RWO ibmc-block-gold 108m data-volume-arcgis-relational-store-pfxpx-mcap-0 Bound pvc-28635771-0e4a-475d-b6aa-2012c48dd470 20Gi RWO ibmc-block-gold 111m data-volume-arcgis-relational-store-pfxpx-yjnr-0 Bound pvc-0e2dbc22-11af-4c5c-b5e7-b00d21a060fe 20Gi RWO ibmc-block-gold 100m data-volume-arcgis-spatiotemporal-index-store-dejcm-coordinator-0 Bound pvc-2e0ca0f0-c830-4353-abf0-196ce0e75b87 20Gi RWO ibmc-block-gold 108m prometheus-volume-arcgis-rest-metrics-api-nmbtw-0 Bound pvc-fbc5c0ce-26eb-441f-8161-2191fd113a80 30Gi RWO ibmc-block-gold 108m queue-data-volume-arcgis-queue-store-cgatl-0 Bound pvc-93451297-0e3d-4a56-bf4e-cff9bda43fb7 20Gi RWO ibmc-block-gold 108m Average of 182 gigabyes of required capacity. Role Variables - Installation \u00a4 ibm_entitlement_key \u00a4 Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None mas_catalog_source \u00a4 Defines the catalog to be used to install MAS. You can set it to ibm-operator-catalog for both release as well as for development install Optional Environment Variable: MAS_CATALOG_SOURCE Default Value: ibm-operator-catalog mas_arcgis_channel \u00a4 Subscription channel for IBM Maximo Location Services for Esri. Optional Environment Variable: MAS_ARCGIS_CHANNEL Default Value: 9.1.x Role Variables - MAS Configuration \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Example Playbooks \u00a4 Install IBM Maximo Location Services for Esri \u00a4 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxx roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.arcgis License \u00a4 EPL-2.0","title":"arcgis"},{"location":"roles/arcgis/#arcgis","text":"Installs IBM Maximo Location Services for Esri . This dependency is an alternative solution if you are planning to leverage geospatial and map features with Maximo Spatial. The biggest benefit of using it is that you could have both IBM Maximo Location Services for Esri and Maximo Spatial deployed and running into the same cluster, which improves significantly your overall networking performance. Note: IBM Maximo Location Services for Esri will make use of MAS cluster issuers while managing internal and public certificates thus, you while using suite_dns to setup cluster issuer and public certificates for your MAS instances, these are automatically reused for your instance of IBM Maximo Location Services for Esri .","title":"arcgis"},{"location":"roles/arcgis/#deployment-details","text":"Here are the full deployment details for a default installation, considering number of running pods, cpu/memory and storage utilization: oc get deployments -n mas-$MAS_INSTANCE_ID-arcgis NAME READY UP-TO-DATE AVAILABLE AGE arcgis-enterprise-apps 1/1 1 1 101m arcgis-enterprise-manager 1/1 1 1 114m arcgis-enterprise-portal 1/1 1 1 101m arcgis-enterprise-web-style-app 1/1 1 1 101m arcgis-featureserver-webhook-processor 1/1 1 1 74m arcgis-gpserver-webhook-processor 1/1 1 1 74m arcgis-help 1/1 1 1 114m arcgis-ingress-controller 1/1 1 1 115m arcgis-javascript-api 1/1 1 1 101m arcgis-ki7vnxghb8526ejnxiqcf-mapserver 1/1 1 1 82m arcgis-kphc76hvhto7lzv74xxts-featureserver 1/1 1 1 73m arcgis-kvrl4t01w78hbbyl1fsof-mapserver 1/1 1 1 77m arcgis-private-ingress-controller 2/2 2 2 106m arcgis-rest-administrator-api 1/1 1 1 114m arcgis-rest-services-api 1/1 1 1 97m arcgis-service-lifecycle-manager 1/1 1 1 97m arcgis-system-cachingcontrollers-gpserver 1/1 1 1 89m arcgis-system-cachingcontrollers-gpsyncserver 1/1 1 1 89m arcgis-system-cachingtools-gpserver 1/1 1 1 89m arcgis-system-cachingtools-gpsyncserver 1/1 1 1 89m arcgis-system-featureservicetools-gpserver 1/1 1 1 84m arcgis-system-featureservicetools-gpsyncserver 1/1 1 1 84m arcgis-system-publishingtools-gpserver 1/1 1 1 89m arcgis-system-publishingtools-gpsyncserver 3/3 3 3 89m arcgis-system-reportingtools-gpserver 1/1 1 1 89m arcgis-system-spatialanalysistools-gpserver 1/1 1 1 82m arcgis-system-spatialanalysistools-gpsyncserver 1/1 1 1 82m arcgis-system-synctools-gpserver 1/1 1 1 84m arcgis-system-synctools-gpsyncserver 1/1 1 1 84m arcgis-utilities-geocodingtools-gpserver 1/1 1 1 80m arcgis-utilities-geocodingtools-gpsyncserver 1/1 1 1 80m arcgis-utilities-geometry-geometryserver 1/1 1 1 81m arcgis-utilities-offlinepackaging-gpserver 1/1 1 1 79m arcgis-utilities-offlinepackaging-gpsyncserver 1/1 1 1 79m arcgis-utilities-printingtools-gpserver 1/1 1 1 80m arcgis-utilities-symbols-symbolserver- 1/1 1 1 79m ibm-mas-arcgis-entitymgr-ws 1/1 1 1 118m ibm-mas-arcgis-operator 1/1 1 1 121m Total of 49 running pods. oc adm top pods -n mas-$MAS_INSTANCE_ID-arcgis NAME CPU(cores) MEMORY(bytes) arcgis-enterprise-apps 1m 105Mi arcgis-enterprise-manager 0m 132Mi arcgis-enterprise-portal 1m 144Mi arcgis-enterprise-web-style-app 1m 80Mi arcgis-featureserver-webhook-processor 2m 426Mi arcgis-gpserver-webhook-processor 5m 438Mi arcgis-help 1m 96Mi arcgis-in-memory-store 5m 378Mi arcgis-ingress-controller 2m 122Mi arcgis-javascript-api 0m 12Mi arcgis-ki7vnxghb8526ejnxiqcf-mapserver 8m 1102Mi arcgis-kphc76hvhto7lzv74xxts-featureserver 4m 2250Mi arcgis-kvrl4t01w78hbbyl1fsof-mapserver 8m 934Mi arcgis-object-store 29m 3355Mi arcgis-private-ingress-controller 4m 114Mi arcgis-private-ingress-controller 2m 113Mi arcgis-queue-store-cgatl-0 16m 186Mi arcgis-relational-store-pfxpx-mcap-0 14m 894Mi arcgis-relational-store-pfxpx-yjnr-0 4m 687Mi arcgis-rest-administrator-api 26m 706Mi arcgis-rest-metrics-api-nmbtw-0 4m 62Mi arcgis-rest-portal-api-rpcnv-0 4m 678Mi arcgis-rest-services-api 24m 876Mi arcgis-service-lifecycle-manager 6m 744Mi arcgis-spatiotemporal-index-store-dejcm-coordinator-0 4m 3612Mi arcgis-system-cachingcontrollers-gpserver 12m 919Mi arcgis-system-cachingcontrollers-gpsyncserver 2m 1179Mi arcgis-system-cachingtools-gpserver 7m 906Mi arcgis-system-cachingtools-gpsyncserver 1m 1273Mi arcgis-system-featureservicetools-gpserver 7m 985Mi arcgis-system-featureservicetools-gpsyncserver 6m 977Mi arcgis-system-publishingtools-gpserver 13m 1043Mi arcgis-system-publishingtools-gpsyncserver 8m 939Mi arcgis-system-publishingtools-gpsyncserver 10m 1189Mi arcgis-system-publishingtools-gpsyncserver 16m 891Mi arcgis-system-reportingtools-gpserver 20m 636Mi arcgis-system-spatialanalysistools-gpserver 8m 982Mi arcgis-system-spatialanalysistools-gpsyncserver 14m 927Mi arcgis-system-synctools-gpserver 11m 1093Mi arcgis-system-synctools-gpsyncserver 23m 960Mi arcgis-utilities-geocodingtools-gpserver 24m 959Mi arcgis-utilities-geocodingtools-gpsyncserver 10m 1189Mi arcgis-utilities-geometry-geometryserver 6m 1053Mi arcgis-utilities-offlinepackaging-gpserver 19m 983Mi arcgis-utilities-offlinepackaging-gpsyncserver 6m 914Mi arcgis-utilities-printingtools-gpserver 21m 1323Mi arcgis-utilities-symbols-symbolserver 9m 580Mi ibm-mas-arcgis-entitymgr-ws 1222m 201Mi ibm-mas-arcgis-operator 0m 48Mi Average of 1650 milicores (1.65 vCPUs) and 40 gigabytes of memory RAM. oc get pvc -n mas-$MAS_INSTANCE_ID-arcgis NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE arcgis-in-memory-store-feiz3-0-data-volume Bound pvc-432e190c-dbe4-44d5-828b-bf8126a326fe 20Gi RWO ibmc-block-gold 108m arcgis-rest-portal-api-rpcnv-0-portal-sharing-volume Bound pvc-a1201747-6b7e-42b8-beed-c842c21cfa01 20Gi RWO ibmc-block-gold 62m data-volume-arcgis-object-store-o0vq5-awsrx-0 Bound pvc-6c97716f-20d4-46b2-a110-706546dda95d 32Gi RWO ibmc-block-gold 108m data-volume-arcgis-relational-store-pfxpx-mcap-0 Bound pvc-28635771-0e4a-475d-b6aa-2012c48dd470 20Gi RWO ibmc-block-gold 111m data-volume-arcgis-relational-store-pfxpx-yjnr-0 Bound pvc-0e2dbc22-11af-4c5c-b5e7-b00d21a060fe 20Gi RWO ibmc-block-gold 100m data-volume-arcgis-spatiotemporal-index-store-dejcm-coordinator-0 Bound pvc-2e0ca0f0-c830-4353-abf0-196ce0e75b87 20Gi RWO ibmc-block-gold 108m prometheus-volume-arcgis-rest-metrics-api-nmbtw-0 Bound pvc-fbc5c0ce-26eb-441f-8161-2191fd113a80 30Gi RWO ibmc-block-gold 108m queue-data-volume-arcgis-queue-store-cgatl-0 Bound pvc-93451297-0e3d-4a56-bf4e-cff9bda43fb7 20Gi RWO ibmc-block-gold 108m Average of 182 gigabyes of required capacity.","title":"Deployment details"},{"location":"roles/arcgis/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/arcgis/#ibm_entitlement_key","text":"Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None","title":"ibm_entitlement_key"},{"location":"roles/arcgis/#mas_catalog_source","text":"Defines the catalog to be used to install MAS. You can set it to ibm-operator-catalog for both release as well as for development install Optional Environment Variable: MAS_CATALOG_SOURCE Default Value: ibm-operator-catalog","title":"mas_catalog_source"},{"location":"roles/arcgis/#mas_arcgis_channel","text":"Subscription channel for IBM Maximo Location Services for Esri. Optional Environment Variable: MAS_ARCGIS_CHANNEL Default Value: 9.1.x","title":"mas_arcgis_channel"},{"location":"roles/arcgis/#role-variables-mas-configuration","text":"","title":"Role Variables - MAS Configuration"},{"location":"roles/arcgis/#mas_instance_id","text":"The instance ID of Maximo Application Suite. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/arcgis/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/arcgis/#example-playbooks","text":"","title":"Example Playbooks"},{"location":"roles/arcgis/#install-ibm-maximo-location-services-for-esri","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxx roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.arcgis","title":"Install IBM Maximo Location Services for Esri"},{"location":"roles/arcgis/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_bucket_access_point/","text":"aws_bucket_access_point \u00a4 This role will create an access point and associates it with the specified s3/aws bucket in the targeted AWS account. Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Role Variables \u00a4 aws_access_point_name \u00a4 The name you want to assign to this access point. Required. Environment Variable: AWS_ACCESS_POINT_NAME Default Value: access-point-c1 aws_access_point_bucket_name \u00a4 The name of the bucket that you want to associate this access point with. Required. Environment Variable: COS_BUCKET_NAME Default Value: None aws_access_point_region \u00a4 The region where the bucket is located. Required. Environment Variable: AWS_REGION Default Value: us-east-2 aws_access_point_username \u00a4 The AWS account or username who is allowed access to the actions defined in by the access point policy. By default, the defined aws_access_point_username will have read-only permissions to the bucket objects through the created access point alias. Required. Environment Variable: AWS_ACCESS_POINT_USERNAME Default Value: None Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_access_point_name: \"{{ lookup('env', 'AWS_ACCESS_POINT_NAME') | default('access-point-c1', True) }}\" aws_access_point_bucket_name: \"{{ lookup('env', 'COS_BUCKET_NAME') }}\" aws_access_point_region: \"{{ lookup('env', 'AWS_REGION') | default('us-east-2', True) }}\" aws_access_point_username: \"{{ lookup('env', 'AWS_ACCESS_POINT_USERNAME') }}\" roles: - ibm.mas_devops.aws_bucket_access_point Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_ACCESS_POINT_NAME=my-aws-access-point export COS_BUCKET_NAME=my-aws-bucket export AWS_ACCESS_POINT_USERNAME=my-aws-username ROLE_NAME=aws_bucket_access_point ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aws_bucket_access_point"},{"location":"roles/aws_bucket_access_point/#aws_bucket_access_point","text":"This role will create an access point and associates it with the specified s3/aws bucket in the targeted AWS account.","title":"aws_bucket_access_point"},{"location":"roles/aws_bucket_access_point/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/aws_bucket_access_point/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_bucket_access_point/#aws_access_point_name","text":"The name you want to assign to this access point. Required. Environment Variable: AWS_ACCESS_POINT_NAME Default Value: access-point-c1","title":"aws_access_point_name"},{"location":"roles/aws_bucket_access_point/#aws_access_point_bucket_name","text":"The name of the bucket that you want to associate this access point with. Required. Environment Variable: COS_BUCKET_NAME Default Value: None","title":"aws_access_point_bucket_name"},{"location":"roles/aws_bucket_access_point/#aws_access_point_region","text":"The region where the bucket is located. Required. Environment Variable: AWS_REGION Default Value: us-east-2","title":"aws_access_point_region"},{"location":"roles/aws_bucket_access_point/#aws_access_point_username","text":"The AWS account or username who is allowed access to the actions defined in by the access point policy. By default, the defined aws_access_point_username will have read-only permissions to the bucket objects through the created access point alias. Required. Environment Variable: AWS_ACCESS_POINT_USERNAME Default Value: None","title":"aws_access_point_username"},{"location":"roles/aws_bucket_access_point/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_access_point_name: \"{{ lookup('env', 'AWS_ACCESS_POINT_NAME') | default('access-point-c1', True) }}\" aws_access_point_bucket_name: \"{{ lookup('env', 'COS_BUCKET_NAME') }}\" aws_access_point_region: \"{{ lookup('env', 'AWS_REGION') | default('us-east-2', True) }}\" aws_access_point_username: \"{{ lookup('env', 'AWS_ACCESS_POINT_USERNAME') }}\" roles: - ibm.mas_devops.aws_bucket_access_point","title":"Example Playbook"},{"location":"roles/aws_bucket_access_point/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_ACCESS_POINT_NAME=my-aws-access-point export COS_BUCKET_NAME=my-aws-bucket export AWS_ACCESS_POINT_USERNAME=my-aws-username ROLE_NAME=aws_bucket_access_point ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aws_bucket_access_point/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_documentdb_user/","text":"aws_documentdb_user \u00a4 This role creates a docdb user for MAS instance and saves username and password as k8 Secret in specified config directory Prerequisites \u00a4 To run this role with providers you must have already installed the Mongo Shell . Role variables \u00a4 mas_instance_id \u00a4 Required.The instance ID of Maximo Application Suite required for creating docdb user credentials secret Environment Variable: MAS_INSTANCE_ID docdb_host \u00a4 AWS DocumentDB Instance Host Address, Required if docdb_hosts is not set Environment Variable: DOCDB_HOST docdb_port \u00a4 AWS DocumentDB Port Address, Required if docdb_hosts is not set Environment Variable: DOCDB_PORT docdb_hosts \u00a4 AWS DocumentDB Instance Host Address & Port Address, Required if both docdb_host & docdb_port are not set. docdb_hosts takes precedence if both docdb_hosts and (docdb_host & docdb_port) are set Environment Variable: DOCDB_HOSTS docdb_master_username \u00a4 Required. AWS DocumentDB Master Username Environment Variable: DOCDB_MASTER_USERNAME docdb_master_password \u00a4 Required. AWS DocumentDB Master Password Environment Variable: DOCDB_MASTER_PASSWORD - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 docdb_master_username: test-user docdb_master_password: test-pass-*** docdb_host: test1.aws-01.... docdb_port: 27017 roles: - ibm.mas_devops.aws_documentdb_user - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 docdb_master_username: test-user docdb_master_password: test-pass-*** docdb_hosts: test1.aws-01:27017,test1.aws-02:27017,test1.aws-03:27017 roles: - ibm.mas_devops.aws_documentdb_user","title":"aws_documentdb_user"},{"location":"roles/aws_documentdb_user/#aws_documentdb_user","text":"This role creates a docdb user for MAS instance and saves username and password as k8 Secret in specified config directory","title":"aws_documentdb_user"},{"location":"roles/aws_documentdb_user/#prerequisites","text":"To run this role with providers you must have already installed the Mongo Shell .","title":"Prerequisites"},{"location":"roles/aws_documentdb_user/#role-variables","text":"","title":"Role variables"},{"location":"roles/aws_documentdb_user/#mas_instance_id","text":"Required.The instance ID of Maximo Application Suite required for creating docdb user credentials secret Environment Variable: MAS_INSTANCE_ID","title":"mas_instance_id"},{"location":"roles/aws_documentdb_user/#docdb_host","text":"AWS DocumentDB Instance Host Address, Required if docdb_hosts is not set Environment Variable: DOCDB_HOST","title":"docdb_host"},{"location":"roles/aws_documentdb_user/#docdb_port","text":"AWS DocumentDB Port Address, Required if docdb_hosts is not set Environment Variable: DOCDB_PORT","title":"docdb_port"},{"location":"roles/aws_documentdb_user/#docdb_hosts","text":"AWS DocumentDB Instance Host Address & Port Address, Required if both docdb_host & docdb_port are not set. docdb_hosts takes precedence if both docdb_hosts and (docdb_host & docdb_port) are set Environment Variable: DOCDB_HOSTS","title":"docdb_hosts"},{"location":"roles/aws_documentdb_user/#docdb_master_username","text":"Required. AWS DocumentDB Master Username Environment Variable: DOCDB_MASTER_USERNAME","title":"docdb_master_username"},{"location":"roles/aws_documentdb_user/#docdb_master_password","text":"Required. AWS DocumentDB Master Password Environment Variable: DOCDB_MASTER_PASSWORD - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 docdb_master_username: test-user docdb_master_password: test-pass-*** docdb_host: test1.aws-01.... docdb_port: 27017 roles: - ibm.mas_devops.aws_documentdb_user - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 docdb_master_username: test-user docdb_master_password: test-pass-*** docdb_hosts: test1.aws-01:27017,test1.aws-02:27017,test1.aws-03:27017 roles: - ibm.mas_devops.aws_documentdb_user","title":"docdb_master_password"},{"location":"roles/aws_policy/","text":"aws_policy \u00a4 This role will create an AWS IAM Policy from a JSON file in the targeted AWS account. Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Role Variables \u00a4 aws_policy_name \u00a4 AWS Policy name. Required. Environment Variable: AWS_POLICY_NAME Default Value: None aws_policy_json_file_path_local \u00a4 Local path for the AWS Policy json file. The AWS Policy json file should be structured as the sample found in /files/policy-template-sample.json Required. Environment Variable: AWS_POLICY_JSON_FILE_PATH_LOCAL Default Value: None Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_policy: \"{{ lookup('env', 'AWS_POLICY_NAME') }}\" aws_policy_json_file_path_local: \"{{ lookup('env', 'AWS_POLICY_JSON_FILE_PATH_LOCAL') }}\" roles: - ibm.mas_devops.aws_policy Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_POLICY_NAME=my-aws-policy export AWS_POLICY_JSON_FILE_PATH_LOCAL=/tmp/local/my-aws-policy.json ROLE_NAME=aws_policy ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aws_policy"},{"location":"roles/aws_policy/#aws_policy","text":"This role will create an AWS IAM Policy from a JSON file in the targeted AWS account.","title":"aws_policy"},{"location":"roles/aws_policy/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/aws_policy/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_policy/#aws_policy_name","text":"AWS Policy name. Required. Environment Variable: AWS_POLICY_NAME Default Value: None","title":"aws_policy_name"},{"location":"roles/aws_policy/#aws_policy_json_file_path_local","text":"Local path for the AWS Policy json file. The AWS Policy json file should be structured as the sample found in /files/policy-template-sample.json Required. Environment Variable: AWS_POLICY_JSON_FILE_PATH_LOCAL Default Value: None","title":"aws_policy_json_file_path_local"},{"location":"roles/aws_policy/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_policy: \"{{ lookup('env', 'AWS_POLICY_NAME') }}\" aws_policy_json_file_path_local: \"{{ lookup('env', 'AWS_POLICY_JSON_FILE_PATH_LOCAL') }}\" roles: - ibm.mas_devops.aws_policy","title":"Example Playbook"},{"location":"roles/aws_policy/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_POLICY_NAME=my-aws-policy export AWS_POLICY_JSON_FILE_PATH_LOCAL=/tmp/local/my-aws-policy.json ROLE_NAME=aws_policy ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aws_policy/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_route53/","text":"aws_route53 \u00a4 This role will create an AWS Route53 public hosted zone in the targeted AWS Account. For further details on how to create and configure an AWS Route53 instance, refer to AWS Route53 documentation . Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Role Variables \u00a4 route53_hosted_zone_name \u00a4 AWS Route53 Hosted Zone name. Required. Environment Variable: ROUTE53_HOSTED_ZONE_NAME Default Value: None route53_hosted_zone_region \u00a4 AWS Route53 Hosted Zone region. Required. Environment Variable: ROUTE53_HOSTED_ZONE_REGION Default Value: Same value as defined in AWS_REGION , or if none defined, then us-east-2 is the defaulted region. Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: route53_hosted_zone_name: \"{{ lookup('env', 'ROUTE53_HOSTED_ZONE_NAME') }}\" # mycompany.com route53_hosted_zone_region: \"{{ lookup('env', 'ROUTE53_HOSTED_ZONE_REGION') }}\" # us-east-2 roles: - ibm.mas_devops.aws_route53 Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export ROUTE53_HOSTED_ZONE_NAME=mycompany.com export ROUTE53_HOSTED_ZONE_REGION=us-east-2 ROLE_NAME=aws_route53 ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aws_route53"},{"location":"roles/aws_route53/#aws_route53","text":"This role will create an AWS Route53 public hosted zone in the targeted AWS Account. For further details on how to create and configure an AWS Route53 instance, refer to AWS Route53 documentation .","title":"aws_route53"},{"location":"roles/aws_route53/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/aws_route53/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_route53/#route53_hosted_zone_name","text":"AWS Route53 Hosted Zone name. Required. Environment Variable: ROUTE53_HOSTED_ZONE_NAME Default Value: None","title":"route53_hosted_zone_name"},{"location":"roles/aws_route53/#route53_hosted_zone_region","text":"AWS Route53 Hosted Zone region. Required. Environment Variable: ROUTE53_HOSTED_ZONE_REGION Default Value: Same value as defined in AWS_REGION , or if none defined, then us-east-2 is the defaulted region.","title":"route53_hosted_zone_region"},{"location":"roles/aws_route53/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: route53_hosted_zone_name: \"{{ lookup('env', 'ROUTE53_HOSTED_ZONE_NAME') }}\" # mycompany.com route53_hosted_zone_region: \"{{ lookup('env', 'ROUTE53_HOSTED_ZONE_REGION') }}\" # us-east-2 roles: - ibm.mas_devops.aws_route53","title":"Example Playbook"},{"location":"roles/aws_route53/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export ROUTE53_HOSTED_ZONE_NAME=mycompany.com export ROUTE53_HOSTED_ZONE_REGION=us-east-2 ROLE_NAME=aws_route53 ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aws_route53/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_user_creation/","text":"aws_user_creation \u00a4 This role will create an AWS IAM Username and corresponding IAM Access Key ID and Secret Access Key in the targeted AWS account. Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Role Variables \u00a4 aws_username \u00a4 AWS Username. Required. Environment Variable: AWS_USERNAME Default Value: None aws_username_create_access_key_flag \u00a4 Flag that defines if IAM Access Key ID and Secret Access Key should be created for the AWS Username. If set to False , then only the AWS Username will be created but no IAM Access Key ID and Secret Access Key. Optional Environment Variable: AWS_USERNAME_CREATE_ACCESS_KEY_FLAG Default Value: True . aws_username_access_key_id \u00a4 Defines an existing IAM Access Key ID for your AWS username. If both aws_username_access_key_id and aws_username_secret_access_key are defined, then aws_username_create_access_key_flag will be automatically forced to False , therefore if you want to create new pair of credentials for the username, do not set this property. Optional Environment Variable: AWS_USERNAME_ACCESS_KEY_ID Default Value: None. aws_username_secret_access_key \u00a4 Defines and existing IAM Secret Access Key for your AWS username. If both aws_username_access_key_id and aws_username_secret_access_key are defined, then aws_username_create_access_key_flag will be automatically forced to False , therefore if you want to create new pair of credentials for the username, do not set this property. Optional Environment Variable: AWS_USERNAME_SECRET_ACCESS_KEY Default Value: None. aws_policy_arn \u00a4 If set, then it will attach the corresponding policy to the AWS Username's permissions. Optional Environment Variable: AWS_POLICY_ARN Default Value: None. Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_username: \"{{ lookup('env', 'AWS_USERNAME') }}\" aws_username_create_access_key_flag: \"{{ lookup('env', 'AWS_USERNAME_CREATE_ACCESS_KEY_FLAG') }}\" aws_policy_arn: \"{{ lookup('env', 'AWS_POLICY_ARN') }}\" roles: - ibm.mas_devops.aws_policy Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_USERNAME=my-aws-username export AWS_USERNAME_CREATE_ACCESS_KEY_FLAG=True export AWS_POLICY_ARN=arn:aws:iam::my-id:policy/my-policy-name ROLE_NAME=aws_user_creation ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aws_user_creation"},{"location":"roles/aws_user_creation/#aws_user_creation","text":"This role will create an AWS IAM Username and corresponding IAM Access Key ID and Secret Access Key in the targeted AWS account.","title":"aws_user_creation"},{"location":"roles/aws_user_creation/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/aws_user_creation/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_user_creation/#aws_username","text":"AWS Username. Required. Environment Variable: AWS_USERNAME Default Value: None","title":"aws_username"},{"location":"roles/aws_user_creation/#aws_username_create_access_key_flag","text":"Flag that defines if IAM Access Key ID and Secret Access Key should be created for the AWS Username. If set to False , then only the AWS Username will be created but no IAM Access Key ID and Secret Access Key. Optional Environment Variable: AWS_USERNAME_CREATE_ACCESS_KEY_FLAG Default Value: True .","title":"aws_username_create_access_key_flag"},{"location":"roles/aws_user_creation/#aws_username_access_key_id","text":"Defines an existing IAM Access Key ID for your AWS username. If both aws_username_access_key_id and aws_username_secret_access_key are defined, then aws_username_create_access_key_flag will be automatically forced to False , therefore if you want to create new pair of credentials for the username, do not set this property. Optional Environment Variable: AWS_USERNAME_ACCESS_KEY_ID Default Value: None.","title":"aws_username_access_key_id"},{"location":"roles/aws_user_creation/#aws_username_secret_access_key","text":"Defines and existing IAM Secret Access Key for your AWS username. If both aws_username_access_key_id and aws_username_secret_access_key are defined, then aws_username_create_access_key_flag will be automatically forced to False , therefore if you want to create new pair of credentials for the username, do not set this property. Optional Environment Variable: AWS_USERNAME_SECRET_ACCESS_KEY Default Value: None.","title":"aws_username_secret_access_key"},{"location":"roles/aws_user_creation/#aws_policy_arn","text":"If set, then it will attach the corresponding policy to the AWS Username's permissions. Optional Environment Variable: AWS_POLICY_ARN Default Value: None.","title":"aws_policy_arn"},{"location":"roles/aws_user_creation/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_username: \"{{ lookup('env', 'AWS_USERNAME') }}\" aws_username_create_access_key_flag: \"{{ lookup('env', 'AWS_USERNAME_CREATE_ACCESS_KEY_FLAG') }}\" aws_policy_arn: \"{{ lookup('env', 'AWS_POLICY_ARN') }}\" roles: - ibm.mas_devops.aws_policy","title":"Example Playbook"},{"location":"roles/aws_user_creation/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_USERNAME=my-aws-username export AWS_USERNAME_CREATE_ACCESS_KEY_FLAG=True export AWS_POLICY_ARN=arn:aws:iam::my-id:policy/my-policy-name ROLE_NAME=aws_user_creation ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aws_user_creation/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_vpc/","text":"aws_vpc \u00a4 This role will create VPC with specified CIDR IP Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Role Variables \u00a4 mas_config_dir \u00a4 Generate k8 resources like username and password Secret for created user will be saved in this directory Required Environment Variable: MAS_CONFIG_DIR Default Value: None. aws_region \u00a4 Specify AWS Region where vpc will be created Optional Environment Variable: AWS_REGION Default Value: us-east-1 vpc_action \u00a4 Specify action(provision/deprovision) to performed by the role Optional Environment Variable: VPC_ACTION Default Value: provision vpc_cidr \u00a4 Specify IP Address CIDR range for VPC Required Environment Variable: VPC_CIDR Default Value: None vpc_name \u00a4 Specify Name for VPC Required Environment Variable: VPC_NAME Default Value: None Example Playbook \u00a4 - hosts: localhost vars: mas_config_dir: ~/masconfig vpc_name: test-vpc vpc_cidr: 10.0.0.0/16 vpc_action: provision roles: - ibm.mas_devops.aws_vpc Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export VPC_NAME=test-vpc export VPC_CIDR='10.0.0.0/16' export VPC_ACTION=provision export MAS_CONFIG_DIR=/pathtoconfig ROLE_NAME=aws_vpc ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aws_vpc"},{"location":"roles/aws_vpc/#aws_vpc","text":"This role will create VPC with specified CIDR IP","title":"aws_vpc"},{"location":"roles/aws_vpc/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/aws_vpc/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_vpc/#mas_config_dir","text":"Generate k8 resources like username and password Secret for created user will be saved in this directory Required Environment Variable: MAS_CONFIG_DIR Default Value: None.","title":"mas_config_dir"},{"location":"roles/aws_vpc/#aws_region","text":"Specify AWS Region where vpc will be created Optional Environment Variable: AWS_REGION Default Value: us-east-1","title":"aws_region"},{"location":"roles/aws_vpc/#vpc_action","text":"Specify action(provision/deprovision) to performed by the role Optional Environment Variable: VPC_ACTION Default Value: provision","title":"vpc_action"},{"location":"roles/aws_vpc/#vpc_cidr","text":"Specify IP Address CIDR range for VPC Required Environment Variable: VPC_CIDR Default Value: None","title":"vpc_cidr"},{"location":"roles/aws_vpc/#vpc_name","text":"Specify Name for VPC Required Environment Variable: VPC_NAME Default Value: None","title":"vpc_name"},{"location":"roles/aws_vpc/#example-playbook","text":"- hosts: localhost vars: mas_config_dir: ~/masconfig vpc_name: test-vpc vpc_cidr: 10.0.0.0/16 vpc_action: provision roles: - ibm.mas_devops.aws_vpc","title":"Example Playbook"},{"location":"roles/aws_vpc/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export VPC_NAME=test-vpc export VPC_CIDR='10.0.0.0/16' export VPC_ACTION=provision export MAS_CONFIG_DIR=/pathtoconfig ROLE_NAME=aws_vpc ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aws_vpc/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cert_manager/","text":"cert_manager \u00a4 This role deploys the Red Hat Certificate Manager Operator into the target OpenShift cluster. The operator will be installed into the cert-manager-operator namespace, and the operand will be created in the cert-manager namespace. Certificate Manager provides certificate management capabilities for Kubernetes and OpenShift clusters, enabling automated certificate provisioning and renewal. Prerequisites \u00a4 Red Hat Operators CatalogSource must be installed in the cluster Cluster administrator access Role Variables \u00a4 General Variables \u00a4 cert_manager_action \u00a4 Specifies which operation to perform on the Certificate Manager operator. Optional Environment Variable: CERT_MANAGER_ACTION Default Value: install Purpose : Controls what action the role executes against the Certificate Manager operator. This allows the same role to handle installation, removal, or no action on the cert-manager deployment. When to use : - Use install (default) for initial deployment or to ensure cert-manager is present - Use uninstall to remove cert-manager (use with extreme caution) - Use none to skip cert-manager operations while running broader playbooks Valid values : install , uninstall , none Impact : - install : Deploys Red Hat Certificate Manager Operator to cert-manager-operator namespace and creates operand in cert-manager namespace - uninstall : Removes cert-manager operator and operand (destructive operation) - none : Role takes no action Related variables : None Note : WARNING - Certificate Manager is a cluster-wide dependency used by MAS, SLS, and other components. Uninstalling it will break certificate management for all dependent applications. Only use uninstall if you are certain no applications depend on it. Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: cert_manager_action: install roles: - ibm.mas_devops.cert_manager Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export CERT_MANAGER_ACTION=install ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"cert_manager"},{"location":"roles/cert_manager/#cert_manager","text":"This role deploys the Red Hat Certificate Manager Operator into the target OpenShift cluster. The operator will be installed into the cert-manager-operator namespace, and the operand will be created in the cert-manager namespace. Certificate Manager provides certificate management capabilities for Kubernetes and OpenShift clusters, enabling automated certificate provisioning and renewal.","title":"cert_manager"},{"location":"roles/cert_manager/#prerequisites","text":"Red Hat Operators CatalogSource must be installed in the cluster Cluster administrator access","title":"Prerequisites"},{"location":"roles/cert_manager/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cert_manager/#general-variables","text":"","title":"General Variables"},{"location":"roles/cert_manager/#cert_manager_action","text":"Specifies which operation to perform on the Certificate Manager operator. Optional Environment Variable: CERT_MANAGER_ACTION Default Value: install Purpose : Controls what action the role executes against the Certificate Manager operator. This allows the same role to handle installation, removal, or no action on the cert-manager deployment. When to use : - Use install (default) for initial deployment or to ensure cert-manager is present - Use uninstall to remove cert-manager (use with extreme caution) - Use none to skip cert-manager operations while running broader playbooks Valid values : install , uninstall , none Impact : - install : Deploys Red Hat Certificate Manager Operator to cert-manager-operator namespace and creates operand in cert-manager namespace - uninstall : Removes cert-manager operator and operand (destructive operation) - none : Role takes no action Related variables : None Note : WARNING - Certificate Manager is a cluster-wide dependency used by MAS, SLS, and other components. Uninstalling it will break certificate management for all dependent applications. Only use uninstall if you are certain no applications depend on it.","title":"cert_manager_action"},{"location":"roles/cert_manager/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: cert_manager_action: install roles: - ibm.mas_devops.cert_manager","title":"Example Playbook"},{"location":"roles/cert_manager/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export CERT_MANAGER_ACTION=install ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/cert_manager/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cis/","text":"cis \u00a4 This role provides support for Configuring IBM Cloud Internet Services.During CIS provisioning it performs four tasks during provisioning in given order: 1. Provision CIS Instance in customer account 2. Add customer domain to customer's CIS Instance 3. Configure Domain settings in customer CIS Instance 4. Add DNS Records of type `NS` for customer's Domain nameservers to Master CIS Account During CIS Instance deprovisioing role will perform following tasks: 1. Delete DNS Record from Master Account 2. Delete Domain from Customer Account 3. Delete Customer CIS Instance Role Variables \u00a4 cis_action \u00a4 Required. Action to be performed by CIS role. Valid values are provision or deprovision Environment Variable: CIS_ACTION Default Value: provision cis_plan \u00a4 Required. The plan type of the service Environment Variable: CIS_PLAN Default Value: standard ibmcloud_apikey \u00a4 Required. Provide your IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY Default Value: None ibmcloud_resourcegroup \u00a4 Provide the name of the resource group which will own the CIS instance. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default master_ibmcloud_api_key \u00a4 Required. Provide IBM Cloud API Key for Account where Master CIS Instance is running. Environment Variable: MASTER_IBMCLOUD_APIKEY Default Value: None master_cis_resource_group \u00a4 Required. Provide the name of the resource group which owns the Master CIS instance. Environment Variable: MASTER_CIS_RESOURCE_GROUP Default Value: manager master_cis_resource_name \u00a4 Required. Master CIS Instance name Environment Variable: MASTER_CIS_RESOURCE_NAME Default Value: {{mas_instance_id}}-cis master_cis_base_domain \u00a4 Required. Domain from Master CIS Instance - Environment Variable: MASTER_CIS_BASE_DOMAIN mas_instance_id \u00a4 Used as suffix string to define CIS Service name. Environment Variable: MAS_INSTANCE_ID Default Value: None cluster_name \u00a4 Used as prefix string to define CIS Service name. Environment Variable: CLUSTER_NAME Default Value: None mas_config_dir \u00a4 Local directory to save the generated CIS Instance details as ConfigMap. Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbook \u00a4 Create CIS Instance alongwith save Instance details in MAS_CONFIG_DIR path as ConfigMap - hosts: localhost any_errors_fatal: true vars: cis_action: provision mas_instance_id: masinst1 mas_config_dir: ~/masconfig ibmcloud_apikey: \"****\" master_ibmcloud_api_key: \"******\" cluster_name: \"test\" roles: - ibm.mas_devops.cis","title":"cis"},{"location":"roles/cis/#cis","text":"This role provides support for Configuring IBM Cloud Internet Services.During CIS provisioning it performs four tasks during provisioning in given order: 1. Provision CIS Instance in customer account 2. Add customer domain to customer's CIS Instance 3. Configure Domain settings in customer CIS Instance 4. Add DNS Records of type `NS` for customer's Domain nameservers to Master CIS Account During CIS Instance deprovisioing role will perform following tasks: 1. Delete DNS Record from Master Account 2. Delete Domain from Customer Account 3. Delete Customer CIS Instance","title":"cis"},{"location":"roles/cis/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cis/#cis_action","text":"Required. Action to be performed by CIS role. Valid values are provision or deprovision Environment Variable: CIS_ACTION Default Value: provision","title":"cis_action"},{"location":"roles/cis/#cis_plan","text":"Required. The plan type of the service Environment Variable: CIS_PLAN Default Value: standard","title":"cis_plan"},{"location":"roles/cis/#ibmcloud_apikey","text":"Required. Provide your IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/cis/#ibmcloud_resourcegroup","text":"Provide the name of the resource group which will own the CIS instance. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/cis/#master_ibmcloud_api_key","text":"Required. Provide IBM Cloud API Key for Account where Master CIS Instance is running. Environment Variable: MASTER_IBMCLOUD_APIKEY Default Value: None","title":"master_ibmcloud_api_key"},{"location":"roles/cis/#master_cis_resource_group","text":"Required. Provide the name of the resource group which owns the Master CIS instance. Environment Variable: MASTER_CIS_RESOURCE_GROUP Default Value: manager","title":"master_cis_resource_group"},{"location":"roles/cis/#master_cis_resource_name","text":"Required. Master CIS Instance name Environment Variable: MASTER_CIS_RESOURCE_NAME Default Value: {{mas_instance_id}}-cis","title":"master_cis_resource_name"},{"location":"roles/cis/#master_cis_base_domain","text":"Required. Domain from Master CIS Instance - Environment Variable: MASTER_CIS_BASE_DOMAIN","title":"master_cis_base_domain"},{"location":"roles/cis/#mas_instance_id","text":"Used as suffix string to define CIS Service name. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cis/#cluster_name","text":"Used as prefix string to define CIS Service name. Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/cis/#mas_config_dir","text":"Local directory to save the generated CIS Instance details as ConfigMap. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/cis/#example-playbook","text":"Create CIS Instance alongwith save Instance details in MAS_CONFIG_DIR path as ConfigMap - hosts: localhost any_errors_fatal: true vars: cis_action: provision mas_instance_id: masinst1 mas_config_dir: ~/masconfig ibmcloud_apikey: \"****\" master_ibmcloud_api_key: \"******\" cluster_name: \"test\" roles: - ibm.mas_devops.cis","title":"Example Playbook"},{"location":"roles/configure_manage_eventstreams/","text":"configure_manage_eventstreams \u00a4 This role configures manage to use IBM Cloud Eventstreams. NOTE This role inserts dummy kafka password during configuration (not the actual one),so user has to follow below guide to configure kafka password manually via Manage Dashboard. Role Variables \u00a4 mas_instance_id \u00a4 Required Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \u00a4 Required Environment Variable: MAS_WORKSPACE_ID Default Value: None ibmcloud_apikey \u00a4 Required Environment Variable: IBMCLOUD_APIKEY Default Value: None ibmcloud_region \u00a4 Optional Environment Variable: IBMCLOUD_REGION Default Value: us-east ibmcloud_resourcegroup \u00a4 IBM Cloud Resource Group Name where the IBM Cloud Eventstreams is provisioned. - Optional - Environment Variable: IBMCLOUD_RESOURCEGROUP - Default Value: Default eventstreams_name \u00a4 IBM Cloud Eventstreams Service Name - Required - Environment Variable: EVENTSTREAMS_NAME - Default Value: None eventstreams_location \u00a4 IBM Cloud Eventstreams Service Location - Optional - Environment Variable: EVENTSTREAMS_LOCATION - Default Value: us-east db2wh_dbname \u00a4 DB2 Database name where configurations will be done for Manage to use IBM Cloud Eventstreams - Optional - Environment Variable: DB2WH_DBNAME - Default Value: BLUDB cpd_meta_namespace \u00a4 Required Environment Variable: CPD_NAMESPACE Default Value: None db2_instance_name \u00a4 Required to build up pod name running db2 Required Environment Variable: DB2_INSTANCE_NAME Default Value: None mas_app_id \u00a4 Optional Environment Variable: MAS_APP_ID Default Value: manage mas_app_ws_fqn \u00a4 Fully Qualified Name for MAS Application - Optional - Environment Variable: MAS_APP_WS_FQN - Default Value: manageworkspaces.apps.mas.ibm.com Example Playbook \u00a4 Configures IBM Cloud Evenstreams service with Manage - hosts: localhost any_errors_fatal: true vars: mas_instance_id: 'test-instance' mas_workspace_id: 'main' ibmcloud_apikey: 'test-api-key' eventstreams_name: 'test-es' cpd_meta_namespace: 'db2u' db2_instance_name: 'test-db2' roles: - ibm.mas_devops.configure_manage_cfg","title":"configure_manage_eventstreams"},{"location":"roles/configure_manage_eventstreams/#configure_manage_eventstreams","text":"This role configures manage to use IBM Cloud Eventstreams. NOTE This role inserts dummy kafka password during configuration (not the actual one),so user has to follow below guide to configure kafka password manually via Manage Dashboard.","title":"configure_manage_eventstreams"},{"location":"roles/configure_manage_eventstreams/#role-variables","text":"","title":"Role Variables"},{"location":"roles/configure_manage_eventstreams/#mas_instance_id","text":"Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/configure_manage_eventstreams/#mas_workspace_id","text":"Required Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/configure_manage_eventstreams/#ibmcloud_apikey","text":"Required Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/configure_manage_eventstreams/#ibmcloud_region","text":"Optional Environment Variable: IBMCLOUD_REGION Default Value: us-east","title":"ibmcloud_region"},{"location":"roles/configure_manage_eventstreams/#ibmcloud_resourcegroup","text":"IBM Cloud Resource Group Name where the IBM Cloud Eventstreams is provisioned. - Optional - Environment Variable: IBMCLOUD_RESOURCEGROUP - Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/configure_manage_eventstreams/#eventstreams_name","text":"IBM Cloud Eventstreams Service Name - Required - Environment Variable: EVENTSTREAMS_NAME - Default Value: None","title":"eventstreams_name"},{"location":"roles/configure_manage_eventstreams/#eventstreams_location","text":"IBM Cloud Eventstreams Service Location - Optional - Environment Variable: EVENTSTREAMS_LOCATION - Default Value: us-east","title":"eventstreams_location"},{"location":"roles/configure_manage_eventstreams/#db2wh_dbname","text":"DB2 Database name where configurations will be done for Manage to use IBM Cloud Eventstreams - Optional - Environment Variable: DB2WH_DBNAME - Default Value: BLUDB","title":"db2wh_dbname"},{"location":"roles/configure_manage_eventstreams/#cpd_meta_namespace","text":"Required Environment Variable: CPD_NAMESPACE Default Value: None","title":"cpd_meta_namespace"},{"location":"roles/configure_manage_eventstreams/#db2_instance_name","text":"Required to build up pod name running db2 Required Environment Variable: DB2_INSTANCE_NAME Default Value: None","title":"db2_instance_name"},{"location":"roles/configure_manage_eventstreams/#mas_app_id","text":"Optional Environment Variable: MAS_APP_ID Default Value: manage","title":"mas_app_id"},{"location":"roles/configure_manage_eventstreams/#mas_app_ws_fqn","text":"Fully Qualified Name for MAS Application - Optional - Environment Variable: MAS_APP_WS_FQN - Default Value: manageworkspaces.apps.mas.ibm.com","title":"mas_app_ws_fqn"},{"location":"roles/configure_manage_eventstreams/#example-playbook","text":"Configures IBM Cloud Evenstreams service with Manage - hosts: localhost any_errors_fatal: true vars: mas_instance_id: 'test-instance' mas_workspace_id: 'main' ibmcloud_apikey: 'test-api-key' eventstreams_name: 'test-es' cpd_meta_namespace: 'db2u' db2_instance_name: 'test-db2' roles: - ibm.mas_devops.configure_manage_cfg","title":"Example Playbook"},{"location":"roles/cos/","text":"cos \u00a4 This role provides support for: Provisioning and Configuring Cloud Object Storage in MAS. It currently supports two providers: In-cluster Ceph Object Storage leveraging OpenShift Container Storage IBM Cloud Object Storage Deprovision Cloud Object Store. It currently supports one provider: IBM Cloud Object Storage Currently this role only supports generating a system-scoped ObjectStorageCfg resource, but the generated file can be modified if you wish to use other scopes. Role Variables - General \u00a4 cos_type \u00a4 Which COS provider to use; can be set to either ibm for IBM Cloud Object Storage or ocs for OpenShift Container Storage Required Environment Variable: COS_TYPE Default Value: None cos_action \u00a4 Which action you want to run for the COS instance. You can either provision or deprovision a COS instance in your IBM Cloud account. Required Environment Variable: COS_ACTION Default Value: provision ocp_ingress_tls_secret_name \u00a4 This can be set to the name of the cluster default router certificate's secret, for use in rare cases where the role is unable to determine the secret name automatically. Optional Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Role Variables - IBM COS \u00a4 cos_instance_name \u00a4 Provide an optional name for the Object Storage instance. This is only used when cos_type is set to ibm for IBM Cloud Object Storage. Environment Variable: COS_INSTANCE_NAME Default Value: Object Storage for MAS , if mas_instance_id is set the MAS instance ID will be appended to this name. cos_location_info \u00a4 Required. The location where the instance available - Environment Variable: COS_LOCATION - Default Value: global cos_plan_type \u00a4 Required (For Provisioning). The plan type of the service - Environment Variable: COS_PLAN - Default Value: standard cos_url \u00a4 Required (For Provisioning). The COS region location url endpoint. Needed to generage a system-scoped ObjectStorageCfg resource configuration file for MAS. - Environment Variable: COS_REGION_LOCATION_URL - Default Value: https://s3.us.cloud-object-storage.appdomain.cloud cos_resource_key_iam_role \u00a4 Provide an optional role when cos service credential is getting created during COS provisioning. - Environment Variable: COS_RESOURCE_KEY_IAM_ROLE - Default Value: Manager cos_use_hmac \u00a4 Set to false to disable the use of HMAC encrypted credentials, however doing so will prevent use of this COS instance with Maximo Application Suite. Optional Environment Variable: COS_USE_HMAC Default: true cos_apikey \u00a4 Required if cos_type is set to ibm . Provide your less priviledged IBM Cloud API Key for COS only Environment Variable: COS_APIKEY Default Value: ibmcloud_apikey ibmcloud_apikey \u00a4 Required if cos_type is set to ibm . Provide your IBM Cloud API Key that will be used as the default API Key across multiple roles in this collection. Environment Variable: IBMCLOUD_APIKEY Default Value: None cos_resourcegroup \u00a4 Only used when cos_type is set to ibm . Provide the name of the resource group which will own the COS instance. Environment Variable: COS_RESOURCEGROUP Default Value: ibmcloud_resourcegroup ibmcloud_resourcegroup \u00a4 Only used when cos_type is set to ibm . Provide the name of the default resource group used across multiple roles in this collection. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default Role Variables - MAS Configuration \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the ObjectStorageCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated ObjectStorageCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None include_cluster_ingress_cert_chain \u00a4 Optional. When set to True , includes the complete certificates chain in the generated MAS configuration, when a trusted certificate authority is found in your cluster's ingress. Optional Environment Variable: INCLUDE_CLUSTER_INGRESS_CERT_CHAIN Default: False Example Playbook \u00a4 Create the Ceph Object store on the existing OCS cluster and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ocs mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos Create the IBM Cloud Object storage Instance and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm ibmcloud_apikey: <Your IBM Cloud API Key> mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos License \u00a4 EPL-2.0","title":"cos"},{"location":"roles/cos/#cos","text":"This role provides support for: Provisioning and Configuring Cloud Object Storage in MAS. It currently supports two providers: In-cluster Ceph Object Storage leveraging OpenShift Container Storage IBM Cloud Object Storage Deprovision Cloud Object Store. It currently supports one provider: IBM Cloud Object Storage Currently this role only supports generating a system-scoped ObjectStorageCfg resource, but the generated file can be modified if you wish to use other scopes.","title":"cos"},{"location":"roles/cos/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/cos/#cos_type","text":"Which COS provider to use; can be set to either ibm for IBM Cloud Object Storage or ocs for OpenShift Container Storage Required Environment Variable: COS_TYPE Default Value: None","title":"cos_type"},{"location":"roles/cos/#cos_action","text":"Which action you want to run for the COS instance. You can either provision or deprovision a COS instance in your IBM Cloud account. Required Environment Variable: COS_ACTION Default Value: provision","title":"cos_action"},{"location":"roles/cos/#ocp_ingress_tls_secret_name","text":"This can be set to the name of the cluster default router certificate's secret, for use in rare cases where the role is unable to determine the secret name automatically. Optional Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default","title":"ocp_ingress_tls_secret_name"},{"location":"roles/cos/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/cos/#role-variables-ibm-cos","text":"","title":"Role Variables - IBM COS"},{"location":"roles/cos/#cos_instance_name","text":"Provide an optional name for the Object Storage instance. This is only used when cos_type is set to ibm for IBM Cloud Object Storage. Environment Variable: COS_INSTANCE_NAME Default Value: Object Storage for MAS , if mas_instance_id is set the MAS instance ID will be appended to this name.","title":"cos_instance_name"},{"location":"roles/cos/#cos_location_info","text":"Required. The location where the instance available - Environment Variable: COS_LOCATION - Default Value: global","title":"cos_location_info"},{"location":"roles/cos/#cos_plan_type","text":"Required (For Provisioning). The plan type of the service - Environment Variable: COS_PLAN - Default Value: standard","title":"cos_plan_type"},{"location":"roles/cos/#cos_url","text":"Required (For Provisioning). The COS region location url endpoint. Needed to generage a system-scoped ObjectStorageCfg resource configuration file for MAS. - Environment Variable: COS_REGION_LOCATION_URL - Default Value: https://s3.us.cloud-object-storage.appdomain.cloud","title":"cos_url"},{"location":"roles/cos/#cos_resource_key_iam_role","text":"Provide an optional role when cos service credential is getting created during COS provisioning. - Environment Variable: COS_RESOURCE_KEY_IAM_ROLE - Default Value: Manager","title":"cos_resource_key_iam_role"},{"location":"roles/cos/#cos_use_hmac","text":"Set to false to disable the use of HMAC encrypted credentials, however doing so will prevent use of this COS instance with Maximo Application Suite. Optional Environment Variable: COS_USE_HMAC Default: true","title":"cos_use_hmac"},{"location":"roles/cos/#cos_apikey","text":"Required if cos_type is set to ibm . Provide your less priviledged IBM Cloud API Key for COS only Environment Variable: COS_APIKEY Default Value: ibmcloud_apikey","title":"cos_apikey"},{"location":"roles/cos/#ibmcloud_apikey","text":"Required if cos_type is set to ibm . Provide your IBM Cloud API Key that will be used as the default API Key across multiple roles in this collection. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/cos/#cos_resourcegroup","text":"Only used when cos_type is set to ibm . Provide the name of the resource group which will own the COS instance. Environment Variable: COS_RESOURCEGROUP Default Value: ibmcloud_resourcegroup","title":"cos_resourcegroup"},{"location":"roles/cos/#ibmcloud_resourcegroup","text":"Only used when cos_type is set to ibm . Provide the name of the default resource group used across multiple roles in this collection. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/cos/#role-variables-mas-configuration","text":"","title":"Role Variables - MAS Configuration"},{"location":"roles/cos/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the ObjectStorageCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cos/#mas_config_dir","text":"Local directory to save the generated ObjectStorageCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/cos/#include_cluster_ingress_cert_chain","text":"Optional. When set to True , includes the complete certificates chain in the generated MAS configuration, when a trusted certificate authority is found in your cluster's ingress. Optional Environment Variable: INCLUDE_CLUSTER_INGRESS_CERT_CHAIN Default: False","title":"include_cluster_ingress_cert_chain"},{"location":"roles/cos/#example-playbook","text":"Create the Ceph Object store on the existing OCS cluster and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ocs mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos Create the IBM Cloud Object storage Instance and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm ibmcloud_apikey: <Your IBM Cloud API Key> mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos","title":"Example Playbook"},{"location":"roles/cos/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cos_bucket/","text":"cos_bucket \u00a4 This role extends support to create or deprovision Cloud Object Storage buckets. Role Variables \u00a4 cos_type \u00a4 Required. Which COS provider to use; can be set to either ibm for IBM Cloud Object Storage or aws for S3 bucket types (aws support under development). Environment Variable: COS_TYPE Default Value: None cos_bucket_action \u00a4 Required. Which action you want to run for the COS bucket. You can either create or delete a COS bucket. Environment Variable: COS_BUCKET_ACTION Default Value: create Role Variables - IBM Cloud Object Storage buckets \u00a4 cos_bucket_name \u00a4 Optional name for your IBM Cloud Object Storage bucket. Environment Variable: COS_BUCKET_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID-bucket cos_bucket_storage_class \u00a4 Optional. IBM Cloud Object Storage bucket storage class. Supported options are smart , vault , cold and flex . For more details, see IBM Cloud Object Storage documentation Environment Variable: COS_BUCKET_STORAGE_CLASS Default Value: smart cos_instance_name \u00a4 Provide the Object Storage instance name, will be used to find the targeted COS instance to create/deprovision the buckets. This is only used when cos_type is set to ibm for IBM Cloud Object Storage. Environment Variable: COS_INSTANCE_NAME Default Value: None cos_location_info \u00a4 Required. The location where the COS instance is available Environment Variable: COS_LOCATION Default Value: global cos_bucket_region_location_type \u00a4 Required. This defines the resiliency of your COS bucket. Supported options are cross_region_location (Highest availability) or region_location (Best performance). For more details, see IBM Cloud Object Storage documentation Environment Variable: COS_BUCKET_REGION_LOCATION_TYPE Default Value: cross_region_location cos_bucket_region_location: \"{{ lookup('env', 'COS_BUCKET_REGION_LOCATION') | default(bucket_cross_reg_loc, true) }}\" cos_bucket_region_location \u00a4 Required. This defines the specific region of your COS bucket. For cross_region_location type, the supported regions are us , ap and eu . For region_location type, the supported regions are au-syd , eu-de , eu-gb , jp-tok , us-east , us-south , ca-tor , jp-osa and br-sao . For more details, see IBM Cloud Object Storage documentation ibmcloud_region \u00a4 Optional. For cross region location type buckets, the IBM Cloud region can be used as alternative to determine which cross region location to be used while creating the buckets. - Environment Variable: IBMCLOUD_REGION - Default Value: us-east cos_url \u00a4 Required (For bucket creation). The COS region location url endpoint. Needed to specify the COS bucket region location. - Environment Variable: COS_REGION_LOCATION_URL - Default Value: https://s3.us.cloud-object-storage.appdomain.cloud cos_plan_type \u00a4 Required (For Provisioning). The plan type of the service - Environment Variable: COS_PLAN - Default Value: standard resource_key_iam_role \u00a4 Provide an optional role when cos service credential is getting created during COS bucket creation. - Environment Variable: RESOURCE_KEY_IAM_ROLE - Default Value: Manager ibmcloud_apikey \u00a4 Required if cos_type is set to ibm . Provide your IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY Default Value: None ibmcloud_resourcegroup \u00a4 Only used when cos_type is set to ibm . Provide the name of the resource group which will own the COS instance for the targeted buckets. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default Role Variables - AWS S3 Buckets \u00a4 To run this role successfully for AWS s3 buckets, you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. aws_bucket_name \u00a4 Optional name for your AWS/S3 bucket. Environment Variable: COS_BUCKET_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID-bucket aws_region \u00a4 The region where the bucket is located. Required. Environment Variable: AWS_REGION Default Value: us-east-2 aws_bucket_versioning_flag \u00a4 Flag to define if versioning should be enabled for the bucket Optional. Environment Variable: COS_BUCKET_VERSIONING_FLAG Default Value: True aws_bucket_encryption \u00a4 JSON formatted string to define default encryption configuration for AWS S3 bucket. Optional. Environment Variable: COS_BUCKET_ENCRYPTION Default Value: None aws_bucket_force_deletion_flag \u00a4 Deletes S3 AWS bucket objects prior deleting the S3 bucket. This option only works if versioning is not enabled in the bucket. Note: To delete AWS bucket, cos_bucket_action must be set to delete . Optional. Environment Variable: COS_BUCKET_FORCE_DELETION_FLAG Default Value: True Example Playbook \u00a4 Create the IBM Cloud Object storage bucket. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm cos_bucket_action: create cos_bucket_name: my-ibm-bucket cos_instance_name: my-ibmcos-instance-name ibmcloud_apikey: my-ibm-cloud-apikey roles: - ibm.mas_devops.cos_bucket Create the AWS S3 storage bucket. - hosts: localhost any_errors_fatal: true vars: cos_type: aws cos_bucket_action: create aws_bucket_name: my-aws-bucket aws_region: us-east-2 aws_bucket_versioning_flag: True aws_bucket_encryption: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}' roles: - ibm.mas_devops.cos_bucket License \u00a4 EPL-2.0","title":"cos_bucket"},{"location":"roles/cos_bucket/#cos_bucket","text":"This role extends support to create or deprovision Cloud Object Storage buckets.","title":"cos_bucket"},{"location":"roles/cos_bucket/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cos_bucket/#cos_type","text":"Required. Which COS provider to use; can be set to either ibm for IBM Cloud Object Storage or aws for S3 bucket types (aws support under development). Environment Variable: COS_TYPE Default Value: None","title":"cos_type"},{"location":"roles/cos_bucket/#cos_bucket_action","text":"Required. Which action you want to run for the COS bucket. You can either create or delete a COS bucket. Environment Variable: COS_BUCKET_ACTION Default Value: create","title":"cos_bucket_action"},{"location":"roles/cos_bucket/#role-variables-ibm-cloud-object-storage-buckets","text":"","title":"Role Variables - IBM Cloud Object Storage buckets"},{"location":"roles/cos_bucket/#cos_bucket_name","text":"Optional name for your IBM Cloud Object Storage bucket. Environment Variable: COS_BUCKET_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID-bucket","title":"cos_bucket_name"},{"location":"roles/cos_bucket/#cos_bucket_storage_class","text":"Optional. IBM Cloud Object Storage bucket storage class. Supported options are smart , vault , cold and flex . For more details, see IBM Cloud Object Storage documentation Environment Variable: COS_BUCKET_STORAGE_CLASS Default Value: smart","title":"cos_bucket_storage_class"},{"location":"roles/cos_bucket/#cos_instance_name","text":"Provide the Object Storage instance name, will be used to find the targeted COS instance to create/deprovision the buckets. This is only used when cos_type is set to ibm for IBM Cloud Object Storage. Environment Variable: COS_INSTANCE_NAME Default Value: None","title":"cos_instance_name"},{"location":"roles/cos_bucket/#cos_location_info","text":"Required. The location where the COS instance is available Environment Variable: COS_LOCATION Default Value: global","title":"cos_location_info"},{"location":"roles/cos_bucket/#cos_bucket_region_location_type","text":"Required. This defines the resiliency of your COS bucket. Supported options are cross_region_location (Highest availability) or region_location (Best performance). For more details, see IBM Cloud Object Storage documentation Environment Variable: COS_BUCKET_REGION_LOCATION_TYPE Default Value: cross_region_location cos_bucket_region_location: \"{{ lookup('env', 'COS_BUCKET_REGION_LOCATION') | default(bucket_cross_reg_loc, true) }}\"","title":"cos_bucket_region_location_type"},{"location":"roles/cos_bucket/#cos_bucket_region_location","text":"Required. This defines the specific region of your COS bucket. For cross_region_location type, the supported regions are us , ap and eu . For region_location type, the supported regions are au-syd , eu-de , eu-gb , jp-tok , us-east , us-south , ca-tor , jp-osa and br-sao . For more details, see IBM Cloud Object Storage documentation","title":"cos_bucket_region_location"},{"location":"roles/cos_bucket/#ibmcloud_region","text":"Optional. For cross region location type buckets, the IBM Cloud region can be used as alternative to determine which cross region location to be used while creating the buckets. - Environment Variable: IBMCLOUD_REGION - Default Value: us-east","title":"ibmcloud_region"},{"location":"roles/cos_bucket/#cos_url","text":"Required (For bucket creation). The COS region location url endpoint. Needed to specify the COS bucket region location. - Environment Variable: COS_REGION_LOCATION_URL - Default Value: https://s3.us.cloud-object-storage.appdomain.cloud","title":"cos_url"},{"location":"roles/cos_bucket/#cos_plan_type","text":"Required (For Provisioning). The plan type of the service - Environment Variable: COS_PLAN - Default Value: standard","title":"cos_plan_type"},{"location":"roles/cos_bucket/#resource_key_iam_role","text":"Provide an optional role when cos service credential is getting created during COS bucket creation. - Environment Variable: RESOURCE_KEY_IAM_ROLE - Default Value: Manager","title":"resource_key_iam_role"},{"location":"roles/cos_bucket/#ibmcloud_apikey","text":"Required if cos_type is set to ibm . Provide your IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/cos_bucket/#ibmcloud_resourcegroup","text":"Only used when cos_type is set to ibm . Provide the name of the resource group which will own the COS instance for the targeted buckets. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/cos_bucket/#role-variables-aws-s3-buckets","text":"To run this role successfully for AWS s3 buckets, you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Role Variables - AWS S3 Buckets"},{"location":"roles/cos_bucket/#aws_bucket_name","text":"Optional name for your AWS/S3 bucket. Environment Variable: COS_BUCKET_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID-bucket","title":"aws_bucket_name"},{"location":"roles/cos_bucket/#aws_region","text":"The region where the bucket is located. Required. Environment Variable: AWS_REGION Default Value: us-east-2","title":"aws_region"},{"location":"roles/cos_bucket/#aws_bucket_versioning_flag","text":"Flag to define if versioning should be enabled for the bucket Optional. Environment Variable: COS_BUCKET_VERSIONING_FLAG Default Value: True","title":"aws_bucket_versioning_flag"},{"location":"roles/cos_bucket/#aws_bucket_encryption","text":"JSON formatted string to define default encryption configuration for AWS S3 bucket. Optional. Environment Variable: COS_BUCKET_ENCRYPTION Default Value: None","title":"aws_bucket_encryption"},{"location":"roles/cos_bucket/#aws_bucket_force_deletion_flag","text":"Deletes S3 AWS bucket objects prior deleting the S3 bucket. This option only works if versioning is not enabled in the bucket. Note: To delete AWS bucket, cos_bucket_action must be set to delete . Optional. Environment Variable: COS_BUCKET_FORCE_DELETION_FLAG Default Value: True","title":"aws_bucket_force_deletion_flag"},{"location":"roles/cos_bucket/#example-playbook","text":"Create the IBM Cloud Object storage bucket. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm cos_bucket_action: create cos_bucket_name: my-ibm-bucket cos_instance_name: my-ibmcos-instance-name ibmcloud_apikey: my-ibm-cloud-apikey roles: - ibm.mas_devops.cos_bucket Create the AWS S3 storage bucket. - hosts: localhost any_errors_fatal: true vars: cos_type: aws cos_bucket_action: create aws_bucket_name: my-aws-bucket aws_region: us-east-2 aws_bucket_versioning_flag: True aws_bucket_encryption: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}' roles: - ibm.mas_devops.cos_bucket","title":"Example Playbook"},{"location":"roles/cos_bucket/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d/","text":"cp4d \u00a4 This role installs or upgrades IBM Cloud Pak for Data Operator in the target cluster. It assumes that you have already installed the IBM Maximo Operator Catalog and configured Certificate Manager in the target cluster. These actions are performed by the ibm_catalogs cert_manager roles in this collection. Cloud Pak for Data will be configured as a specialized installation Info A specialized installation allows a user with project administrator permissions to install the software after a cluster administrator completes the initial cluster setup. A specialized installation also facilitates strict division between Red Hat OpenShift Container Platform projects (Kubernetes namespaces). In a specialized installation, the IBM Cloud Pak foundational services operators are installed in the ibm-common-services project and the Cloud Pak for Data operators are installed in a separate project (typically cpd-operators). Each project has a dedicated: Operator group, which specifies the OwnNamespace installation mode NamespaceScope Operator, which allows the operators in the project to manage operators and service workloads in specific projects In this way, you can specify different settings for the IBM Cloud Pak foundational services and for the Cloud Pak for Data operators. Currently supported Cloud Pak for Data release versions are: 5.1.3 5.2.0 Tip For more information about CPD versioning, see IBM Cloud Pak for data Operator and operand versions 5.1.x Cloud Pak for Data Version Mapping \u00a4 Users can choose to install a specific version of Cloud Pak for Data by setting CPD_PRODUCT_VERSION variable. However, by default, the version of Cloud Pak for Data will be determined by the version of the Maximo Operator Catalog that is installed in the cluster. If CPD_PRODUCT_VERSION variable is not defined, and the role is not able to find the Maximo Operator Catalog, then the role will default to installing the Cloud Pak for Data version supported by the latest released MAS catalog. Upgrade \u00a4 The role will automatically install or upgrade (if targeted to an existing CPD deployment) the corresponding Zen version associated to the chosen Cloud Pak for Data release, for example: Cloud Pak for Data release version 5.1.3 installs Zen/Control Plane version 6.1.1 Cloud Pak for Data release version 5.2.0 installs Zen/Control Plane version 6.2.0 Tip For more information about IBM Cloud Pak for Data upgrade process, refer to the Cloud Pak for Data official documentation . Cloud Pak for Data Deployment Details \u00a4 Cloud Pak for Data 5.x leverages Cloud Pak Foundational Services v4, which runs its deployments in isolated/dedicated scope model, that means that its dependencies will be grouped and installed within the Cloud Pak for Data related projects/namespaces. There are only two namespaces that will be used: CPD instance namespace (e.g ibm-cpd ) and CPD operators namespace (e.g ibm-cpd-operators ). In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 17h ibm-common-service-operator 1/1 1 1 17h ibm-namespace-scope-operator 1/1 1 1 17h ibm-zen-operator 1/1 1 1 17h meta-api-deploy 1/1 1 1 17h operand-deployment-lifecycle-manager 1/1 1 1 17h postgresql-operator-controller-manager-1-18-7 1/1 1 1 17h In the ibm-cpd namespace: oc -n ibm-cpd get zenservice,ibmcpd,deployments,sts,pvc NAME VERSION STATUS AGE zenservice.zen.cpd.ibm.com/lite-cr 6.0.1 Completed 17h NAME AGE ibmcpd.cpd.ibm.com/ibmcpd-cr 17h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ibm-mcs-hubwork 1/1 1 1 17h deployment.apps/ibm-mcs-placement 1/1 1 1 17h deployment.apps/ibm-mcs-storage 1/1 1 1 17h deployment.apps/ibm-nginx 3/3 3 3 16h deployment.apps/ibm-nginx-tester 1/1 1 1 16h deployment.apps/usermgmt 3/3 3 3 16h deployment.apps/zen-audit 2/2 2 2 16h deployment.apps/zen-core 3/3 3 3 16h deployment.apps/zen-core-api 3/3 3 3 16h deployment.apps/zen-watchdog 2/2 2 2 16h deployment.apps/zen-watcher 1/1 1 1 16h NAME READY AGE statefulset.apps/zen-minio 3/3 17h NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/export-zen-minio-0 Bound pvc-b2a2a729-13c1-4e7f-b672-0b5efc6aa40a 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/export-zen-minio-1 Bound pvc-7e772a3a-8849-4291-8e14-501f49e79182 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/export-zen-minio-2 Bound pvc-e0dd31dc-916d-4b15-9d9c-351db0a2b47f 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/ibm-cs-postgres-backup Bound pvc-ef788b99-784f-4531-a1b3-12611f112551 20Gi RWO ibmc-block-gold 16h persistentvolumeclaim/ibm-zen-objectstore-backup-pvc Bound pvc-d5e61dcf-65a3-4930-9cbf-ab80d04dda00 20Gi RWO ibmc-block-gold 16h persistentvolumeclaim/zen-metastore-edb-1 Bound pvc-19d44f17-05ab-4dc0-bb5d-1b5f15ffd201 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/zen-metastore-edb-2 Bound pvc-741ea444-b6f0-44ff-a123-bb4615d97381 20Gi RWO ibmc-block-gold 17h Tip You can retrieve the Initial Cloud Pak for Data password from the admin-user-details secret: oc -n ibm-cpd get secret admin-user-details -o jsonpath=\"{.data.initial_admin_password}\" | base64 -d Role Variables \u00a4 Installation Variables \u00a4 cpd_product_version \u00a4 Cloud Pak for Data release version to install. Required Environment Variable: CPD_PRODUCT_VERSION Default: Determined by installed MAS catalog version Purpose : Specifies which CP4D release version to install or upgrade to. The version determines which Zen/Control Plane version and features are available. When to use : - Set explicitly when you need a specific CP4D version - Leave unset to use version matching the installed MAS catalog - Required for reproducible deployments - Must match supported versions (currently 5.1.3, 5.2.0) Valid values : 5.1.3 , 5.2.0 (or other supported versions) Impact : - 5.1.3 : Installs Zen/Control Plane 6.1.1 - 5.2.0 : Installs Zen/Control Plane 6.2.0 Different versions have different features and compatibility requirements. Related variables : - Determines compatible service versions - Affects cpd_scale_config options Note : If not set and MAS catalog is not found, defaults to CP4D version supported by latest MAS catalog. For version-specific details, see CP4D Operator and operand versions . ibm_entitlement_key \u00a4 IBM entitlement key for accessing container images. Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Provides authentication to IBM Container Registry for pulling CP4D container images. This key grants access to entitled software. When to use : - Always required for CP4D installation - Obtain from IBM Container Library - Must have valid entitlement for CP4D Valid values : Valid IBM entitlement key string from your IBM account Impact : Without a valid key, image pulls will fail and CP4D installation cannot proceed. Key must have CP4D entitlement. Related variables : - cpd_entitlement_key : CP4D-specific override (primarily for development) Note : Keep this key secure and do not commit to source control. The key is tied to your IBM account and entitlements. Can be overridden by cpd_entitlement_key for CP4D-specific scenarios. cpd_entitlement_key \u00a4 CP4D-specific entitlement key override (primarily for development). Optional Environment Variable: CPD_ENTITLEMENT_KEY Default: None (uses ibm_entitlement_key ) Purpose : Provides a CP4D-specific entitlement key that overrides the general ibm_entitlement_key . Primarily used in development scenarios. When to use : - Leave unset for standard deployments (uses ibm_entitlement_key ) - Set when you need a different key specifically for CP4D - Useful in development/testing with separate entitlements Valid values : Valid IBM entitlement key string with CP4D entitlement Impact : When set, this key is used instead of ibm_entitlement_key for CP4D image pulls. If not set, falls back to ibm_entitlement_key . Related variables : - ibm_entitlement_key : General entitlement key (used if this is not set) Note : Most deployments should use ibm_entitlement_key only. This override is primarily for development scenarios where different keys are needed for different components. cpd_primary_storage_class \u00a4 Primary storage class for CP4D (must support ReadWriteMany). Required (if known storage classes not available) Environment Variable: CPD_PRIMARY_STORAGE_CLASS Default: ibmc-file-gold-gid , ocs-storagecluster-cephfs , or azurefiles-premium (if available) Purpose : Specifies the storage class for CP4D primary storage, which requires ReadWriteMany (RWX) access mode for file storage. When to use : - Leave unset for automatic detection if known storage classes exist - Set explicitly when using custom or non-standard storage classes - Must support RWX access mode for shared file storage Valid values : Storage class name supporting ReadWriteMany access mode Impact : CP4D uses this for shared file storage across pods. Incorrect storage class or one not supporting RWX will cause deployment to fail. Related variables : - cpd_metadata_storage_class : Separate storage for metadata (RWO) Note : Known supported classes: ibmc-file-gold-gid (IBM Cloud), ocs-storagecluster-cephfs (OCS), azurefiles-premium (Azure). See CP4D Storage Considerations for details. cpd_metadata_storage_class \u00a4 Storage class for CP4D Zen metadata database (must support ReadWriteOnce). Required (if known storage classes not available) Environment Variable: CPD_METADATA_STORAGE_CLASS Default: ibmc-block-gold , ocs-storagecluster-ceph-rbd , or managed-premium (if available) Purpose : Specifies the storage class for CP4D Zen metadata database, which requires ReadWriteOnce (RWO) access mode for block storage. When to use : - Leave unset for automatic detection if known storage classes exist - Set explicitly when using custom or non-standard storage classes - Must support RWO access mode for block storage Valid values : Storage class name supporting ReadWriteOnce access mode Impact : CP4D Zen metadata database uses this for persistent storage. Incorrect storage class or one not supporting RWO will cause deployment to fail. Related variables : - cpd_primary_storage_class : Separate storage for primary/file storage (RWX) Note : Known supported classes: ibmc-block-gold (IBM Cloud), ocs-storagecluster-ceph-rbd (OCS), managed-premium (Azure). Block storage typically provides better performance for databases. cpd_operators_namespace \u00a4 Namespace for CP4D operators installation. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default: ibm-cpd-operators Purpose : Specifies the namespace where CP4D operators will be installed. This follows the specialized installation model with separate operator and instance namespaces. When to use : - Use default ( ibm-cpd-operators ) for standard deployments - Set custom namespace for specific organizational requirements - Must be different from cpd_instance_namespace Valid values : Valid Kubernetes namespace name Impact : All CP4D operators (platform, zen, common services, etc.) are installed in this namespace. The namespace must not conflict with the instance namespace. Related variables : - cpd_instance_namespace : Separate namespace for CP4D instance workloads Note : CP4D uses a specialized installation model with operators in one namespace ( ibm-cpd-operators ) and instance workloads in another ( ibm-cpd ). This provides better isolation and management. cpd_instance_namespace \u00a4 Namespace for CP4D instance workloads. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default: ibm-cpd Purpose : Specifies the namespace where CP4D instance workloads (Zen, services, databases) will be deployed. Operators in cpd_operators_namespace watch and manage resources in this namespace. When to use : - Use default ( ibm-cpd ) for standard deployments - Set custom namespace for specific organizational requirements - Must be different from cpd_operators_namespace Valid values : Valid Kubernetes namespace name Impact : All CP4D instance workloads (Zen, MinIO, PostgreSQL, services) are deployed in this namespace. Operators watch this namespace for custom resources. Related variables : - cpd_operators_namespace : Separate namespace for CP4D operators Note : CP4D uses a specialized installation model with operators in ibm-cpd-operators and instance workloads in ibm-cpd . This separation provides better isolation and follows CP4D best practices. cpd_scale_config \u00a4 Resource scaling configuration for CP4D instance. Optional Environment Variable: CPD_SCALE_CONFIG Default: medium Purpose : Adjusts resource allocation (CPU, memory, replicas) for CP4D components to match workload requirements and increase processing capacity. When to use : - Use medium (default) for standard production deployments - Use small for development/test environments - Use large for high-capacity production environments - Adjust based on expected workload and performance requirements Valid values : small , medium , large (or other supported scale configurations) Impact : Determines resource requests/limits and replica counts for CP4D components. Larger scales require more cluster resources but provide better performance and capacity. Related variables : - cpd_product_version : Available scale options may vary by version Note : See Managing resources for detailed resource requirements per scale configuration. Ensure your cluster has sufficient resources for the selected scale. cpd_admin_username \u00a4 CP4D admin username for API authentication. Optional Environment Variable: CPD_ADMIN_USERNAME Default: cpadmin Purpose : Specifies the CP4D admin username for authenticating with CP4D APIs. Used when the role needs to interact with CP4D services. When to use : - Leave as default ( cpadmin ) if you haven't changed the initial admin username - Set explicitly if you changed the admin username after CP4D installation - Required for API operations that need admin authentication Valid values : Valid CP4D admin username string Impact : Used for CP4D API authentication. Incorrect username will cause API operations to fail. Related variables : - cpd_admin_password : Password for this admin user Note : The default cpadmin is the standard CP4D admin username. Only change if you've customized the admin username after installation. cpd_admin_password \u00a4 CP4D admin password for API authentication. Optional Environment Variable: CPD_ADMIN_PASSWORD Default: Retrieved from admin-user-details secret in cpd_instance_namespace Purpose : Specifies the CP4D admin password for authenticating with CP4D APIs. Used when the role needs to interact with CP4D services. When to use : - Leave unset (recommended) to auto-retrieve from cluster secret - Set explicitly if you changed the admin password after CP4D installation - Required for API operations that need admin authentication Valid values : Valid CP4D admin password string Impact : Used for CP4D API authentication. If not set, the role retrieves the initial admin password from the cluster. Incorrect password will cause API operations to fail. Related variables : - cpd_admin_username : Username for this admin password - cpd_instance_namespace : Namespace containing the password secret Note : The role automatically retrieves the initial admin password from the admin-user-details secret if not provided. Only set this if you've changed the password after installation. Keep passwords secure and do not commit to source control. Example Playbook \u00a4 Install Cloud Pak for Data 5.1.3 \u00a4 - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.1.3 cpd_primary_storage_class: ibmc-file-gold-gid cpd_metadata_storage_class: ibmc-block-gold roles: - ibm.mas_devops.cp4d Install Cloud Pak for Data 5.2.0 \u00a4 - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.2.0 cpd_primary_storage_class: ibmc-file-gold-gid cpd_metadata_storage_class: ibmc-block-gold roles: - ibm.mas_devops.cp4d Run Role Playbook \u00a4 export CPD_PRODUCT_VERSION=5.2.0 export CPD_PRIMARY_STORAGE_CLASS=ibmc-file-gold-gid export CPD_METADATA_STORAGE_CLASS=ibmc-block-gold export IBM_ENTITLEMENT_KEY=xxxxx ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"cp4d"},{"location":"roles/cp4d/#cp4d","text":"This role installs or upgrades IBM Cloud Pak for Data Operator in the target cluster. It assumes that you have already installed the IBM Maximo Operator Catalog and configured Certificate Manager in the target cluster. These actions are performed by the ibm_catalogs cert_manager roles in this collection. Cloud Pak for Data will be configured as a specialized installation Info A specialized installation allows a user with project administrator permissions to install the software after a cluster administrator completes the initial cluster setup. A specialized installation also facilitates strict division between Red Hat OpenShift Container Platform projects (Kubernetes namespaces). In a specialized installation, the IBM Cloud Pak foundational services operators are installed in the ibm-common-services project and the Cloud Pak for Data operators are installed in a separate project (typically cpd-operators). Each project has a dedicated: Operator group, which specifies the OwnNamespace installation mode NamespaceScope Operator, which allows the operators in the project to manage operators and service workloads in specific projects In this way, you can specify different settings for the IBM Cloud Pak foundational services and for the Cloud Pak for Data operators. Currently supported Cloud Pak for Data release versions are: 5.1.3 5.2.0 Tip For more information about CPD versioning, see IBM Cloud Pak for data Operator and operand versions 5.1.x","title":"cp4d"},{"location":"roles/cp4d/#cloud-pak-for-data-version-mapping","text":"Users can choose to install a specific version of Cloud Pak for Data by setting CPD_PRODUCT_VERSION variable. However, by default, the version of Cloud Pak for Data will be determined by the version of the Maximo Operator Catalog that is installed in the cluster. If CPD_PRODUCT_VERSION variable is not defined, and the role is not able to find the Maximo Operator Catalog, then the role will default to installing the Cloud Pak for Data version supported by the latest released MAS catalog.","title":"Cloud Pak for Data Version Mapping"},{"location":"roles/cp4d/#upgrade","text":"The role will automatically install or upgrade (if targeted to an existing CPD deployment) the corresponding Zen version associated to the chosen Cloud Pak for Data release, for example: Cloud Pak for Data release version 5.1.3 installs Zen/Control Plane version 6.1.1 Cloud Pak for Data release version 5.2.0 installs Zen/Control Plane version 6.2.0 Tip For more information about IBM Cloud Pak for Data upgrade process, refer to the Cloud Pak for Data official documentation .","title":"Upgrade"},{"location":"roles/cp4d/#cloud-pak-for-data-deployment-details","text":"Cloud Pak for Data 5.x leverages Cloud Pak Foundational Services v4, which runs its deployments in isolated/dedicated scope model, that means that its dependencies will be grouped and installed within the Cloud Pak for Data related projects/namespaces. There are only two namespaces that will be used: CPD instance namespace (e.g ibm-cpd ) and CPD operators namespace (e.g ibm-cpd-operators ). In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 17h ibm-common-service-operator 1/1 1 1 17h ibm-namespace-scope-operator 1/1 1 1 17h ibm-zen-operator 1/1 1 1 17h meta-api-deploy 1/1 1 1 17h operand-deployment-lifecycle-manager 1/1 1 1 17h postgresql-operator-controller-manager-1-18-7 1/1 1 1 17h In the ibm-cpd namespace: oc -n ibm-cpd get zenservice,ibmcpd,deployments,sts,pvc NAME VERSION STATUS AGE zenservice.zen.cpd.ibm.com/lite-cr 6.0.1 Completed 17h NAME AGE ibmcpd.cpd.ibm.com/ibmcpd-cr 17h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ibm-mcs-hubwork 1/1 1 1 17h deployment.apps/ibm-mcs-placement 1/1 1 1 17h deployment.apps/ibm-mcs-storage 1/1 1 1 17h deployment.apps/ibm-nginx 3/3 3 3 16h deployment.apps/ibm-nginx-tester 1/1 1 1 16h deployment.apps/usermgmt 3/3 3 3 16h deployment.apps/zen-audit 2/2 2 2 16h deployment.apps/zen-core 3/3 3 3 16h deployment.apps/zen-core-api 3/3 3 3 16h deployment.apps/zen-watchdog 2/2 2 2 16h deployment.apps/zen-watcher 1/1 1 1 16h NAME READY AGE statefulset.apps/zen-minio 3/3 17h NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/export-zen-minio-0 Bound pvc-b2a2a729-13c1-4e7f-b672-0b5efc6aa40a 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/export-zen-minio-1 Bound pvc-7e772a3a-8849-4291-8e14-501f49e79182 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/export-zen-minio-2 Bound pvc-e0dd31dc-916d-4b15-9d9c-351db0a2b47f 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/ibm-cs-postgres-backup Bound pvc-ef788b99-784f-4531-a1b3-12611f112551 20Gi RWO ibmc-block-gold 16h persistentvolumeclaim/ibm-zen-objectstore-backup-pvc Bound pvc-d5e61dcf-65a3-4930-9cbf-ab80d04dda00 20Gi RWO ibmc-block-gold 16h persistentvolumeclaim/zen-metastore-edb-1 Bound pvc-19d44f17-05ab-4dc0-bb5d-1b5f15ffd201 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/zen-metastore-edb-2 Bound pvc-741ea444-b6f0-44ff-a123-bb4615d97381 20Gi RWO ibmc-block-gold 17h Tip You can retrieve the Initial Cloud Pak for Data password from the admin-user-details secret: oc -n ibm-cpd get secret admin-user-details -o jsonpath=\"{.data.initial_admin_password}\" | base64 -d","title":"Cloud Pak for Data Deployment Details"},{"location":"roles/cp4d/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d/#installation-variables","text":"","title":"Installation Variables"},{"location":"roles/cp4d/#cpd_product_version","text":"Cloud Pak for Data release version to install. Required Environment Variable: CPD_PRODUCT_VERSION Default: Determined by installed MAS catalog version Purpose : Specifies which CP4D release version to install or upgrade to. The version determines which Zen/Control Plane version and features are available. When to use : - Set explicitly when you need a specific CP4D version - Leave unset to use version matching the installed MAS catalog - Required for reproducible deployments - Must match supported versions (currently 5.1.3, 5.2.0) Valid values : 5.1.3 , 5.2.0 (or other supported versions) Impact : - 5.1.3 : Installs Zen/Control Plane 6.1.1 - 5.2.0 : Installs Zen/Control Plane 6.2.0 Different versions have different features and compatibility requirements. Related variables : - Determines compatible service versions - Affects cpd_scale_config options Note : If not set and MAS catalog is not found, defaults to CP4D version supported by latest MAS catalog. For version-specific details, see CP4D Operator and operand versions .","title":"cpd_product_version"},{"location":"roles/cp4d/#ibm_entitlement_key","text":"IBM entitlement key for accessing container images. Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Provides authentication to IBM Container Registry for pulling CP4D container images. This key grants access to entitled software. When to use : - Always required for CP4D installation - Obtain from IBM Container Library - Must have valid entitlement for CP4D Valid values : Valid IBM entitlement key string from your IBM account Impact : Without a valid key, image pulls will fail and CP4D installation cannot proceed. Key must have CP4D entitlement. Related variables : - cpd_entitlement_key : CP4D-specific override (primarily for development) Note : Keep this key secure and do not commit to source control. The key is tied to your IBM account and entitlements. Can be overridden by cpd_entitlement_key for CP4D-specific scenarios.","title":"ibm_entitlement_key"},{"location":"roles/cp4d/#cpd_entitlement_key","text":"CP4D-specific entitlement key override (primarily for development). Optional Environment Variable: CPD_ENTITLEMENT_KEY Default: None (uses ibm_entitlement_key ) Purpose : Provides a CP4D-specific entitlement key that overrides the general ibm_entitlement_key . Primarily used in development scenarios. When to use : - Leave unset for standard deployments (uses ibm_entitlement_key ) - Set when you need a different key specifically for CP4D - Useful in development/testing with separate entitlements Valid values : Valid IBM entitlement key string with CP4D entitlement Impact : When set, this key is used instead of ibm_entitlement_key for CP4D image pulls. If not set, falls back to ibm_entitlement_key . Related variables : - ibm_entitlement_key : General entitlement key (used if this is not set) Note : Most deployments should use ibm_entitlement_key only. This override is primarily for development scenarios where different keys are needed for different components.","title":"cpd_entitlement_key"},{"location":"roles/cp4d/#cpd_primary_storage_class","text":"Primary storage class for CP4D (must support ReadWriteMany). Required (if known storage classes not available) Environment Variable: CPD_PRIMARY_STORAGE_CLASS Default: ibmc-file-gold-gid , ocs-storagecluster-cephfs , or azurefiles-premium (if available) Purpose : Specifies the storage class for CP4D primary storage, which requires ReadWriteMany (RWX) access mode for file storage. When to use : - Leave unset for automatic detection if known storage classes exist - Set explicitly when using custom or non-standard storage classes - Must support RWX access mode for shared file storage Valid values : Storage class name supporting ReadWriteMany access mode Impact : CP4D uses this for shared file storage across pods. Incorrect storage class or one not supporting RWX will cause deployment to fail. Related variables : - cpd_metadata_storage_class : Separate storage for metadata (RWO) Note : Known supported classes: ibmc-file-gold-gid (IBM Cloud), ocs-storagecluster-cephfs (OCS), azurefiles-premium (Azure). See CP4D Storage Considerations for details.","title":"cpd_primary_storage_class"},{"location":"roles/cp4d/#cpd_metadata_storage_class","text":"Storage class for CP4D Zen metadata database (must support ReadWriteOnce). Required (if known storage classes not available) Environment Variable: CPD_METADATA_STORAGE_CLASS Default: ibmc-block-gold , ocs-storagecluster-ceph-rbd , or managed-premium (if available) Purpose : Specifies the storage class for CP4D Zen metadata database, which requires ReadWriteOnce (RWO) access mode for block storage. When to use : - Leave unset for automatic detection if known storage classes exist - Set explicitly when using custom or non-standard storage classes - Must support RWO access mode for block storage Valid values : Storage class name supporting ReadWriteOnce access mode Impact : CP4D Zen metadata database uses this for persistent storage. Incorrect storage class or one not supporting RWO will cause deployment to fail. Related variables : - cpd_primary_storage_class : Separate storage for primary/file storage (RWX) Note : Known supported classes: ibmc-block-gold (IBM Cloud), ocs-storagecluster-ceph-rbd (OCS), managed-premium (Azure). Block storage typically provides better performance for databases.","title":"cpd_metadata_storage_class"},{"location":"roles/cp4d/#cpd_operators_namespace","text":"Namespace for CP4D operators installation. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default: ibm-cpd-operators Purpose : Specifies the namespace where CP4D operators will be installed. This follows the specialized installation model with separate operator and instance namespaces. When to use : - Use default ( ibm-cpd-operators ) for standard deployments - Set custom namespace for specific organizational requirements - Must be different from cpd_instance_namespace Valid values : Valid Kubernetes namespace name Impact : All CP4D operators (platform, zen, common services, etc.) are installed in this namespace. The namespace must not conflict with the instance namespace. Related variables : - cpd_instance_namespace : Separate namespace for CP4D instance workloads Note : CP4D uses a specialized installation model with operators in one namespace ( ibm-cpd-operators ) and instance workloads in another ( ibm-cpd ). This provides better isolation and management.","title":"cpd_operators_namespace"},{"location":"roles/cp4d/#cpd_instance_namespace","text":"Namespace for CP4D instance workloads. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default: ibm-cpd Purpose : Specifies the namespace where CP4D instance workloads (Zen, services, databases) will be deployed. Operators in cpd_operators_namespace watch and manage resources in this namespace. When to use : - Use default ( ibm-cpd ) for standard deployments - Set custom namespace for specific organizational requirements - Must be different from cpd_operators_namespace Valid values : Valid Kubernetes namespace name Impact : All CP4D instance workloads (Zen, MinIO, PostgreSQL, services) are deployed in this namespace. Operators watch this namespace for custom resources. Related variables : - cpd_operators_namespace : Separate namespace for CP4D operators Note : CP4D uses a specialized installation model with operators in ibm-cpd-operators and instance workloads in ibm-cpd . This separation provides better isolation and follows CP4D best practices.","title":"cpd_instance_namespace"},{"location":"roles/cp4d/#cpd_scale_config","text":"Resource scaling configuration for CP4D instance. Optional Environment Variable: CPD_SCALE_CONFIG Default: medium Purpose : Adjusts resource allocation (CPU, memory, replicas) for CP4D components to match workload requirements and increase processing capacity. When to use : - Use medium (default) for standard production deployments - Use small for development/test environments - Use large for high-capacity production environments - Adjust based on expected workload and performance requirements Valid values : small , medium , large (or other supported scale configurations) Impact : Determines resource requests/limits and replica counts for CP4D components. Larger scales require more cluster resources but provide better performance and capacity. Related variables : - cpd_product_version : Available scale options may vary by version Note : See Managing resources for detailed resource requirements per scale configuration. Ensure your cluster has sufficient resources for the selected scale.","title":"cpd_scale_config"},{"location":"roles/cp4d/#cpd_admin_username","text":"CP4D admin username for API authentication. Optional Environment Variable: CPD_ADMIN_USERNAME Default: cpadmin Purpose : Specifies the CP4D admin username for authenticating with CP4D APIs. Used when the role needs to interact with CP4D services. When to use : - Leave as default ( cpadmin ) if you haven't changed the initial admin username - Set explicitly if you changed the admin username after CP4D installation - Required for API operations that need admin authentication Valid values : Valid CP4D admin username string Impact : Used for CP4D API authentication. Incorrect username will cause API operations to fail. Related variables : - cpd_admin_password : Password for this admin user Note : The default cpadmin is the standard CP4D admin username. Only change if you've customized the admin username after installation.","title":"cpd_admin_username"},{"location":"roles/cp4d/#cpd_admin_password","text":"CP4D admin password for API authentication. Optional Environment Variable: CPD_ADMIN_PASSWORD Default: Retrieved from admin-user-details secret in cpd_instance_namespace Purpose : Specifies the CP4D admin password for authenticating with CP4D APIs. Used when the role needs to interact with CP4D services. When to use : - Leave unset (recommended) to auto-retrieve from cluster secret - Set explicitly if you changed the admin password after CP4D installation - Required for API operations that need admin authentication Valid values : Valid CP4D admin password string Impact : Used for CP4D API authentication. If not set, the role retrieves the initial admin password from the cluster. Incorrect password will cause API operations to fail. Related variables : - cpd_admin_username : Username for this admin password - cpd_instance_namespace : Namespace containing the password secret Note : The role automatically retrieves the initial admin password from the admin-user-details secret if not provided. Only set this if you've changed the password after installation. Keep passwords secure and do not commit to source control.","title":"cpd_admin_password"},{"location":"roles/cp4d/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/cp4d/#install-cloud-pak-for-data-513","text":"- hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.1.3 cpd_primary_storage_class: ibmc-file-gold-gid cpd_metadata_storage_class: ibmc-block-gold roles: - ibm.mas_devops.cp4d","title":"Install Cloud Pak for Data 5.1.3"},{"location":"roles/cp4d/#install-cloud-pak-for-data-520","text":"- hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.2.0 cpd_primary_storage_class: ibmc-file-gold-gid cpd_metadata_storage_class: ibmc-block-gold roles: - ibm.mas_devops.cp4d","title":"Install Cloud Pak for Data 5.2.0"},{"location":"roles/cp4d/#run-role-playbook","text":"export CPD_PRODUCT_VERSION=5.2.0 export CPD_PRIMARY_STORAGE_CLASS=ibmc-file-gold-gid export CPD_METADATA_STORAGE_CLASS=ibmc-block-gold export IBM_ENTITLEMENT_KEY=xxxxx ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/cp4d/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d_admin_pwd_update/","text":"cp4d_admin_pwd_update \u00a4 This role will update the password on an existing cp4d instance. By default it will update the password to a randomly generated new password only when the instance is still using the 'initial_admin_password' although using the 'cp4d_admin_password_force_update' variable referenced below will override this to update the password regardless of the current one being used. The new password will be added to the same yaml file that the 'initial_admin_password' was generated into - 'admin-user-details' by default. Role Variables \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the cp4d password updater will target. Environment Variable: MAS_INSTANCE_ID Default Value: None cp4d_namespace \u00a4 The instance of cp4d in your cluster that the cp4d password updater will target - defaults to 'ibm-cpd'. Environment Variable: CP4D_NAMESPACE Default Value: 'ibm-cpd' cp4d_admin_credentials_secret_name \u00a4 The secret inside your cp4d instance that stores the intial admin password - defaults to 'admin-user-details'. Environment Variable: CP4D_ADMIN_CREDENTIALS_SECRET_NAME Default Value: 'admin-user-details' cp4d_admin_username \u00a4 The username for your cp4d instance - defaults to 'admin'. Environment Variable: CP4D_ADMIN_USERNAME Default Value: 'admin' cp4d_admin_password \u00a4 The password for your cp4d insrance - an optional addition as the password updater will attempt to collect this value from the 'cp4d_admin_credentials_secret_name' secret. Optional Environment Variable: CP4D_ADMIN_PASSWORD Default Value: None cp4d_admin_password_force_update \u00a4 Typically the password updater will only update the password if the cp4d instance is using the initial password provided in the secret - setting this value to 'True' will ensure that is resets the password regardless. Environment Variable: CP4D_ADMIN_PASSWORD_FORCE_UPDATE Default Value: 'False' Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" cp4d_namespace: ibm-cpd cp4d_admin_credentials_secret_name: admin-user-details cp4d_admin_username: admin cp4d_admin_password: password123 cp4d_admin_password_force_update: True roles: - ibm.mas_devops.cp4d_admin_pwd_update","title":"cp4d_admin_pwd_update"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_admin_pwd_update","text":"This role will update the password on an existing cp4d instance. By default it will update the password to a randomly generated new password only when the instance is still using the 'initial_admin_password' although using the 'cp4d_admin_password_force_update' variable referenced below will override this to update the password regardless of the current one being used. The new password will be added to the same yaml file that the 'initial_admin_password' was generated into - 'admin-user-details' by default.","title":"cp4d_admin_pwd_update"},{"location":"roles/cp4d_admin_pwd_update/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d_admin_pwd_update/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the cp4d password updater will target. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_namespace","text":"The instance of cp4d in your cluster that the cp4d password updater will target - defaults to 'ibm-cpd'. Environment Variable: CP4D_NAMESPACE Default Value: 'ibm-cpd'","title":"cp4d_namespace"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_admin_credentials_secret_name","text":"The secret inside your cp4d instance that stores the intial admin password - defaults to 'admin-user-details'. Environment Variable: CP4D_ADMIN_CREDENTIALS_SECRET_NAME Default Value: 'admin-user-details'","title":"cp4d_admin_credentials_secret_name"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_admin_username","text":"The username for your cp4d instance - defaults to 'admin'. Environment Variable: CP4D_ADMIN_USERNAME Default Value: 'admin'","title":"cp4d_admin_username"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_admin_password","text":"The password for your cp4d insrance - an optional addition as the password updater will attempt to collect this value from the 'cp4d_admin_credentials_secret_name' secret. Optional Environment Variable: CP4D_ADMIN_PASSWORD Default Value: None","title":"cp4d_admin_password"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_admin_password_force_update","text":"Typically the password updater will only update the password if the cp4d instance is using the initial password provided in the secret - setting this value to 'True' will ensure that is resets the password regardless. Environment Variable: CP4D_ADMIN_PASSWORD_FORCE_UPDATE Default Value: 'False'","title":"cp4d_admin_password_force_update"},{"location":"roles/cp4d_admin_pwd_update/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" cp4d_namespace: ibm-cpd cp4d_admin_credentials_secret_name: admin-user-details cp4d_admin_username: admin cp4d_admin_password: password123 cp4d_admin_password_force_update: True roles: - ibm.mas_devops.cp4d_admin_pwd_update","title":"Example Playbook"},{"location":"roles/cp4d_service/","text":"cp4d_service \u00a4 Install or upgrade a chosen CloudPak for Data service. Currently supported Cloud Pak for Data release versions supported are: 5.1.3 5.2.0 The role will automatically install the corresponding CPD service operator channel and custom resource version associated to the chosen Cloud Pak for Data release version. For more information about the specific CPD services channels and versions associated to a particular Cloud Pak for Data release can be found here . Services Supported \u00a4 These services can be deployed and configured using this role: Watson Studio required by Predict Watson Machine Learning required by Predict Analytics Services (Apache Spark) required by Predict Cognos Analytics optional dependency for Manage application Upgrade \u00a4 This role also supports seamlessly CPD services minor version upgrades, as well as patch version upgrades. All you need to do is to define cpd_product_version variable to the version you target to upgrade and run this role for a particular CPD service. It's important that before you upgrade CPD services, the CPD Control Plane/Zen is also upgraded to the same release version. For more information about IBM Cloud Pak for Data upgrade process, refer to the CPD official documentation . Application Support For more information on how Predict and HP Utilities make use of Watson Studio, refer to Predict/HP Utilities documentation Warning The reconcile of many CP4D resources will be marked as Failed multiple times during initial installation, these are misleading status updates , the install is just really slow and the operators can not properly handle this. For example, if you are watching the install of CCS you will see that each rabbitmq-ha pod takes 10-15 minutes to start up and it looks like there is a problem because the pod log will just stop at a certain point. If you see something like this as the last message in the pod log WAL: ra_log_wal init, open tbls: ra_log_open_mem_tables, closed tbls: ra_log_closed_mem_tables be assured that there's nothing wrong, it's just there's a long delay between that message and the next ( starting system coordination ) being logged. Watson Studio \u00a4 Subscriptions related to Watson Studio: cpd-platform-operator ibm-cpd-wsl ibm-cpd-ccs ibm-cpd-datarefinery ibm-cpd-ws-runtimes Search Engine Dependency CPD 5.1.3 : Uses Elasticsearch operator ( ibm-elasticsearch-operator ) CPD 5.2.0 : Uses OpenSearch operator ( ibm-opensearch-operator ) Watson Studio is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: For CPD 5.1.3: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 83m ibm-cpd-datarefinery-operator 1/1 1 1 83m ibm-cpd-ws-operator 1/1 1 1 83m ibm-cpd-ws-runtimes-operator 1/1 1 1 83m ibm-elasticsearch-operator-ibm-es-controller-manager 1/1 1 1 83m For CPD 5.2.0: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 83m ibm-cpd-datarefinery-operator 1/1 1 1 83m ibm-cpd-ws-operator 1/1 1 1 83m ibm-cpd-ws-runtimes-operator 1/1 1 1 83m ibm-opensearch-operator-controller-manager 1/1 1 1 83m In the ibm-cpd namespace: oc -n ibm-cpd get ccs,ws,datarefinery,notebookruntimes,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 9.0.0 9.0.0 Completed 82m NAME VERSION RECONCILED STATUS AGE ws.ws.cpd.ibm.com/ws-cr 9.0.0 9.0.0 Completed 83m NAME VERSION RECONCILED STATUS AGE datarefinery.datarefinery.cpd.ibm.com/datarefinery-cr 9.0.0 9.0.0 Completed 36m NAME NLP MODELS VERSION RECONCILED STATUS AGE notebookruntime.ws.cpd.ibm.com/ibm-cpd-ws-runtime-241-py 9.0.0 9.0.0 Completed 22m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/asset-files-api 1/1 1 1 60m deployment.apps/ax-cdsx-jupyter-notebooks-converter-deploy 1/1 1 1 15m deployment.apps/ax-cdsx-notebooks-job-manager-deploy 1/1 1 1 15m deployment.apps/ax-environments-api-deploy 1/1 1 1 53m deployment.apps/ax-environments-ui-deploy 1/1 1 1 53m deployment.apps/ax-wdp-notebooks-api-deploy 1/1 1 1 15m deployment.apps/ax-ws-notebooks-ui-deploy 1/1 1 1 15m deployment.apps/catalog-api 2/2 2 2 69m deployment.apps/dataview-api-service 1/1 1 1 48m deployment.apps/dc-main 1/1 1 1 65m deployment.apps/event-logger-api 1/1 1 1 59m deployment.apps/ibm-0100-model-viewer-prod 1/1 1 1 14m deployment.apps/jobs-api 1/1 1 1 49m deployment.apps/jobs-ui 1/1 1 1 49m deployment.apps/ngp-projects-api 1/1 1 1 60m deployment.apps/portal-catalog 1/1 1 1 65m deployment.apps/portal-common-api 1/1 1 1 60m deployment.apps/portal-job-manager 1/1 1 1 60m deployment.apps/portal-main 1/1 1 1 60m deployment.apps/portal-ml-dl 1/1 1 1 14m deployment.apps/portal-notifications 1/1 1 1 60m deployment.apps/portal-projects 1/1 1 1 60m deployment.apps/redis-ha-haproxy 1/1 1 1 78m deployment.apps/runtime-assemblies-operator 1/1 1 1 59m deployment.apps/runtime-manager-api 1/1 1 1 59m deployment.apps/spaces 1/1 1 1 48m deployment.apps/task-credentials 1/1 1 1 48m deployment.apps/wdp-connect-connection 1/1 1 1 66m deployment.apps/wdp-connect-connector 1/1 1 1 66m deployment.apps/wdp-connect-flight 1/1 1 1 66m deployment.apps/wdp-dataprep 1/1 1 1 29m deployment.apps/wdp-dataview 1/1 1 1 48m deployment.apps/wdp-shaper 1/1 1 1 29m deployment.apps/wkc-search 1/1 1 1 66m deployment.apps/wml-main 1/1 1 1 48m NAME READY AGE statefulset.apps/elasticsea-0ac3-ib-6fb9-es-server-esnodes 3/3 74m statefulset.apps/rabbitmq-ha 3/3 79m statefulset.apps/redis-ha-server 3/3 79m statefulset.apps/wdp-couchdb 3/3 79m Watson Machine Learning \u00a4 Subscriptions related to Watson Machine Learning: cpd-platform-operator ibm-cpd-wml ibm-cpd-ccs Search Engine Dependency CPD 5.1.3 : Uses Elasticsearch operator ( ibm-elasticsearch-operator ) CPD 5.2.0 : Uses OpenSearch operator ( ibm-opensearch-operator ) Watson Machine Learning is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: For CPD 5.1.3: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 134m ibm-cpd-datarefinery-operator 1/1 1 1 134m ibm-cpd-wml-operator 1/1 1 1 49m ibm-elasticsearch-operator-ibm-es-controller-manager 1/1 1 1 134m For CPD 5.2.0: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 134m ibm-cpd-datarefinery-operator 1/1 1 1 134m ibm-cpd-wml-operator 1/1 1 1 49m ibm-opensearch-operator-controller-manager 1/1 1 1 134m In the ibm-cpd namespace: oc -n ibm-cpd get ccs,wmlbase,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 9.0.0 9.0.0 Completed 133m NAME VERSION BUILD STATUS RECONCILED AGE wmlbase.wml.cpd.ibm.com/wml-cr 5.0.0 5.0.0-918 Completed 5.0.0 50m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/wml-deployment-envoy 1/1 1 1 23m deployment.apps/wml-deployment-manager 1/1 1 1 19m deployment.apps/wml-main 1/1 1 1 99m deployment.apps/wml-repositoryv4 1/1 1 1 16m deployment.apps/wmltraining 1/1 1 1 15m deployment.apps/wmltrainingorchestrator 1/1 1 1 14m NAME READY AGE statefulset.apps/wml-cpd-etcd 3/3 26m statefulset.apps/wml-deployment-agent 1/1 21m Analytics Engine \u00a4 Subscriptions related to Analytics Engine: cpd-platform-operator analyticsengine-operator Analytics Engine is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ae-operator 1/1 1 1 31m In the ibm-cpd namespace: oc -n ibm-cpd get analyticsengine,deployments NAME VERSION RECONCILED STATUS AGE analyticsengine.ae.cpd.ibm.com/analyticsengine-sample 5.0.0 5.0.0 Completed 31m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/spark-hb-br-recovery 1/1 1 1 11m deployment.apps/spark-hb-control-plane 1/1 1 1 19m deployment.apps/spark-hb-create-trust-store 1/1 1 1 25m deployment.apps/spark-hb-deployer-agent 1/1 1 1 19m deployment.apps/spark-hb-nginx 1/1 1 1 19m deployment.apps/spark-hb-register-hb-dataplane 1/1 1 1 10m deployment.apps/spark-hb-ui 1/1 1 1 19m Cognos Analytics \u00a4 Subscriptions related to Cognos Analytics (in the ibm-cpd-operators namespace): cpd-platform-operator ibm-ca-operator-controller-manager Cognos Analytics is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-ca-operator-controller-manager 1/1 1 1 19m In the ibm-cpd namespace: oc -n ibm-cpd get caservice,deployments NAME AGE caservice.ca.cpd.ibm.com/ca-addon-cr 19m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cognos-analytics-cognos-analytics-addon 1/1 1 1 9m17s Role Variables - Installation \u00a4 cpd_service_name \u00a4 Name of the service to install, supported values are: wsl , wml , spark , and ca Required Environment Variable: CPD_SERVICE_NAME Default Value: None cpd_product_version \u00a4 The product version (also known as operand version) of this service to install. Required Environment Variable: CPD_PRODUCT_VERSION Default Value: Defined by the installed MAS catalog version cpd_service_storage_class \u00a4 This is used to set spec.storageClass in all CPD services that uses file storage class (read-write-many RWX). Required , unless IBMCloud storage classes are available. Environment Variable: CPD_SERVICE_STORAGE_CLASS Default Value: Auto determined if default storage classes are provided and available by your cloud provider. i.e ibmc-file for IBM Cloud, efs for AWS. cpd_service_block_storage_class \u00a4 This is used to set spec.blockStorageClass in all CPD services that uses block storage class (read-write-only RWO). Required , unless IBMCloud storage classes are available. Environment Variable: CPD_SERVICE_BLOCK_STORAGE_CLASS Default Value: Auto determined if default storage classes are provided and available by your cloud provider. i.e ibmc-block for IBM Cloud, gp2 for AWS. cpd_instance_namespace \u00a4 Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default Value: ibm-cpd cpd_operator_namespace \u00a4 Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default Value: ibm-cpd-operators cpd_admin_username \u00a4 The CP4D Admin username to authenticate with CP4D APIs. If you didn't change the initial admin username after installing CP4D then you don't need to provide this. Optional Environment Variable: CPD_ADMIN_USERNAME Default Value: admin (CPD 4.6) cpadmin (CPD 4.8 and newer) cpd_admin_password \u00a4 The CP4D Admin User password to call CP4D API to provision Discovery Instance. If you didn't change the initial admin password after CP4D install, you don't need to provide it. The initial admin user password for admin or cpdamin will be used. Optional Environment Variable: CPD_ADMIN_PASSWORD Default Value: CPD 4.6: Looked up from the admin-user-details secret in the cpd_instance_namespace namespace CPD 4.8 and newer: Looked up from the ibm-iam-bindinfo-platform-auth-idp-credentials secret in the cpd_instance_namespace namespace cpd_service_scale_config \u00a4 Adjust and scale the resources for your Cloud Pak for Data services to increase processing capacity. For more information, refer to Managing resources in IBM Cloud Pak for Data documentation. Optional Environment Variable: CPD_SERVICE_SCALE_CONFIG Default Value: small Role Variables - Watson Studio \u00a4 cpd_wsl_project_name \u00a4 Stores the CP4D Watson Studio Project name that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_NAME Default Value: wsl-mas-${mas_instance_id}-hputilities cpd_wsl_project_description \u00a4 Optional - Stores the CP4D Watson Studio Project description that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_DESCRIPTION Default Value: Watson Studio Project for Maximo Application Suite Role Variables - MAS Configuration Generation \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that a generated configuration will target. If this or mas_config_dir are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated resource definition. This can be used to manually configure a MAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbook \u00a4 Install Watson Studio on CPD 5.1.3 \u00a4 --- - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.1.3 cpd_service_storage_class: ibmc-file-gold-gid cpd_service_name: wsl roles: - ibm.mas_devops.cp4d_service Install Watson Studio on CPD 5.2.0 \u00a4 --- - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.2.0 cpd_service_storage_class: ibmc-file-gold-gid cpd_service_name: wsl roles: - ibm.mas_devops.cp4d_service License \u00a4 EPL-2.0","title":"cp4d_service"},{"location":"roles/cp4d_service/#cp4d_service","text":"Install or upgrade a chosen CloudPak for Data service. Currently supported Cloud Pak for Data release versions supported are: 5.1.3 5.2.0 The role will automatically install the corresponding CPD service operator channel and custom resource version associated to the chosen Cloud Pak for Data release version. For more information about the specific CPD services channels and versions associated to a particular Cloud Pak for Data release can be found here .","title":"cp4d_service"},{"location":"roles/cp4d_service/#services-supported","text":"These services can be deployed and configured using this role: Watson Studio required by Predict Watson Machine Learning required by Predict Analytics Services (Apache Spark) required by Predict Cognos Analytics optional dependency for Manage application","title":"Services Supported"},{"location":"roles/cp4d_service/#upgrade","text":"This role also supports seamlessly CPD services minor version upgrades, as well as patch version upgrades. All you need to do is to define cpd_product_version variable to the version you target to upgrade and run this role for a particular CPD service. It's important that before you upgrade CPD services, the CPD Control Plane/Zen is also upgraded to the same release version. For more information about IBM Cloud Pak for Data upgrade process, refer to the CPD official documentation . Application Support For more information on how Predict and HP Utilities make use of Watson Studio, refer to Predict/HP Utilities documentation Warning The reconcile of many CP4D resources will be marked as Failed multiple times during initial installation, these are misleading status updates , the install is just really slow and the operators can not properly handle this. For example, if you are watching the install of CCS you will see that each rabbitmq-ha pod takes 10-15 minutes to start up and it looks like there is a problem because the pod log will just stop at a certain point. If you see something like this as the last message in the pod log WAL: ra_log_wal init, open tbls: ra_log_open_mem_tables, closed tbls: ra_log_closed_mem_tables be assured that there's nothing wrong, it's just there's a long delay between that message and the next ( starting system coordination ) being logged.","title":"Upgrade"},{"location":"roles/cp4d_service/#watson-studio","text":"Subscriptions related to Watson Studio: cpd-platform-operator ibm-cpd-wsl ibm-cpd-ccs ibm-cpd-datarefinery ibm-cpd-ws-runtimes Search Engine Dependency CPD 5.1.3 : Uses Elasticsearch operator ( ibm-elasticsearch-operator ) CPD 5.2.0 : Uses OpenSearch operator ( ibm-opensearch-operator ) Watson Studio is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: For CPD 5.1.3: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 83m ibm-cpd-datarefinery-operator 1/1 1 1 83m ibm-cpd-ws-operator 1/1 1 1 83m ibm-cpd-ws-runtimes-operator 1/1 1 1 83m ibm-elasticsearch-operator-ibm-es-controller-manager 1/1 1 1 83m For CPD 5.2.0: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 83m ibm-cpd-datarefinery-operator 1/1 1 1 83m ibm-cpd-ws-operator 1/1 1 1 83m ibm-cpd-ws-runtimes-operator 1/1 1 1 83m ibm-opensearch-operator-controller-manager 1/1 1 1 83m In the ibm-cpd namespace: oc -n ibm-cpd get ccs,ws,datarefinery,notebookruntimes,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 9.0.0 9.0.0 Completed 82m NAME VERSION RECONCILED STATUS AGE ws.ws.cpd.ibm.com/ws-cr 9.0.0 9.0.0 Completed 83m NAME VERSION RECONCILED STATUS AGE datarefinery.datarefinery.cpd.ibm.com/datarefinery-cr 9.0.0 9.0.0 Completed 36m NAME NLP MODELS VERSION RECONCILED STATUS AGE notebookruntime.ws.cpd.ibm.com/ibm-cpd-ws-runtime-241-py 9.0.0 9.0.0 Completed 22m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/asset-files-api 1/1 1 1 60m deployment.apps/ax-cdsx-jupyter-notebooks-converter-deploy 1/1 1 1 15m deployment.apps/ax-cdsx-notebooks-job-manager-deploy 1/1 1 1 15m deployment.apps/ax-environments-api-deploy 1/1 1 1 53m deployment.apps/ax-environments-ui-deploy 1/1 1 1 53m deployment.apps/ax-wdp-notebooks-api-deploy 1/1 1 1 15m deployment.apps/ax-ws-notebooks-ui-deploy 1/1 1 1 15m deployment.apps/catalog-api 2/2 2 2 69m deployment.apps/dataview-api-service 1/1 1 1 48m deployment.apps/dc-main 1/1 1 1 65m deployment.apps/event-logger-api 1/1 1 1 59m deployment.apps/ibm-0100-model-viewer-prod 1/1 1 1 14m deployment.apps/jobs-api 1/1 1 1 49m deployment.apps/jobs-ui 1/1 1 1 49m deployment.apps/ngp-projects-api 1/1 1 1 60m deployment.apps/portal-catalog 1/1 1 1 65m deployment.apps/portal-common-api 1/1 1 1 60m deployment.apps/portal-job-manager 1/1 1 1 60m deployment.apps/portal-main 1/1 1 1 60m deployment.apps/portal-ml-dl 1/1 1 1 14m deployment.apps/portal-notifications 1/1 1 1 60m deployment.apps/portal-projects 1/1 1 1 60m deployment.apps/redis-ha-haproxy 1/1 1 1 78m deployment.apps/runtime-assemblies-operator 1/1 1 1 59m deployment.apps/runtime-manager-api 1/1 1 1 59m deployment.apps/spaces 1/1 1 1 48m deployment.apps/task-credentials 1/1 1 1 48m deployment.apps/wdp-connect-connection 1/1 1 1 66m deployment.apps/wdp-connect-connector 1/1 1 1 66m deployment.apps/wdp-connect-flight 1/1 1 1 66m deployment.apps/wdp-dataprep 1/1 1 1 29m deployment.apps/wdp-dataview 1/1 1 1 48m deployment.apps/wdp-shaper 1/1 1 1 29m deployment.apps/wkc-search 1/1 1 1 66m deployment.apps/wml-main 1/1 1 1 48m NAME READY AGE statefulset.apps/elasticsea-0ac3-ib-6fb9-es-server-esnodes 3/3 74m statefulset.apps/rabbitmq-ha 3/3 79m statefulset.apps/redis-ha-server 3/3 79m statefulset.apps/wdp-couchdb 3/3 79m","title":"Watson Studio"},{"location":"roles/cp4d_service/#watson-machine-learning","text":"Subscriptions related to Watson Machine Learning: cpd-platform-operator ibm-cpd-wml ibm-cpd-ccs Search Engine Dependency CPD 5.1.3 : Uses Elasticsearch operator ( ibm-elasticsearch-operator ) CPD 5.2.0 : Uses OpenSearch operator ( ibm-opensearch-operator ) Watson Machine Learning is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: For CPD 5.1.3: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 134m ibm-cpd-datarefinery-operator 1/1 1 1 134m ibm-cpd-wml-operator 1/1 1 1 49m ibm-elasticsearch-operator-ibm-es-controller-manager 1/1 1 1 134m For CPD 5.2.0: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 134m ibm-cpd-datarefinery-operator 1/1 1 1 134m ibm-cpd-wml-operator 1/1 1 1 49m ibm-opensearch-operator-controller-manager 1/1 1 1 134m In the ibm-cpd namespace: oc -n ibm-cpd get ccs,wmlbase,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 9.0.0 9.0.0 Completed 133m NAME VERSION BUILD STATUS RECONCILED AGE wmlbase.wml.cpd.ibm.com/wml-cr 5.0.0 5.0.0-918 Completed 5.0.0 50m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/wml-deployment-envoy 1/1 1 1 23m deployment.apps/wml-deployment-manager 1/1 1 1 19m deployment.apps/wml-main 1/1 1 1 99m deployment.apps/wml-repositoryv4 1/1 1 1 16m deployment.apps/wmltraining 1/1 1 1 15m deployment.apps/wmltrainingorchestrator 1/1 1 1 14m NAME READY AGE statefulset.apps/wml-cpd-etcd 3/3 26m statefulset.apps/wml-deployment-agent 1/1 21m","title":"Watson Machine Learning"},{"location":"roles/cp4d_service/#analytics-engine","text":"Subscriptions related to Analytics Engine: cpd-platform-operator analyticsengine-operator Analytics Engine is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ae-operator 1/1 1 1 31m In the ibm-cpd namespace: oc -n ibm-cpd get analyticsengine,deployments NAME VERSION RECONCILED STATUS AGE analyticsengine.ae.cpd.ibm.com/analyticsengine-sample 5.0.0 5.0.0 Completed 31m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/spark-hb-br-recovery 1/1 1 1 11m deployment.apps/spark-hb-control-plane 1/1 1 1 19m deployment.apps/spark-hb-create-trust-store 1/1 1 1 25m deployment.apps/spark-hb-deployer-agent 1/1 1 1 19m deployment.apps/spark-hb-nginx 1/1 1 1 19m deployment.apps/spark-hb-register-hb-dataplane 1/1 1 1 10m deployment.apps/spark-hb-ui 1/1 1 1 19m","title":"Analytics Engine"},{"location":"roles/cp4d_service/#cognos-analytics","text":"Subscriptions related to Cognos Analytics (in the ibm-cpd-operators namespace): cpd-platform-operator ibm-ca-operator-controller-manager Cognos Analytics is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-ca-operator-controller-manager 1/1 1 1 19m In the ibm-cpd namespace: oc -n ibm-cpd get caservice,deployments NAME AGE caservice.ca.cpd.ibm.com/ca-addon-cr 19m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cognos-analytics-cognos-analytics-addon 1/1 1 1 9m17s","title":"Cognos Analytics"},{"location":"roles/cp4d_service/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/cp4d_service/#cpd_service_name","text":"Name of the service to install, supported values are: wsl , wml , spark , and ca Required Environment Variable: CPD_SERVICE_NAME Default Value: None","title":"cpd_service_name"},{"location":"roles/cp4d_service/#cpd_product_version","text":"The product version (also known as operand version) of this service to install. Required Environment Variable: CPD_PRODUCT_VERSION Default Value: Defined by the installed MAS catalog version","title":"cpd_product_version"},{"location":"roles/cp4d_service/#cpd_service_storage_class","text":"This is used to set spec.storageClass in all CPD services that uses file storage class (read-write-many RWX). Required , unless IBMCloud storage classes are available. Environment Variable: CPD_SERVICE_STORAGE_CLASS Default Value: Auto determined if default storage classes are provided and available by your cloud provider. i.e ibmc-file for IBM Cloud, efs for AWS.","title":"cpd_service_storage_class"},{"location":"roles/cp4d_service/#cpd_service_block_storage_class","text":"This is used to set spec.blockStorageClass in all CPD services that uses block storage class (read-write-only RWO). Required , unless IBMCloud storage classes are available. Environment Variable: CPD_SERVICE_BLOCK_STORAGE_CLASS Default Value: Auto determined if default storage classes are provided and available by your cloud provider. i.e ibmc-block for IBM Cloud, gp2 for AWS.","title":"cpd_service_block_storage_class"},{"location":"roles/cp4d_service/#cpd_instance_namespace","text":"Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default Value: ibm-cpd","title":"cpd_instance_namespace"},{"location":"roles/cp4d_service/#cpd_operator_namespace","text":"Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default Value: ibm-cpd-operators","title":"cpd_operator_namespace"},{"location":"roles/cp4d_service/#cpd_admin_username","text":"The CP4D Admin username to authenticate with CP4D APIs. If you didn't change the initial admin username after installing CP4D then you don't need to provide this. Optional Environment Variable: CPD_ADMIN_USERNAME Default Value: admin (CPD 4.6) cpadmin (CPD 4.8 and newer)","title":"cpd_admin_username"},{"location":"roles/cp4d_service/#cpd_admin_password","text":"The CP4D Admin User password to call CP4D API to provision Discovery Instance. If you didn't change the initial admin password after CP4D install, you don't need to provide it. The initial admin user password for admin or cpdamin will be used. Optional Environment Variable: CPD_ADMIN_PASSWORD Default Value: CPD 4.6: Looked up from the admin-user-details secret in the cpd_instance_namespace namespace CPD 4.8 and newer: Looked up from the ibm-iam-bindinfo-platform-auth-idp-credentials secret in the cpd_instance_namespace namespace","title":"cpd_admin_password"},{"location":"roles/cp4d_service/#cpd_service_scale_config","text":"Adjust and scale the resources for your Cloud Pak for Data services to increase processing capacity. For more information, refer to Managing resources in IBM Cloud Pak for Data documentation. Optional Environment Variable: CPD_SERVICE_SCALE_CONFIG Default Value: small","title":"cpd_service_scale_config"},{"location":"roles/cp4d_service/#role-variables-watson-studio","text":"","title":"Role Variables - Watson Studio"},{"location":"roles/cp4d_service/#cpd_wsl_project_name","text":"Stores the CP4D Watson Studio Project name that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_NAME Default Value: wsl-mas-${mas_instance_id}-hputilities","title":"cpd_wsl_project_name"},{"location":"roles/cp4d_service/#cpd_wsl_project_description","text":"Optional - Stores the CP4D Watson Studio Project description that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_DESCRIPTION Default Value: Watson Studio Project for Maximo Application Suite","title":"cpd_wsl_project_description"},{"location":"roles/cp4d_service/#role-variables-mas-configuration-generation","text":"","title":"Role Variables - MAS Configuration Generation"},{"location":"roles/cp4d_service/#mas_instance_id","text":"The instance ID of Maximo Application Suite that a generated configuration will target. If this or mas_config_dir are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cp4d_service/#mas_config_dir","text":"Local directory to save the generated resource definition. This can be used to manually configure a MAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/cp4d_service/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/cp4d_service/#install-watson-studio-on-cpd-513","text":"--- - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.1.3 cpd_service_storage_class: ibmc-file-gold-gid cpd_service_name: wsl roles: - ibm.mas_devops.cp4d_service","title":"Install Watson Studio on CPD 5.1.3"},{"location":"roles/cp4d_service/#install-watson-studio-on-cpd-520","text":"--- - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.2.0 cpd_service_storage_class: ibmc-file-gold-gid cpd_service_name: wsl roles: - ibm.mas_devops.cp4d_service","title":"Install Watson Studio on CPD 5.2.0"},{"location":"roles/cp4d_service/#license","text":"EPL-2.0","title":"License"},{"location":"roles/db2/","text":"db2 \u00a4 This role creates or upgrades a Db2 instance using the Db2u Operator. When installing db2, the db2u operator will now be installed into the same namespace as the db2 instance ( db2ucluster ). If you already have db2 operator and db2 instances running in separate namespaces, this role will take care of migrating (by deleting & reinstalling) the db2 operators from ibm-common-services to the namespace defined by db2_namespace property (in case of a new role execution for a db2 install or db2 upgrade). A private root CA certificate is created and is used to secure the TLS connections to the database. A Db2 Warehouse cluster will be created along with a public TLS encrypted route to allow external access to the cluster (access is via the ssl-server nodeport port on the -db2u-engn-svc service). Internal access is via the -db2u-engn-svc service and port 50001. Both the external route and the internal service use the same server certificate. The private root CA certificate and the server certificate are available from the db2u-ca and db2u-certificate secrets in the db2 namespace. The default user is db2inst1 and the password is available in the instancepassword secret in the same namespace. You can examine the deployed resources in the db2 namespace. This example assumes the default namespace db2u : oc -n db2u get db2ucluster NAME STATE MAINTENANCESTATE AGE db2u-db01 Ready None 29m It typically takes 20-30 minutes from the db2ucluster being created till it is ready. If the db2ucluster is not ready after that period then check that all the PersistentVolumeClaims in the db2 namespace are ready and that the pods in the namespace are not stuck in init state. If the c-<db2_instance_name>-db2u-0 pod is running then you can exec into the pod and check the /var/log/db2u.log for any issue. If the mas_instance_id and mas_config_dir are provided then the role will generate the JdbcCfg yaml that can be used to configure MAS to connect to this database. It does not apply the yaml to the cluster but does provide you with the yaml files to apply if needed. When upgrading db2, specify the existing namespace where the db2uCluster instances exist. All the instances under that namespace will be upgraded to the db2 version specified. The version of db2 must match the channel of db2 being used for the upgrade. Role Variables \u00a4 Installation Variables \u00a4 common_services_namespace \u00a4 OpenShift namespace where IBM Common Services is installed. Optional Environment Variable: COMMON_SERVICES_NAMESPACE Default Value: ibm-common-services Purpose : Specifies the namespace containing IBM Common Services, which provides shared services used by Db2 operator. This is needed for the role to locate and interact with Common Services components. When to use : - Leave as default ( ibm-common-services ) for standard installations - Set only if Common Services is installed in a non-standard namespace - Required when Common Services namespace differs from default Valid values : Any valid Kubernetes namespace name where IBM Common Services is installed Impact : The role uses this to locate Common Services resources. Incorrect namespace will cause the role to fail when trying to access Common Services components. Related variables : Works with db2_namespace to manage operator and instance placement. Note : The default ibm-common-services is the standard namespace for Common Services. Only change if your installation uses a different namespace. db2_action \u00a4 Specifies which operation to perform on the Db2 database. Optional Environment Variable: DB2_ACTION Default: install Purpose : Controls what action the role executes against Db2 instances. This allows the same role to handle installation, upgrades, backups, and restores of Db2 databases. When to use : - Use install (default) for initial Db2 deployment - Use upgrade to upgrade all Db2 instances in the namespace to a new version - Use backup to create a backup of Db2 data - Use restore to restore Db2 from a backup Valid values : install , upgrade , backup , restore Impact : - install : Creates new Db2 operator and instance - upgrade : Upgrades ALL instances in db2_namespace to db2_version (affects all instances in namespace) - backup : Creates backup of Db2 data - restore : Restores Db2 from backup Related variables : - db2_version : Required for upgrade action to specify target version - db2_namespace : All instances in this namespace are affected by upgrade Note : WARNING - When using upgrade , ALL Db2 instances in the specified namespace will be upgraded. Plan accordingly and ensure db2_version matches the operator channel. db2_namespace \u00a4 OpenShift namespace where Db2 operator and instances will be deployed. Optional Environment Variable: DB2_NAMESPACE Default: db2u Purpose : Specifies the namespace for deploying the Db2u operator and Db2 instances (Db2uCluster custom resources). Starting with recent versions, the operator is installed in the same namespace as the instances. When to use : - Use default ( db2u ) for standard single-instance deployments - Set to a custom namespace when organizing multiple Db2 deployments - Must match existing namespace when upgrading Db2 instances Valid values : Any valid Kubernetes namespace name (e.g., db2u , db2-prod , mas-db2 ) Impact : All Db2 resources (operator, instances, secrets, services) are created in this namespace. When upgrading, ALL instances in this namespace will be upgraded together. Related variables : - db2_action : When set to upgrade , affects all instances in this namespace - db2_instance_name : Instance created within this namespace Note : The role handles migration of operators from ibm-common-services to this namespace if needed. All instances in the namespace are upgraded together when using db2_action=upgrade . db2_channel \u00a4 Subscription channel for the Db2 Universal Operator. Optional Environment Variable: DB2_CHANNEL Default: The default channel from the operator package (automatically selected) Purpose : Specifies which operator channel to subscribe to, determining which Db2 operator version stream you receive. Channels typically correspond to major Db2 versions (e.g., v2.2 , v3.0 ). When to use : - Leave unset to use the default channel (recommended for new installations) - Set explicitly when you need a specific operator version stream - Must match db2_version when upgrading (version must be available in the channel) Valid values : Valid Db2 operator channel names (e.g., v2.2 , v3.0 , v3.1 ) Impact : Determines which operator version is installed and which Db2 engine versions are available. Changing channels may require operator migration. Related variables : - db2_version : Must be compatible with the selected channel - When upgrading, version must match channel Note : The operator channel determines available Db2 engine versions. Check the operator package for available channels and their supported versions. db2_instance_name \u00a4 Unique name for the Db2 instance (Db2uCluster resource). Required Environment Variable: DB2_INSTANCE_NAME Default: None Purpose : Defines the name of the Db2uCluster custom resource that will be created. This name is used to identify the Db2 instance and is embedded in resource names (services, pods, secrets). When to use : - Always required for Db2 installation - Use descriptive names that indicate purpose (e.g., maximo-db , iot-db , manage-db ) - Must be unique within the namespace Valid values : Valid Kubernetes resource name (lowercase alphanumeric with hyphens, e.g., db01 , mas-db , manage-db ) Impact : This name is used throughout the Db2 deployment: - Db2uCluster resource name - Service names (e.g., {instance_name}-db2u-engn-svc ) - Pod names (e.g., c-{instance_name}-db2u-0 ) - Secret names Related variables : - db2_dbname : Name of the database within this instance - Used in generated JdbcCfg when mas_instance_id is provided Note : Choose a meaningful name as it appears in many resource names and cannot be easily changed after creation. ibm_entitlement_key \u00a4 IBM Container Library entitlement key for accessing Db2 container images. Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Provides authentication credentials to pull Db2 container images from the IBM Container Registry. This key is associated with your IBM entitlement and grants access to licensed software. When to use : - Always required for Db2 installation - Obtain from IBM Container Library - Same key used across all IBM MAS components Valid values : Valid IBM entitlement key string Impact : Without a valid key, Db2 container images cannot be pulled and installation will fail. The key is stored in an image pull secret in the Db2 namespace. Related variables : Used across multiple roles for accessing IBM container images. Note : Keep this key secure and do not commit to source control. The key is tied to your IBM entitlement and should be treated as a credential. Obtain from the IBM Container Library using your IBM ID. db2_dbname \u00a4 Name of the database to create within the Db2 instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB Purpose : Specifies the name of the database that will be created inside the Db2 instance. A Db2 instance can contain multiple databases, and this defines the primary database name. When to use : - Use default ( BLUDB ) for standard MAS deployments - Set to a custom name when organizational standards require specific naming - Must match the database name expected by applications connecting to Db2 Valid values : Valid Db2 database name (uppercase alphanumeric, up to 8 characters, e.g., BLUDB , MAXDB , MASDB ) Impact : This database name is used in connection strings and JDBC configurations. Applications must use this exact name to connect to the database. Related variables : - db2_instance_name : The instance containing this database - Used in generated JdbcCfg for MAS configuration Note : The default BLUDB is standard for Db2 Warehouse. Db2 database names are typically uppercase and limited to 8 characters. db2_version \u00a4 Db2 engine version to use for installation or upgrade. Optional Environment Variable: DB2_VERSION Default: Latest version supported by the installed Db2 operator (auto-detected from db2u-release configmap) Purpose : Specifies which Db2 engine version to deploy or upgrade to. The version must be compatible with the installed operator channel. When to use : - Leave unset for new installations to use the latest supported version (recommended) - Set explicitly when you need a specific Db2 version - Required when upgrading to specify the target version - Must match the operator channel capabilities Valid values : Valid Db2 engine version supported by the operator (e.g., 11.5.8.0 , 11.5.9.0 ) Impact : Determines which Db2 engine version is deployed. When upgrading, ALL instances in db2_namespace will be upgraded to this version. Version must be available in the operator's supported versions list. Related variables : - db2_channel : Version must be compatible with the operator channel - db2_action : When set to upgrade , this version is the target - db2_namespace : All instances in namespace upgraded to this version Note : Supported versions are listed in the db2u-release configmap in ibm-common-services namespace. Ensure the version matches your operator channel. When upgrading, all instances in the namespace are upgraded together. db2_type \u00a4 Db2 instance type optimized for specific workload patterns. Optional Environment Variable: DB2_TYPE Default: db2wh Purpose : Determines the Db2 instance configuration optimized for either data warehouse (analytical) or online transaction processing workloads. This affects resource allocation and database tuning. When to use : - Use db2wh (default) for MAS Manage and analytical workloads - Use db2oltp for high-transaction workloads requiring OLTP optimization - Choose based on your primary use case Valid values : db2wh , db2oltp Impact : - db2wh : Optimized for data warehouse/analytical queries (recommended for MAS Manage) - db2oltp : Optimized for online transaction processing with high concurrency Related variables : - db2_workload : Further refines workload optimization - db2_table_org : Table organization should align with instance type Note : MAS Manage typically uses db2wh for optimal performance with its analytical workload patterns. db2_timezone \u00a4 Server timezone for the Db2 instance. Optional Environment Variable: DB2_TIMEZONE Default: GMT Purpose : Sets the timezone used by the Db2 server for timestamp operations and scheduling. This affects how Db2 interprets and stores timestamp data. When to use : - Use default ( GMT ) for globally distributed systems - Set to local timezone when all users are in the same timezone - Must match MAS Manage timezone if using Manage with this database Valid values : Valid timezone codes (e.g., GMT , EST , PST , CET , UTC ) Impact : Affects timestamp interpretation and storage in the database. Mismatched timezones between Db2 and applications can cause data inconsistencies. Related variables : - MAS_APP_SETTINGS_SERVER_TIMEZONE : Must be set to the same value for MAS Manage - Affects all timestamp operations in the database Note : IMPORTANT - If using this Db2 instance with MAS Manage, you must also set MAS_APP_SETTINGS_SERVER_TIMEZONE to the same value to ensure consistent timestamp handling. db2_4k_device_support \u00a4 Controls 4K sector device support in Db2. Optional Environment Variable: DB2_4K_DEVICE_SUPPORT Default: ON Purpose : Enables or disables support for storage devices with 4K sector sizes. Modern storage systems often use 4K sectors instead of traditional 512-byte sectors. When to use : - Leave as ON (default) for modern storage systems with 4K sectors - Set to OFF only if using legacy storage with 512-byte sectors - Check your storage specifications if unsure Valid values : ON , OFF Impact : - ON : Enables 4K sector support (required for most modern storage) - OFF : Uses traditional 512-byte sector mode (legacy storage only) Related variables : Works with storage class configuration to ensure compatibility. Note : Most modern storage systems use 4K sectors. Keep this ON unless you have specific legacy storage requirements. db2_workload \u00a4 Workload profile for Db2 instance optimization. Optional Environment Variable: DB2_WORKLOAD Default: ANALYTICS Purpose : Configures Db2 with predefined settings optimized for specific workload patterns. This affects memory allocation, query optimization, and other performance parameters. When to use : - Use ANALYTICS (default) for general analytical and MAS workloads - Use PUREDATA_OLAP for pure data warehouse/OLAP workloads - Choose based on your primary query patterns Valid values : ANALYTICS , PUREDATA_OLAP Impact : - ANALYTICS : Balanced optimization for mixed analytical workloads (recommended for MAS) - PUREDATA_OLAP : Optimized specifically for OLAP/data warehouse queries Related variables : - db2_type : Should align with workload choice - db2_table_org : Table organization should match workload pattern Note : The default ANALYTICS is suitable for most MAS deployments. Only change if you have specific OLAP requirements. db2_table_org \u00a4 Default table organization for database tables. Optional Environment Variable: DB2_TABLE_ORG Default: ROW Purpose : Determines the default storage organization for tables in the database. Row-organized tables are optimized for transactional workloads, while column-organized tables are optimized for analytical queries. When to use : - Use ROW (default) for MAS Manage and mixed workloads - Use COLUMN only for pure analytical/reporting workloads - Choose based on your primary query patterns Valid values : ROW , COLUMN Impact : - ROW : Tables stored row-by-row (better for OLTP, updates, and mixed workloads) - COLUMN : Tables stored column-by-column (better for analytical queries and aggregations) Related variables : - db2_type : Should align with table organization choice - db2_workload : Workload profile should match table organization Note : MAS Manage requires ROW organization. Only use COLUMN for dedicated analytical databases. This sets the default; individual tables can override this setting. db2_ldap_username \u00a4 Username for Db2 LDAP authentication. Optional Environment Variable: DB2_LDAP_USERNAME Default: None (uses default db2inst1 user) Purpose : Defines a custom LDAP username for Db2 authentication instead of the default db2inst1 user. When set, this user is configured in Db2's local LDAP registry and used in MAS JDBC configuration. When to use : - Set when you need a custom database user for MAS connections - Set when organizational policies require specific usernames - Leave unset to use the default db2inst1 user - Must be set together with db2_ldap_password Valid values : Valid LDAP username string Impact : When set, this user is created in Db2's LDAP registry and used in the generated JdbcCfg for MAS. The user will have necessary database permissions configured. Related variables : - db2_ldap_password : Required when this is set - db2_rotate_password : Can auto-generate password for this user - Used in generated JdbcCfg for MAS configuration Note : If not set, MAS will use the default db2inst1 user. When set, you must also provide db2_ldap_password . db2_ldap_password \u00a4 Password for the Db2 LDAP user. Optional Environment Variable: DB2_LDAP_PASSWORD Default: None Purpose : Sets the password for the LDAP user specified in db2_ldap_username . This password is configured in Db2's local LDAP registry and used for database authentication. When to use : - Required when db2_ldap_username is set - Set to a strong password meeting security requirements - Can be omitted if using db2_rotate_password for auto-generation Valid values : Strong password string meeting security requirements Impact : This password is stored in Db2's LDAP registry and used for database authentication. It's also included in the generated JdbcCfg for MAS. Related variables : - db2_ldap_username : Must be set together with this password - db2_rotate_password : Alternative to manually setting password Note : Keep this password secure and do not commit to source control. If using db2_rotate_password=true , this password will be auto-generated and you don't need to provide it. db2_rotate_password \u00a4 Enables automatic password generation and rotation for Db2 LDAP user. Optional Environment Variable: DB2_LDAP_ROTATE_PASSWORD Default: False Purpose : Automates password management by generating a new random password for the Db2 LDAP user and updating both Db2 and MAS configurations. This improves security by enabling regular password rotation. When to use : - Set to True for automated password management - Set to True for regular password rotation as a security practice - Leave as False when manually managing passwords - Useful for automated deployments where manual password management is impractical Valid values : True , False Impact : - When True : Role generates a strong random password, configures it in Db2, and updates MAS JdbcCfg - When False : You must provide db2_ldap_password manually Related variables : - db2_ldap_username : User whose password will be rotated - db2_ldap_password : Not needed when rotation is enabled Note : When enabled, the role handles all password management automatically. The generated password is stored securely in Kubernetes secrets and MAS configuration. Storage Variables \u00a4 We recommend reviewing the Db2 documentation about the certified storage options for Db2 on Red Hat OpenShift. Please ensure your storage class meets the specified deployment requirements for Db2. https://www.ibm.com/docs/en/db2/11.5?topic=storage-certified-options db2_meta_storage_class \u00a4 Storage class for Db2 metadata storage (must support ReadWriteMany). Required Environment Variable: DB2_META_STORAGE_CLASS Default: ibmc-file-gold (if available in cluster) Purpose : Specifies the storage class for Db2 metadata storage, which requires ReadWriteMany (RWX) access mode for shared access across Db2 pods. When to use : - Always required for Db2 installation - Must be a storage class supporting RWX access mode - Typically file-based storage (NFS, IBM Cloud File Storage, etc.) Valid values : Any storage class name supporting ReadWriteMany access mode Impact : Metadata storage is critical for Db2 operation. Incorrect storage class or one not supporting RWX will cause Db2 deployment to fail. Related variables : - db2_meta_storage_size : Size of metadata volume - db2_meta_storage_accessmode : Should be ReadWriteMany Note : See Db2 certified storage options for supported storage. File-based storage classes typically support RWX (e.g., ibmc-file-gold , ocs-storagecluster-cephfs ). db2_meta_storage_size \u00a4 Size of the metadata persistent volume. Optional Environment Variable: DB2_META_STORAGE_SIZE Default: 10Gi Purpose : Specifies the size of the persistent volume for Db2 metadata storage. Metadata includes system catalogs, configuration files, and other operational data. When to use : - Use default ( 10Gi ) for most deployments - Increase for large-scale deployments with many databases or complex configurations - Monitor usage and adjust if metadata volume fills up Valid values : Kubernetes storage size format (e.g., 10Gi , 20Gi , 50Gi ) Impact : Insufficient metadata storage can cause Db2 operational issues. The volume must have enough space for system catalogs and configuration data. Related variables : - db2_meta_storage_class : Storage class for this volume - db2_meta_storage_accessmode : Access mode for this volume Note : The default 10Gi is sufficient for most deployments. Metadata storage requirements are typically much smaller than data storage. db2_meta_storage_accessmode \u00a4 Access mode for metadata storage volume. Optional Environment Variable: DB2_META_STORAGE_ACCESSMODE Default: ReadWriteMany Purpose : Defines the Kubernetes access mode for the metadata persistent volume. ReadWriteMany (RWX) is required for Db2 metadata to be accessible from multiple pods. When to use : - Leave as default ( ReadWriteMany ) for standard Db2 deployments - RWX is required for Db2 high availability and proper operation Valid values : ReadWriteMany (RWX) Impact : Db2 requires RWX access mode for metadata storage. Using a different access mode will cause deployment failures. Related variables : - db2_meta_storage_class : Must support the specified access mode - db2_meta_storage_size : Size of this volume Note : Do not change from the default ReadWriteMany . Db2 metadata requires RWX access mode for proper operation. db2_data_storage_class \u00a4 Storage class for Db2 user data storage (must support ReadWriteOnce). Required Environment Variable: DB2_DATA_STORAGE_CLASS Default: ibmc-block-gold (if available in cluster) Purpose : Specifies the storage class for Db2 user data storage, which requires ReadWriteOnce (RWO) access mode. This is where database tables and user data are stored. When to use : - Always required for Db2 installation - Must be a storage class supporting RWO access mode - Typically block-based storage for performance (SAN, IBM Cloud Block Storage, etc.) Valid values : Any storage class name supporting ReadWriteOnce access mode Impact : Data storage is critical for Db2 performance and capacity. Choose high-performance storage for production workloads. Incorrect storage class or one not supporting RWO will cause Db2 deployment to fail. Related variables : - db2_data_storage_size : Size of data volume - db2_data_storage_accessmode : Should be ReadWriteOnce Note : See Db2 certified storage options for supported storage. Block-based storage classes typically provide better performance for databases (e.g., ibmc-block-gold , ocs-storagecluster-ceph-rbd ). db2_data_storage_size \u00a4 Size of the user data persistent volume. Optional Environment Variable: DB2_DATA_STORAGE_SIZE Default: 50Gi Purpose : Specifies the size of the persistent volume for Db2 user data storage. This is where database tables, indexes, and all user data are stored. When to use : - Use default ( 50Gi ) for development/test environments - Increase significantly for production based on data volume estimates - Plan for data growth over time - Monitor usage and expand as needed Valid values : Kubernetes storage size format (e.g., 50Gi , 100Gi , 500Gi , 1Ti ) Impact : Insufficient data storage will cause database operations to fail when the volume fills up. Size appropriately for your data volume plus growth. Related variables : - db2_data_storage_class : Storage class for this volume - db2_data_storage_accessmode : Access mode for this volume Note : The default 50Gi is suitable for small deployments only. Production MAS Manage deployments typically require 500Gi or more. Plan storage capacity based on expected data volume and growth. db2_data_storage_accessmode \u00a4 Access mode for user data storage volume. Optional Environment Variable: DB2_DATA_STORAGE_ACCESSMODE Default: ReadWriteOnce Purpose : Defines the Kubernetes access mode for the user data persistent volume. ReadWriteOnce (RWO) is standard for Db2 data storage as it's accessed by a single pod at a time. When to use : - Leave as default ( ReadWriteOnce ) for standard Db2 deployments - RWO is the correct mode for Db2 data storage Valid values : ReadWriteOnce (RWO) Impact : Db2 data storage uses RWO access mode. Using a different access mode may cause deployment issues or performance problems. Related variables : - db2_data_storage_class : Must support the specified access mode - db2_data_storage_size : Size of this volume Note : Do not change from the default ReadWriteOnce . Db2 data storage is designed for RWO access mode. db2_backup_storage_class \u00a4 Storage class for Db2 backup storage (must support ReadWriteMany). Optional Environment Variable: DB2_BACKUP_STORAGE_CLASS Default: ibmc-file-gold (if available in cluster) Purpose : Specifies the storage class for Db2 backup storage, which requires ReadWriteMany (RWX) access mode for shared access during backup operations. When to use : - Use default for standard backup storage configuration - Must be a storage class supporting RWX access mode - Set to None to disable backup storage (not recommended for production) - Typically file-based storage (NFS, IBM Cloud File Storage, etc.) Valid values : Any storage class name supporting ReadWriteMany access mode, or None to disable Impact : - When set: Backup storage is configured for Db2 backups - When set to None : Backup storage is dropped from Db2uCluster CR (backups not possible) Related variables : - db2_backup_storage_size : Size of backup volume - db2_backup_storage_accessmode : Should be ReadWriteMany Note : WARNING - Setting to None disables backup capability. Only do this for non-production environments. Production systems should always have backup storage configured. db2_backup_storage_size \u00a4 Size of the backup persistent volume. Optional Environment Variable: DB2_BACKUP_STORAGE_SIZE Default: 50Gi Purpose : Specifies the size of the persistent volume for Db2 backup storage. This volume stores database backups for disaster recovery and point-in-time recovery. When to use : - Use default ( 50Gi ) for small databases or development environments - Increase based on database size and backup retention requirements - Plan for multiple full backups plus transaction logs - Monitor usage and expand as database grows Valid values : Kubernetes storage size format (e.g., 50Gi , 100Gi , 500Gi , 1Ti ) Impact : Insufficient backup storage will cause backup operations to fail. Size should accommodate multiple full backups based on your retention policy. Related variables : - db2_backup_storage_class : Storage class for this volume - db2_backup_storage_accessmode : Access mode for this volume - db2_data_storage_size : Backup storage should be sized relative to data storage Note : Plan backup storage as a multiple of your data storage size. A common practice is 2-3x the data storage size to accommodate multiple full backups and transaction logs. db2_backup_storage_accessmode \u00a4 Access mode for backup storage volume. Optional Environment Variable: DB2_BACKUP_STORAGE_ACCESSMODE Default: ReadWriteMany Purpose : Defines the Kubernetes access mode for the backup persistent volume. ReadWriteMany (RWX) is required for Db2 backup operations to be accessible from multiple pods. When to use : - Leave as default ( ReadWriteMany ) for standard Db2 backup configuration - RWX is required for Db2 backup operations Valid values : ReadWriteMany (RWX) Impact : Db2 backup storage requires RWX access mode. Using a different access mode will cause backup operations to fail. Related variables : - db2_backup_storage_class : Must support the specified access mode - db2_backup_storage_size : Size of this volume Note : Do not change from the default ReadWriteMany . Db2 backup operations require RWX access mode for proper operation. - Default: ReadWriteMany db2_logs_storage_class \u00a4 Storage class used for transaction logs. This must support ReadWriteOnce(RWO) access mode. Optional Environment Variable: DB2_LOGS_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. Set to None will drop the logs storage on DB2ucluster CR. db2_logs_storage_size \u00a4 Size of transaction logs persistent volume. Optional Environment Variable: DB2_LOGS_STORAGE_SIZE Default: 10Gi db2_logs_storage_accessmode \u00a4 The access mode for the storage. Optional Environment Variable: DB2_LOGS_STORAGE_ACCESSMODE Default: ReadWriteOnce db2_temp_storage_class \u00a4 Storage class used for temporary data. This must support ReadWriteMany(RWX) access mode. Optional Environment Variable: DB2_TEMP_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. Set to None will drop the tempts storage on DB2ucluster CR. db2_temp_storage_size \u00a4 Size of temporary persistent volume. Optional Environment Variable: DB2_TEMP_STORAGE_SIZE Default: 10Gi db2_temp_storage_accessmode \u00a4 The access mode for the storage. This must support ReadWriteOnce(RWO) access mode. Optional Environment Variable: DB2_TEMP_STORAGE_ACCESSMODE Default: ReadWriteOnce Resource Request Variables \u00a4 These variables allow you to customize the resources available to the Db2 pod in your cluster. In most circumstances you will want to set these properties because it's impossible for us to provide a default value that will be appropriate for all users. We have set defaults that are suitable for deploying Db2 onto a dedicated worker node with 4cpu and 16gb memory. Tip Note that you must take into account the system overhead on any given node when setting these parameters, if you set the requests equal to the number of CPU or amount of memory on your node then the scheduler will not be able to schedule the Db2 pod because not 100% of the worker nodes' resource will be available to pod on that node, even if there's only a single pod on it. Db2 is sensitive to both CPU and memory issues, particularly memory, we recommend setting requests and limits to the same values, ensuring the scheduler always reserves the resources that Db2 expects to be available to it. db2_cpu_requests \u00a4 Define the Kubernetes CPU request for the Db2 pod. Optional Environment Variable: DB2_CPU_REQUESTS Default: 4000m db2_cpu_limits \u00a4 Define the Kubernetes CPU limit for the Db2 pod. Optional Environment Variable: DB2_CPU_LIMITS Default: 6000m db2_memory_requests \u00a4 Define the Kubernetes memory request for the Db2 pod. Optional Environment Variable: DB2_MEMORY_REQUESTS Default: 8Gi db2_memory_limits \u00a4 Define the Kubernetes memory limit for the Db2 pod. Optional Environment Variable: DB2_MEMORY_LIMITS Default: 16Gi Node Label Affinity Variables \u00a4 Specify both db2_affinity_key and db2_affinity_value to configure requiredDuringSchedulingIgnoredDuringExecution affinity with appropriately labelled nodes. db2_affinity_key \u00a4 Specify the key of a node label to declare affinity with. Optional Environment Variable: DB2_AFFINITY_KEY Default: None db2_affinity_value \u00a4 Specify the value of a node label to declare affinity with. Optional Environment Variable: DB2_AFFINITY_VALUE Default: None Node Taint Toleration Variables \u00a4 Specify db2_tolerate_key , db2_tolerate_value , and db2_tolerate_effect to configure a toleration policy to allow the db2 instance to be scheduled on nodes with the specified taint. db2_tolerate_key \u00a4 Specify the key of the taint that is to be tolerated. Optional Environment Variable: DB2_TOLERATE_KEY Default: None db2_tolerate_value \u00a4 Specify the value of the taint that is to be tolerated. Optional Environment Variable: DB2_TOLERATE_VALUE Default: None db2_tolerate_effect \u00a4 Specify the type of taint effect that will be tolerated ( NoSchedule , PreferNoSchedule , or NoExecute ). Optional Environment Variable: DB2_TOLERATE_EFFECT Default: None Role Variables - DB2UCluster Database Configuration Settings \u00a4 The following variables will overwrite DB2UCluster default properties for the DB2 configuration sections: spec.environment.database.dbConfig spec.environment.instance.dbmConfig spec.environment.instance.registry db2_database_db_config \u00a4 Overwrites the db2ucluster database configuration settings under spec.environment.database.dbConfig section. Optional Environment Variable: DB2_DATABASE_DB_CONFIG Default: None db2_instance_dbm_config \u00a4 Overwrites the db2ucluster instance database configuration settings under spec.environment.instance.dbmConfig section. Important Do not set instance_memory . The Db2 engine does not know Db2 is running inside a container, setting dbmConfig.INSTANCE_MEMORY: automatic will cause it to read the cgroups of the node and potentially go beyond the pod memory limit. Db2U has logic built in to use a normalized percentage that takes into account the memory limit and free memory of the node. Optional Environment Variable: DB2_INSTANCE_DBM_CONFIG Default: None db2_instance_registry \u00a4 Overwrites the db2ucluster instance database configuration settings under spec.environment.instance.registry section. You can define parameters to be included in this section using semicolon separated values. Optional Environment Variable: DB2_INSTANCE_REGISTRY Default: None MPP System Variables \u00a4 Warning Do not use these variables if you intend to use the Db2 instance with IBM Maximo Application Suite; no MAS application supports Db2 MPP db2_mln_count \u00a4 The number of logical nodes (i.e. database partitions to create). Note: ensure that the application using this Db2 can support Db2 MPP (which is created when DB2_MLN_COUNT is greater than 1). Optional Environment Variable: 'DB2_MLN_COUNT Default: 1 db2_num_pods \u00a4 The number of Db2 pods to create in the instance. Note that db2_num_pods must be less than or equal to db2_mln_count . A single db2u pod can contain multiple logical nodes. So be sure to avoid specifying a large number for db2_mln_count while specifying a small number for db2_num_pods . If in doubt, make db2_mln_count = db2_num_pods . For more information refer to the Db2 documentation . Optional Environment Variable: DB2_NUM_PODS Default: 1 MAS Configuration Variables \u00a4 mas_instance_id \u00a4 Providing this and mas_config_dir will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_INSTANCE_ID Default: None mas_config_dir \u00a4 Providing this and mas_instance_id will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_CONFIG_DIR Default: None mas_config_scope \u00a4 Supported values are system , ws , app , or wsapp , this is only used when both mas_config_dir and mas_instance_id are set. Optional Environment Variable: MAS_CONFIG_SCOPE Default: system mas_workspace_id \u00a4 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Optional Environment Variable: MAS_WORKSPACE_ID Default: None mas_application_id \u00a4 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either app or wsapp Optional Environment Variable: 'MAS_APP_ID Default: None Role Variables - Backup and Restore \u00a4 masbr_confirm_cluster \u00a4 Set true or false to indicate the role whether to confirm the currently connected cluster before running the backup or restore job. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false masbr_copy_timeout_sec \u00a4 Set the transfer files timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) masbr_job_timezone \u00a4 Set the time zone for creating scheduled backup job. If not set a value for this variable, this role will use UTC time zone when creating a CronJob for running scheduled backup job. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None masbr_storage_local_folder \u00a4 Set local path to save the backup files. Required Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None masbr_backup_type \u00a4 Set full or incr to indicate the role to create a full backup or incremental backup. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full masbr_backup_from_version \u00a4 Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). This variable is only valid when MASBR_BACKUP_TYPE=incr . If not set a value for this variable, this role will try to find the latest full backup version from the specified storage location. Optional Environment Variable: MASBR_BACKUP_FROM_VERSION Default: None masbr_backup_schedule \u00a4 Set Cron expression to create a scheduled backup. If not set a value for this varialbe, this role will create an on-demand backup. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None masbr_restore_from_version \u00a4 Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) Required only when DB2_ACTION=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Example Playbook \u00a4 Install Db2 \u00a4 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxxx # Configuration for the Db2 cluster db2_instance_name: db2u-db01 db2_meta_storage_class: \"ibmc-file-gold\" db2_data_storage_class: \"ibmc-block-gold\" db2_backup_storage_class: \"ibmc-file-gold\" db2_logs_storage_class: \"ibmc-block-gold\" db2_temp_storage_class: \"ibmc-block-gold\" # Create the MAS JdbcCfg & Secret resource definitions mas_instance_id: inst1 mas_config_dir: /home/david/masconfig roles: - ibm.mas_devops.db2 Backup Db2 \u00a4 - hosts: localhost any_errors_fatal: true vars: db2_action: backup db2_instance_name: db2u-db01 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.db2 Restore Db2 \u00a4 - hosts: localhost any_errors_fatal: true vars: db2_action: restore db2_instance_name: db2u-db01 masbr_restore_from_version: 20240621021316 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.db2 Run Role Playbook \u00a4 export IBM_ENTITLEMENT_KEY=xxxxx export DB2_INSTANCE_NAME=db2u-db01 export DB2_META_STORAGE_CLASS=ibmc-file-gold export DB2_DATA_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/masconfig ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"db2"},{"location":"roles/db2/#db2","text":"This role creates or upgrades a Db2 instance using the Db2u Operator. When installing db2, the db2u operator will now be installed into the same namespace as the db2 instance ( db2ucluster ). If you already have db2 operator and db2 instances running in separate namespaces, this role will take care of migrating (by deleting & reinstalling) the db2 operators from ibm-common-services to the namespace defined by db2_namespace property (in case of a new role execution for a db2 install or db2 upgrade). A private root CA certificate is created and is used to secure the TLS connections to the database. A Db2 Warehouse cluster will be created along with a public TLS encrypted route to allow external access to the cluster (access is via the ssl-server nodeport port on the -db2u-engn-svc service). Internal access is via the -db2u-engn-svc service and port 50001. Both the external route and the internal service use the same server certificate. The private root CA certificate and the server certificate are available from the db2u-ca and db2u-certificate secrets in the db2 namespace. The default user is db2inst1 and the password is available in the instancepassword secret in the same namespace. You can examine the deployed resources in the db2 namespace. This example assumes the default namespace db2u : oc -n db2u get db2ucluster NAME STATE MAINTENANCESTATE AGE db2u-db01 Ready None 29m It typically takes 20-30 minutes from the db2ucluster being created till it is ready. If the db2ucluster is not ready after that period then check that all the PersistentVolumeClaims in the db2 namespace are ready and that the pods in the namespace are not stuck in init state. If the c-<db2_instance_name>-db2u-0 pod is running then you can exec into the pod and check the /var/log/db2u.log for any issue. If the mas_instance_id and mas_config_dir are provided then the role will generate the JdbcCfg yaml that can be used to configure MAS to connect to this database. It does not apply the yaml to the cluster but does provide you with the yaml files to apply if needed. When upgrading db2, specify the existing namespace where the db2uCluster instances exist. All the instances under that namespace will be upgraded to the db2 version specified. The version of db2 must match the channel of db2 being used for the upgrade.","title":"db2"},{"location":"roles/db2/#role-variables","text":"","title":"Role Variables"},{"location":"roles/db2/#installation-variables","text":"","title":"Installation Variables"},{"location":"roles/db2/#common_services_namespace","text":"OpenShift namespace where IBM Common Services is installed. Optional Environment Variable: COMMON_SERVICES_NAMESPACE Default Value: ibm-common-services Purpose : Specifies the namespace containing IBM Common Services, which provides shared services used by Db2 operator. This is needed for the role to locate and interact with Common Services components. When to use : - Leave as default ( ibm-common-services ) for standard installations - Set only if Common Services is installed in a non-standard namespace - Required when Common Services namespace differs from default Valid values : Any valid Kubernetes namespace name where IBM Common Services is installed Impact : The role uses this to locate Common Services resources. Incorrect namespace will cause the role to fail when trying to access Common Services components. Related variables : Works with db2_namespace to manage operator and instance placement. Note : The default ibm-common-services is the standard namespace for Common Services. Only change if your installation uses a different namespace.","title":"common_services_namespace"},{"location":"roles/db2/#db2_action","text":"Specifies which operation to perform on the Db2 database. Optional Environment Variable: DB2_ACTION Default: install Purpose : Controls what action the role executes against Db2 instances. This allows the same role to handle installation, upgrades, backups, and restores of Db2 databases. When to use : - Use install (default) for initial Db2 deployment - Use upgrade to upgrade all Db2 instances in the namespace to a new version - Use backup to create a backup of Db2 data - Use restore to restore Db2 from a backup Valid values : install , upgrade , backup , restore Impact : - install : Creates new Db2 operator and instance - upgrade : Upgrades ALL instances in db2_namespace to db2_version (affects all instances in namespace) - backup : Creates backup of Db2 data - restore : Restores Db2 from backup Related variables : - db2_version : Required for upgrade action to specify target version - db2_namespace : All instances in this namespace are affected by upgrade Note : WARNING - When using upgrade , ALL Db2 instances in the specified namespace will be upgraded. Plan accordingly and ensure db2_version matches the operator channel.","title":"db2_action"},{"location":"roles/db2/#db2_namespace","text":"OpenShift namespace where Db2 operator and instances will be deployed. Optional Environment Variable: DB2_NAMESPACE Default: db2u Purpose : Specifies the namespace for deploying the Db2u operator and Db2 instances (Db2uCluster custom resources). Starting with recent versions, the operator is installed in the same namespace as the instances. When to use : - Use default ( db2u ) for standard single-instance deployments - Set to a custom namespace when organizing multiple Db2 deployments - Must match existing namespace when upgrading Db2 instances Valid values : Any valid Kubernetes namespace name (e.g., db2u , db2-prod , mas-db2 ) Impact : All Db2 resources (operator, instances, secrets, services) are created in this namespace. When upgrading, ALL instances in this namespace will be upgraded together. Related variables : - db2_action : When set to upgrade , affects all instances in this namespace - db2_instance_name : Instance created within this namespace Note : The role handles migration of operators from ibm-common-services to this namespace if needed. All instances in the namespace are upgraded together when using db2_action=upgrade .","title":"db2_namespace"},{"location":"roles/db2/#db2_channel","text":"Subscription channel for the Db2 Universal Operator. Optional Environment Variable: DB2_CHANNEL Default: The default channel from the operator package (automatically selected) Purpose : Specifies which operator channel to subscribe to, determining which Db2 operator version stream you receive. Channels typically correspond to major Db2 versions (e.g., v2.2 , v3.0 ). When to use : - Leave unset to use the default channel (recommended for new installations) - Set explicitly when you need a specific operator version stream - Must match db2_version when upgrading (version must be available in the channel) Valid values : Valid Db2 operator channel names (e.g., v2.2 , v3.0 , v3.1 ) Impact : Determines which operator version is installed and which Db2 engine versions are available. Changing channels may require operator migration. Related variables : - db2_version : Must be compatible with the selected channel - When upgrading, version must match channel Note : The operator channel determines available Db2 engine versions. Check the operator package for available channels and their supported versions.","title":"db2_channel"},{"location":"roles/db2/#db2_instance_name","text":"Unique name for the Db2 instance (Db2uCluster resource). Required Environment Variable: DB2_INSTANCE_NAME Default: None Purpose : Defines the name of the Db2uCluster custom resource that will be created. This name is used to identify the Db2 instance and is embedded in resource names (services, pods, secrets). When to use : - Always required for Db2 installation - Use descriptive names that indicate purpose (e.g., maximo-db , iot-db , manage-db ) - Must be unique within the namespace Valid values : Valid Kubernetes resource name (lowercase alphanumeric with hyphens, e.g., db01 , mas-db , manage-db ) Impact : This name is used throughout the Db2 deployment: - Db2uCluster resource name - Service names (e.g., {instance_name}-db2u-engn-svc ) - Pod names (e.g., c-{instance_name}-db2u-0 ) - Secret names Related variables : - db2_dbname : Name of the database within this instance - Used in generated JdbcCfg when mas_instance_id is provided Note : Choose a meaningful name as it appears in many resource names and cannot be easily changed after creation.","title":"db2_instance_name"},{"location":"roles/db2/#ibm_entitlement_key","text":"IBM Container Library entitlement key for accessing Db2 container images. Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Provides authentication credentials to pull Db2 container images from the IBM Container Registry. This key is associated with your IBM entitlement and grants access to licensed software. When to use : - Always required for Db2 installation - Obtain from IBM Container Library - Same key used across all IBM MAS components Valid values : Valid IBM entitlement key string Impact : Without a valid key, Db2 container images cannot be pulled and installation will fail. The key is stored in an image pull secret in the Db2 namespace. Related variables : Used across multiple roles for accessing IBM container images. Note : Keep this key secure and do not commit to source control. The key is tied to your IBM entitlement and should be treated as a credential. Obtain from the IBM Container Library using your IBM ID.","title":"ibm_entitlement_key"},{"location":"roles/db2/#db2_dbname","text":"Name of the database to create within the Db2 instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB Purpose : Specifies the name of the database that will be created inside the Db2 instance. A Db2 instance can contain multiple databases, and this defines the primary database name. When to use : - Use default ( BLUDB ) for standard MAS deployments - Set to a custom name when organizational standards require specific naming - Must match the database name expected by applications connecting to Db2 Valid values : Valid Db2 database name (uppercase alphanumeric, up to 8 characters, e.g., BLUDB , MAXDB , MASDB ) Impact : This database name is used in connection strings and JDBC configurations. Applications must use this exact name to connect to the database. Related variables : - db2_instance_name : The instance containing this database - Used in generated JdbcCfg for MAS configuration Note : The default BLUDB is standard for Db2 Warehouse. Db2 database names are typically uppercase and limited to 8 characters.","title":"db2_dbname"},{"location":"roles/db2/#db2_version","text":"Db2 engine version to use for installation or upgrade. Optional Environment Variable: DB2_VERSION Default: Latest version supported by the installed Db2 operator (auto-detected from db2u-release configmap) Purpose : Specifies which Db2 engine version to deploy or upgrade to. The version must be compatible with the installed operator channel. When to use : - Leave unset for new installations to use the latest supported version (recommended) - Set explicitly when you need a specific Db2 version - Required when upgrading to specify the target version - Must match the operator channel capabilities Valid values : Valid Db2 engine version supported by the operator (e.g., 11.5.8.0 , 11.5.9.0 ) Impact : Determines which Db2 engine version is deployed. When upgrading, ALL instances in db2_namespace will be upgraded to this version. Version must be available in the operator's supported versions list. Related variables : - db2_channel : Version must be compatible with the operator channel - db2_action : When set to upgrade , this version is the target - db2_namespace : All instances in namespace upgraded to this version Note : Supported versions are listed in the db2u-release configmap in ibm-common-services namespace. Ensure the version matches your operator channel. When upgrading, all instances in the namespace are upgraded together.","title":"db2_version"},{"location":"roles/db2/#db2_type","text":"Db2 instance type optimized for specific workload patterns. Optional Environment Variable: DB2_TYPE Default: db2wh Purpose : Determines the Db2 instance configuration optimized for either data warehouse (analytical) or online transaction processing workloads. This affects resource allocation and database tuning. When to use : - Use db2wh (default) for MAS Manage and analytical workloads - Use db2oltp for high-transaction workloads requiring OLTP optimization - Choose based on your primary use case Valid values : db2wh , db2oltp Impact : - db2wh : Optimized for data warehouse/analytical queries (recommended for MAS Manage) - db2oltp : Optimized for online transaction processing with high concurrency Related variables : - db2_workload : Further refines workload optimization - db2_table_org : Table organization should align with instance type Note : MAS Manage typically uses db2wh for optimal performance with its analytical workload patterns.","title":"db2_type"},{"location":"roles/db2/#db2_timezone","text":"Server timezone for the Db2 instance. Optional Environment Variable: DB2_TIMEZONE Default: GMT Purpose : Sets the timezone used by the Db2 server for timestamp operations and scheduling. This affects how Db2 interprets and stores timestamp data. When to use : - Use default ( GMT ) for globally distributed systems - Set to local timezone when all users are in the same timezone - Must match MAS Manage timezone if using Manage with this database Valid values : Valid timezone codes (e.g., GMT , EST , PST , CET , UTC ) Impact : Affects timestamp interpretation and storage in the database. Mismatched timezones between Db2 and applications can cause data inconsistencies. Related variables : - MAS_APP_SETTINGS_SERVER_TIMEZONE : Must be set to the same value for MAS Manage - Affects all timestamp operations in the database Note : IMPORTANT - If using this Db2 instance with MAS Manage, you must also set MAS_APP_SETTINGS_SERVER_TIMEZONE to the same value to ensure consistent timestamp handling.","title":"db2_timezone"},{"location":"roles/db2/#db2_4k_device_support","text":"Controls 4K sector device support in Db2. Optional Environment Variable: DB2_4K_DEVICE_SUPPORT Default: ON Purpose : Enables or disables support for storage devices with 4K sector sizes. Modern storage systems often use 4K sectors instead of traditional 512-byte sectors. When to use : - Leave as ON (default) for modern storage systems with 4K sectors - Set to OFF only if using legacy storage with 512-byte sectors - Check your storage specifications if unsure Valid values : ON , OFF Impact : - ON : Enables 4K sector support (required for most modern storage) - OFF : Uses traditional 512-byte sector mode (legacy storage only) Related variables : Works with storage class configuration to ensure compatibility. Note : Most modern storage systems use 4K sectors. Keep this ON unless you have specific legacy storage requirements.","title":"db2_4k_device_support"},{"location":"roles/db2/#db2_workload","text":"Workload profile for Db2 instance optimization. Optional Environment Variable: DB2_WORKLOAD Default: ANALYTICS Purpose : Configures Db2 with predefined settings optimized for specific workload patterns. This affects memory allocation, query optimization, and other performance parameters. When to use : - Use ANALYTICS (default) for general analytical and MAS workloads - Use PUREDATA_OLAP for pure data warehouse/OLAP workloads - Choose based on your primary query patterns Valid values : ANALYTICS , PUREDATA_OLAP Impact : - ANALYTICS : Balanced optimization for mixed analytical workloads (recommended for MAS) - PUREDATA_OLAP : Optimized specifically for OLAP/data warehouse queries Related variables : - db2_type : Should align with workload choice - db2_table_org : Table organization should match workload pattern Note : The default ANALYTICS is suitable for most MAS deployments. Only change if you have specific OLAP requirements.","title":"db2_workload"},{"location":"roles/db2/#db2_table_org","text":"Default table organization for database tables. Optional Environment Variable: DB2_TABLE_ORG Default: ROW Purpose : Determines the default storage organization for tables in the database. Row-organized tables are optimized for transactional workloads, while column-organized tables are optimized for analytical queries. When to use : - Use ROW (default) for MAS Manage and mixed workloads - Use COLUMN only for pure analytical/reporting workloads - Choose based on your primary query patterns Valid values : ROW , COLUMN Impact : - ROW : Tables stored row-by-row (better for OLTP, updates, and mixed workloads) - COLUMN : Tables stored column-by-column (better for analytical queries and aggregations) Related variables : - db2_type : Should align with table organization choice - db2_workload : Workload profile should match table organization Note : MAS Manage requires ROW organization. Only use COLUMN for dedicated analytical databases. This sets the default; individual tables can override this setting.","title":"db2_table_org"},{"location":"roles/db2/#db2_ldap_username","text":"Username for Db2 LDAP authentication. Optional Environment Variable: DB2_LDAP_USERNAME Default: None (uses default db2inst1 user) Purpose : Defines a custom LDAP username for Db2 authentication instead of the default db2inst1 user. When set, this user is configured in Db2's local LDAP registry and used in MAS JDBC configuration. When to use : - Set when you need a custom database user for MAS connections - Set when organizational policies require specific usernames - Leave unset to use the default db2inst1 user - Must be set together with db2_ldap_password Valid values : Valid LDAP username string Impact : When set, this user is created in Db2's LDAP registry and used in the generated JdbcCfg for MAS. The user will have necessary database permissions configured. Related variables : - db2_ldap_password : Required when this is set - db2_rotate_password : Can auto-generate password for this user - Used in generated JdbcCfg for MAS configuration Note : If not set, MAS will use the default db2inst1 user. When set, you must also provide db2_ldap_password .","title":"db2_ldap_username"},{"location":"roles/db2/#db2_ldap_password","text":"Password for the Db2 LDAP user. Optional Environment Variable: DB2_LDAP_PASSWORD Default: None Purpose : Sets the password for the LDAP user specified in db2_ldap_username . This password is configured in Db2's local LDAP registry and used for database authentication. When to use : - Required when db2_ldap_username is set - Set to a strong password meeting security requirements - Can be omitted if using db2_rotate_password for auto-generation Valid values : Strong password string meeting security requirements Impact : This password is stored in Db2's LDAP registry and used for database authentication. It's also included in the generated JdbcCfg for MAS. Related variables : - db2_ldap_username : Must be set together with this password - db2_rotate_password : Alternative to manually setting password Note : Keep this password secure and do not commit to source control. If using db2_rotate_password=true , this password will be auto-generated and you don't need to provide it.","title":"db2_ldap_password"},{"location":"roles/db2/#db2_rotate_password","text":"Enables automatic password generation and rotation for Db2 LDAP user. Optional Environment Variable: DB2_LDAP_ROTATE_PASSWORD Default: False Purpose : Automates password management by generating a new random password for the Db2 LDAP user and updating both Db2 and MAS configurations. This improves security by enabling regular password rotation. When to use : - Set to True for automated password management - Set to True for regular password rotation as a security practice - Leave as False when manually managing passwords - Useful for automated deployments where manual password management is impractical Valid values : True , False Impact : - When True : Role generates a strong random password, configures it in Db2, and updates MAS JdbcCfg - When False : You must provide db2_ldap_password manually Related variables : - db2_ldap_username : User whose password will be rotated - db2_ldap_password : Not needed when rotation is enabled Note : When enabled, the role handles all password management automatically. The generated password is stored securely in Kubernetes secrets and MAS configuration.","title":"db2_rotate_password"},{"location":"roles/db2/#storage-variables","text":"We recommend reviewing the Db2 documentation about the certified storage options for Db2 on Red Hat OpenShift. Please ensure your storage class meets the specified deployment requirements for Db2. https://www.ibm.com/docs/en/db2/11.5?topic=storage-certified-options","title":"Storage Variables"},{"location":"roles/db2/#db2_meta_storage_class","text":"Storage class for Db2 metadata storage (must support ReadWriteMany). Required Environment Variable: DB2_META_STORAGE_CLASS Default: ibmc-file-gold (if available in cluster) Purpose : Specifies the storage class for Db2 metadata storage, which requires ReadWriteMany (RWX) access mode for shared access across Db2 pods. When to use : - Always required for Db2 installation - Must be a storage class supporting RWX access mode - Typically file-based storage (NFS, IBM Cloud File Storage, etc.) Valid values : Any storage class name supporting ReadWriteMany access mode Impact : Metadata storage is critical for Db2 operation. Incorrect storage class or one not supporting RWX will cause Db2 deployment to fail. Related variables : - db2_meta_storage_size : Size of metadata volume - db2_meta_storage_accessmode : Should be ReadWriteMany Note : See Db2 certified storage options for supported storage. File-based storage classes typically support RWX (e.g., ibmc-file-gold , ocs-storagecluster-cephfs ).","title":"db2_meta_storage_class"},{"location":"roles/db2/#db2_meta_storage_size","text":"Size of the metadata persistent volume. Optional Environment Variable: DB2_META_STORAGE_SIZE Default: 10Gi Purpose : Specifies the size of the persistent volume for Db2 metadata storage. Metadata includes system catalogs, configuration files, and other operational data. When to use : - Use default ( 10Gi ) for most deployments - Increase for large-scale deployments with many databases or complex configurations - Monitor usage and adjust if metadata volume fills up Valid values : Kubernetes storage size format (e.g., 10Gi , 20Gi , 50Gi ) Impact : Insufficient metadata storage can cause Db2 operational issues. The volume must have enough space for system catalogs and configuration data. Related variables : - db2_meta_storage_class : Storage class for this volume - db2_meta_storage_accessmode : Access mode for this volume Note : The default 10Gi is sufficient for most deployments. Metadata storage requirements are typically much smaller than data storage.","title":"db2_meta_storage_size"},{"location":"roles/db2/#db2_meta_storage_accessmode","text":"Access mode for metadata storage volume. Optional Environment Variable: DB2_META_STORAGE_ACCESSMODE Default: ReadWriteMany Purpose : Defines the Kubernetes access mode for the metadata persistent volume. ReadWriteMany (RWX) is required for Db2 metadata to be accessible from multiple pods. When to use : - Leave as default ( ReadWriteMany ) for standard Db2 deployments - RWX is required for Db2 high availability and proper operation Valid values : ReadWriteMany (RWX) Impact : Db2 requires RWX access mode for metadata storage. Using a different access mode will cause deployment failures. Related variables : - db2_meta_storage_class : Must support the specified access mode - db2_meta_storage_size : Size of this volume Note : Do not change from the default ReadWriteMany . Db2 metadata requires RWX access mode for proper operation.","title":"db2_meta_storage_accessmode"},{"location":"roles/db2/#db2_data_storage_class","text":"Storage class for Db2 user data storage (must support ReadWriteOnce). Required Environment Variable: DB2_DATA_STORAGE_CLASS Default: ibmc-block-gold (if available in cluster) Purpose : Specifies the storage class for Db2 user data storage, which requires ReadWriteOnce (RWO) access mode. This is where database tables and user data are stored. When to use : - Always required for Db2 installation - Must be a storage class supporting RWO access mode - Typically block-based storage for performance (SAN, IBM Cloud Block Storage, etc.) Valid values : Any storage class name supporting ReadWriteOnce access mode Impact : Data storage is critical for Db2 performance and capacity. Choose high-performance storage for production workloads. Incorrect storage class or one not supporting RWO will cause Db2 deployment to fail. Related variables : - db2_data_storage_size : Size of data volume - db2_data_storage_accessmode : Should be ReadWriteOnce Note : See Db2 certified storage options for supported storage. Block-based storage classes typically provide better performance for databases (e.g., ibmc-block-gold , ocs-storagecluster-ceph-rbd ).","title":"db2_data_storage_class"},{"location":"roles/db2/#db2_data_storage_size","text":"Size of the user data persistent volume. Optional Environment Variable: DB2_DATA_STORAGE_SIZE Default: 50Gi Purpose : Specifies the size of the persistent volume for Db2 user data storage. This is where database tables, indexes, and all user data are stored. When to use : - Use default ( 50Gi ) for development/test environments - Increase significantly for production based on data volume estimates - Plan for data growth over time - Monitor usage and expand as needed Valid values : Kubernetes storage size format (e.g., 50Gi , 100Gi , 500Gi , 1Ti ) Impact : Insufficient data storage will cause database operations to fail when the volume fills up. Size appropriately for your data volume plus growth. Related variables : - db2_data_storage_class : Storage class for this volume - db2_data_storage_accessmode : Access mode for this volume Note : The default 50Gi is suitable for small deployments only. Production MAS Manage deployments typically require 500Gi or more. Plan storage capacity based on expected data volume and growth.","title":"db2_data_storage_size"},{"location":"roles/db2/#db2_data_storage_accessmode","text":"Access mode for user data storage volume. Optional Environment Variable: DB2_DATA_STORAGE_ACCESSMODE Default: ReadWriteOnce Purpose : Defines the Kubernetes access mode for the user data persistent volume. ReadWriteOnce (RWO) is standard for Db2 data storage as it's accessed by a single pod at a time. When to use : - Leave as default ( ReadWriteOnce ) for standard Db2 deployments - RWO is the correct mode for Db2 data storage Valid values : ReadWriteOnce (RWO) Impact : Db2 data storage uses RWO access mode. Using a different access mode may cause deployment issues or performance problems. Related variables : - db2_data_storage_class : Must support the specified access mode - db2_data_storage_size : Size of this volume Note : Do not change from the default ReadWriteOnce . Db2 data storage is designed for RWO access mode.","title":"db2_data_storage_accessmode"},{"location":"roles/db2/#db2_backup_storage_class","text":"Storage class for Db2 backup storage (must support ReadWriteMany). Optional Environment Variable: DB2_BACKUP_STORAGE_CLASS Default: ibmc-file-gold (if available in cluster) Purpose : Specifies the storage class for Db2 backup storage, which requires ReadWriteMany (RWX) access mode for shared access during backup operations. When to use : - Use default for standard backup storage configuration - Must be a storage class supporting RWX access mode - Set to None to disable backup storage (not recommended for production) - Typically file-based storage (NFS, IBM Cloud File Storage, etc.) Valid values : Any storage class name supporting ReadWriteMany access mode, or None to disable Impact : - When set: Backup storage is configured for Db2 backups - When set to None : Backup storage is dropped from Db2uCluster CR (backups not possible) Related variables : - db2_backup_storage_size : Size of backup volume - db2_backup_storage_accessmode : Should be ReadWriteMany Note : WARNING - Setting to None disables backup capability. Only do this for non-production environments. Production systems should always have backup storage configured.","title":"db2_backup_storage_class"},{"location":"roles/db2/#db2_backup_storage_size","text":"Size of the backup persistent volume. Optional Environment Variable: DB2_BACKUP_STORAGE_SIZE Default: 50Gi Purpose : Specifies the size of the persistent volume for Db2 backup storage. This volume stores database backups for disaster recovery and point-in-time recovery. When to use : - Use default ( 50Gi ) for small databases or development environments - Increase based on database size and backup retention requirements - Plan for multiple full backups plus transaction logs - Monitor usage and expand as database grows Valid values : Kubernetes storage size format (e.g., 50Gi , 100Gi , 500Gi , 1Ti ) Impact : Insufficient backup storage will cause backup operations to fail. Size should accommodate multiple full backups based on your retention policy. Related variables : - db2_backup_storage_class : Storage class for this volume - db2_backup_storage_accessmode : Access mode for this volume - db2_data_storage_size : Backup storage should be sized relative to data storage Note : Plan backup storage as a multiple of your data storage size. A common practice is 2-3x the data storage size to accommodate multiple full backups and transaction logs.","title":"db2_backup_storage_size"},{"location":"roles/db2/#db2_backup_storage_accessmode","text":"Access mode for backup storage volume. Optional Environment Variable: DB2_BACKUP_STORAGE_ACCESSMODE Default: ReadWriteMany Purpose : Defines the Kubernetes access mode for the backup persistent volume. ReadWriteMany (RWX) is required for Db2 backup operations to be accessible from multiple pods. When to use : - Leave as default ( ReadWriteMany ) for standard Db2 backup configuration - RWX is required for Db2 backup operations Valid values : ReadWriteMany (RWX) Impact : Db2 backup storage requires RWX access mode. Using a different access mode will cause backup operations to fail. Related variables : - db2_backup_storage_class : Must support the specified access mode - db2_backup_storage_size : Size of this volume Note : Do not change from the default ReadWriteMany . Db2 backup operations require RWX access mode for proper operation. - Default: ReadWriteMany","title":"db2_backup_storage_accessmode"},{"location":"roles/db2/#db2_logs_storage_class","text":"Storage class used for transaction logs. This must support ReadWriteOnce(RWO) access mode. Optional Environment Variable: DB2_LOGS_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. Set to None will drop the logs storage on DB2ucluster CR.","title":"db2_logs_storage_class"},{"location":"roles/db2/#db2_logs_storage_size","text":"Size of transaction logs persistent volume. Optional Environment Variable: DB2_LOGS_STORAGE_SIZE Default: 10Gi","title":"db2_logs_storage_size"},{"location":"roles/db2/#db2_logs_storage_accessmode","text":"The access mode for the storage. Optional Environment Variable: DB2_LOGS_STORAGE_ACCESSMODE Default: ReadWriteOnce","title":"db2_logs_storage_accessmode"},{"location":"roles/db2/#db2_temp_storage_class","text":"Storage class used for temporary data. This must support ReadWriteMany(RWX) access mode. Optional Environment Variable: DB2_TEMP_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. Set to None will drop the tempts storage on DB2ucluster CR.","title":"db2_temp_storage_class"},{"location":"roles/db2/#db2_temp_storage_size","text":"Size of temporary persistent volume. Optional Environment Variable: DB2_TEMP_STORAGE_SIZE Default: 10Gi","title":"db2_temp_storage_size"},{"location":"roles/db2/#db2_temp_storage_accessmode","text":"The access mode for the storage. This must support ReadWriteOnce(RWO) access mode. Optional Environment Variable: DB2_TEMP_STORAGE_ACCESSMODE Default: ReadWriteOnce","title":"db2_temp_storage_accessmode"},{"location":"roles/db2/#resource-request-variables","text":"These variables allow you to customize the resources available to the Db2 pod in your cluster. In most circumstances you will want to set these properties because it's impossible for us to provide a default value that will be appropriate for all users. We have set defaults that are suitable for deploying Db2 onto a dedicated worker node with 4cpu and 16gb memory. Tip Note that you must take into account the system overhead on any given node when setting these parameters, if you set the requests equal to the number of CPU or amount of memory on your node then the scheduler will not be able to schedule the Db2 pod because not 100% of the worker nodes' resource will be available to pod on that node, even if there's only a single pod on it. Db2 is sensitive to both CPU and memory issues, particularly memory, we recommend setting requests and limits to the same values, ensuring the scheduler always reserves the resources that Db2 expects to be available to it.","title":"Resource Request Variables"},{"location":"roles/db2/#db2_cpu_requests","text":"Define the Kubernetes CPU request for the Db2 pod. Optional Environment Variable: DB2_CPU_REQUESTS Default: 4000m","title":"db2_cpu_requests"},{"location":"roles/db2/#db2_cpu_limits","text":"Define the Kubernetes CPU limit for the Db2 pod. Optional Environment Variable: DB2_CPU_LIMITS Default: 6000m","title":"db2_cpu_limits"},{"location":"roles/db2/#db2_memory_requests","text":"Define the Kubernetes memory request for the Db2 pod. Optional Environment Variable: DB2_MEMORY_REQUESTS Default: 8Gi","title":"db2_memory_requests"},{"location":"roles/db2/#db2_memory_limits","text":"Define the Kubernetes memory limit for the Db2 pod. Optional Environment Variable: DB2_MEMORY_LIMITS Default: 16Gi","title":"db2_memory_limits"},{"location":"roles/db2/#node-label-affinity-variables","text":"Specify both db2_affinity_key and db2_affinity_value to configure requiredDuringSchedulingIgnoredDuringExecution affinity with appropriately labelled nodes.","title":"Node Label Affinity Variables"},{"location":"roles/db2/#db2_affinity_key","text":"Specify the key of a node label to declare affinity with. Optional Environment Variable: DB2_AFFINITY_KEY Default: None","title":"db2_affinity_key"},{"location":"roles/db2/#db2_affinity_value","text":"Specify the value of a node label to declare affinity with. Optional Environment Variable: DB2_AFFINITY_VALUE Default: None","title":"db2_affinity_value"},{"location":"roles/db2/#node-taint-toleration-variables","text":"Specify db2_tolerate_key , db2_tolerate_value , and db2_tolerate_effect to configure a toleration policy to allow the db2 instance to be scheduled on nodes with the specified taint.","title":"Node Taint Toleration Variables"},{"location":"roles/db2/#db2_tolerate_key","text":"Specify the key of the taint that is to be tolerated. Optional Environment Variable: DB2_TOLERATE_KEY Default: None","title":"db2_tolerate_key"},{"location":"roles/db2/#db2_tolerate_value","text":"Specify the value of the taint that is to be tolerated. Optional Environment Variable: DB2_TOLERATE_VALUE Default: None","title":"db2_tolerate_value"},{"location":"roles/db2/#db2_tolerate_effect","text":"Specify the type of taint effect that will be tolerated ( NoSchedule , PreferNoSchedule , or NoExecute ). Optional Environment Variable: DB2_TOLERATE_EFFECT Default: None","title":"db2_tolerate_effect"},{"location":"roles/db2/#role-variables-db2ucluster-database-configuration-settings","text":"The following variables will overwrite DB2UCluster default properties for the DB2 configuration sections: spec.environment.database.dbConfig spec.environment.instance.dbmConfig spec.environment.instance.registry","title":"Role Variables - DB2UCluster Database Configuration Settings"},{"location":"roles/db2/#db2_database_db_config","text":"Overwrites the db2ucluster database configuration settings under spec.environment.database.dbConfig section. Optional Environment Variable: DB2_DATABASE_DB_CONFIG Default: None","title":"db2_database_db_config"},{"location":"roles/db2/#db2_instance_dbm_config","text":"Overwrites the db2ucluster instance database configuration settings under spec.environment.instance.dbmConfig section. Important Do not set instance_memory . The Db2 engine does not know Db2 is running inside a container, setting dbmConfig.INSTANCE_MEMORY: automatic will cause it to read the cgroups of the node and potentially go beyond the pod memory limit. Db2U has logic built in to use a normalized percentage that takes into account the memory limit and free memory of the node. Optional Environment Variable: DB2_INSTANCE_DBM_CONFIG Default: None","title":"db2_instance_dbm_config"},{"location":"roles/db2/#db2_instance_registry","text":"Overwrites the db2ucluster instance database configuration settings under spec.environment.instance.registry section. You can define parameters to be included in this section using semicolon separated values. Optional Environment Variable: DB2_INSTANCE_REGISTRY Default: None","title":"db2_instance_registry"},{"location":"roles/db2/#mpp-system-variables","text":"Warning Do not use these variables if you intend to use the Db2 instance with IBM Maximo Application Suite; no MAS application supports Db2 MPP","title":"MPP System Variables"},{"location":"roles/db2/#db2_mln_count","text":"The number of logical nodes (i.e. database partitions to create). Note: ensure that the application using this Db2 can support Db2 MPP (which is created when DB2_MLN_COUNT is greater than 1). Optional Environment Variable: 'DB2_MLN_COUNT Default: 1","title":"db2_mln_count"},{"location":"roles/db2/#db2_num_pods","text":"The number of Db2 pods to create in the instance. Note that db2_num_pods must be less than or equal to db2_mln_count . A single db2u pod can contain multiple logical nodes. So be sure to avoid specifying a large number for db2_mln_count while specifying a small number for db2_num_pods . If in doubt, make db2_mln_count = db2_num_pods . For more information refer to the Db2 documentation . Optional Environment Variable: DB2_NUM_PODS Default: 1","title":"db2_num_pods"},{"location":"roles/db2/#mas-configuration-variables","text":"","title":"MAS Configuration Variables"},{"location":"roles/db2/#mas_instance_id","text":"Providing this and mas_config_dir will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/db2/#mas_config_dir","text":"Providing this and mas_instance_id will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_CONFIG_DIR Default: None","title":"mas_config_dir"},{"location":"roles/db2/#mas_config_scope","text":"Supported values are system , ws , app , or wsapp , this is only used when both mas_config_dir and mas_instance_id are set. Optional Environment Variable: MAS_CONFIG_SCOPE Default: system","title":"mas_config_scope"},{"location":"roles/db2/#mas_workspace_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Optional Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/db2/#mas_application_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either app or wsapp Optional Environment Variable: 'MAS_APP_ID Default: None","title":"mas_application_id"},{"location":"roles/db2/#role-variables-backup-and-restore","text":"","title":"Role Variables - Backup and Restore"},{"location":"roles/db2/#masbr_confirm_cluster","text":"Set true or false to indicate the role whether to confirm the currently connected cluster before running the backup or restore job. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false","title":"masbr_confirm_cluster"},{"location":"roles/db2/#masbr_copy_timeout_sec","text":"Set the transfer files timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours)","title":"masbr_copy_timeout_sec"},{"location":"roles/db2/#masbr_job_timezone","text":"Set the time zone for creating scheduled backup job. If not set a value for this variable, this role will use UTC time zone when creating a CronJob for running scheduled backup job. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None","title":"masbr_job_timezone"},{"location":"roles/db2/#masbr_storage_local_folder","text":"Set local path to save the backup files. Required Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None","title":"masbr_storage_local_folder"},{"location":"roles/db2/#masbr_backup_type","text":"Set full or incr to indicate the role to create a full backup or incremental backup. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full","title":"masbr_backup_type"},{"location":"roles/db2/#masbr_backup_from_version","text":"Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). This variable is only valid when MASBR_BACKUP_TYPE=incr . If not set a value for this variable, this role will try to find the latest full backup version from the specified storage location. Optional Environment Variable: MASBR_BACKUP_FROM_VERSION Default: None","title":"masbr_backup_from_version"},{"location":"roles/db2/#masbr_backup_schedule","text":"Set Cron expression to create a scheduled backup. If not set a value for this varialbe, this role will create an on-demand backup. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None","title":"masbr_backup_schedule"},{"location":"roles/db2/#masbr_restore_from_version","text":"Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) Required only when DB2_ACTION=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None","title":"masbr_restore_from_version"},{"location":"roles/db2/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/db2/#install-db2","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxxx # Configuration for the Db2 cluster db2_instance_name: db2u-db01 db2_meta_storage_class: \"ibmc-file-gold\" db2_data_storage_class: \"ibmc-block-gold\" db2_backup_storage_class: \"ibmc-file-gold\" db2_logs_storage_class: \"ibmc-block-gold\" db2_temp_storage_class: \"ibmc-block-gold\" # Create the MAS JdbcCfg & Secret resource definitions mas_instance_id: inst1 mas_config_dir: /home/david/masconfig roles: - ibm.mas_devops.db2","title":"Install Db2"},{"location":"roles/db2/#backup-db2","text":"- hosts: localhost any_errors_fatal: true vars: db2_action: backup db2_instance_name: db2u-db01 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.db2","title":"Backup Db2"},{"location":"roles/db2/#restore-db2","text":"- hosts: localhost any_errors_fatal: true vars: db2_action: restore db2_instance_name: db2u-db01 masbr_restore_from_version: 20240621021316 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.db2","title":"Restore Db2"},{"location":"roles/db2/#run-role-playbook","text":"export IBM_ENTITLEMENT_KEY=xxxxx export DB2_INSTANCE_NAME=db2u-db01 export DB2_META_STORAGE_CLASS=ibmc-file-gold export DB2_DATA_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/masconfig ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/db2/#license","text":"EPL-2.0","title":"License"},{"location":"roles/dro/","text":"dro [Data Reporter Operator] \u00a4 DRO will be supported on the following MAS versions - MAS 8.10.6 + - MAS 8.11.2 + - MAS 9.0 + Installs Data Reporter Operator in the redhat-marketplace namespace. If mas_instance_id and the others associated parameters are provided then the role will also generate a configuration file that can be directly applied to IBM Maximo Application Suite. Role Variables - Installation \u00a4 dro_action \u00a4 Inform the role whether to perform an install or uninstall of Data Reporter Operator. Supported values are install and uninstall . Optional Environment Variable: DRO_ACTION Default: install dro_namespace \u00a4 DRO can be installed on a different namespace, on certain type of OCP clusters where redhat* namespaces have restricted access, User can configure and install DRO on a custom namespace of their choosing by supplying a name using DRO_NAMESPACE Environment Variable: DRO_NAMESPACE Default Value: redhat-marketplace ibm_entitlement_key \u00a4 Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None dro_storage_class \u00a4 Required. Storage class where DRO will be installed. MAS ansible playbooks will automatically try to determine a RWO (Read Write Once) storage class from a cluster if DRO_STORAGE_CLASS is not supplied. If a cluster is setup with a customized storage solution, please provide a valid RWO storage class name using DRO_STORAGE_CLASS. Optional Environment Variable: DRO_STORAGE_CLASS Default Value: None Note : The storage class must support the RWO(Read Write Once) access Mode Role Variables - BASCfg Generation \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the BasCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated BasCfg resource definition. This can be used to manually configure a MAS instance to connect to BAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None dro_endpoint_url \u00a4 DRO url from ibm-data-reporter route found in redhat-marketplace namespace, this variable is needed if you wish to connect to an existing DRO instance. Optional Environment Variable: DRO_ENDPOINT_URL Default Value: None dro_api_key \u00a4 DRO api_key is a token obtained from ibm-data-reporter-operator-api-token secret found in redhat-marketplace namespace, this variable is needed if you wish to connect to an existing DRO instance. Optional Environment Variable: DRO_APIKEY Default Value: None dro_crt_path \u00a4 DRO uses default OCP cluster ingress certificates. these can be obtained from either router-certs-default secret found in openshift-ingress namespace or trustedCA config map found in openshift-config namespace, copy the contents of tls.crt into a .pem file and provide the filepath of the .pem file to DRO_CERTIFICATE_PATH , this variable is needed if you wish to connect to an existing DRO instance. Optional Environment Variable: DRO_CERTIFICATE_PATH Default Value: None dro_contact.email \u00a4 Sets the Contact e-mail address used by the MAS instance's DRO configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: DRO_CONTACT_EMAIL Default Value: None dro_contact.first_name \u00a4 Sets the Contact first name used by the MAS instance's DRO configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: DRO_CONTACT_FIRSTNAME Default Value: None dro_contact.last_name \u00a4 Sets the Contact last name used by the MAS instance's DRO configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: DRO_CONTACT_LASTNAME Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None mas_pod_templates_dir \u00a4 Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-bascfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the BasCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None include_cluster_ingress_cert_chain \u00a4 Optional. When set to True , includes the complete certificates chain in the generated MAS configuration, when a trusted certificate authority is found in your cluster's ingress. Optional Environment Variable: INCLUDE_CLUSTER_INGRESS_CERT_CHAIN Default: False Example Playbook \u00a4 Install in-cluster and generate MAS configuration \u00a4 To install DRO export IBM_ENTITLEMENT_KEY=<valid ibm entitlement key> export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export DRO_ACTION=install export MAS_CONFIG_DIR=<valid local path to the config folder> export MAS_INSTANCE_ID=<valid mas instance id> export DRO_STORAGE_CLASS=<valid storage class name> export ROLE_NAME='dro' export DRO_NAMESPACE=ibm-dro ansible-playbook playbooks/run_role.yml To connect to an existing DRO export DRO_ENDPOINT_URL=<valid DRO url> export DRO_APIKEY=<valid DRO apikey> export DRO_CERTIFICATE_PATH=/temp/cert.pem export IBM_ENTITLEMENT_KEY=<valid ibm entitlement key> export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export MAS_CONFIG_DIR=<valid local path to the config folder> export MAS_INSTANCE_ID=<valid mas instance id> export DRO_ACTION=install export ROLE_NAME='dro' ansible-playbook playbooks/run_role.yml To uninstall DRO export DRO_ACTION=uninstall export ROLE_NAME='dro' export DRO_NAMESPACE=ibm-dro ansible-playbook playbooks/run_role.yml - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig dro_contact: email: 'john@email.com' first_name: 'john' last_name: 'winter' roles: - ibm.mas_devops.dro License \u00a4 EPL-2.0","title":"dro"},{"location":"roles/dro/#dro-data-reporter-operator","text":"DRO will be supported on the following MAS versions - MAS 8.10.6 + - MAS 8.11.2 + - MAS 9.0 + Installs Data Reporter Operator in the redhat-marketplace namespace. If mas_instance_id and the others associated parameters are provided then the role will also generate a configuration file that can be directly applied to IBM Maximo Application Suite.","title":"dro [Data Reporter Operator]"},{"location":"roles/dro/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/dro/#dro_action","text":"Inform the role whether to perform an install or uninstall of Data Reporter Operator. Supported values are install and uninstall . Optional Environment Variable: DRO_ACTION Default: install","title":"dro_action"},{"location":"roles/dro/#dro_namespace","text":"DRO can be installed on a different namespace, on certain type of OCP clusters where redhat* namespaces have restricted access, User can configure and install DRO on a custom namespace of their choosing by supplying a name using DRO_NAMESPACE Environment Variable: DRO_NAMESPACE Default Value: redhat-marketplace","title":"dro_namespace"},{"location":"roles/dro/#ibm_entitlement_key","text":"Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None","title":"ibm_entitlement_key"},{"location":"roles/dro/#dro_storage_class","text":"Required. Storage class where DRO will be installed. MAS ansible playbooks will automatically try to determine a RWO (Read Write Once) storage class from a cluster if DRO_STORAGE_CLASS is not supplied. If a cluster is setup with a customized storage solution, please provide a valid RWO storage class name using DRO_STORAGE_CLASS. Optional Environment Variable: DRO_STORAGE_CLASS Default Value: None Note : The storage class must support the RWO(Read Write Once) access Mode","title":"dro_storage_class"},{"location":"roles/dro/#role-variables-bascfg-generation","text":"","title":"Role Variables - BASCfg Generation"},{"location":"roles/dro/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the BasCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/dro/#mas_config_dir","text":"Local directory to save the generated BasCfg resource definition. This can be used to manually configure a MAS instance to connect to BAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/dro/#dro_endpoint_url","text":"DRO url from ibm-data-reporter route found in redhat-marketplace namespace, this variable is needed if you wish to connect to an existing DRO instance. Optional Environment Variable: DRO_ENDPOINT_URL Default Value: None","title":"dro_endpoint_url"},{"location":"roles/dro/#dro_api_key","text":"DRO api_key is a token obtained from ibm-data-reporter-operator-api-token secret found in redhat-marketplace namespace, this variable is needed if you wish to connect to an existing DRO instance. Optional Environment Variable: DRO_APIKEY Default Value: None","title":"dro_api_key"},{"location":"roles/dro/#dro_crt_path","text":"DRO uses default OCP cluster ingress certificates. these can be obtained from either router-certs-default secret found in openshift-ingress namespace or trustedCA config map found in openshift-config namespace, copy the contents of tls.crt into a .pem file and provide the filepath of the .pem file to DRO_CERTIFICATE_PATH , this variable is needed if you wish to connect to an existing DRO instance. Optional Environment Variable: DRO_CERTIFICATE_PATH Default Value: None","title":"dro_crt_path"},{"location":"roles/dro/#dro_contactemail","text":"Sets the Contact e-mail address used by the MAS instance's DRO configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: DRO_CONTACT_EMAIL Default Value: None","title":"dro_contact.email"},{"location":"roles/dro/#dro_contactfirst_name","text":"Sets the Contact first name used by the MAS instance's DRO configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: DRO_CONTACT_FIRSTNAME Default Value: None","title":"dro_contact.first_name"},{"location":"roles/dro/#dro_contactlast_name","text":"Sets the Contact last name used by the MAS instance's DRO configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: DRO_CONTACT_LASTNAME Default Value: None","title":"dro_contact.last_name"},{"location":"roles/dro/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/dro/#mas_pod_templates_dir","text":"Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-bascfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the BasCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None","title":"mas_pod_templates_dir"},{"location":"roles/dro/#include_cluster_ingress_cert_chain","text":"Optional. When set to True , includes the complete certificates chain in the generated MAS configuration, when a trusted certificate authority is found in your cluster's ingress. Optional Environment Variable: INCLUDE_CLUSTER_INGRESS_CERT_CHAIN Default: False","title":"include_cluster_ingress_cert_chain"},{"location":"roles/dro/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/dro/#install-in-cluster-and-generate-mas-configuration","text":"To install DRO export IBM_ENTITLEMENT_KEY=<valid ibm entitlement key> export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export DRO_ACTION=install export MAS_CONFIG_DIR=<valid local path to the config folder> export MAS_INSTANCE_ID=<valid mas instance id> export DRO_STORAGE_CLASS=<valid storage class name> export ROLE_NAME='dro' export DRO_NAMESPACE=ibm-dro ansible-playbook playbooks/run_role.yml To connect to an existing DRO export DRO_ENDPOINT_URL=<valid DRO url> export DRO_APIKEY=<valid DRO apikey> export DRO_CERTIFICATE_PATH=/temp/cert.pem export IBM_ENTITLEMENT_KEY=<valid ibm entitlement key> export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export MAS_CONFIG_DIR=<valid local path to the config folder> export MAS_INSTANCE_ID=<valid mas instance id> export DRO_ACTION=install export ROLE_NAME='dro' ansible-playbook playbooks/run_role.yml To uninstall DRO export DRO_ACTION=uninstall export ROLE_NAME='dro' export DRO_NAMESPACE=ibm-dro ansible-playbook playbooks/run_role.yml - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig dro_contact: email: 'john@email.com' first_name: 'john' last_name: 'winter' roles: - ibm.mas_devops.dro","title":"Install in-cluster and generate MAS configuration"},{"location":"roles/dro/#license","text":"EPL-2.0","title":"License"},{"location":"roles/eck/","text":"eck \u00a4 This role provides support to install Elastic Cloud on Kubernetes (ECK). Elasticsearch is configured with a default user named elastic , you can obtain the password for this user by running the following command: oc -n eck get secret mas-es-elastic-user -o go-template='{{.data.elastic | base64decode}}'; echo Role Variables \u00a4 eck_action \u00a4 Action to perform on ECK installation. Optional Environment Variable: ECK_ACTION Default: install Purpose : Specifies the action to perform on the Elastic Cloud on Kubernetes (ECK) deployment. When to use : - Use install (default and only supported value) to deploy ECK - Future versions may support additional actions Valid values : install Impact : Determines the operation performed on ECK. Currently only installation is supported. Related variables : - eck_enable_elasticsearch : Enable Elasticsearch component - eck_enable_kibana : Enable Kibana component - eck_enable_logstash : Enable Logstash component - eck_enable_filebeat : Enable Filebeat component Note : This role installs ECK operator and optionally deploys Elasticsearch, Kibana, Logstash, and Filebeat components based on enable flags. eck_enable_elasticsearch \u00a4 Enable Elasticsearch deployment. Optional Environment Variable: ECK_ENABLE_ELASTICSEARCH Default: false Purpose : Controls whether Elasticsearch is deployed as part of the ECK installation. When to use : - Set to true to deploy Elasticsearch for log storage and search - Leave as false if using external Elasticsearch or not needed - Required for Kibana and Logstash functionality Valid values : true , false Impact : - true : Deploys Elasticsearch cluster in ECK namespace - false : Skips Elasticsearch deployment Related variables : - eck_enable_kibana : Kibana requires Elasticsearch - eck_enable_logstash : Logstash can send to local or remote Elasticsearch - es_domain : Custom domain for Elasticsearch access Note : Default user elastic is created. Retrieve password with: oc -n eck get secret mas-es-elastic-user -o go-template='{{.data.elastic | base64decode}}' eck_enable_kibana \u00a4 Enable Kibana deployment. Optional Environment Variable: ECK_ENABLE_KIBANA Default: false Purpose : Controls whether Kibana is deployed as part of the ECK installation for log visualization and analysis. When to use : - Set to true to deploy Kibana for log visualization - Leave as false if not using Kibana UI - Requires Elasticsearch to be enabled Valid values : true , false Impact : - true : Deploys Kibana instance connected to Elasticsearch - false : Skips Kibana deployment Related variables : - eck_enable_elasticsearch : Must be true for Kibana to function - kibana_domain : Custom domain for Kibana access - letsencrypt_email : For LetsEncrypt certificate Note : Kibana requires Elasticsearch. Ensure eck_enable_elasticsearch=true when enabling Kibana. eck_enable_logstash \u00a4 Enable Logstash deployment. Optional Environment Variable: ECK_ENABLE_LOGSTASH Default: false Purpose : Controls whether Logstash is deployed as part of the ECK installation for log processing and forwarding. When to use : - Set to true to deploy Logstash for log processing - Leave as false if not using Logstash pipeline - Can send logs to local or remote Elasticsearch Valid values : true , false Impact : - true : Deploys Logstash instance for log processing - false : Skips Logstash deployment Related variables : - eck_remote_es_hosts : Remote Elasticsearch hosts for log forwarding - eck_remote_es_username : Username for remote Elasticsearch - eck_remote_es_password : Password for remote Elasticsearch Note : When remote Elasticsearch variables are set, Logstash forwards logs to the remote instance. Otherwise, logs are sent to local Elasticsearch (if enabled). eck_enable_filebeat \u00a4 Enable Filebeat deployment. Optional Environment Variable: ECK_ENABLE_FILEBEAT Default: false Purpose : Controls whether Filebeat is deployed as part of the ECK installation for log collection from cluster nodes. When to use : - Set to true to deploy Filebeat for log collection - Leave as false if not collecting node logs - Filebeat collects logs from Kubernetes nodes Valid values : true , false Impact : - true : Deploys Filebeat DaemonSet for log collection - false : Skips Filebeat deployment Related variables : - eck_enable_elasticsearch : Filebeat sends logs to Elasticsearch - eck_enable_logstash : Alternative log processing pipeline Note : Filebeat runs as a DaemonSet on cluster nodes to collect logs and forward them to Elasticsearch or Logstash. Role Variables - Remote Elasticsearch \u00a4 When eck_remote_es_hosts , eck_remote_es_username , and eck_remote_es_password are all set, and eck_enable_logstash is true , the Logstash server will be configured to send log messages to the remote Elasticsearch instance defined. eck_remote_es_hosts \u00a4 Remote Elasticsearch host list. Optional (required for remote Elasticsearch) Environment Variable: ECK_REMOTE_ES_HOSTS Default: None Purpose : Specifies one or more remote Elasticsearch hosts for Logstash to forward logs to instead of local Elasticsearch. When to use : - Set when forwarding logs to external Elasticsearch cluster - Required along with eck_remote_es_username and eck_remote_es_password - Only applies when eck_enable_logstash=true Valid values : Comma-separated list of Elasticsearch hosts (e.g., https://es1.example.com:9200,https://es2.example.com:9200 ) Impact : When set with credentials, Logstash forwards logs to remote Elasticsearch instead of local instance. Related variables : - eck_remote_es_username : Username for remote Elasticsearch - eck_remote_es_password : Password for remote Elasticsearch - eck_enable_logstash : Must be true for remote forwarding Note : All three remote Elasticsearch variables must be set together for remote forwarding to work. eck_remote_es_username \u00a4 Remote Elasticsearch username. Optional (required for remote Elasticsearch) Environment Variable: ECK_REMOTE_ES_USERNAME Default: None Purpose : Specifies the username for authenticating with remote Elasticsearch when forwarding logs via Logstash. When to use : - Set when forwarding logs to external Elasticsearch cluster - Required along with eck_remote_es_hosts and eck_remote_es_password - Only applies when eck_enable_logstash=true Valid values : Valid Elasticsearch username string Impact : Used by Logstash to authenticate with remote Elasticsearch. Without valid credentials, log forwarding will fail. Related variables : - eck_remote_es_hosts : Remote Elasticsearch hosts - eck_remote_es_password : Password for authentication - eck_enable_logstash : Must be true for remote forwarding Note : SECURITY - All three remote Elasticsearch variables must be set together. Credentials are stored securely in Kubernetes secrets. eck_remote_es_password \u00a4 Remote Elasticsearch password. Optional (required for remote Elasticsearch) Environment Variable: ECK_REMOTE_ES_PASSWORD Default: None Purpose : Specifies the password for authenticating with remote Elasticsearch when forwarding logs via Logstash. When to use : - Set when forwarding logs to external Elasticsearch cluster - Required along with eck_remote_es_hosts and eck_remote_es_username - Only applies when eck_enable_logstash=true Valid values : Valid Elasticsearch password string Impact : Used by Logstash to authenticate with remote Elasticsearch. Without valid credentials, log forwarding will fail. Related variables : - eck_remote_es_hosts : Remote Elasticsearch hosts - eck_remote_es_username : Username for authentication - eck_enable_logstash : Must be true for remote forwarding Note : SECURITY - Store password securely. All three remote Elasticsearch variables must be set together. Credentials are stored in Kubernetes secrets. Role Variables - Domains and Certificates \u00a4 Elasticsearch and Kibana can be configured with a custom domain and a certificate signed by LetsEncrypt . es_domain \u00a4 Custom domain for Elasticsearch access. Optional Environment Variable: ECK_ELASTICSEARCH_DOMAIN Default: None Purpose : Specifies a custom domain for accessing Elasticsearch, enabling external access with proper DNS routing. When to use : - Set when external access to Elasticsearch is required - Must be routable to the target OCP cluster - Used with LetsEncrypt for automatic certificate generation Valid values : Valid domain name routable to the cluster (e.g., es.example.com ) Impact : When set, creates a route with custom domain for Elasticsearch access. Without it, uses default cluster route. Related variables : - letsencrypt_email : Required for automatic certificate generation - eck_enable_elasticsearch : Must be true Note : Domain must be routable to the cluster. When combined with letsencrypt_email , automatically provisions LetsEncrypt certificate using HTTP solver. kibana_domain \u00a4 Custom domain for Kibana access. Optional Environment Variable: ECK_KIBANA_DOMAIN Default: None Purpose : Specifies a custom domain for accessing Kibana UI, enabling external access with proper DNS routing. When to use : - Set when external access to Kibana is required - Must be routable to the target OCP cluster - Used with LetsEncrypt for automatic certificate generation Valid values : Valid domain name routable to the cluster (e.g., kibana.example.com ) Impact : When set, creates a route with custom domain for Kibana access. Without it, uses default cluster route. Related variables : - letsencrypt_email : Required for automatic certificate generation - eck_enable_kibana : Must be true Note : Domain must be routable to the cluster. When combined with letsencrypt_email , automatically provisions LetsEncrypt certificate using HTTP solver. letsencrypt_email \u00a4 Email for LetsEncrypt certificate registration. Optional Environment Variable: LETSENCRYPT_EMAIL Default: None Purpose : Specifies the email address for registering LetsEncrypt certificates when using custom domains for Elasticsearch or Kibana. When to use : - Set when using custom domains ( es_domain or kibana_domain ) - Required for automatic LetsEncrypt certificate provisioning - Email receives certificate expiration notifications Valid values : Valid email address Impact : When set with custom domains, automatically configures LetsEncrypt Issuer and provisions certificates using HTTP solver via Cert-Manager. Related variables : - es_domain : Elasticsearch custom domain - kibana_domain : Kibana custom domain Note : Requires Cert-Manager to be installed in the cluster. The Issuer uses LetsEncrypt production environment with HTTP-01 challenge solver. Email receives important certificate notifications. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: eck_action: install eck_enable_elasticsearch: true eck_enable_kibana: true eck_enable_logstash: true roles: - ibm.mas_devops.eck License \u00a4 EPL-2.0","title":"eck"},{"location":"roles/eck/#eck","text":"This role provides support to install Elastic Cloud on Kubernetes (ECK). Elasticsearch is configured with a default user named elastic , you can obtain the password for this user by running the following command: oc -n eck get secret mas-es-elastic-user -o go-template='{{.data.elastic | base64decode}}'; echo","title":"eck"},{"location":"roles/eck/#role-variables","text":"","title":"Role Variables"},{"location":"roles/eck/#eck_action","text":"Action to perform on ECK installation. Optional Environment Variable: ECK_ACTION Default: install Purpose : Specifies the action to perform on the Elastic Cloud on Kubernetes (ECK) deployment. When to use : - Use install (default and only supported value) to deploy ECK - Future versions may support additional actions Valid values : install Impact : Determines the operation performed on ECK. Currently only installation is supported. Related variables : - eck_enable_elasticsearch : Enable Elasticsearch component - eck_enable_kibana : Enable Kibana component - eck_enable_logstash : Enable Logstash component - eck_enable_filebeat : Enable Filebeat component Note : This role installs ECK operator and optionally deploys Elasticsearch, Kibana, Logstash, and Filebeat components based on enable flags.","title":"eck_action"},{"location":"roles/eck/#eck_enable_elasticsearch","text":"Enable Elasticsearch deployment. Optional Environment Variable: ECK_ENABLE_ELASTICSEARCH Default: false Purpose : Controls whether Elasticsearch is deployed as part of the ECK installation. When to use : - Set to true to deploy Elasticsearch for log storage and search - Leave as false if using external Elasticsearch or not needed - Required for Kibana and Logstash functionality Valid values : true , false Impact : - true : Deploys Elasticsearch cluster in ECK namespace - false : Skips Elasticsearch deployment Related variables : - eck_enable_kibana : Kibana requires Elasticsearch - eck_enable_logstash : Logstash can send to local or remote Elasticsearch - es_domain : Custom domain for Elasticsearch access Note : Default user elastic is created. Retrieve password with: oc -n eck get secret mas-es-elastic-user -o go-template='{{.data.elastic | base64decode}}'","title":"eck_enable_elasticsearch"},{"location":"roles/eck/#eck_enable_kibana","text":"Enable Kibana deployment. Optional Environment Variable: ECK_ENABLE_KIBANA Default: false Purpose : Controls whether Kibana is deployed as part of the ECK installation for log visualization and analysis. When to use : - Set to true to deploy Kibana for log visualization - Leave as false if not using Kibana UI - Requires Elasticsearch to be enabled Valid values : true , false Impact : - true : Deploys Kibana instance connected to Elasticsearch - false : Skips Kibana deployment Related variables : - eck_enable_elasticsearch : Must be true for Kibana to function - kibana_domain : Custom domain for Kibana access - letsencrypt_email : For LetsEncrypt certificate Note : Kibana requires Elasticsearch. Ensure eck_enable_elasticsearch=true when enabling Kibana.","title":"eck_enable_kibana"},{"location":"roles/eck/#eck_enable_logstash","text":"Enable Logstash deployment. Optional Environment Variable: ECK_ENABLE_LOGSTASH Default: false Purpose : Controls whether Logstash is deployed as part of the ECK installation for log processing and forwarding. When to use : - Set to true to deploy Logstash for log processing - Leave as false if not using Logstash pipeline - Can send logs to local or remote Elasticsearch Valid values : true , false Impact : - true : Deploys Logstash instance for log processing - false : Skips Logstash deployment Related variables : - eck_remote_es_hosts : Remote Elasticsearch hosts for log forwarding - eck_remote_es_username : Username for remote Elasticsearch - eck_remote_es_password : Password for remote Elasticsearch Note : When remote Elasticsearch variables are set, Logstash forwards logs to the remote instance. Otherwise, logs are sent to local Elasticsearch (if enabled).","title":"eck_enable_logstash"},{"location":"roles/eck/#eck_enable_filebeat","text":"Enable Filebeat deployment. Optional Environment Variable: ECK_ENABLE_FILEBEAT Default: false Purpose : Controls whether Filebeat is deployed as part of the ECK installation for log collection from cluster nodes. When to use : - Set to true to deploy Filebeat for log collection - Leave as false if not collecting node logs - Filebeat collects logs from Kubernetes nodes Valid values : true , false Impact : - true : Deploys Filebeat DaemonSet for log collection - false : Skips Filebeat deployment Related variables : - eck_enable_elasticsearch : Filebeat sends logs to Elasticsearch - eck_enable_logstash : Alternative log processing pipeline Note : Filebeat runs as a DaemonSet on cluster nodes to collect logs and forward them to Elasticsearch or Logstash.","title":"eck_enable_filebeat"},{"location":"roles/eck/#role-variables-remote-elasticsearch","text":"When eck_remote_es_hosts , eck_remote_es_username , and eck_remote_es_password are all set, and eck_enable_logstash is true , the Logstash server will be configured to send log messages to the remote Elasticsearch instance defined.","title":"Role Variables - Remote Elasticsearch"},{"location":"roles/eck/#eck_remote_es_hosts","text":"Remote Elasticsearch host list. Optional (required for remote Elasticsearch) Environment Variable: ECK_REMOTE_ES_HOSTS Default: None Purpose : Specifies one or more remote Elasticsearch hosts for Logstash to forward logs to instead of local Elasticsearch. When to use : - Set when forwarding logs to external Elasticsearch cluster - Required along with eck_remote_es_username and eck_remote_es_password - Only applies when eck_enable_logstash=true Valid values : Comma-separated list of Elasticsearch hosts (e.g., https://es1.example.com:9200,https://es2.example.com:9200 ) Impact : When set with credentials, Logstash forwards logs to remote Elasticsearch instead of local instance. Related variables : - eck_remote_es_username : Username for remote Elasticsearch - eck_remote_es_password : Password for remote Elasticsearch - eck_enable_logstash : Must be true for remote forwarding Note : All three remote Elasticsearch variables must be set together for remote forwarding to work.","title":"eck_remote_es_hosts"},{"location":"roles/eck/#eck_remote_es_username","text":"Remote Elasticsearch username. Optional (required for remote Elasticsearch) Environment Variable: ECK_REMOTE_ES_USERNAME Default: None Purpose : Specifies the username for authenticating with remote Elasticsearch when forwarding logs via Logstash. When to use : - Set when forwarding logs to external Elasticsearch cluster - Required along with eck_remote_es_hosts and eck_remote_es_password - Only applies when eck_enable_logstash=true Valid values : Valid Elasticsearch username string Impact : Used by Logstash to authenticate with remote Elasticsearch. Without valid credentials, log forwarding will fail. Related variables : - eck_remote_es_hosts : Remote Elasticsearch hosts - eck_remote_es_password : Password for authentication - eck_enable_logstash : Must be true for remote forwarding Note : SECURITY - All three remote Elasticsearch variables must be set together. Credentials are stored securely in Kubernetes secrets.","title":"eck_remote_es_username"},{"location":"roles/eck/#eck_remote_es_password","text":"Remote Elasticsearch password. Optional (required for remote Elasticsearch) Environment Variable: ECK_REMOTE_ES_PASSWORD Default: None Purpose : Specifies the password for authenticating with remote Elasticsearch when forwarding logs via Logstash. When to use : - Set when forwarding logs to external Elasticsearch cluster - Required along with eck_remote_es_hosts and eck_remote_es_username - Only applies when eck_enable_logstash=true Valid values : Valid Elasticsearch password string Impact : Used by Logstash to authenticate with remote Elasticsearch. Without valid credentials, log forwarding will fail. Related variables : - eck_remote_es_hosts : Remote Elasticsearch hosts - eck_remote_es_username : Username for authentication - eck_enable_logstash : Must be true for remote forwarding Note : SECURITY - Store password securely. All three remote Elasticsearch variables must be set together. Credentials are stored in Kubernetes secrets.","title":"eck_remote_es_password"},{"location":"roles/eck/#role-variables-domains-and-certificates","text":"Elasticsearch and Kibana can be configured with a custom domain and a certificate signed by LetsEncrypt .","title":"Role Variables - Domains and Certificates"},{"location":"roles/eck/#es_domain","text":"Custom domain for Elasticsearch access. Optional Environment Variable: ECK_ELASTICSEARCH_DOMAIN Default: None Purpose : Specifies a custom domain for accessing Elasticsearch, enabling external access with proper DNS routing. When to use : - Set when external access to Elasticsearch is required - Must be routable to the target OCP cluster - Used with LetsEncrypt for automatic certificate generation Valid values : Valid domain name routable to the cluster (e.g., es.example.com ) Impact : When set, creates a route with custom domain for Elasticsearch access. Without it, uses default cluster route. Related variables : - letsencrypt_email : Required for automatic certificate generation - eck_enable_elasticsearch : Must be true Note : Domain must be routable to the cluster. When combined with letsencrypt_email , automatically provisions LetsEncrypt certificate using HTTP solver.","title":"es_domain"},{"location":"roles/eck/#kibana_domain","text":"Custom domain for Kibana access. Optional Environment Variable: ECK_KIBANA_DOMAIN Default: None Purpose : Specifies a custom domain for accessing Kibana UI, enabling external access with proper DNS routing. When to use : - Set when external access to Kibana is required - Must be routable to the target OCP cluster - Used with LetsEncrypt for automatic certificate generation Valid values : Valid domain name routable to the cluster (e.g., kibana.example.com ) Impact : When set, creates a route with custom domain for Kibana access. Without it, uses default cluster route. Related variables : - letsencrypt_email : Required for automatic certificate generation - eck_enable_kibana : Must be true Note : Domain must be routable to the cluster. When combined with letsencrypt_email , automatically provisions LetsEncrypt certificate using HTTP solver.","title":"kibana_domain"},{"location":"roles/eck/#letsencrypt_email","text":"Email for LetsEncrypt certificate registration. Optional Environment Variable: LETSENCRYPT_EMAIL Default: None Purpose : Specifies the email address for registering LetsEncrypt certificates when using custom domains for Elasticsearch or Kibana. When to use : - Set when using custom domains ( es_domain or kibana_domain ) - Required for automatic LetsEncrypt certificate provisioning - Email receives certificate expiration notifications Valid values : Valid email address Impact : When set with custom domains, automatically configures LetsEncrypt Issuer and provisions certificates using HTTP solver via Cert-Manager. Related variables : - es_domain : Elasticsearch custom domain - kibana_domain : Kibana custom domain Note : Requires Cert-Manager to be installed in the cluster. The Issuer uses LetsEncrypt production environment with HTTP-01 challenge solver. Email receives important certificate notifications.","title":"letsencrypt_email"},{"location":"roles/eck/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: eck_action: install eck_enable_elasticsearch: true eck_enable_kibana: true eck_enable_logstash: true roles: - ibm.mas_devops.eck","title":"Example Playbook"},{"location":"roles/eck/#license","text":"EPL-2.0","title":"License"},{"location":"roles/entitlement_key_rotation/","text":"entitlement_key_rotation \u00a4 This role creates/updates the entitlement username and password that are stored in the secrets used to pull images throughout all MAS related namespaces for one or multiple clusters. The main secret that is updated by this role is the ibm-entitlement which holds the credentials needed to pull the MAS images used by MAS Core or the MAS applications. By default, this role will search for all MAS related namespaces that might contain the secret that holds the entitlement key to be updated. The list of namespaces to be updated with new username/password credentials are: All namespaces starting with mas- , which means by default it will update the ibm-entitlement secret with the new username/password credentials for all MAS namespaces/instances in the cluster. SLS namespace - holds ibm-entitlement which pulls Suite License Services related images. openshift-marketplace - holds wiot-docker-local which pulls the pre-release/development catalog source image for ibm-operator-catalog . Requires the artifactory_username and artifactory_token to be set. Note This role uses ocp_login to login into the target clusters, therefore make sure you export the corresponding environment variables accordingly to the cluster type you want to target. Role Variables \u00a4 artifactory_username \u00a4 Required to rotate the ibm-entitlement and wiotp-docker-local secret credentials which is used to pull images across MAS namespaces for development installs and pre-release catalog sources. Environment Variable: ARTIFACTORY_USERNAME Default Value: None artifactory_token \u00a4 Required to rotate the ibm-entitlement and wiotp-docker-local secret credentials which is used to pull images across MAS namespaces for development installs and pre-release catalog sources. Environment Variable: ARTIFACTORY_TOKEN Default Value: None mas_entitlement_username \u00a4 Required to rotate the ibm-entitlement secret credentials only, which is used to pull images across MAS namespaces for MAS installs using release catalog sources. Environment Variable: MAS_ENTITLEMENT_USERNAME Default Value: None mas_entitlement_key \u00a4 Required to rotate the ibm-entitlement secret credentials only, which is used to pull images across MAS namespaces for MAS installs using release catalog sources. Environment Variable: MAS_ENTITLEMENT_KEY Default Value: None cluster_name \u00a4 Required. The target cluster to rotate the credentials/entitlement key secrets. Environment Variable: CLUSTER_NAME Default Value: None sls_namespace \u00a4 Optional. Defines the SLS namespace that holds ibm-entitlement secret which pulls Suite License Services related images. Environment Variable: SLS_NAMESPACE Default Value: ibm-sls Role Variables - Advanced mode \u00a4 Use the following variables to change the default behavior of this role to only rotate the entitlement key for specific clusters or namespaces, instead of running it for all MAS related namespaces. mas_clusters_entitlement_key_rotation_list \u00a4 Optionally define a list of clusters to loop through the entitlement key rotation. Environment Variable: MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST Default Value: If not set, the cluster_name property will be used to target the cluster while executing this role. Example: export MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST='cluster1,cluster2' mas_namespaces_entitlement_key_rotation_list \u00a4 Optionally define a specific list of namespaces to loop through the entitlement key rotation. Environment Variable: MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST Default Value: If not set, all MAS related namespaces for all MAS instances will be target for entitlement key rotation. Example: export MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST='ibm-sls,openshift-marketplace' Example Playbook \u00a4 Rotate entitlement credentials across all MAS instances for a given target cluster: - hosts: localhost any_errors_fatal: true vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME') }}\" cluster_type: \"{{ lookup('env', 'CLUSTER_TYPE') }}\" ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" artifactory_username: \"{{ lookup('env', 'ARTIFACTORY_USERNAME') }}\" artifactory_token: \"{{ lookup('env', 'ARTIFACTORY_TOKEN') }}\" mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.entitlement_key_rotation Rotate entitlement credentials across a specific list of namespaces, targeting multiple clusters: - hosts: localhost any_errors_fatal: true vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME') }}\" # this is the original cluster that will keep the login session context at the end of the rotation loop. cluster_type: \"{{ lookup('env', 'CLUSTER_TYPE') }}\" ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" artifactory_username: \"{{ lookup('env', 'ARTIFACTORY_USERNAME') }}\" artifactory_token: \"{{ lookup('env', 'ARTIFACTORY_TOKEN') }}\" mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" mas_clusters_entitlement_key_rotation_list: \"{{ lookup('env', 'MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST') }}\" mas_namespaces_entitlement_key_rotation_list: \"{{ lookup('env', 'MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST') }}\" roles: - ibm.mas_devops.entitlement_key_rotation License \u00a4 EPL-2.0","title":"entitlement_key_rotation"},{"location":"roles/entitlement_key_rotation/#entitlement_key_rotation","text":"This role creates/updates the entitlement username and password that are stored in the secrets used to pull images throughout all MAS related namespaces for one or multiple clusters. The main secret that is updated by this role is the ibm-entitlement which holds the credentials needed to pull the MAS images used by MAS Core or the MAS applications. By default, this role will search for all MAS related namespaces that might contain the secret that holds the entitlement key to be updated. The list of namespaces to be updated with new username/password credentials are: All namespaces starting with mas- , which means by default it will update the ibm-entitlement secret with the new username/password credentials for all MAS namespaces/instances in the cluster. SLS namespace - holds ibm-entitlement which pulls Suite License Services related images. openshift-marketplace - holds wiot-docker-local which pulls the pre-release/development catalog source image for ibm-operator-catalog . Requires the artifactory_username and artifactory_token to be set. Note This role uses ocp_login to login into the target clusters, therefore make sure you export the corresponding environment variables accordingly to the cluster type you want to target.","title":"entitlement_key_rotation"},{"location":"roles/entitlement_key_rotation/#role-variables","text":"","title":"Role Variables"},{"location":"roles/entitlement_key_rotation/#artifactory_username","text":"Required to rotate the ibm-entitlement and wiotp-docker-local secret credentials which is used to pull images across MAS namespaces for development installs and pre-release catalog sources. Environment Variable: ARTIFACTORY_USERNAME Default Value: None","title":"artifactory_username"},{"location":"roles/entitlement_key_rotation/#artifactory_token","text":"Required to rotate the ibm-entitlement and wiotp-docker-local secret credentials which is used to pull images across MAS namespaces for development installs and pre-release catalog sources. Environment Variable: ARTIFACTORY_TOKEN Default Value: None","title":"artifactory_token"},{"location":"roles/entitlement_key_rotation/#mas_entitlement_username","text":"Required to rotate the ibm-entitlement secret credentials only, which is used to pull images across MAS namespaces for MAS installs using release catalog sources. Environment Variable: MAS_ENTITLEMENT_USERNAME Default Value: None","title":"mas_entitlement_username"},{"location":"roles/entitlement_key_rotation/#mas_entitlement_key","text":"Required to rotate the ibm-entitlement secret credentials only, which is used to pull images across MAS namespaces for MAS installs using release catalog sources. Environment Variable: MAS_ENTITLEMENT_KEY Default Value: None","title":"mas_entitlement_key"},{"location":"roles/entitlement_key_rotation/#cluster_name","text":"Required. The target cluster to rotate the credentials/entitlement key secrets. Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/entitlement_key_rotation/#sls_namespace","text":"Optional. Defines the SLS namespace that holds ibm-entitlement secret which pulls Suite License Services related images. Environment Variable: SLS_NAMESPACE Default Value: ibm-sls","title":"sls_namespace"},{"location":"roles/entitlement_key_rotation/#role-variables-advanced-mode","text":"Use the following variables to change the default behavior of this role to only rotate the entitlement key for specific clusters or namespaces, instead of running it for all MAS related namespaces.","title":"Role Variables - Advanced mode"},{"location":"roles/entitlement_key_rotation/#mas_clusters_entitlement_key_rotation_list","text":"Optionally define a list of clusters to loop through the entitlement key rotation. Environment Variable: MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST Default Value: If not set, the cluster_name property will be used to target the cluster while executing this role. Example: export MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST='cluster1,cluster2'","title":"mas_clusters_entitlement_key_rotation_list"},{"location":"roles/entitlement_key_rotation/#mas_namespaces_entitlement_key_rotation_list","text":"Optionally define a specific list of namespaces to loop through the entitlement key rotation. Environment Variable: MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST Default Value: If not set, all MAS related namespaces for all MAS instances will be target for entitlement key rotation. Example: export MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST='ibm-sls,openshift-marketplace'","title":"mas_namespaces_entitlement_key_rotation_list"},{"location":"roles/entitlement_key_rotation/#example-playbook","text":"Rotate entitlement credentials across all MAS instances for a given target cluster: - hosts: localhost any_errors_fatal: true vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME') }}\" cluster_type: \"{{ lookup('env', 'CLUSTER_TYPE') }}\" ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" artifactory_username: \"{{ lookup('env', 'ARTIFACTORY_USERNAME') }}\" artifactory_token: \"{{ lookup('env', 'ARTIFACTORY_TOKEN') }}\" mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.entitlement_key_rotation Rotate entitlement credentials across a specific list of namespaces, targeting multiple clusters: - hosts: localhost any_errors_fatal: true vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME') }}\" # this is the original cluster that will keep the login session context at the end of the rotation loop. cluster_type: \"{{ lookup('env', 'CLUSTER_TYPE') }}\" ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" artifactory_username: \"{{ lookup('env', 'ARTIFACTORY_USERNAME') }}\" artifactory_token: \"{{ lookup('env', 'ARTIFACTORY_TOKEN') }}\" mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" mas_clusters_entitlement_key_rotation_list: \"{{ lookup('env', 'MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST') }}\" mas_namespaces_entitlement_key_rotation_list: \"{{ lookup('env', 'MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST') }}\" roles: - ibm.mas_devops.entitlement_key_rotation","title":"Example Playbook"},{"location":"roles/entitlement_key_rotation/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_jdbc/","text":"gencfg_jdbc \u00a4 This role is used to configure database in Maximo Application Suite. It will use the database SSL certificate if ssl_enabled flag is true. The db_pem-file defines the location of the pem file used for JDBC connection in MAS installation. Role Variables - Data Source \u00a4 db_instance_id \u00a4 Defines the instance id that is used for the db configure in MAS installation Required Environment Variable: DB_INSTANCE_ID Default: dbinst db_username \u00a4 Defines the username that is used for the db configure in MAS installation Required Environment Variable: MAS_JDBC_USER Default: None jdbc_instance_password \u00a4 Defines the password that is used to connect to db in MAS installation Required Environment Variable: MAS_JDBC_PASSWORD Default: None jdbc_url \u00a4 Defines the jdbc URL that is used to connect to db in MAS installation: Required Environment Variable: MAS_JDBC_URL Default: None Tip Example URL strings: IBM Db2 (insecure): jdbc:db2://dbserverxx:50000/maxdbxx IBM Db2 (secure): jdbc:db2://dbserverxx:50000/maxdbxx:sslConnection=true Oracle Database: jdbc:oracle:thin:@dbserverxx:1521:maximo Microsoft SQL Server (insecure): jdbc:sqlserver://;serverName=dbserverxx;portNumber=1433;databaseName=msdbxx;integratedSecurity=false;sendStringParametersAsUnicode=false;selectMethod=cursor;encrypt=false;trustServerCertificate=false; Microsoft SQL Server (secure): jdbc:sqlserver://;serverName=dbserverxx;portNumber=1433;databaseName=msdbxx;integratedSecurity=false;sendStringParametersAsUnicode=false;selectMethod=cursor;encrypt=true;trustServerCertificate=true; db_pem_file \u00a4 Defines the location of the pem file used for JDBC connection in MAS installation Optional Environment Variable: MAS_JDBC_CERT_LOCAL_FILE Default: None Role Variables - MAS Configuration \u00a4 mas_config_scope \u00a4 Configure whether to generate a binding suitable for System, Workspace, Application, or Workspace-Application use within MAS ( system , ws , app , or wsapp ). Required Environment Variable: MAS_CONFIG_SCOPE Default: None mas_config_dir \u00a4 Configure the destination directory for the generated yaml file. Required Environment Variable: MAS_CONFIG_DIR Default: None mas_instance_id \u00a4 MAS Instance ID we are generating a configuration for. Required Environment Variable: MAS_INSTANCE_ID Default: None mas_workspace_id \u00a4 Set the workspace ID when generating a configuration for workspace or workspace-application scope. Required if mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None mas_application_id \u00a4 Set the application ID when generating a configuration for application or workspace-application scope. Required if mas_config_scope is set to either app or wsapp Environment Variable: MAS_APP_ID Default: None ssl_enabled \u00a4 Some applications in MAS are unable to determine whether SSL is enabled or disable via the JDBC string, and require this additional setting. Make sure to set this to match the setting in jdbc_url . Required Environment Variable: SSL_ENABLED Default: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default: None Example Playbook \u00a4 --- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_jdbc License \u00a4 EPL-2.0","title":"gencfg_jdbc"},{"location":"roles/gencfg_jdbc/#gencfg_jdbc","text":"This role is used to configure database in Maximo Application Suite. It will use the database SSL certificate if ssl_enabled flag is true. The db_pem-file defines the location of the pem file used for JDBC connection in MAS installation.","title":"gencfg_jdbc"},{"location":"roles/gencfg_jdbc/#role-variables-data-source","text":"","title":"Role Variables - Data Source"},{"location":"roles/gencfg_jdbc/#db_instance_id","text":"Defines the instance id that is used for the db configure in MAS installation Required Environment Variable: DB_INSTANCE_ID Default: dbinst","title":"db_instance_id"},{"location":"roles/gencfg_jdbc/#db_username","text":"Defines the username that is used for the db configure in MAS installation Required Environment Variable: MAS_JDBC_USER Default: None","title":"db_username"},{"location":"roles/gencfg_jdbc/#jdbc_instance_password","text":"Defines the password that is used to connect to db in MAS installation Required Environment Variable: MAS_JDBC_PASSWORD Default: None","title":"jdbc_instance_password"},{"location":"roles/gencfg_jdbc/#jdbc_url","text":"Defines the jdbc URL that is used to connect to db in MAS installation: Required Environment Variable: MAS_JDBC_URL Default: None Tip Example URL strings: IBM Db2 (insecure): jdbc:db2://dbserverxx:50000/maxdbxx IBM Db2 (secure): jdbc:db2://dbserverxx:50000/maxdbxx:sslConnection=true Oracle Database: jdbc:oracle:thin:@dbserverxx:1521:maximo Microsoft SQL Server (insecure): jdbc:sqlserver://;serverName=dbserverxx;portNumber=1433;databaseName=msdbxx;integratedSecurity=false;sendStringParametersAsUnicode=false;selectMethod=cursor;encrypt=false;trustServerCertificate=false; Microsoft SQL Server (secure): jdbc:sqlserver://;serverName=dbserverxx;portNumber=1433;databaseName=msdbxx;integratedSecurity=false;sendStringParametersAsUnicode=false;selectMethod=cursor;encrypt=true;trustServerCertificate=true;","title":"jdbc_url"},{"location":"roles/gencfg_jdbc/#db_pem_file","text":"Defines the location of the pem file used for JDBC connection in MAS installation Optional Environment Variable: MAS_JDBC_CERT_LOCAL_FILE Default: None","title":"db_pem_file"},{"location":"roles/gencfg_jdbc/#role-variables-mas-configuration","text":"","title":"Role Variables - MAS Configuration"},{"location":"roles/gencfg_jdbc/#mas_config_scope","text":"Configure whether to generate a binding suitable for System, Workspace, Application, or Workspace-Application use within MAS ( system , ws , app , or wsapp ). Required Environment Variable: MAS_CONFIG_SCOPE Default: None","title":"mas_config_scope"},{"location":"roles/gencfg_jdbc/#mas_config_dir","text":"Configure the destination directory for the generated yaml file. Required Environment Variable: MAS_CONFIG_DIR Default: None","title":"mas_config_dir"},{"location":"roles/gencfg_jdbc/#mas_instance_id","text":"MAS Instance ID we are generating a configuration for. Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/gencfg_jdbc/#mas_workspace_id","text":"Set the workspace ID when generating a configuration for workspace or workspace-application scope. Required if mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/gencfg_jdbc/#mas_application_id","text":"Set the application ID when generating a configuration for application or workspace-application scope. Required if mas_config_scope is set to either app or wsapp Environment Variable: MAS_APP_ID Default: None","title":"mas_application_id"},{"location":"roles/gencfg_jdbc/#ssl_enabled","text":"Some applications in MAS are unable to determine whether SSL is enabled or disable via the JDBC string, and require this additional setting. Make sure to set this to match the setting in jdbc_url . Required Environment Variable: SSL_ENABLED Default: None","title":"ssl_enabled"},{"location":"roles/gencfg_jdbc/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default: None","title":"custom_labels"},{"location":"roles/gencfg_jdbc/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_jdbc","title":"Example Playbook"},{"location":"roles/gencfg_jdbc/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_mongo/","text":"gencfg_mongo \u00a4 This role is used to generate mongo configuration in Maximo Application Suite This generated mongo configuration can be used as an input to the suite_config role, to configure a MAS instance to connect with an existing Mongo cluster Role Variables \u00a4 mongodb_namespace \u00a4 The generated Mongo Config file name will be suffixed with this namespace value eg, mongo-< >.yml Environment Variable: MONGODB_NAMESPACE Default: mongoce mongodb_admin_username \u00a4 Required. MongoDB admin username Environment Variable: MONGODB_ADMIN_USERNAME Default: None mongodb_admin_password \u00a4 Required. MongoDB admin password Environment Variable: MONGODB_ADMIN_PASSWORD Default: None mongodb_authentication_mechanism \u00a4 Required. MongoDB authentication mechanism. Specify DEFAULT for SCRAM-SHA-256 or SCRAM-SHA-1. For LDAP authentication use PLAIN Environment Variable: MONGODB_AUTHENTICATION_MECHANISM Default: DEFAULT mongodb_authentication_database \u00a4 Required. MongoDB authentication database. This value must be $external if PLAIN has been specified for mongodb_authentication_mechanism Environment Variable: MONGODB_AUTHENTICATION_DATABASE Default: admin mongodb_hosts \u00a4 Required. In case if there are multiple instances, the host address should be seperated by a ,. Example: docdb-1.abc.ca-central-1.docdb.amazonaws.com:27017,docdb-2.def.ca-central-1.docdb.amazonaws.com:27017 Environment Variable: MONGODB_HOSTS Default: None mongodb_retry_writes \u00a4 Set to true if MongoDB support retryable writes. In case if retryable writes is not supported (like in case of Amazon DocumentDB), set to false Optional Environment Variable: MONGODB_RETRY_WRITES Default: true mongodb_ca_pem_local_file \u00a4 Required. defines the CA pem file's local file path Environment Variable: MONGODB_CA_PEM_LOCAL_FILE Default: None mas_instance_id \u00a4 Required. The instance ID of Maximo Application Suite for which the MongoCfg configuration will generate Environment Variable: MAS_INSTANCE_ID Default: None mas_config_dir \u00a4 Required. Local directory to save the generated MongoCfg resource definition. This can be used as an input to the suite_config role, to configure a MAS instance to connect with an existing Mongo cluster Environment Variable: MAS_CONFIG_DIR Default: None Example Playbook \u00a4 --- - hosts: localhost any_errors_fatal: true vars: mongodb_namespace: mongoce mongodb_admin_username: mongoadmin mongodb_admin_password: mongo-strong-password mongodb_hosts: docdb-1.abc.ca-central-1.docdb.amazonaws.com:27017,docdb-2.def.ca-central-1.docdb.amazonaws.com:27017 mongodb_retry_writes: false mongodb_ca_pem_local_file: /tmp/mongo-ca.pem mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.gencfg_mongo License \u00a4 EPL-2.0","title":"gencfg_mongo"},{"location":"roles/gencfg_mongo/#gencfg_mongo","text":"This role is used to generate mongo configuration in Maximo Application Suite This generated mongo configuration can be used as an input to the suite_config role, to configure a MAS instance to connect with an existing Mongo cluster","title":"gencfg_mongo"},{"location":"roles/gencfg_mongo/#role-variables","text":"","title":"Role Variables"},{"location":"roles/gencfg_mongo/#mongodb_namespace","text":"The generated Mongo Config file name will be suffixed with this namespace value eg, mongo-< >.yml Environment Variable: MONGODB_NAMESPACE Default: mongoce","title":"mongodb_namespace"},{"location":"roles/gencfg_mongo/#mongodb_admin_username","text":"Required. MongoDB admin username Environment Variable: MONGODB_ADMIN_USERNAME Default: None","title":"mongodb_admin_username"},{"location":"roles/gencfg_mongo/#mongodb_admin_password","text":"Required. MongoDB admin password Environment Variable: MONGODB_ADMIN_PASSWORD Default: None","title":"mongodb_admin_password"},{"location":"roles/gencfg_mongo/#mongodb_authentication_mechanism","text":"Required. MongoDB authentication mechanism. Specify DEFAULT for SCRAM-SHA-256 or SCRAM-SHA-1. For LDAP authentication use PLAIN Environment Variable: MONGODB_AUTHENTICATION_MECHANISM Default: DEFAULT","title":"mongodb_authentication_mechanism"},{"location":"roles/gencfg_mongo/#mongodb_authentication_database","text":"Required. MongoDB authentication database. This value must be $external if PLAIN has been specified for mongodb_authentication_mechanism Environment Variable: MONGODB_AUTHENTICATION_DATABASE Default: admin","title":"mongodb_authentication_database"},{"location":"roles/gencfg_mongo/#mongodb_hosts","text":"Required. In case if there are multiple instances, the host address should be seperated by a ,. Example: docdb-1.abc.ca-central-1.docdb.amazonaws.com:27017,docdb-2.def.ca-central-1.docdb.amazonaws.com:27017 Environment Variable: MONGODB_HOSTS Default: None","title":"mongodb_hosts"},{"location":"roles/gencfg_mongo/#mongodb_retry_writes","text":"Set to true if MongoDB support retryable writes. In case if retryable writes is not supported (like in case of Amazon DocumentDB), set to false Optional Environment Variable: MONGODB_RETRY_WRITES Default: true","title":"mongodb_retry_writes"},{"location":"roles/gencfg_mongo/#mongodb_ca_pem_local_file","text":"Required. defines the CA pem file's local file path Environment Variable: MONGODB_CA_PEM_LOCAL_FILE Default: None","title":"mongodb_ca_pem_local_file"},{"location":"roles/gencfg_mongo/#mas_instance_id","text":"Required. The instance ID of Maximo Application Suite for which the MongoCfg configuration will generate Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/gencfg_mongo/#mas_config_dir","text":"Required. Local directory to save the generated MongoCfg resource definition. This can be used as an input to the suite_config role, to configure a MAS instance to connect with an existing Mongo cluster Environment Variable: MAS_CONFIG_DIR Default: None","title":"mas_config_dir"},{"location":"roles/gencfg_mongo/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true vars: mongodb_namespace: mongoce mongodb_admin_username: mongoadmin mongodb_admin_password: mongo-strong-password mongodb_hosts: docdb-1.abc.ca-central-1.docdb.amazonaws.com:27017,docdb-2.def.ca-central-1.docdb.amazonaws.com:27017 mongodb_retry_writes: false mongodb_ca_pem_local_file: /tmp/mongo-ca.pem mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.gencfg_mongo","title":"Example Playbook"},{"location":"roles/gencfg_mongo/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_watsonstudio/","text":"gencfg_watsonstudio \u00a4 This role is used to configure WatsonStudio in Maximo Application Suite. Role Variables \u00a4 mas_instance_id \u00a4 Providing this and mas_config_dir will instruct the role to generate a WatsonStudioCfg template that can be used to configure MAS to connect to WatsonStudio. Environment Variable: MAS_INSTANCE_ID Default: None mas_workspace_id \u00a4 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None CPD_ADMIN_USERNAME \u00a4 Defines the username that is used for the WatsonStudio configure in MAS installation Environment Variable: CPD_ADMIN_USERNAME Default: None CPD_ADMIN_PASSWORD \u00a4 Defines the password that is used to connect to WatsonStudio in MAS installation Environment Variable: CPD_ADMIN_PASSWORD Default: None CPD_ADMIN_URL \u00a4 Defines the url that is used to connect to WatsonStudio in MAS installation Environment Variable: CPD_ADMIN_URL Default: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default: None Example Playbook \u00a4 --- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_watsonstudio License \u00a4 EPL-2.0","title":"gencfg_watsonstudio"},{"location":"roles/gencfg_watsonstudio/#gencfg_watsonstudio","text":"This role is used to configure WatsonStudio in Maximo Application Suite.","title":"gencfg_watsonstudio"},{"location":"roles/gencfg_watsonstudio/#role-variables","text":"","title":"Role Variables"},{"location":"roles/gencfg_watsonstudio/#mas_instance_id","text":"Providing this and mas_config_dir will instruct the role to generate a WatsonStudioCfg template that can be used to configure MAS to connect to WatsonStudio. Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/gencfg_watsonstudio/#mas_workspace_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/gencfg_watsonstudio/#cpd_admin_username","text":"Defines the username that is used for the WatsonStudio configure in MAS installation Environment Variable: CPD_ADMIN_USERNAME Default: None","title":"CPD_ADMIN_USERNAME"},{"location":"roles/gencfg_watsonstudio/#cpd_admin_password","text":"Defines the password that is used to connect to WatsonStudio in MAS installation Environment Variable: CPD_ADMIN_PASSWORD Default: None","title":"CPD_ADMIN_PASSWORD"},{"location":"roles/gencfg_watsonstudio/#cpd_admin_url","text":"Defines the url that is used to connect to WatsonStudio in MAS installation Environment Variable: CPD_ADMIN_URL Default: None","title":"CPD_ADMIN_URL"},{"location":"roles/gencfg_watsonstudio/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default: None","title":"custom_labels"},{"location":"roles/gencfg_watsonstudio/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_watsonstudio","title":"Example Playbook"},{"location":"roles/gencfg_watsonstudio/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_workspace/","text":"gencfg_workspace \u00a4 This role is used to generate a Workspace custom resource that can be applied to Maximo Application Suite manually, or using the suite_config role. The configuration will be saved to local disk in the directory specified by the mas_config_dir variable. Role Variables \u00a4 mas_instance_id \u00a4 Required. The MAS instance ID that the workspace will be used in Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \u00a4 Required. The ID of the workspace Environment Variable: MAS_WORKSPACE_ID Default Value: None mas_workspace_name \u00a4 Required. The display name for the workspace Environment Variable: MAS_WORKSPACE_NAME Default Value: None mas_config_dir \u00a4 Required. The directory to save the configuration to. Environment Variable: MAS_CONFIG_DIR Default Value: None custom_labels \u00a4 Optional. List of comma separated key=value pairs for setting custom labels on instance specific resources. Environment Variable: CUSTOM_LABELS Default Value: None Example Playbook \u00a4 --- - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_workspace_id: \"masdev\" mas_workspace_name: \"MAS Development\" mas_config_dir: \"/home/david/masconfig/inst1\" roles: - ibm.mas_devops.gencfg_workspace License \u00a4 EPL-2.0","title":"gencfg_workspace"},{"location":"roles/gencfg_workspace/#gencfg_workspace","text":"This role is used to generate a Workspace custom resource that can be applied to Maximo Application Suite manually, or using the suite_config role. The configuration will be saved to local disk in the directory specified by the mas_config_dir variable.","title":"gencfg_workspace"},{"location":"roles/gencfg_workspace/#role-variables","text":"","title":"Role Variables"},{"location":"roles/gencfg_workspace/#mas_instance_id","text":"Required. The MAS instance ID that the workspace will be used in Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/gencfg_workspace/#mas_workspace_id","text":"Required. The ID of the workspace Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/gencfg_workspace/#mas_workspace_name","text":"Required. The display name for the workspace Environment Variable: MAS_WORKSPACE_NAME Default Value: None","title":"mas_workspace_name"},{"location":"roles/gencfg_workspace/#mas_config_dir","text":"Required. The directory to save the configuration to. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/gencfg_workspace/#custom_labels","text":"Optional. List of comma separated key=value pairs for setting custom labels on instance specific resources. Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/gencfg_workspace/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_workspace_id: \"masdev\" mas_workspace_name: \"MAS Development\" mas_config_dir: \"/home/david/masconfig/inst1\" roles: - ibm.mas_devops.gencfg_workspace","title":"Example Playbook"},{"location":"roles/gencfg_workspace/#license","text":"EPL-2.0","title":"License"},{"location":"roles/grafana/","text":"grafana \u00a4 Installs and configures an instance of Grafana for use with IBM Maximo Application Suite, using the community grafana operator Note The credentials for the grafana admin user are stored in grafana-admin-credentials secret in the grafana namespace. A route is created in the grafana namespace to allow access to the grafana UI. Role Variables \u00a4 grafana_action \u00a4 Action to perform on Grafana installation. Optional Environment Variable: GRAFANA_ACTION Default: install Purpose : Specifies whether to install, uninstall, or upgrade Grafana for use with MAS monitoring. When to use : - Use install (default) for new Grafana deployments - Use update to upgrade from Grafana v4 to v5 - Use uninstall to remove Grafana Valid values : install , uninstall , update Impact : - install : Deploys Grafana operator and instance - update : Upgrades from v4 to v5 (creates new instance with new URL) - uninstall : Removes Grafana operator and instance Related variables : - grafana_major_version : Version to install/upgrade to - grafana_v4_namespace / grafana_v5_namespace : Namespaces for different versions Note : IMPORTANT - When upgrading from v4 to v5, the new instance will have a different URL and will NOT inherit the user database. Admin password will be reset, and user accounts must be recreated. The v4 instance remains until manually removed. grafana_major_version \u00a4 Grafana operator major version. Optional Environment Variable: GRAFANA_MAJOR_VERSION Default: 5 Purpose : Specifies which major version of the Grafana operator to install. When to use : - Use default ( 5 ) for new installations (recommended) - Set to 4 only for legacy deployments - Version 5 is the current supported version Valid values : 4 , 5 Impact : Determines which Grafana operator version is deployed. Version 5 uses a different namespace and has breaking changes from version 4. Related variables : - grafana_action : Use update to upgrade from v4 to v5 - grafana_v4_namespace : Namespace for v4 installation - grafana_v5_namespace : Namespace for v5 installation Note : Version 5 is recommended for all new deployments. Upgrading from v4 to v5 requires using grafana_action=update and results in a new instance with different URL and no user database migration. grafana_v4_namespace \u00a4 Namespace for Grafana v4 installation. Optional Environment Variable: GRAFANA_NAMESPACE Default: grafana Purpose : Specifies the Kubernetes namespace where Grafana operator v4 and instance are installed. When to use : - Only applies when grafana_major_version=4 - Use default ( grafana ) for standard deployments - Override only if namespace conflicts exist Valid values : Valid Kubernetes namespace name Impact : Determines where Grafana v4 resources are created. Namespace must not conflict with v5 installation. Related variables : - grafana_major_version : Must be 4 for this to apply - grafana_v5_namespace : Separate namespace for v5 Note : When upgrading from v4 to v5, both namespaces coexist. The v4 instance remains in this namespace until manually removed. grafana_v5_namespace \u00a4 Namespace for Grafana v5 installation. Optional Environment Variable: GRAFANA_V5_NAMESPACE Default: grafana5 Purpose : Specifies the Kubernetes namespace where Grafana operator v5 and instance are installed. When to use : - Applies when grafana_major_version=5 (default) - Use default ( grafana5 ) for standard deployments - Override only if namespace conflicts exist Valid values : Valid Kubernetes namespace name Impact : Determines where Grafana v5 resources are created. Uses separate namespace from v4 to allow coexistence during upgrades. Related variables : - grafana_major_version : Must be 5 for this to apply - grafana_v4_namespace : Separate namespace for v4 Note : The separate namespace allows v4 and v5 to coexist during upgrades. After upgrading, the v4 instance remains in its namespace until manually removed. grafana_instance_storage_class \u00a4 Storage class for Grafana user data. Required (if supported storage class not available) Environment Variable: GRAFANA_INSTANCE_STORAGE_CLASS Default: Auto-detected from ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium Purpose : Specifies the storage class for Grafana's persistent volume that stores user data, dashboards, and configurations. When to use : - Leave unset to auto-detect from supported storage classes - Set explicitly if none of the default classes are available - Required if cluster has no supported storage classes Valid values : Any storage class supporting ReadWriteOnce (RWO) access mode. ReadWriteMany (RWX) classes also work. Impact : Determines where Grafana user data is stored. Incorrect or unavailable storage class will cause deployment to fail. Related variables : - grafana_instance_storage_size : Size of the storage volume Note : RWO access mode is sufficient for Grafana. RWX classes (like ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium ) also work but are not required. The role auto-detects these common classes if available. grafana_instance_storage_size \u00a4 Storage volume size for Grafana user data. Optional Environment Variable: GRAFANA_INSTANCE_STORAGE_SIZE Default: 10Gi Purpose : Specifies the size of the persistent volume used to store Grafana user data, dashboards, and configurations. When to use : - Use default ( 10Gi ) for most deployments - Increase for environments with many dashboards or extensive data - Consider growth over time for dashboard storage Valid values : Kubernetes storage size format (e.g., 10Gi , 20Gi , 50Gi ) Impact : Determines available storage for Grafana data. Insufficient size may limit dashboard creation. Excessive size wastes storage resources. Related variables : - grafana_instance_storage_class : Storage class for the volume Note : The default 10Gi is sufficient for typical MAS monitoring deployments. When upgrading from v4 to v5, the new instance inherits the v4 storage size unless explicitly overridden. Example Playbook \u00a4 - hosts: localhost vars: grafana_instance_storage_class: \"ibmc-file-gold-gid\" grafana_instance_storage_class: \"15Gi\" roles: - ibm.mas_devops.grafana To Upgrade from Grafana Operator from V4 to V5 - hosts: localhost vars: grafana_action: \"update\" roles: - ibm.mas_devops.grafana Note note that the upgraded v5 grafana inherits the storage class and size from the v4 configuration unless they are defined as environment variables. License \u00a4 EPL-2.0","title":"grafana"},{"location":"roles/grafana/#grafana","text":"Installs and configures an instance of Grafana for use with IBM Maximo Application Suite, using the community grafana operator Note The credentials for the grafana admin user are stored in grafana-admin-credentials secret in the grafana namespace. A route is created in the grafana namespace to allow access to the grafana UI.","title":"grafana"},{"location":"roles/grafana/#role-variables","text":"","title":"Role Variables"},{"location":"roles/grafana/#grafana_action","text":"Action to perform on Grafana installation. Optional Environment Variable: GRAFANA_ACTION Default: install Purpose : Specifies whether to install, uninstall, or upgrade Grafana for use with MAS monitoring. When to use : - Use install (default) for new Grafana deployments - Use update to upgrade from Grafana v4 to v5 - Use uninstall to remove Grafana Valid values : install , uninstall , update Impact : - install : Deploys Grafana operator and instance - update : Upgrades from v4 to v5 (creates new instance with new URL) - uninstall : Removes Grafana operator and instance Related variables : - grafana_major_version : Version to install/upgrade to - grafana_v4_namespace / grafana_v5_namespace : Namespaces for different versions Note : IMPORTANT - When upgrading from v4 to v5, the new instance will have a different URL and will NOT inherit the user database. Admin password will be reset, and user accounts must be recreated. The v4 instance remains until manually removed.","title":"grafana_action"},{"location":"roles/grafana/#grafana_major_version","text":"Grafana operator major version. Optional Environment Variable: GRAFANA_MAJOR_VERSION Default: 5 Purpose : Specifies which major version of the Grafana operator to install. When to use : - Use default ( 5 ) for new installations (recommended) - Set to 4 only for legacy deployments - Version 5 is the current supported version Valid values : 4 , 5 Impact : Determines which Grafana operator version is deployed. Version 5 uses a different namespace and has breaking changes from version 4. Related variables : - grafana_action : Use update to upgrade from v4 to v5 - grafana_v4_namespace : Namespace for v4 installation - grafana_v5_namespace : Namespace for v5 installation Note : Version 5 is recommended for all new deployments. Upgrading from v4 to v5 requires using grafana_action=update and results in a new instance with different URL and no user database migration.","title":"grafana_major_version"},{"location":"roles/grafana/#grafana_v4_namespace","text":"Namespace for Grafana v4 installation. Optional Environment Variable: GRAFANA_NAMESPACE Default: grafana Purpose : Specifies the Kubernetes namespace where Grafana operator v4 and instance are installed. When to use : - Only applies when grafana_major_version=4 - Use default ( grafana ) for standard deployments - Override only if namespace conflicts exist Valid values : Valid Kubernetes namespace name Impact : Determines where Grafana v4 resources are created. Namespace must not conflict with v5 installation. Related variables : - grafana_major_version : Must be 4 for this to apply - grafana_v5_namespace : Separate namespace for v5 Note : When upgrading from v4 to v5, both namespaces coexist. The v4 instance remains in this namespace until manually removed.","title":"grafana_v4_namespace"},{"location":"roles/grafana/#grafana_v5_namespace","text":"Namespace for Grafana v5 installation. Optional Environment Variable: GRAFANA_V5_NAMESPACE Default: grafana5 Purpose : Specifies the Kubernetes namespace where Grafana operator v5 and instance are installed. When to use : - Applies when grafana_major_version=5 (default) - Use default ( grafana5 ) for standard deployments - Override only if namespace conflicts exist Valid values : Valid Kubernetes namespace name Impact : Determines where Grafana v5 resources are created. Uses separate namespace from v4 to allow coexistence during upgrades. Related variables : - grafana_major_version : Must be 5 for this to apply - grafana_v4_namespace : Separate namespace for v4 Note : The separate namespace allows v4 and v5 to coexist during upgrades. After upgrading, the v4 instance remains in its namespace until manually removed.","title":"grafana_v5_namespace"},{"location":"roles/grafana/#grafana_instance_storage_class","text":"Storage class for Grafana user data. Required (if supported storage class not available) Environment Variable: GRAFANA_INSTANCE_STORAGE_CLASS Default: Auto-detected from ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium Purpose : Specifies the storage class for Grafana's persistent volume that stores user data, dashboards, and configurations. When to use : - Leave unset to auto-detect from supported storage classes - Set explicitly if none of the default classes are available - Required if cluster has no supported storage classes Valid values : Any storage class supporting ReadWriteOnce (RWO) access mode. ReadWriteMany (RWX) classes also work. Impact : Determines where Grafana user data is stored. Incorrect or unavailable storage class will cause deployment to fail. Related variables : - grafana_instance_storage_size : Size of the storage volume Note : RWO access mode is sufficient for Grafana. RWX classes (like ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium ) also work but are not required. The role auto-detects these common classes if available.","title":"grafana_instance_storage_class"},{"location":"roles/grafana/#grafana_instance_storage_size","text":"Storage volume size for Grafana user data. Optional Environment Variable: GRAFANA_INSTANCE_STORAGE_SIZE Default: 10Gi Purpose : Specifies the size of the persistent volume used to store Grafana user data, dashboards, and configurations. When to use : - Use default ( 10Gi ) for most deployments - Increase for environments with many dashboards or extensive data - Consider growth over time for dashboard storage Valid values : Kubernetes storage size format (e.g., 10Gi , 20Gi , 50Gi ) Impact : Determines available storage for Grafana data. Insufficient size may limit dashboard creation. Excessive size wastes storage resources. Related variables : - grafana_instance_storage_class : Storage class for the volume Note : The default 10Gi is sufficient for typical MAS monitoring deployments. When upgrading from v4 to v5, the new instance inherits the v4 storage size unless explicitly overridden.","title":"grafana_instance_storage_size"},{"location":"roles/grafana/#example-playbook","text":"- hosts: localhost vars: grafana_instance_storage_class: \"ibmc-file-gold-gid\" grafana_instance_storage_class: \"15Gi\" roles: - ibm.mas_devops.grafana To Upgrade from Grafana Operator from V4 to V5 - hosts: localhost vars: grafana_action: \"update\" roles: - ibm.mas_devops.grafana Note note that the upgraded v5 grafana inherits the storage class and size from the v4 configuration unless they are defined as environment variables.","title":"Example Playbook"},{"location":"roles/grafana/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ibm_catalogs/","text":"ibm_catalogs \u00a4 This role installs the IBM Maximo Operator Catalog , which is a curated Operator Catalog derived from the IBM Operator Catalog , with all content certified compatible with IBM Maximo Application Suite: Additional, for IBM employees only, the pre-release development operator catalog can be installed, this is achieved by setting both the artifactory_username and artifactory_token variables. Role Variables \u00a4 General Variables \u00a4 mas_catalog_version \u00a4 Version of the IBM Maximo Operator Catalog to install. Optional Environment Variable: MAS_CATALOG_VERSION Default Value: @@MAS_LATEST_CATALOG@@ (latest stable version) Purpose : Specifies which version of the IBM Maximo Operator Catalog to install. The catalog provides certified operators compatible with MAS, including MAS Core, applications, and dependencies. When to use : - Leave as default to install the latest stable catalog version (recommended) - Set explicitly when you need a specific catalog version - Set to match your MAS version requirements - Use specific version for reproducible deployments Valid values : Valid catalog version string (e.g., v8-240625-amd64 , v9-250115-amd64 ) Impact : Determines which operator versions are available for installation. The catalog version must be compatible with your target MAS version. Using an incompatible catalog version may prevent MAS installation or upgrades. Related variables : The catalog version affects which MAS and application versions can be installed. Note : The default value is automatically updated to the latest stable catalog version. For production deployments, consider pinning to a specific version for consistency and reproducibility. Development Variables \u00a4 artifactory_username \u00a4 Artifactory username for accessing pre-release development catalogs (IBM employees only). Optional Environment Variable: ARTIFACTORY_USERNAME Default Value: None Purpose : Provides authentication to IBM Artifactory for installing development catalog sources containing pre-release MAS operators. This enables testing of upcoming MAS versions before general availability. When to use : - Only for IBM employees with Artifactory access - Only for development/testing of pre-release MAS versions - Must be set together with artifactory_token - Never use in production environments Valid values : Valid IBM Artifactory username Impact : When set with artifactory_token , enables installation of development catalog sources. Without both credentials, only production catalogs are available. Related variables : - artifactory_token : Required together with this username - Both must be set to enable development catalog access Note : IBM EMPLOYEES ONLY - This is for pre-release testing only. Never use development catalogs in production. Keep credentials secure and do not commit to source control. artifactory_token \u00a4 Artifactory API token for accessing pre-release development catalogs (IBM employees only). Optional Environment Variable: ARTIFACTORY_TOKEN Default Value: None Purpose : Provides API token authentication to IBM Artifactory for installing development catalog sources containing pre-release MAS operators. This enables testing of upcoming MAS versions before general availability. When to use : - Only for IBM employees with Artifactory access - Only for development/testing of pre-release MAS versions - Must be set together with artifactory_username - Never use in production environments Valid values : Valid IBM Artifactory API token string Impact : When set with artifactory_username , enables installation of development catalog sources. Without both credentials, only production catalogs are available. Related variables : - artifactory_username : Required together with this token - Both must be set to enable development catalog access Note : IBM EMPLOYEES ONLY - This is for pre-release testing only. Never use development catalogs in production. Keep this token secure and do not commit to source control. Generate tokens from IBM Artifactory. Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost roles: - ibm.mas_devops.ibm_catalogs Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=ibm_catalogs ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"ibm_catalogs"},{"location":"roles/ibm_catalogs/#ibm_catalogs","text":"This role installs the IBM Maximo Operator Catalog , which is a curated Operator Catalog derived from the IBM Operator Catalog , with all content certified compatible with IBM Maximo Application Suite: Additional, for IBM employees only, the pre-release development operator catalog can be installed, this is achieved by setting both the artifactory_username and artifactory_token variables.","title":"ibm_catalogs"},{"location":"roles/ibm_catalogs/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ibm_catalogs/#general-variables","text":"","title":"General Variables"},{"location":"roles/ibm_catalogs/#mas_catalog_version","text":"Version of the IBM Maximo Operator Catalog to install. Optional Environment Variable: MAS_CATALOG_VERSION Default Value: @@MAS_LATEST_CATALOG@@ (latest stable version) Purpose : Specifies which version of the IBM Maximo Operator Catalog to install. The catalog provides certified operators compatible with MAS, including MAS Core, applications, and dependencies. When to use : - Leave as default to install the latest stable catalog version (recommended) - Set explicitly when you need a specific catalog version - Set to match your MAS version requirements - Use specific version for reproducible deployments Valid values : Valid catalog version string (e.g., v8-240625-amd64 , v9-250115-amd64 ) Impact : Determines which operator versions are available for installation. The catalog version must be compatible with your target MAS version. Using an incompatible catalog version may prevent MAS installation or upgrades. Related variables : The catalog version affects which MAS and application versions can be installed. Note : The default value is automatically updated to the latest stable catalog version. For production deployments, consider pinning to a specific version for consistency and reproducibility.","title":"mas_catalog_version"},{"location":"roles/ibm_catalogs/#development-variables","text":"","title":"Development Variables"},{"location":"roles/ibm_catalogs/#artifactory_username","text":"Artifactory username for accessing pre-release development catalogs (IBM employees only). Optional Environment Variable: ARTIFACTORY_USERNAME Default Value: None Purpose : Provides authentication to IBM Artifactory for installing development catalog sources containing pre-release MAS operators. This enables testing of upcoming MAS versions before general availability. When to use : - Only for IBM employees with Artifactory access - Only for development/testing of pre-release MAS versions - Must be set together with artifactory_token - Never use in production environments Valid values : Valid IBM Artifactory username Impact : When set with artifactory_token , enables installation of development catalog sources. Without both credentials, only production catalogs are available. Related variables : - artifactory_token : Required together with this username - Both must be set to enable development catalog access Note : IBM EMPLOYEES ONLY - This is for pre-release testing only. Never use development catalogs in production. Keep credentials secure and do not commit to source control.","title":"artifactory_username"},{"location":"roles/ibm_catalogs/#artifactory_token","text":"Artifactory API token for accessing pre-release development catalogs (IBM employees only). Optional Environment Variable: ARTIFACTORY_TOKEN Default Value: None Purpose : Provides API token authentication to IBM Artifactory for installing development catalog sources containing pre-release MAS operators. This enables testing of upcoming MAS versions before general availability. When to use : - Only for IBM employees with Artifactory access - Only for development/testing of pre-release MAS versions - Must be set together with artifactory_username - Never use in production environments Valid values : Valid IBM Artifactory API token string Impact : When set with artifactory_username , enables installation of development catalog sources. Without both credentials, only production catalogs are available. Related variables : - artifactory_username : Required together with this token - Both must be set to enable development catalog access Note : IBM EMPLOYEES ONLY - This is for pre-release testing only. Never use development catalogs in production. Keep this token secure and do not commit to source control. Generate tokens from IBM Artifactory.","title":"artifactory_token"},{"location":"roles/ibm_catalogs/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost roles: - ibm.mas_devops.ibm_catalogs","title":"Example Playbook"},{"location":"roles/ibm_catalogs/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=ibm_catalogs ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/ibm_catalogs/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ibmcloud_resource_key/","text":"ibmcloud_resource_key \u00a4 Create IBM Cloud resource keys (apikeys for specific services associated to the account) Role Variables \u00a4 service_instance \u00a4 Name of the service that the key will be referencing. (Eg: DB2, Mongo, cp4d, etc) Required Environment Variable: SERVICE_INSTANCE service_resource_key_name \u00a4 Name of the key to either create or delete. If unset will be dervied from the service_instance as \" service_instance _resource-key\" Optional Environment Variable: SERVICE_RESOURCE_KEY_NAME delete_service_key \u00a4 Set this to true to force deletion of the service_resource_key_name provided Optional Environment Variable: DELETE_SERVICE_KEY Default: false output_service_key_details_to_file \u00a4 If set will output a json file with the full service key details and credentials as \"service-key_ service_resource_key_name .json\" Optional Environment Variable: OUTPUT_SERVICE_KEY_DETAILS_TO_FILE Default: false output_dir \u00a4 Location to output the service key details json file if output_service_key_details_to_file is set Optional Environment Variable: OUTPUT_DIR Default: . (which will set the directory file in ibm/mas_devops) Example Playbook \u00a4 --- - hosts: localhost any_errors_fatal: true vars: ibmcloud_apikey: xxx service_instance: xxx service_resource_key_name: xxx output_service_key_details_to_file: True OR False delete_service_key: True OR False roles: - ibmcloud_resource_key License \u00a4 EPL-2.0","title":"ibmcloud_resource_key"},{"location":"roles/ibmcloud_resource_key/#ibmcloud_resource_key","text":"Create IBM Cloud resource keys (apikeys for specific services associated to the account)","title":"ibmcloud_resource_key"},{"location":"roles/ibmcloud_resource_key/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ibmcloud_resource_key/#service_instance","text":"Name of the service that the key will be referencing. (Eg: DB2, Mongo, cp4d, etc) Required Environment Variable: SERVICE_INSTANCE","title":"service_instance"},{"location":"roles/ibmcloud_resource_key/#service_resource_key_name","text":"Name of the key to either create or delete. If unset will be dervied from the service_instance as \" service_instance _resource-key\" Optional Environment Variable: SERVICE_RESOURCE_KEY_NAME","title":"service_resource_key_name"},{"location":"roles/ibmcloud_resource_key/#delete_service_key","text":"Set this to true to force deletion of the service_resource_key_name provided Optional Environment Variable: DELETE_SERVICE_KEY Default: false","title":"delete_service_key"},{"location":"roles/ibmcloud_resource_key/#output_service_key_details_to_file","text":"If set will output a json file with the full service key details and credentials as \"service-key_ service_resource_key_name .json\" Optional Environment Variable: OUTPUT_SERVICE_KEY_DETAILS_TO_FILE Default: false","title":"output_service_key_details_to_file"},{"location":"roles/ibmcloud_resource_key/#output_dir","text":"Location to output the service key details json file if output_service_key_details_to_file is set Optional Environment Variable: OUTPUT_DIR Default: . (which will set the directory file in ibm/mas_devops)","title":"output_dir"},{"location":"roles/ibmcloud_resource_key/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true vars: ibmcloud_apikey: xxx service_instance: xxx service_resource_key_name: xxx output_service_key_details_to_file: True OR False delete_service_key: True OR False roles: - ibmcloud_resource_key","title":"Example Playbook"},{"location":"roles/ibmcloud_resource_key/#license","text":"EPL-2.0","title":"License"},{"location":"roles/kafka/","text":"kafka \u00a4 This role provides support to install a Kafka Cluster using Strimzi , Red Hat AMQ Streams , IBM Event Streams or AWS MSK and generate configuration that can be directly applied to Maximo Application Suite. Both Strimzi and Red Hat AMQ streams component are massively scalable, distributed, and high-performance data streaming platform based on the Apache Kafka project. Both offer a distributed backbone that allows microservices and other applications to share data with high throughput and low latency. As more applications move to Kubernetes and Red Hat OpenShift, it is increasingly important to be able to run the communication infrastructure on the same platform. Red Hat OpenShift, as a highly scalable platform, is a natural fit for messaging technologies such as Kafka. The AMQ streams component makes running and managing Apache Kafka OpenShift native through the use of powerful operators that simplify the deployment, configuration, management, and use of Apache Kafka on Red Hat OpenShift. The AMQ streams component is part of the Red Hat AMQ family, which also includes the AMQ broker, a longtime innovation leader in Java\u2122 Message Service (JMS) and polyglot messaging, as well as the AMQ interconnect router, a wide-area, peer-to-peer messaging solution. Under the covers, AMQ streams leverages Strimzi's architecture, resources and configurations. Note: The MAS license does not include entitlement for AMQ streams. The MAS Devops Collection supports this Kafka deployment as an example only. Therefore, we recommend the use of Strimzi for an opensource Kafka provider. Tip The role will generate a yaml file containing the definition of a Secret and KafkaCfg resource that can be used to configure the deployed cluster as the MAS system Kafka. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/kafkacfg-amqstreams-system.yaml or used in conjunction with the suite_config role. Role Variables \u00a4 General Variables \u00a4 kafka_action \u00a4 Specifies which operation to perform on the Kafka cluster. Optional Environment Variable: KAFKA_ACTION Default Value: install Purpose : Controls what action the role executes against Kafka deployments. This allows the same role to handle installation, upgrades, and removal of Kafka clusters. When to use : - Use install (default) for initial Kafka deployment - Use upgrade to upgrade existing Kafka cluster (Strimzi and Red Hat AMQ Streams only) - Use uninstall to remove Kafka cluster and operator Valid values : install , upgrade , uninstall Impact : - install : Deploys Kafka operator and creates cluster - upgrade : Upgrades existing Kafka cluster (only for Strimzi and Red Hat providers) - uninstall : Removes Kafka cluster and operator Related variables : - kafka_provider : Upgrade action only supported for strimzi and redhat providers - kafka_version : Target version when upgrading Note : The upgrade action is only available for Strimzi and Red Hat AMQ Streams providers. IBM Event Streams and AWS MSK do not support in-place upgrades through this role. kafka_provider \u00a4 Specifies which Kafka provider to use for deployment. Optional Environment Variable: KAFKA_PROVIDER Default Value: strimzi Purpose : Determines which Kafka implementation to deploy. Different providers offer different features, licensing models, and deployment targets (on-cluster vs cloud-managed). When to use : - Use strimzi (default) for open-source Kafka on OpenShift (recommended, no additional license required) - Use redhat for Red Hat AMQ Streams (requires separate license not included with MAS) - Use ibm for IBM Event Streams on IBM Cloud (managed service, additional cost) - Use aws for AWS MSK (managed service, additional cost) Valid values : strimzi , redhat , ibm , aws Impact : - strimzi : Deploys open-source Kafka using Strimzi operator (no additional license) - redhat : Deploys AMQ Streams (requires Red Hat AMQ license) - ibm : Provisions IBM Event Streams in IBM Cloud (requires IBM Cloud account and incurs costs) - aws : Provisions AWS MSK in AWS account (requires AWS account and incurs costs) Related variables : - Different providers require different additional variables - kafka_action=upgrade only supported for strimzi and redhat Note : IMPORTANT - MAS license does NOT include entitlement for Red Hat AMQ Streams. Strimzi is recommended for open-source Kafka. IBM and AWS providers provision managed cloud services with additional costs. Red Hat AMQ Streams & Strimzi Variables \u00a4 kafka_version \u00a4 Kafka version to deploy (Strimzi and Red Hat AMQ Streams only). Optional Environment Variable: KAFKA_VERSION Default Value: 3.8.0 for AMQ Streams, 3.9.0 for Strimzi Purpose : Specifies which Apache Kafka version to deploy when using Strimzi or Red Hat AMQ Streams providers. The version must be supported by the installed operator. When to use : - Leave as default for standard deployments - Set explicitly when you need a specific Kafka version - Verify version compatibility with operator before changing Valid values : Valid Kafka version supported by the operator (e.g., 3.8.0 , 3.9.0 ) Impact : Determines which Kafka version is deployed. Incompatible versions will cause deployment failures. Related variables : - kafka_provider : Only applies to strimzi and redhat providers - Operator version determines available Kafka versions Note : Before changing this value, verify the version is supported by your AMQ Streams operator or Strimzi operator . This variable does not apply to IBM Event Streams or AWS MSK providers. kafka_namespace \u00a4 OpenShift namespace where Kafka operator and cluster will be deployed. Optional Environment Variable: KAFKA_NAMESPACE Default Value: amq-streams for AMQ Streams, strimzi for Strimzi Purpose : Specifies the namespace for deploying the Kafka operator and Kafka cluster resources. This isolates Kafka resources from other applications. When to use : - Use default for standard single-cluster deployments - Set to custom namespace when organizing multiple Kafka deployments - Must be unique if deploying multiple Kafka clusters Valid values : Any valid Kubernetes namespace name (e.g., strimzi , amq-streams , kafka-prod ) Impact : All Kafka resources (operator, cluster, topics, users) are created in this namespace. Related variables : - kafka_provider : Default namespace depends on provider ( strimzi or amq-streams ) - kafka_cluster_name : Cluster created within this namespace Note : The default namespace differs by provider: strimzi for Strimzi provider, amq-streams for Red Hat AMQ Streams. This variable only applies to Strimzi and Red Hat providers. kafka_cluster_name \u00a4 Name for the Kafka cluster resource. Optional Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka Purpose : Defines the name of the Kafka custom resource that will be created. This name is used to identify the Kafka cluster and is embedded in resource names. When to use : - Use default ( maskafka ) for standard MAS deployments - Set to custom name when deploying multiple Kafka clusters - Use descriptive names for multi-cluster environments Valid values : Valid Kubernetes resource name (lowercase alphanumeric with hyphens, e.g., maskafka , kafka-prod , mas-kafka ) Impact : This name is used throughout the Kafka deployment in resource names (services, pods, secrets). It appears in the generated KafkaCfg for MAS. Related variables : - kafka_namespace : Cluster created within this namespace - Used in generated KafkaCfg when mas_instance_id is provided Note : Choose a meaningful name as it appears in many resource names. The default maskafka is suitable for single-cluster MAS deployments. kafka_cluster_size \u00a4 Predefined configuration size for the Kafka cluster. Optional Environment Variable: KAFKA_CLUSTER_SIZE Default Value: small Purpose : Selects a predefined resource configuration for the Kafka cluster. Different sizes allocate different amounts of CPU, memory, and replicas for Kafka brokers and Zookeeper nodes. When to use : - Use small (default) for development, test, or small production environments - Use large for production environments with higher throughput requirements - Choose based on expected message volume and performance needs Valid values : small , large Impact : - small : Fewer resources, suitable for dev/test or small workloads - large : More resources (CPU, memory, replicas) for production workloads Related variables : - kafka_storage_size : Storage size should align with cluster size - zookeeper_storage_size : Zookeeper storage should align with cluster size Note : The small configuration is suitable for development and testing. Production environments typically require the large configuration for adequate performance and reliability. kafka_storage_class \u00a4 Storage class for Kafka broker persistent storage (must support ReadWriteOnce). Optional Environment Variable: KAFKA_STORAGE_CLASS Default Value: Auto-detected from available storage classes in cluster Purpose : Specifies the storage class for Kafka broker persistent storage, which requires ReadWriteOnce (RWO) access mode. This is where Kafka stores message logs and data. When to use : - Leave unset for automatic detection (recommended) - Set explicitly when you need a specific storage class - Must be a storage class supporting RWO access mode - Typically block-based storage for performance Valid values : Any storage class name supporting ReadWriteOnce access mode Impact : Kafka broker performance depends heavily on storage performance. Choose high-performance storage for production workloads. Incorrect storage class or one not supporting RWO will cause deployment to fail. Related variables : - kafka_storage_size : Size of storage for Kafka brokers - zookeeper_storage_class : Separate storage class for Zookeeper Note : Block-based storage classes typically provide better performance for Kafka (e.g., ibmc-block-gold , ocs-storagecluster-ceph-rbd ). This variable only applies to Strimzi and Red Hat AMQ Streams providers. kafka_storage_size \u00a4 Size of persistent storage for Kafka brokers. Optional Environment Variable: KAFKA_STORAGE_SIZE Default Value: 100Gi Purpose : Specifies the size of persistent volumes for Kafka broker storage. This is where Kafka stores all message logs and topic data. When to use : - Use default ( 100Gi ) for development/test environments - Increase significantly for production based on message volume and retention - Plan for message throughput and retention period - Monitor usage and expand as needed Valid values : Kubernetes storage size format (e.g., 100Gi , 500Gi , 1Ti , 2Ti ) Impact : Insufficient storage will cause Kafka to stop accepting messages when volumes fill up. Size appropriately for your message volume and retention requirements. Related variables : - kafka_storage_class : Storage class for these volumes - kafka_cluster_size : Larger clusters may need more storage - Message retention settings affect storage requirements Note : The default 100Gi is suitable for small deployments only. Production environments typically require 500Gi or more depending on message volume and retention. Plan storage based on daily message volume \u00d7 retention days. zookeeper_storage_class \u00a4 Storage class for Zookeeper persistent storage (must support ReadWriteOnce). Optional Environment Variable: ZOOKEEPER_STORAGE_CLASS Default Value: Auto-detected from available storage classes in cluster Purpose : Specifies the storage class for Zookeeper persistent storage, which requires ReadWriteOnce (RWO) access mode. Zookeeper stores cluster metadata and coordination data. When to use : - Leave unset for automatic detection (recommended) - Set explicitly when you need a specific storage class - Must be a storage class supporting RWO access mode - Can be same as or different from kafka_storage_class Valid values : Any storage class name supporting ReadWriteOnce access mode Impact : Zookeeper is critical for Kafka cluster coordination. Reliable storage is essential. Incorrect storage class or one not supporting RWO will cause deployment to fail. Related variables : - zookeeper_storage_size : Size of storage for Zookeeper - kafka_storage_class : Separate storage class for Kafka brokers Note : Zookeeper storage requirements are typically much smaller than Kafka broker storage. This variable only applies to Strimzi and Red Hat AMQ Streams providers. zookeeper_storage_size \u00a4 Size of persistent storage for Zookeeper nodes. Optional Environment Variable: ZOOKEEPER_STORAGE_SIZE Default Value: 10Gi Purpose : Specifies the size of persistent volumes for Zookeeper storage. Zookeeper stores cluster metadata, configuration, and coordination data. When to use : - Use default ( 10Gi ) for most deployments - Increase only for very large Kafka clusters with many topics/partitions - Zookeeper storage needs are typically much smaller than Kafka broker storage Valid values : Kubernetes storage size format (e.g., 10Gi , 20Gi , 50Gi ) Impact : Insufficient Zookeeper storage can cause cluster coordination issues. However, Zookeeper storage requirements are typically modest. Related variables : - zookeeper_storage_class : Storage class for these volumes - kafka_storage_size : Kafka broker storage (typically much larger) Note : The default 10Gi is sufficient for most deployments. Zookeeper storage requirements grow slowly with cluster size. Only increase if you have a very large number of topics and partitions. kafka_user_name \u00a4 Username for MAS to authenticate with Kafka. Optional Environment Variable: KAFKA_USER_NAME Default Value: masuser Purpose : Defines the Kafka user that will be created for MAS to authenticate with the Kafka cluster. This user is configured with appropriate permissions for MAS operations. When to use : - Use default ( masuser ) for standard MAS deployments - Set to custom name when organizational policies require specific usernames - Must be unique within the Kafka cluster Valid values : Valid Kafka username string (e.g., masuser , mas-kafka-user ) Impact : This user is created in Kafka with necessary permissions and credentials are included in the generated KafkaCfg for MAS. Related variables : - kafka_user_password : Password for this user (Strimzi 0.25.0+/AMQ Streams 2.x+) - Used in generated KafkaCfg when mas_instance_id is provided Note : The default masuser is suitable for most deployments. This variable only applies to Strimzi and Red Hat AMQ Streams providers. kafka_user_password \u00a4 Password for the Kafka user (Strimzi 0.25.0+/AMQ Streams 2.x+). Optional Environment Variable: KAFKA_USER_PASSWORD Default Value: Randomly generated if not specified Purpose : Sets the password for the Kafka user specified in kafka_user_name . This password is used for SCRAM-SHA authentication. When to use : - Leave unset for automatic random password generation (recommended) - Set explicitly when you need a specific password - Requires Strimzi operator 0.25.0+ or AMQ Streams 2.x+ Valid values : Strong password string meeting security requirements Impact : This password is stored in Kafka secrets and included in the generated KafkaCfg for MAS. If not set, a secure random password is generated automatically. Related variables : - kafka_user_name : User for which this password is set - Used in generated KafkaCfg when mas_instance_id is provided Note : Automatic password generation is recommended for security. This feature requires Strimzi operator version 0.25.0 or AMQ Streams 2.x+. Keep passwords secure and do not commit to source control. mas_instance_id \u00a4 MAS instance ID for generating KafkaCfg configuration. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None Purpose : Identifies which MAS instance the generated KafkaCfg will target. When set (along with mas_config_dir ), the role generates a KafkaCfg resource for configuring MAS to use this Kafka cluster. When to use : - Set when you want to generate MAS configuration automatically - Must match the instance ID of your MAS installation - Required together with mas_config_dir for KafkaCfg generation Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : When set with mas_config_dir , generates a KafkaCfg YAML file that can be applied to configure MAS. Without this, no MAS configuration is generated. Related variables : - mas_config_dir : Required together with this for KafkaCfg generation - Generated KafkaCfg targets this MAS instance Note : If either mas_instance_id or mas_config_dir is not set, the role will not generate a KafkaCfg template. You'll need to configure MAS manually. mas_config_dir \u00a4 Local directory path where generated KafkaCfg will be saved. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None Purpose : Specifies where to save the generated KafkaCfg YAML file. This file can be manually applied to configure MAS or used with the suite_config role for automated configuration. When to use : - Set when you want to generate MAS configuration automatically - Use the same directory across all MAS setup roles for consistency - Required together with mas_instance_id for KafkaCfg generation Valid values : Any valid local filesystem path (e.g., /home/user/masconfig , ~/masconfig , ./config ) Impact : When set with mas_instance_id , generates kafkacfg-{provider}-system.yaml in this directory. The file can be applied with oc apply or used with suite_config role. Related variables : - mas_instance_id : Required together with this for KafkaCfg generation - kafka_provider : Affects the generated filename Note : If either mas_instance_id or mas_config_dir is not set, no KafkaCfg template is generated. Ensure the directory exists and is writable. custom_labels \u00a4 Comma-separated list of key=value labels to apply to Kafka resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Purpose : Adds Kubernetes labels to Kafka-related resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,component=kafka ) Impact : Labels are applied to Kafka-related resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect Kafka functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management. Note : This variable applies to Strimzi and Red Hat AMQ Streams providers. Labels help with resource organization and are especially useful in multi-tenant environments. IBM Cloud Evenstreams Role Variables \u00a4 ibmcloud_apikey \u00a4 Defines IBM Cloud API Key. This API Key needs to have access to manage (provision/deprovision) IBM Cloud Event Streams. Required Environment Variable: IBMCLOUD_APIKEY Default Value: None eventstreams_resourcegroup \u00a4 Defines the IBM Cloud Resource Group to target the Event Streams instance. Optional Environment Variable: EVENTSTREAMS_RESOURCEGROUP Default Value: Default or value defined by IBMCLOUD_RESOURCEGROUP eventstreams_name \u00a4 Event Streams instance name. Required Environment Variable: EVENTSTREAMS_NAME Default Value: None eventstreams_plan \u00a4 Event Streams instance plan. Optional Environment Variable: EVENTSTREAMS_PLAN Default Value: standard eventstreams_location \u00a4 Optional Environment Variable: EVENTSTREAMS_LOCATION Default Value: us-east or value defined by IBMCLOUD_REGION eventstreams_retention \u00a4 Event Streams topic retention period (in miliseconds). Optional Environment Variable: EVENTSTREAMS_RETENTION Default Value: 1209600000 eventstreams_create_manage_jms_topic \u00a4 Defines whether to create specific Manage application JMS topics by default. Optional Environment Variable: EVENTSTREAMS_CREATE_MANAGE_JMS_TOPICS Default Value: True mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # Set storage class suitable for use on IBM Cloud ROKS kafka_storage_class: ibmc-block-gold # Generate a KafkaCfg template mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.kafka AWS MSK Variables \u00a4 Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. kafka_version \u00a4 The version of Kafka to deploy for AWS MSK. Environment Variable: KAFKA_VERSION Default Value: 3.3.1 kafka_cluster_name \u00a4 The name of the Kafka cluster that will be created Required Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka aws_region \u00a4 Required Environment Variable: AWS_REGION Default Value: None vpc_id \u00a4 The AWS Virtual Private Cloud identifier (VPC ID) where the MSK instance will be hosted. Required Environment Variable: VPC_ID Default Value: None aws_msk_cidr_az1 \u00a4 The CIDR address for the first Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ1 Default Value: None aws_msk_cidr_az2 \u00a4 The CIDR address for the second Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ2 Default Value: None aws_msk_cidr_az3 \u00a4 The CIDR address for the third Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ3 Default Value: None aws_msk_ingress_cidr \u00a4 The IPv4 CIDR address for ingress connection. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_INGRESS_CIDR Default Value: None aws_msk_egress_cidr \u00a4 The IPv4 CIDR address for egress connection. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_EGRESS_CIDR Default Value: None aws_kafka_user_name \u00a4 The name of the user to setup in the cluster for MAS. Required Environment Variable: AWS_KAFKA_USER_NAME Default Value: None aws_kafka_user_password \u00a4 The password of the user to setup in the cluster for MAS. Optional Environment Variable: AWS_KAFKA_USER_PASSWORD Default Value: None aws_msk_instance_type \u00a4 The type/flavor of your MSK instance. Optional Environment Variable: AWS_MSK_INSTANCE_TYPE Default Value: kafka.m5.large aws_msk_volume_size \u00a4 The storage/volume size of your MSK instance. Optional Environment Variable: AWS_MSK_VOLUME_SIZE Default Value: 100 aws_msk_instance_number \u00a4 The number of broker/instances of your MSK instance. Optional Environment Variable: AWS_MSK_INSTANCE_NUMBER Default Value: 3 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None aws_msk_secret \u00a4 AWS MSK Secret name. The secret name must begin with the prefix AmazonMSK_. If this is not set, then default secret name will be AmazonMSK_SECRET_{{kafka_cluster_name}} Optional Environment Variable: AWS_MSK_SECRET Default Value: AmazonMSK_SECRET_{{kafka_cluster_name}}' Install AWS MSK \u00a4 - hosts: localhost any_errors_fatal: true vars: aws_region: ca-central-1 aws_access_key_id: ***** aws_secret_access_key: ***** kafka_version: 3.3.1 kafka_provider: aws kafka_action: install kafka_cluster_name: msk-abcd0zyxw kafka_namespace: msk-abcd0zyxw vpc_id: vpc-07088da510b3c35c5 aws_kafka_user_name: mskuser-abcd0zyxw aws_msk_instance_type: kafka.t3.small aws_msk_volume_size: 100 aws_msk_instance_number: 3 aws_msk_cidr_az1: \"10.0.128.0/20\" aws_msk_cidr_az2: \"10.0.144.0/20\" aws_msk_cidr_az3: \"10.0.160.0/20\" aws_msk_ingress_cidr: \"10.0.0.0/16\" aws_msk_egress_cidr: \"10.0.0.0/16\" # Generate a KafkaCfg template mas_config_dir: /var/tmp/masconfigdir mas_instance_id: abcd0zyxw roles: - ibm.mas_devops.kafka Uninstall AWS MSK \u00a4 - hosts: localhost any_errors_fatal: true vars: aws_region: ca-central-1 aws_access_key_id: ***** aws_secret_access_key: ***** vpc_id: vpc-07088da510b3c35c5 kafka_provider: aws kafka_action: uninstall kafka_cluster_name: msk-abcd0zyxw aws_msk_cidr_az1: \"10.0.128.0/20\" aws_msk_cidr_az2: \"10.0.144.0/20\" aws_msk_cidr_az3: \"10.0.160.0/20\" roles: - ibm.mas_devops.kafka Run Role Playbook \u00a4 export KAFKA_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=masinst1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"kafka"},{"location":"roles/kafka/#kafka","text":"This role provides support to install a Kafka Cluster using Strimzi , Red Hat AMQ Streams , IBM Event Streams or AWS MSK and generate configuration that can be directly applied to Maximo Application Suite. Both Strimzi and Red Hat AMQ streams component are massively scalable, distributed, and high-performance data streaming platform based on the Apache Kafka project. Both offer a distributed backbone that allows microservices and other applications to share data with high throughput and low latency. As more applications move to Kubernetes and Red Hat OpenShift, it is increasingly important to be able to run the communication infrastructure on the same platform. Red Hat OpenShift, as a highly scalable platform, is a natural fit for messaging technologies such as Kafka. The AMQ streams component makes running and managing Apache Kafka OpenShift native through the use of powerful operators that simplify the deployment, configuration, management, and use of Apache Kafka on Red Hat OpenShift. The AMQ streams component is part of the Red Hat AMQ family, which also includes the AMQ broker, a longtime innovation leader in Java\u2122 Message Service (JMS) and polyglot messaging, as well as the AMQ interconnect router, a wide-area, peer-to-peer messaging solution. Under the covers, AMQ streams leverages Strimzi's architecture, resources and configurations. Note: The MAS license does not include entitlement for AMQ streams. The MAS Devops Collection supports this Kafka deployment as an example only. Therefore, we recommend the use of Strimzi for an opensource Kafka provider. Tip The role will generate a yaml file containing the definition of a Secret and KafkaCfg resource that can be used to configure the deployed cluster as the MAS system Kafka. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/kafkacfg-amqstreams-system.yaml or used in conjunction with the suite_config role.","title":"kafka"},{"location":"roles/kafka/#role-variables","text":"","title":"Role Variables"},{"location":"roles/kafka/#general-variables","text":"","title":"General Variables"},{"location":"roles/kafka/#kafka_action","text":"Specifies which operation to perform on the Kafka cluster. Optional Environment Variable: KAFKA_ACTION Default Value: install Purpose : Controls what action the role executes against Kafka deployments. This allows the same role to handle installation, upgrades, and removal of Kafka clusters. When to use : - Use install (default) for initial Kafka deployment - Use upgrade to upgrade existing Kafka cluster (Strimzi and Red Hat AMQ Streams only) - Use uninstall to remove Kafka cluster and operator Valid values : install , upgrade , uninstall Impact : - install : Deploys Kafka operator and creates cluster - upgrade : Upgrades existing Kafka cluster (only for Strimzi and Red Hat providers) - uninstall : Removes Kafka cluster and operator Related variables : - kafka_provider : Upgrade action only supported for strimzi and redhat providers - kafka_version : Target version when upgrading Note : The upgrade action is only available for Strimzi and Red Hat AMQ Streams providers. IBM Event Streams and AWS MSK do not support in-place upgrades through this role.","title":"kafka_action"},{"location":"roles/kafka/#kafka_provider","text":"Specifies which Kafka provider to use for deployment. Optional Environment Variable: KAFKA_PROVIDER Default Value: strimzi Purpose : Determines which Kafka implementation to deploy. Different providers offer different features, licensing models, and deployment targets (on-cluster vs cloud-managed). When to use : - Use strimzi (default) for open-source Kafka on OpenShift (recommended, no additional license required) - Use redhat for Red Hat AMQ Streams (requires separate license not included with MAS) - Use ibm for IBM Event Streams on IBM Cloud (managed service, additional cost) - Use aws for AWS MSK (managed service, additional cost) Valid values : strimzi , redhat , ibm , aws Impact : - strimzi : Deploys open-source Kafka using Strimzi operator (no additional license) - redhat : Deploys AMQ Streams (requires Red Hat AMQ license) - ibm : Provisions IBM Event Streams in IBM Cloud (requires IBM Cloud account and incurs costs) - aws : Provisions AWS MSK in AWS account (requires AWS account and incurs costs) Related variables : - Different providers require different additional variables - kafka_action=upgrade only supported for strimzi and redhat Note : IMPORTANT - MAS license does NOT include entitlement for Red Hat AMQ Streams. Strimzi is recommended for open-source Kafka. IBM and AWS providers provision managed cloud services with additional costs.","title":"kafka_provider"},{"location":"roles/kafka/#red-hat-amq-streams-strimzi-variables","text":"","title":"Red Hat AMQ Streams &amp; Strimzi Variables"},{"location":"roles/kafka/#kafka_version","text":"Kafka version to deploy (Strimzi and Red Hat AMQ Streams only). Optional Environment Variable: KAFKA_VERSION Default Value: 3.8.0 for AMQ Streams, 3.9.0 for Strimzi Purpose : Specifies which Apache Kafka version to deploy when using Strimzi or Red Hat AMQ Streams providers. The version must be supported by the installed operator. When to use : - Leave as default for standard deployments - Set explicitly when you need a specific Kafka version - Verify version compatibility with operator before changing Valid values : Valid Kafka version supported by the operator (e.g., 3.8.0 , 3.9.0 ) Impact : Determines which Kafka version is deployed. Incompatible versions will cause deployment failures. Related variables : - kafka_provider : Only applies to strimzi and redhat providers - Operator version determines available Kafka versions Note : Before changing this value, verify the version is supported by your AMQ Streams operator or Strimzi operator . This variable does not apply to IBM Event Streams or AWS MSK providers.","title":"kafka_version"},{"location":"roles/kafka/#kafka_namespace","text":"OpenShift namespace where Kafka operator and cluster will be deployed. Optional Environment Variable: KAFKA_NAMESPACE Default Value: amq-streams for AMQ Streams, strimzi for Strimzi Purpose : Specifies the namespace for deploying the Kafka operator and Kafka cluster resources. This isolates Kafka resources from other applications. When to use : - Use default for standard single-cluster deployments - Set to custom namespace when organizing multiple Kafka deployments - Must be unique if deploying multiple Kafka clusters Valid values : Any valid Kubernetes namespace name (e.g., strimzi , amq-streams , kafka-prod ) Impact : All Kafka resources (operator, cluster, topics, users) are created in this namespace. Related variables : - kafka_provider : Default namespace depends on provider ( strimzi or amq-streams ) - kafka_cluster_name : Cluster created within this namespace Note : The default namespace differs by provider: strimzi for Strimzi provider, amq-streams for Red Hat AMQ Streams. This variable only applies to Strimzi and Red Hat providers.","title":"kafka_namespace"},{"location":"roles/kafka/#kafka_cluster_name","text":"Name for the Kafka cluster resource. Optional Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka Purpose : Defines the name of the Kafka custom resource that will be created. This name is used to identify the Kafka cluster and is embedded in resource names. When to use : - Use default ( maskafka ) for standard MAS deployments - Set to custom name when deploying multiple Kafka clusters - Use descriptive names for multi-cluster environments Valid values : Valid Kubernetes resource name (lowercase alphanumeric with hyphens, e.g., maskafka , kafka-prod , mas-kafka ) Impact : This name is used throughout the Kafka deployment in resource names (services, pods, secrets). It appears in the generated KafkaCfg for MAS. Related variables : - kafka_namespace : Cluster created within this namespace - Used in generated KafkaCfg when mas_instance_id is provided Note : Choose a meaningful name as it appears in many resource names. The default maskafka is suitable for single-cluster MAS deployments.","title":"kafka_cluster_name"},{"location":"roles/kafka/#kafka_cluster_size","text":"Predefined configuration size for the Kafka cluster. Optional Environment Variable: KAFKA_CLUSTER_SIZE Default Value: small Purpose : Selects a predefined resource configuration for the Kafka cluster. Different sizes allocate different amounts of CPU, memory, and replicas for Kafka brokers and Zookeeper nodes. When to use : - Use small (default) for development, test, or small production environments - Use large for production environments with higher throughput requirements - Choose based on expected message volume and performance needs Valid values : small , large Impact : - small : Fewer resources, suitable for dev/test or small workloads - large : More resources (CPU, memory, replicas) for production workloads Related variables : - kafka_storage_size : Storage size should align with cluster size - zookeeper_storage_size : Zookeeper storage should align with cluster size Note : The small configuration is suitable for development and testing. Production environments typically require the large configuration for adequate performance and reliability.","title":"kafka_cluster_size"},{"location":"roles/kafka/#kafka_storage_class","text":"Storage class for Kafka broker persistent storage (must support ReadWriteOnce). Optional Environment Variable: KAFKA_STORAGE_CLASS Default Value: Auto-detected from available storage classes in cluster Purpose : Specifies the storage class for Kafka broker persistent storage, which requires ReadWriteOnce (RWO) access mode. This is where Kafka stores message logs and data. When to use : - Leave unset for automatic detection (recommended) - Set explicitly when you need a specific storage class - Must be a storage class supporting RWO access mode - Typically block-based storage for performance Valid values : Any storage class name supporting ReadWriteOnce access mode Impact : Kafka broker performance depends heavily on storage performance. Choose high-performance storage for production workloads. Incorrect storage class or one not supporting RWO will cause deployment to fail. Related variables : - kafka_storage_size : Size of storage for Kafka brokers - zookeeper_storage_class : Separate storage class for Zookeeper Note : Block-based storage classes typically provide better performance for Kafka (e.g., ibmc-block-gold , ocs-storagecluster-ceph-rbd ). This variable only applies to Strimzi and Red Hat AMQ Streams providers.","title":"kafka_storage_class"},{"location":"roles/kafka/#kafka_storage_size","text":"Size of persistent storage for Kafka brokers. Optional Environment Variable: KAFKA_STORAGE_SIZE Default Value: 100Gi Purpose : Specifies the size of persistent volumes for Kafka broker storage. This is where Kafka stores all message logs and topic data. When to use : - Use default ( 100Gi ) for development/test environments - Increase significantly for production based on message volume and retention - Plan for message throughput and retention period - Monitor usage and expand as needed Valid values : Kubernetes storage size format (e.g., 100Gi , 500Gi , 1Ti , 2Ti ) Impact : Insufficient storage will cause Kafka to stop accepting messages when volumes fill up. Size appropriately for your message volume and retention requirements. Related variables : - kafka_storage_class : Storage class for these volumes - kafka_cluster_size : Larger clusters may need more storage - Message retention settings affect storage requirements Note : The default 100Gi is suitable for small deployments only. Production environments typically require 500Gi or more depending on message volume and retention. Plan storage based on daily message volume \u00d7 retention days.","title":"kafka_storage_size"},{"location":"roles/kafka/#zookeeper_storage_class","text":"Storage class for Zookeeper persistent storage (must support ReadWriteOnce). Optional Environment Variable: ZOOKEEPER_STORAGE_CLASS Default Value: Auto-detected from available storage classes in cluster Purpose : Specifies the storage class for Zookeeper persistent storage, which requires ReadWriteOnce (RWO) access mode. Zookeeper stores cluster metadata and coordination data. When to use : - Leave unset for automatic detection (recommended) - Set explicitly when you need a specific storage class - Must be a storage class supporting RWO access mode - Can be same as or different from kafka_storage_class Valid values : Any storage class name supporting ReadWriteOnce access mode Impact : Zookeeper is critical for Kafka cluster coordination. Reliable storage is essential. Incorrect storage class or one not supporting RWO will cause deployment to fail. Related variables : - zookeeper_storage_size : Size of storage for Zookeeper - kafka_storage_class : Separate storage class for Kafka brokers Note : Zookeeper storage requirements are typically much smaller than Kafka broker storage. This variable only applies to Strimzi and Red Hat AMQ Streams providers.","title":"zookeeper_storage_class"},{"location":"roles/kafka/#zookeeper_storage_size","text":"Size of persistent storage for Zookeeper nodes. Optional Environment Variable: ZOOKEEPER_STORAGE_SIZE Default Value: 10Gi Purpose : Specifies the size of persistent volumes for Zookeeper storage. Zookeeper stores cluster metadata, configuration, and coordination data. When to use : - Use default ( 10Gi ) for most deployments - Increase only for very large Kafka clusters with many topics/partitions - Zookeeper storage needs are typically much smaller than Kafka broker storage Valid values : Kubernetes storage size format (e.g., 10Gi , 20Gi , 50Gi ) Impact : Insufficient Zookeeper storage can cause cluster coordination issues. However, Zookeeper storage requirements are typically modest. Related variables : - zookeeper_storage_class : Storage class for these volumes - kafka_storage_size : Kafka broker storage (typically much larger) Note : The default 10Gi is sufficient for most deployments. Zookeeper storage requirements grow slowly with cluster size. Only increase if you have a very large number of topics and partitions.","title":"zookeeper_storage_size"},{"location":"roles/kafka/#kafka_user_name","text":"Username for MAS to authenticate with Kafka. Optional Environment Variable: KAFKA_USER_NAME Default Value: masuser Purpose : Defines the Kafka user that will be created for MAS to authenticate with the Kafka cluster. This user is configured with appropriate permissions for MAS operations. When to use : - Use default ( masuser ) for standard MAS deployments - Set to custom name when organizational policies require specific usernames - Must be unique within the Kafka cluster Valid values : Valid Kafka username string (e.g., masuser , mas-kafka-user ) Impact : This user is created in Kafka with necessary permissions and credentials are included in the generated KafkaCfg for MAS. Related variables : - kafka_user_password : Password for this user (Strimzi 0.25.0+/AMQ Streams 2.x+) - Used in generated KafkaCfg when mas_instance_id is provided Note : The default masuser is suitable for most deployments. This variable only applies to Strimzi and Red Hat AMQ Streams providers.","title":"kafka_user_name"},{"location":"roles/kafka/#kafka_user_password","text":"Password for the Kafka user (Strimzi 0.25.0+/AMQ Streams 2.x+). Optional Environment Variable: KAFKA_USER_PASSWORD Default Value: Randomly generated if not specified Purpose : Sets the password for the Kafka user specified in kafka_user_name . This password is used for SCRAM-SHA authentication. When to use : - Leave unset for automatic random password generation (recommended) - Set explicitly when you need a specific password - Requires Strimzi operator 0.25.0+ or AMQ Streams 2.x+ Valid values : Strong password string meeting security requirements Impact : This password is stored in Kafka secrets and included in the generated KafkaCfg for MAS. If not set, a secure random password is generated automatically. Related variables : - kafka_user_name : User for which this password is set - Used in generated KafkaCfg when mas_instance_id is provided Note : Automatic password generation is recommended for security. This feature requires Strimzi operator version 0.25.0 or AMQ Streams 2.x+. Keep passwords secure and do not commit to source control.","title":"kafka_user_password"},{"location":"roles/kafka/#mas_instance_id","text":"MAS instance ID for generating KafkaCfg configuration. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None Purpose : Identifies which MAS instance the generated KafkaCfg will target. When set (along with mas_config_dir ), the role generates a KafkaCfg resource for configuring MAS to use this Kafka cluster. When to use : - Set when you want to generate MAS configuration automatically - Must match the instance ID of your MAS installation - Required together with mas_config_dir for KafkaCfg generation Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : When set with mas_config_dir , generates a KafkaCfg YAML file that can be applied to configure MAS. Without this, no MAS configuration is generated. Related variables : - mas_config_dir : Required together with this for KafkaCfg generation - Generated KafkaCfg targets this MAS instance Note : If either mas_instance_id or mas_config_dir is not set, the role will not generate a KafkaCfg template. You'll need to configure MAS manually.","title":"mas_instance_id"},{"location":"roles/kafka/#mas_config_dir","text":"Local directory path where generated KafkaCfg will be saved. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None Purpose : Specifies where to save the generated KafkaCfg YAML file. This file can be manually applied to configure MAS or used with the suite_config role for automated configuration. When to use : - Set when you want to generate MAS configuration automatically - Use the same directory across all MAS setup roles for consistency - Required together with mas_instance_id for KafkaCfg generation Valid values : Any valid local filesystem path (e.g., /home/user/masconfig , ~/masconfig , ./config ) Impact : When set with mas_instance_id , generates kafkacfg-{provider}-system.yaml in this directory. The file can be applied with oc apply or used with suite_config role. Related variables : - mas_instance_id : Required together with this for KafkaCfg generation - kafka_provider : Affects the generated filename Note : If either mas_instance_id or mas_config_dir is not set, no KafkaCfg template is generated. Ensure the directory exists and is writable.","title":"mas_config_dir"},{"location":"roles/kafka/#custom_labels","text":"Comma-separated list of key=value labels to apply to Kafka resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Purpose : Adds Kubernetes labels to Kafka-related resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,component=kafka ) Impact : Labels are applied to Kafka-related resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect Kafka functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management. Note : This variable applies to Strimzi and Red Hat AMQ Streams providers. Labels help with resource organization and are especially useful in multi-tenant environments.","title":"custom_labels"},{"location":"roles/kafka/#ibm-cloud-evenstreams-role-variables","text":"","title":"IBM Cloud Evenstreams Role Variables"},{"location":"roles/kafka/#ibmcloud_apikey","text":"Defines IBM Cloud API Key. This API Key needs to have access to manage (provision/deprovision) IBM Cloud Event Streams. Required Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/kafka/#eventstreams_resourcegroup","text":"Defines the IBM Cloud Resource Group to target the Event Streams instance. Optional Environment Variable: EVENTSTREAMS_RESOURCEGROUP Default Value: Default or value defined by IBMCLOUD_RESOURCEGROUP","title":"eventstreams_resourcegroup"},{"location":"roles/kafka/#eventstreams_name","text":"Event Streams instance name. Required Environment Variable: EVENTSTREAMS_NAME Default Value: None","title":"eventstreams_name"},{"location":"roles/kafka/#eventstreams_plan","text":"Event Streams instance plan. Optional Environment Variable: EVENTSTREAMS_PLAN Default Value: standard","title":"eventstreams_plan"},{"location":"roles/kafka/#eventstreams_location","text":"Optional Environment Variable: EVENTSTREAMS_LOCATION Default Value: us-east or value defined by IBMCLOUD_REGION","title":"eventstreams_location"},{"location":"roles/kafka/#eventstreams_retention","text":"Event Streams topic retention period (in miliseconds). Optional Environment Variable: EVENTSTREAMS_RETENTION Default Value: 1209600000","title":"eventstreams_retention"},{"location":"roles/kafka/#eventstreams_create_manage_jms_topic","text":"Defines whether to create specific Manage application JMS topics by default. Optional Environment Variable: EVENTSTREAMS_CREATE_MANAGE_JMS_TOPICS Default Value: True","title":"eventstreams_create_manage_jms_topic"},{"location":"roles/kafka/#mas_instance_id_1","text":"The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/kafka/#mas_config_dir_1","text":"Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/kafka/#custom_labels_1","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/kafka/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Set storage class suitable for use on IBM Cloud ROKS kafka_storage_class: ibmc-block-gold # Generate a KafkaCfg template mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.kafka","title":"Example Playbook"},{"location":"roles/kafka/#aws-msk-variables","text":"","title":"AWS MSK Variables"},{"location":"roles/kafka/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/kafka/#kafka_version_1","text":"The version of Kafka to deploy for AWS MSK. Environment Variable: KAFKA_VERSION Default Value: 3.3.1","title":"kafka_version"},{"location":"roles/kafka/#kafka_cluster_name_1","text":"The name of the Kafka cluster that will be created Required Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka","title":"kafka_cluster_name"},{"location":"roles/kafka/#aws_region","text":"Required Environment Variable: AWS_REGION Default Value: None","title":"aws_region"},{"location":"roles/kafka/#vpc_id","text":"The AWS Virtual Private Cloud identifier (VPC ID) where the MSK instance will be hosted. Required Environment Variable: VPC_ID Default Value: None","title":"vpc_id"},{"location":"roles/kafka/#aws_msk_cidr_az1","text":"The CIDR address for the first Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ1 Default Value: None","title":"aws_msk_cidr_az1"},{"location":"roles/kafka/#aws_msk_cidr_az2","text":"The CIDR address for the second Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ2 Default Value: None","title":"aws_msk_cidr_az2"},{"location":"roles/kafka/#aws_msk_cidr_az3","text":"The CIDR address for the third Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ3 Default Value: None","title":"aws_msk_cidr_az3"},{"location":"roles/kafka/#aws_msk_ingress_cidr","text":"The IPv4 CIDR address for ingress connection. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_INGRESS_CIDR Default Value: None","title":"aws_msk_ingress_cidr"},{"location":"roles/kafka/#aws_msk_egress_cidr","text":"The IPv4 CIDR address for egress connection. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_EGRESS_CIDR Default Value: None","title":"aws_msk_egress_cidr"},{"location":"roles/kafka/#aws_kafka_user_name","text":"The name of the user to setup in the cluster for MAS. Required Environment Variable: AWS_KAFKA_USER_NAME Default Value: None","title":"aws_kafka_user_name"},{"location":"roles/kafka/#aws_kafka_user_password","text":"The password of the user to setup in the cluster for MAS. Optional Environment Variable: AWS_KAFKA_USER_PASSWORD Default Value: None","title":"aws_kafka_user_password"},{"location":"roles/kafka/#aws_msk_instance_type","text":"The type/flavor of your MSK instance. Optional Environment Variable: AWS_MSK_INSTANCE_TYPE Default Value: kafka.m5.large","title":"aws_msk_instance_type"},{"location":"roles/kafka/#aws_msk_volume_size","text":"The storage/volume size of your MSK instance. Optional Environment Variable: AWS_MSK_VOLUME_SIZE Default Value: 100","title":"aws_msk_volume_size"},{"location":"roles/kafka/#aws_msk_instance_number","text":"The number of broker/instances of your MSK instance. Optional Environment Variable: AWS_MSK_INSTANCE_NUMBER Default Value: 3","title":"aws_msk_instance_number"},{"location":"roles/kafka/#mas_instance_id_2","text":"The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/kafka/#mas_config_dir_2","text":"Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/kafka/#custom_labels_2","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/kafka/#aws_msk_secret","text":"AWS MSK Secret name. The secret name must begin with the prefix AmazonMSK_. If this is not set, then default secret name will be AmazonMSK_SECRET_{{kafka_cluster_name}} Optional Environment Variable: AWS_MSK_SECRET Default Value: AmazonMSK_SECRET_{{kafka_cluster_name}}'","title":"aws_msk_secret"},{"location":"roles/kafka/#install-aws-msk","text":"- hosts: localhost any_errors_fatal: true vars: aws_region: ca-central-1 aws_access_key_id: ***** aws_secret_access_key: ***** kafka_version: 3.3.1 kafka_provider: aws kafka_action: install kafka_cluster_name: msk-abcd0zyxw kafka_namespace: msk-abcd0zyxw vpc_id: vpc-07088da510b3c35c5 aws_kafka_user_name: mskuser-abcd0zyxw aws_msk_instance_type: kafka.t3.small aws_msk_volume_size: 100 aws_msk_instance_number: 3 aws_msk_cidr_az1: \"10.0.128.0/20\" aws_msk_cidr_az2: \"10.0.144.0/20\" aws_msk_cidr_az3: \"10.0.160.0/20\" aws_msk_ingress_cidr: \"10.0.0.0/16\" aws_msk_egress_cidr: \"10.0.0.0/16\" # Generate a KafkaCfg template mas_config_dir: /var/tmp/masconfigdir mas_instance_id: abcd0zyxw roles: - ibm.mas_devops.kafka","title":"Install AWS MSK"},{"location":"roles/kafka/#uninstall-aws-msk","text":"- hosts: localhost any_errors_fatal: true vars: aws_region: ca-central-1 aws_access_key_id: ***** aws_secret_access_key: ***** vpc_id: vpc-07088da510b3c35c5 kafka_provider: aws kafka_action: uninstall kafka_cluster_name: msk-abcd0zyxw aws_msk_cidr_az1: \"10.0.128.0/20\" aws_msk_cidr_az2: \"10.0.144.0/20\" aws_msk_cidr_az3: \"10.0.160.0/20\" roles: - ibm.mas_devops.kafka","title":"Uninstall AWS MSK"},{"location":"roles/kafka/#run-role-playbook","text":"export KAFKA_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=masinst1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/kafka/#license","text":"EPL-2.0","title":"License"},{"location":"roles/key_rotation/","text":"key_rotation \u00a4 Create new apikey for user in cloud account and delete the existing one. Role Variables \u00a4 cluster_type \u00a4 Required. Specify the cluster type, supported values are roks , and rosa . Environment Variable: CLUSTER_TYPE Default Value: None Role Variables - ROKS \u00a4 ibmcloud_apikey \u00a4 Required. A new key will be created and this key will be deleted. Environment Variable: IBMCLOUD_APIKEY Default: None ibmcloud_keyname \u00a4 Required. A new key will be created and this key will be deleted. Environment Variable: IBMCLOUD_KEYNAME Default: None ibmcloud_output_keydir \u00a4 Optional. Environment Variable: IBMCLOUD_OUTPUT_KEYDIR Default: '/tmp' Role Variables - ROSA or IPI/AWS \u00a4 The following variables are used when cluster_type = rosa or cluster_type=ipe and cluster_platform=aws . aws_region \u00a4 Required when cluster_type = rosa or cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_REGION Default Value: us-east-1 aws_username \u00a4 Required. Environment Variable: AWS_USERNAME Default: None aws_access_key_id \u00a4 Required. A new key will be created and this key will be deleted. Environment Variable: AWS_ACCESS_KEY_ID Default: None aws_secret_access_key \u00a4 Required. A new key will be created and this key will be deleted. Environment Variable: AWS_SECRET_ACCESS_KEY Default: None Example Playbook \u00a4 - hosts: localhost vars: cluster_type: roks ibmcloud_apikey: ################ ibmcloud_keyname: myapikeyname roles: - ibm.mas_devops.key_rotation License \u00a4 EPL-2.0","title":"key_rotation"},{"location":"roles/key_rotation/#key_rotation","text":"Create new apikey for user in cloud account and delete the existing one.","title":"key_rotation"},{"location":"roles/key_rotation/#role-variables","text":"","title":"Role Variables"},{"location":"roles/key_rotation/#cluster_type","text":"Required. Specify the cluster type, supported values are roks , and rosa . Environment Variable: CLUSTER_TYPE Default Value: None","title":"cluster_type"},{"location":"roles/key_rotation/#role-variables-roks","text":"","title":"Role Variables - ROKS"},{"location":"roles/key_rotation/#ibmcloud_apikey","text":"Required. A new key will be created and this key will be deleted. Environment Variable: IBMCLOUD_APIKEY Default: None","title":"ibmcloud_apikey"},{"location":"roles/key_rotation/#ibmcloud_keyname","text":"Required. A new key will be created and this key will be deleted. Environment Variable: IBMCLOUD_KEYNAME Default: None","title":"ibmcloud_keyname"},{"location":"roles/key_rotation/#ibmcloud_output_keydir","text":"Optional. Environment Variable: IBMCLOUD_OUTPUT_KEYDIR Default: '/tmp'","title":"ibmcloud_output_keydir"},{"location":"roles/key_rotation/#role-variables-rosa-or-ipiaws","text":"The following variables are used when cluster_type = rosa or cluster_type=ipe and cluster_platform=aws .","title":"Role Variables - ROSA or IPI/AWS"},{"location":"roles/key_rotation/#aws_region","text":"Required when cluster_type = rosa or cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_REGION Default Value: us-east-1","title":"aws_region"},{"location":"roles/key_rotation/#aws_username","text":"Required. Environment Variable: AWS_USERNAME Default: None","title":"aws_username"},{"location":"roles/key_rotation/#aws_access_key_id","text":"Required. A new key will be created and this key will be deleted. Environment Variable: AWS_ACCESS_KEY_ID Default: None","title":"aws_access_key_id"},{"location":"roles/key_rotation/#aws_secret_access_key","text":"Required. A new key will be created and this key will be deleted. Environment Variable: AWS_SECRET_ACCESS_KEY Default: None","title":"aws_secret_access_key"},{"location":"roles/key_rotation/#example-playbook","text":"- hosts: localhost vars: cluster_type: roks ibmcloud_apikey: ################ ibmcloud_keyname: myapikeyname roles: - ibm.mas_devops.key_rotation","title":"Example Playbook"},{"location":"roles/key_rotation/#license","text":"EPL-2.0","title":"License"},{"location":"roles/longhorn/","text":"Longhorn provides a single solution to fulfil both the ReadWriteMany and ReadWriteOnce storage requirements for Maximo Application Suite. Longhorn is a lightweight, reliable and easy-to-use distributed block storage system for Kubernetes. Longhorn is free, open source software. Originally developed by Rancher Labs, it is now being developed as a incubating project of the Cloud Native Computing Foundation. The Longhorn UI will be available at https://longhorn-ui-longhorn-system.{clusterdomain} (authentication via OpenShift OAuth). More information: - What is Longhorn? - Longhorn Helm Chart Settings - Longhorn on OpemShift Readme oc -n longhorn-system get deployments NAME READY UP-TO-DATE AVAILABLE AGE csi-attacher 3/3 3 3 38m csi-provisioner 3/3 3 3 38m csi-resizer 3/3 3 3 38m csi-snapshotter 3/3 3 3 38m longhorn-driver-deployer 1/1 1 1 40m longhorn-ui 2/2 2 2 40m Two storage classes will be set up automatically, Maximo Application Suite uses dynamic provisioning and as such will not use the longhorn-static storage class: oc get storageclass | grep longhorn longhorn (default) driver.longhorn.io Delete Immediate true 40m longhorn-static driver.longhorn.io Delete Immediate true 40m Role Variables \u00a4 longhorn_namespace \u00a4 Define the namespace where Longhorn will be installed. Optional Environment Variable: LONGHORN_NAMESPACE Default Value: longhorn-system longhorn_replica_count \u00a4 The replica count in Longhorn determines the number of copies of a volume's data stored across different nodes in a Kubernetes cluster, which directly impacts data availability and resilience. The default replica count of 3 allows the system to tolerate up to two replica failures while maintaining data integrity, but in development system you may prefer to set this to 1 to sacrifice resiliance in favour of reduced storage requirements. Optional Environment Variable: LONGHORN_REPLICA_COUNT Default Value: 3","title":"longhorn"},{"location":"roles/longhorn/#role-variables","text":"","title":"Role Variables"},{"location":"roles/longhorn/#longhorn_namespace","text":"Define the namespace where Longhorn will be installed. Optional Environment Variable: LONGHORN_NAMESPACE Default Value: longhorn-system","title":"longhorn_namespace"},{"location":"roles/longhorn/#longhorn_replica_count","text":"The replica count in Longhorn determines the number of copies of a volume's data stored across different nodes in a Kubernetes cluster, which directly impacts data availability and resilience. The default replica count of 3 allows the system to tolerate up to two replica failures while maintaining data integrity, but in development system you may prefer to set this to 1 to sacrifice resiliance in favour of reduced storage requirements. Optional Environment Variable: LONGHORN_REPLICA_COUNT Default Value: 3","title":"longhorn_replica_count"},{"location":"roles/mirror_case_prepare/","text":"mirror_case_prepare \u00a4 This role generates a mirror manifest file suitable for use with the oc mirror command (or the ibm.mas_devops.mirror_images role) from an IBM CASE bundle. Requirements \u00a4 The ibm-pak plugin must be installed. Role Variables \u00a4 case_name \u00a4 The name of the CASE bundle to prepare for mirroring. Required Environment Variable: CASE_NAME Default: None case_version \u00a4 The version of the CASE bundle to prepare for mirroring. Required Environment Variable: CASE_VERSION Default: None registry_public_host \u00a4 The public hostname for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None registry_public_port \u00a4 The public port for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None registry_prefix \u00a4 The prefix used for the target registry. The images will not be mirrored to the registry at this time but will define the final destination in the form: {host}:{port}/{prefix}/{reponame} Environment Variable: REGISTRY_PREFIX Default: None exclude_images \u00a4 A list of child CASE bundles to exclude from the mirroring process. Optional Environment Variable: None Default: None Role Variables - IBM Pak \u00a4 ibmpak_skip_verify \u00a4 Skip the certification verification when downloading CASE bundles with oc ibm-pak get . Optional Environment Variable: IBMPAK_SKIP_VERIFY Default: False ibmpak_skip_dependencies \u00a4 Skip downloading CASE bundle dependencies with oc ibm-pak get . Optional Environment Variable: IBMPAK_SKIP_DEPENDENCIES Default: False ibmpak_insecure \u00a4 Skip TLS/SSL verification when downloading CASE bundles with oc ibm-pak get . Optional Environment Variable: IBMPAK_INSECURE Default: False Example Playbook \u00a4 - hosts: localhost vars: case_name: ibm-mas case_version: 8.8.1 exclude_images: - ibm-truststore-mgr - ibm-sls - ibm-mas-assist - ibm-mas-iot - ibm-mas-manage registry_public_host: myregistry.com registry_public_port: 32500 registry_prefix: projectName roles: - ibm.mas_devops.mirror_case_prepare License \u00a4 EPL-2.0","title":"mirror_case_prepare"},{"location":"roles/mirror_case_prepare/#mirror_case_prepare","text":"This role generates a mirror manifest file suitable for use with the oc mirror command (or the ibm.mas_devops.mirror_images role) from an IBM CASE bundle.","title":"mirror_case_prepare"},{"location":"roles/mirror_case_prepare/#requirements","text":"The ibm-pak plugin must be installed.","title":"Requirements"},{"location":"roles/mirror_case_prepare/#role-variables","text":"","title":"Role Variables"},{"location":"roles/mirror_case_prepare/#case_name","text":"The name of the CASE bundle to prepare for mirroring. Required Environment Variable: CASE_NAME Default: None","title":"case_name"},{"location":"roles/mirror_case_prepare/#case_version","text":"The version of the CASE bundle to prepare for mirroring. Required Environment Variable: CASE_VERSION Default: None","title":"case_version"},{"location":"roles/mirror_case_prepare/#registry_public_host","text":"The public hostname for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None","title":"registry_public_host"},{"location":"roles/mirror_case_prepare/#registry_public_port","text":"The public port for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None","title":"registry_public_port"},{"location":"roles/mirror_case_prepare/#registry_prefix","text":"The prefix used for the target registry. The images will not be mirrored to the registry at this time but will define the final destination in the form: {host}:{port}/{prefix}/{reponame} Environment Variable: REGISTRY_PREFIX Default: None","title":"registry_prefix"},{"location":"roles/mirror_case_prepare/#exclude_images","text":"A list of child CASE bundles to exclude from the mirroring process. Optional Environment Variable: None Default: None","title":"exclude_images"},{"location":"roles/mirror_case_prepare/#role-variables-ibm-pak","text":"","title":"Role Variables - IBM Pak"},{"location":"roles/mirror_case_prepare/#ibmpak_skip_verify","text":"Skip the certification verification when downloading CASE bundles with oc ibm-pak get . Optional Environment Variable: IBMPAK_SKIP_VERIFY Default: False","title":"ibmpak_skip_verify"},{"location":"roles/mirror_case_prepare/#ibmpak_skip_dependencies","text":"Skip downloading CASE bundle dependencies with oc ibm-pak get . Optional Environment Variable: IBMPAK_SKIP_DEPENDENCIES Default: False","title":"ibmpak_skip_dependencies"},{"location":"roles/mirror_case_prepare/#ibmpak_insecure","text":"Skip TLS/SSL verification when downloading CASE bundles with oc ibm-pak get . Optional Environment Variable: IBMPAK_INSECURE Default: False","title":"ibmpak_insecure"},{"location":"roles/mirror_case_prepare/#example-playbook","text":"- hosts: localhost vars: case_name: ibm-mas case_version: 8.8.1 exclude_images: - ibm-truststore-mgr - ibm-sls - ibm-mas-assist - ibm-mas-iot - ibm-mas-manage registry_public_host: myregistry.com registry_public_port: 32500 registry_prefix: projectName roles: - ibm.mas_devops.mirror_case_prepare","title":"Example Playbook"},{"location":"roles/mirror_case_prepare/#license","text":"EPL-2.0","title":"License"},{"location":"roles/mirror_extras_prepare/","text":"mirror_extras_prepare \u00a4 This role generates a mirror manifest file suitable for use with the oc mirror command (or the ibm.mas_devops.mirror_images role) for a specific set of extra images. Available Extras \u00a4 Extra Versions Description catalog N/A Special extra package for mirroring the IBM Maximo Operator Catalog db2u 1.0.0, 1.0.1 Extra container images missing from the ibm-db2operator CASE bundle mongoce 4.2.6, 4.2.23, 4.4.21 Package containing all images required to use MongoCE Operator in the disconnected environment wd 5.3.1 Extra container images missing from the ibm-watson-discovery CASE bundle odf 4.15 Extra images needed for ODF 4.15 Role Variables \u00a4 extras_name \u00a4 The name of the extras package to prepare for mirroring. Required Environment Variable: EXTRAS_NAME Default: None extras_version \u00a4 The version of the extras package to prepare for mirroring. Required Environment Variable: EXTRAS_VERSION Default: None registry_public_host \u00a4 The public hostname for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None registry_public_port \u00a4 The public port for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None registry_prefix \u00a4 The prefix used for the target registry. The images will not be mirrored to the registry at this time but will define the final destination in the form: {host}:{port}/{prefix}/{reponame} Environment Variable: REGISTRY_PREFIX Default: None Example Playbook \u00a4 - hosts: localhost vars: extras_name: mongoce extras_version: 4.2.6 registry_public_host: myocp-5f1320191125833da1cac8216c06779e-0000.us-south.containers.appdomain.cloud registry_public_port: 32500 registry_prefix: projectName roles: - ibm.mas_devops.mirror_extras_prepare License \u00a4 EPL-2.0","title":"mirror_extras_prepare"},{"location":"roles/mirror_extras_prepare/#mirror_extras_prepare","text":"This role generates a mirror manifest file suitable for use with the oc mirror command (or the ibm.mas_devops.mirror_images role) for a specific set of extra images.","title":"mirror_extras_prepare"},{"location":"roles/mirror_extras_prepare/#available-extras","text":"Extra Versions Description catalog N/A Special extra package for mirroring the IBM Maximo Operator Catalog db2u 1.0.0, 1.0.1 Extra container images missing from the ibm-db2operator CASE bundle mongoce 4.2.6, 4.2.23, 4.4.21 Package containing all images required to use MongoCE Operator in the disconnected environment wd 5.3.1 Extra container images missing from the ibm-watson-discovery CASE bundle odf 4.15 Extra images needed for ODF 4.15","title":"Available Extras"},{"location":"roles/mirror_extras_prepare/#role-variables","text":"","title":"Role Variables"},{"location":"roles/mirror_extras_prepare/#extras_name","text":"The name of the extras package to prepare for mirroring. Required Environment Variable: EXTRAS_NAME Default: None","title":"extras_name"},{"location":"roles/mirror_extras_prepare/#extras_version","text":"The version of the extras package to prepare for mirroring. Required Environment Variable: EXTRAS_VERSION Default: None","title":"extras_version"},{"location":"roles/mirror_extras_prepare/#registry_public_host","text":"The public hostname for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None","title":"registry_public_host"},{"location":"roles/mirror_extras_prepare/#registry_public_port","text":"The public port for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None","title":"registry_public_port"},{"location":"roles/mirror_extras_prepare/#registry_prefix","text":"The prefix used for the target registry. The images will not be mirrored to the registry at this time but will define the final destination in the form: {host}:{port}/{prefix}/{reponame} Environment Variable: REGISTRY_PREFIX Default: None","title":"registry_prefix"},{"location":"roles/mirror_extras_prepare/#example-playbook","text":"- hosts: localhost vars: extras_name: mongoce extras_version: 4.2.6 registry_public_host: myocp-5f1320191125833da1cac8216c06779e-0000.us-south.containers.appdomain.cloud registry_public_port: 32500 registry_prefix: projectName roles: - ibm.mas_devops.mirror_extras_prepare","title":"Example Playbook"},{"location":"roles/mirror_extras_prepare/#license","text":"EPL-2.0","title":"License"},{"location":"roles/mirror_images/","text":"mirror_images \u00a4 Supports mirroring specific images to the target mirror registry","title":"mirror_images"},{"location":"roles/mirror_images/#mirror_images","text":"Supports mirroring specific images to the target mirror registry","title":"mirror_images"},{"location":"roles/mirror_ocp/","text":"mirror_ocp \u00a4 This role supports mirroring the Red Hat Platform and selected content from the Red Hat operator catalogs . Only content in the Red Hat catalogs directly used by IBM Maximo Application Suite is mirrored. Four actions are supported: direct Directly mirror content to your target registry to-filesystem Mirror content to the local filesystem from-filesystem Mirror content from the local filesystem to your target registry Three Catalogs are mirrored, containing the following content: certified-operator-index \u00a4 gpu-operator-certified (required by ibm.mas_devops.nvidia_gpu role) kubeturbo-certified (required by ibm.mas_devops.kubeturbo role) ibm-metrics-operator (required by ibm.mas_devops.dro role) ibm-data-reporter-operator (required by ibm.mas_devops.dro role) community-operator-index \u00a4 grafana-operator (required by ibm.mas_devops.grafana role) strimzi-kafka-operator (required by ibm.mas_devops.kafka role) redhat-operator-index \u00a4 amq-streams (required by ibm.mas_devops.kafka role) openshift-pipelines-operator-rh (required by the MAS CLI) nfd (required by ibm.mas_devops.nvidia_gpu role) aws-efs-csi-driver-operator (required by ibm.mas_devops.ocp_efs role) local-storage-operator (required by ibm.mas_devops.ocs role) odf-operator (required by ibm.mas_devops.ocs role) openshift-cert-manager-operator (required by ibm.mas_devops.cert_manager role) lvms-operator (not directly used, but often used in SNO environments) Requirements \u00a4 oc tool must be installed oc-mirror plugin must be installed Role Variables \u00a4 mirror_mode \u00a4 Set the action to perform ( direct , to-filesystem , from-filesystem ) Required Environment Variable: MIRROR_MODE Default: None Role Variables - Mirror Actions \u00a4 mirror_working_dir \u00a4 Set the working directory for the mirror operations Required Environment Variable: MIRROR_WORKING_DIR Default: None mirror_redhat_platform \u00a4 Enable mirroring of the Red Hat platform images. Optional Environment Variable: MIRROR_REDHAT_PLATFORM Default: False mirror_redhat_operators \u00a4 Enable mirroring of selected content from the Red Hat operator catalogs. Optional Environment Variable: MIRROR_REDHAT_OPERATORS Default: False redhat_pullsecret \u00a4 Path to your Red Hat pull secret, available from: https://console.redhat.com/openshift/install/pull-secret . Required Environment Variable: REDHAT_PULLSECRET Default: None Role Variables - OpenShift Version \u00a4 ocp_release \u00a4 The Red Hat release you are mirroring content for, e.g. 4.19 . Required Environment Variable: OCP_RELEASE Default: None ocp_min_version \u00a4 The minimum version of the Red Hat release to mirror platform content for, e.g. 4.19.10 . Optional Environment Variable: OCP_MIN_VERSION Default: None ocp_max_version \u00a4 The maximimum version of the Red Hat release to mirror platform content for, e.g. 4.19.10 . Optional Environment Variable: OCP_MAX_VERSION Default: None Role Variables - Target Registry \u00a4 registry_public_host \u00a4 The public hostname for the target registry Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None registry_public_port \u00a4 The public port number for the target registry Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None registry_is_ecr \u00a4 Specifies if the target registry is an AWS Elastic Container Registry instance Optional Environment Variable: REGISTRY_IS_ECR Default: false registry_ecr_aws_region \u00a4 The AWS region of the AWS Elastic Container Registry. Only required if the target registry is an AWS Elastic Container Registry instance. Optional Environment Variable: REGISTRY_ECR_AWS_REGION Default: None registry_prefix_redhat \u00a4 The prefix used for the target registry. The images will not be mirrored to the registry at this time but will define the final destination in the form: {host}[:{port}]/{prefix}/{reponame} Optional Environment Variable: REGISTRY_PREFIX_REDHAT Default: None registry_username \u00a4 The username for the target registry. Required Environment Variable: REGISTRY_USERNAME Default: None registry_password \u00a4 The password for the target registry. Required Environment Variable: REGISTRY_PASSWORD Default: None Example Playbook \u00a4 - hosts: localhost vars: registry_public_host: myregistry.mycompany.com registry_public_port: 5000 registry_prefix_redhat: \"ocp416\" registry_username: user1 registry_password: 8934jk77s862! # Not a real password, don't worry security folks mirror_mode: direct mirror_working_dir: /tmp/mirror mirror_redhat_platform: false mirror_redhat_operators: true ocp_release: 4.19 redhat_pullsecret: ~/pull-secret.json roles: - ibm.mas_devops.mirror_ocp License \u00a4 EPL-2.0","title":"mirror_ocp"},{"location":"roles/mirror_ocp/#mirror_ocp","text":"This role supports mirroring the Red Hat Platform and selected content from the Red Hat operator catalogs . Only content in the Red Hat catalogs directly used by IBM Maximo Application Suite is mirrored. Four actions are supported: direct Directly mirror content to your target registry to-filesystem Mirror content to the local filesystem from-filesystem Mirror content from the local filesystem to your target registry Three Catalogs are mirrored, containing the following content:","title":"mirror_ocp"},{"location":"roles/mirror_ocp/#certified-operator-index","text":"gpu-operator-certified (required by ibm.mas_devops.nvidia_gpu role) kubeturbo-certified (required by ibm.mas_devops.kubeturbo role) ibm-metrics-operator (required by ibm.mas_devops.dro role) ibm-data-reporter-operator (required by ibm.mas_devops.dro role)","title":"certified-operator-index"},{"location":"roles/mirror_ocp/#community-operator-index","text":"grafana-operator (required by ibm.mas_devops.grafana role) strimzi-kafka-operator (required by ibm.mas_devops.kafka role)","title":"community-operator-index"},{"location":"roles/mirror_ocp/#redhat-operator-index","text":"amq-streams (required by ibm.mas_devops.kafka role) openshift-pipelines-operator-rh (required by the MAS CLI) nfd (required by ibm.mas_devops.nvidia_gpu role) aws-efs-csi-driver-operator (required by ibm.mas_devops.ocp_efs role) local-storage-operator (required by ibm.mas_devops.ocs role) odf-operator (required by ibm.mas_devops.ocs role) openshift-cert-manager-operator (required by ibm.mas_devops.cert_manager role) lvms-operator (not directly used, but often used in SNO environments)","title":"redhat-operator-index"},{"location":"roles/mirror_ocp/#requirements","text":"oc tool must be installed oc-mirror plugin must be installed","title":"Requirements"},{"location":"roles/mirror_ocp/#role-variables","text":"","title":"Role Variables"},{"location":"roles/mirror_ocp/#mirror_mode","text":"Set the action to perform ( direct , to-filesystem , from-filesystem ) Required Environment Variable: MIRROR_MODE Default: None","title":"mirror_mode"},{"location":"roles/mirror_ocp/#role-variables-mirror-actions","text":"","title":"Role Variables - Mirror Actions"},{"location":"roles/mirror_ocp/#mirror_working_dir","text":"Set the working directory for the mirror operations Required Environment Variable: MIRROR_WORKING_DIR Default: None","title":"mirror_working_dir"},{"location":"roles/mirror_ocp/#mirror_redhat_platform","text":"Enable mirroring of the Red Hat platform images. Optional Environment Variable: MIRROR_REDHAT_PLATFORM Default: False","title":"mirror_redhat_platform"},{"location":"roles/mirror_ocp/#mirror_redhat_operators","text":"Enable mirroring of selected content from the Red Hat operator catalogs. Optional Environment Variable: MIRROR_REDHAT_OPERATORS Default: False","title":"mirror_redhat_operators"},{"location":"roles/mirror_ocp/#redhat_pullsecret","text":"Path to your Red Hat pull secret, available from: https://console.redhat.com/openshift/install/pull-secret . Required Environment Variable: REDHAT_PULLSECRET Default: None","title":"redhat_pullsecret"},{"location":"roles/mirror_ocp/#role-variables-openshift-version","text":"","title":"Role Variables - OpenShift Version"},{"location":"roles/mirror_ocp/#ocp_release","text":"The Red Hat release you are mirroring content for, e.g. 4.19 . Required Environment Variable: OCP_RELEASE Default: None","title":"ocp_release"},{"location":"roles/mirror_ocp/#ocp_min_version","text":"The minimum version of the Red Hat release to mirror platform content for, e.g. 4.19.10 . Optional Environment Variable: OCP_MIN_VERSION Default: None","title":"ocp_min_version"},{"location":"roles/mirror_ocp/#ocp_max_version","text":"The maximimum version of the Red Hat release to mirror platform content for, e.g. 4.19.10 . Optional Environment Variable: OCP_MAX_VERSION Default: None","title":"ocp_max_version"},{"location":"roles/mirror_ocp/#role-variables-target-registry","text":"","title":"Role Variables - Target Registry"},{"location":"roles/mirror_ocp/#registry_public_host","text":"The public hostname for the target registry Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None","title":"registry_public_host"},{"location":"roles/mirror_ocp/#registry_public_port","text":"The public port number for the target registry Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None","title":"registry_public_port"},{"location":"roles/mirror_ocp/#registry_is_ecr","text":"Specifies if the target registry is an AWS Elastic Container Registry instance Optional Environment Variable: REGISTRY_IS_ECR Default: false","title":"registry_is_ecr"},{"location":"roles/mirror_ocp/#registry_ecr_aws_region","text":"The AWS region of the AWS Elastic Container Registry. Only required if the target registry is an AWS Elastic Container Registry instance. Optional Environment Variable: REGISTRY_ECR_AWS_REGION Default: None","title":"registry_ecr_aws_region"},{"location":"roles/mirror_ocp/#registry_prefix_redhat","text":"The prefix used for the target registry. The images will not be mirrored to the registry at this time but will define the final destination in the form: {host}[:{port}]/{prefix}/{reponame} Optional Environment Variable: REGISTRY_PREFIX_REDHAT Default: None","title":"registry_prefix_redhat"},{"location":"roles/mirror_ocp/#registry_username","text":"The username for the target registry. Required Environment Variable: REGISTRY_USERNAME Default: None","title":"registry_username"},{"location":"roles/mirror_ocp/#registry_password","text":"The password for the target registry. Required Environment Variable: REGISTRY_PASSWORD Default: None","title":"registry_password"},{"location":"roles/mirror_ocp/#example-playbook","text":"- hosts: localhost vars: registry_public_host: myregistry.mycompany.com registry_public_port: 5000 registry_prefix_redhat: \"ocp416\" registry_username: user1 registry_password: 8934jk77s862! # Not a real password, don't worry security folks mirror_mode: direct mirror_working_dir: /tmp/mirror mirror_redhat_platform: false mirror_redhat_operators: true ocp_release: 4.19 redhat_pullsecret: ~/pull-secret.json roles: - ibm.mas_devops.mirror_ocp","title":"Example Playbook"},{"location":"roles/mirror_ocp/#license","text":"EPL-2.0","title":"License"},{"location":"roles/mongodb/","text":"mongodb \u00a4 This role currently supports provisioning of mongodb in three different providers: - community - aws (documentdb) - ibm If the selected provider is community then the MongoDB Community Kubernetes Operator will be configured and deployed into the specified namespace. By default a three member MongoDB replica set will be created. The cluster will bind six PVCs, these provide persistence for the data and system logs across the three nodes. Currently there is no support built-in for customizing the cluster beyond this configuration. Tip The role will generate a yaml file containing the definition of a Secret and MongoCfg resource that can be used to configure the deployed instance as the MAS system MongoDb. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/mongocfg-mongoce-system.yaml or used in conjunction with the suite_config role. Prerequisites \u00a4 To run this role with providers as ibm or aws you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role when provider is either ibm or aws . To run the docdb_secret_rotate MONGODB_ACTION when the provider is aws you must have already installed the Mongo Shell . This role will install a GrafanaDashboard used for monitoring the MongoDB instance when the provided is community and you have run the grafana role previously. If you did not run the grafana role then the GrafanaDashboard won't be installed. Role Variables \u00a4 Common Variables \u00a4 mas_instance_id \u00a4 Unique identifier for the MAS instance that will use this MongoDB deployment. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None Purpose : Identifies which MAS instance this MongoDB configuration targets. Used to generate the MongoCfg resource that connects MAS to the MongoDB instance. When to use : - Set when you want to automatically generate MongoCfg for MAS integration - Must be set together with mas_config_dir for MongoCfg generation - Leave unset if manually managing MongoDB configuration for MAS Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : When set with mas_config_dir , generates a MongoCfg YAML file at $MAS_CONFIG_DIR/mongocfg-mongoce-system.yaml . Without this, no MAS configuration file is created. Related variables : Must be set together with mas_config_dir for MongoCfg generation. mas_config_dir \u00a4 Local directory path where the generated MongoCfg resource file will be saved. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None Purpose : Specifies where to save the MongoCfg YAML file that configures MAS to connect to this MongoDB instance. This file can be applied manually or used with the suite_config role for automated MAS configuration. When to use : - Set when you want to automatically generate MongoCfg for MAS integration - Must be set together with mas_instance_id for MongoCfg generation - Use the same directory across multiple dependency roles (mongodb, db2, sls) to collect all configurations - Leave unset if manually managing MongoDB configuration for MAS Valid values : Any valid local filesystem path (e.g., /home/user/masconfig , ~/masconfig , ./config ) Impact : When set with mas_instance_id , creates file mongocfg-mongoce-system.yaml in this directory. The file contains Secret and MongoCfg resources ready to apply to MAS. Related variables : - Must be set together with mas_instance_id for MongoCfg generation - Used by suite_config role to apply configurations mongodb_provider \u00a4 Selects which MongoDB deployment option to use for MAS database requirements. Optional Environment Variable: MONGODB_PROVIDER Default Value: community Purpose : Determines the MongoDB infrastructure provider, which affects deployment architecture, management approach, operational requirements, and cost model. Each provider offers different trade-offs between control, convenience, and cost. When to use : - Use community for self-managed deployments on OpenShift with full control - Use ibm for managed IBM Cloud Databases for MongoDB service - Use aws for managed AWS DocumentDB service (MongoDB-compatible) - Consider operational expertise, cloud platform, and management preferences Valid values : - community - MongoDB Community Edition Operator (self-managed on OpenShift) - ibm - IBM Cloud Databases for MongoDB (managed service) - aws - AWS DocumentDB (managed service, MongoDB-compatible) Impact : - community : Requires cluster storage, manual backup management, and operational overhead - ibm : Requires IBM Cloud account, API key, and incurs IBM Cloud service charges - aws : Requires AWS account, VPC configuration, and incurs AWS service charges Related variables : - When community : Requires mongodb_storage_class and related storage/resource variables - When ibm : Requires ibmcloud_apikey , ibm_mongo_region , ibm_mongo_resourcegroup - When aws : Requires aws_access_key_id , aws_secret_access_key , vpc_id , docdb_* variables - Affects which mongodb_action values are supported Note : Provider cannot be changed after initial deployment. Migration between providers requires backup and restore procedures. mongodb_action \u00a4 Specifies which operation to perform on the MongoDB instance. Optional Environment Variable: MONGODB_ACTION Default Value: install Purpose : Controls what action the role executes against the MongoDB instance. Different providers support different sets of actions based on their capabilities and management model. When to use : - Use install for initial deployment or updates - Use uninstall to remove MongoDB instance (use with caution) - Use backup to create MongoDB backups (community/ibm only) - Use restore to restore from backup (community/ibm only) - Use docdb_secret_rotate to rotate DocumentDB credentials (aws only) - Use destroy-data to delete all data from MongoDB (aws only) - Use create-mongo-service-credentials to generate service credentials (ibm only) Valid values (provider-specific): - community : install , uninstall , backup , restore - aws : install , uninstall , docdb_secret_rotate , destroy-data - ibm : install , uninstall , backup , restore , create-mongo-service-credentials Impact : The action determines what the role will do. Destructive actions like uninstall and destroy-data will permanently delete data. Backup/restore actions require additional variables to be set. Related variables : - mongodb_provider determines which actions are available - Backup actions require masbr_* variables - Restore actions require masbr_restore_from_version - AWS secret rotation requires docdb_* credential variables Note : Always backup data before performing destructive operations. Some actions are irreversible. CE Operator Variables \u00a4 mongodb_namespace \u00a4 OpenShift namespace where the MongoDB Community Operator and MongoDB cluster will be deployed. Optional Environment Variable: MONGODB_NAMESPACE Default Value: mongoce Purpose : Defines the Kubernetes namespace for MongoDB resources, providing isolation and organization for the MongoDB deployment within the cluster. When to use : - Use default mongoce for standard deployments - Change only if you need multiple MongoDB instances or have namespace naming requirements - Ensure namespace doesn't conflict with existing deployments Valid values : Any valid Kubernetes namespace name (lowercase alphanumeric and hyphens) Impact : All MongoDB resources (operator, replica set pods, PVCs, secrets, services) will be created in this namespace. Changing this after deployment requires reinstallation. Related variables : Used in MongoCfg generation to reference MongoDB service endpoints. mongodb_version \u00a4 Specifies the MongoDB version to deploy. Optional Environment Variable: MONGODB_VERSION Default Value: Automatically defined by the mongo version specified in the latest MAS case bundle available Purpose : Controls which MongoDB version is deployed, ensuring compatibility with MAS requirements and enabling version-specific features. The default aligns with the tested and supported version for your MAS release. When to use : - Leave as default for standard deployments (recommended) - Override only when specific version requirements exist - Use for testing compatibility with newer MongoDB versions - Never use to downgrade an existing MongoDB instance Valid values : 7.0.12 , 7.0.22 , 7.0.23 , 8.0.13 , 8.0.17 (check MAS compatibility matrix for supported versions) Impact : Determines MongoDB feature set, performance characteristics, and compatibility. Changing versions may require data migration or compatibility testing. Related variables : - Version upgrades require corresponding mongodb_v*_upgrade flags - Must be compatible with MAS version requirements Note : Never downgrade MongoDB versions . Always create scheduled backups before version changes. Use mongodb_v5_upgrade , mongodb_v6_upgrade , mongodb_v7_upgrade , or mongodb_v8_upgrade flags when upgrading between major versions. mongodb_override_spec \u00a4 Forces the role to use environment variables instead of preserving existing MongoDB spec settings. Optional Environment Variable: MONGODB_OVERRIDE_SPEC Default Value: false Purpose : Controls whether the role preserves existing MongoDB configuration during upgrades/reinstalls or applies new values from environment variables. This prevents accidental configuration changes during routine operations. When to use : - Leave as false (default) to preserve existing settings during upgrades - Set to true only when intentionally changing MongoDB configuration - Use with caution - requires setting all environment variables to match desired state Valid values : true , false Impact : - When false : Existing CPU, memory, storage, and replica settings are preserved during reinstall/upgrade - When true : All settings are taken from environment variables; unset variables revert to defaults Related variables : When set to true , affects these settings: - mongodb_cpu_limits - mongodb_mem_limits - mongodb_cpu_requests - mongodb_mem_requests - mongodb_storage_class - mongodb_storage_capacity_data - mongodb_storage_capacity_logs - mongodb_replicas Note : Check existing MongoDB installation before enabling . If environment variables don't match current spec, resources may be reset to defaults, potentially causing disruption. Unknown settings are not preserved. mongodb_storage_class \u00a4 Name of the Kubernetes storage class for MongoDB persistent volumes. Required when mongodb_provider=community Environment Variable: MONGODB_STORAGE_CLASS Default Value: None Purpose : Specifies which storage class provides persistent volumes for MongoDB data and logs. The storage class determines performance characteristics, availability, and cost of MongoDB storage. When to use : - Always required for Community Edition deployments - Choose based on performance requirements (SSD vs HDD) - Consider backup and snapshot capabilities of the storage class - Verify storage class exists in your cluster before deployment Valid values : Any valid storage class name in your cluster that supports ReadWriteOnce (RWO) access mode Impact : Affects MongoDB performance, reliability, and cost. Six PVCs will be created (data + logs for each of 3 replicas by default). Storage class cannot be changed after deployment without data migration. Related variables : - mongodb_storage_capacity_data - Size of data PVCs - mongodb_storage_capacity_logs - Size of log PVCs - mongodb_replicas - Number of replica sets (affects total PVC count) Note : Storage class must support ReadWriteOnce (RWO) access mode. Verify with oc get storageclass before deployment. mongodb_storage_capacity_data \u00a4 Size of the persistent volume claim (PVC) for MongoDB data storage on each replica set member. Optional Environment Variable: MONGODB_STORAGE_CAPACITY_DATA Default Value: 20Gi Purpose : Determines disk space allocated for storing MongoDB databases, collections, and indexes on each replica. Proper sizing prevents storage exhaustion and ensures adequate space for data growth. When to use : - Increase from default for production environments with large data volumes - Increase for environments with high data growth rates - Use default (20Gi) for development, testing, or small deployments - Consider backup strategy when sizing (larger volumes take longer to backup) Valid values : Any valid Kubernetes storage size (e.g., 20Gi , 100Gi , 500Gi , 1Ti ) Impact : - Larger values consume more cluster storage resources - Cannot be decreased after deployment (PVC expansion only) - Total storage = this value \u00d7 number of replicas - Affects backup and restore duration Related variables : - mongodb_replicas : Total storage = capacity \u00d7 replicas - mongodb_storage_class : Must support volume expansion if you plan to increase size later - mongodb_storage_capacity_logs : Consider balancing data and log storage Note : PVCs can be expanded but not shrunk. Plan for growth when setting initial size. Monitor storage usage to avoid running out of space. mongodb_storage_capacity_logs \u00a4 Size of the persistent volume claim (PVC) for MongoDB log storage on each replica set member. Optional Environment Variable: MONGODB_STORAGE_CAPACITY_LOGS Default Value: 20Gi Purpose : Determines disk space allocated for MongoDB operational logs on each replica. Logs are essential for troubleshooting, auditing, and monitoring MongoDB operations. When to use : - Increase for production environments with verbose logging requirements - Increase if log retention policies require longer history - Use default (20Gi) for standard deployments - Consider log rotation and retention policies when sizing Valid values : Any valid Kubernetes storage size (e.g., 10Gi , 20Gi , 50Gi ) Impact : - Larger values consume more cluster storage resources - Cannot be decreased after deployment (PVC expansion only) - Total log storage = this value \u00d7 number of replicas - Insufficient log space can cause MongoDB operational issues Related variables : - mongodb_replicas : Total log storage = capacity \u00d7 replicas - mongodb_storage_class : Must support volume expansion - mongodb_storage_capacity_data : Consider balancing data and log storage Note : Monitor log usage and implement log rotation to prevent filling log volumes. PVCs can be expanded but not shrunk. mongodb_cpu_limits \u00a4 Maximum CPU cores allocated to each MongoDB container. Optional Environment Variable: MONGODB_CPU_LIMITS Default Value: 1 Purpose : Sets the upper bound on CPU usage for MongoDB containers, preventing any single MongoDB instance from consuming excessive cluster CPU resources. When to use : - Increase for production workloads with high query volumes - Increase for large datasets requiring more processing power - Use default (1 core) for development or light workloads - Set higher than mongodb_cpu_requests to allow burst capacity Valid values : CPU units as decimal (e.g., 0.5 , 1 , 2 , 4 ) or millicores (e.g., 500m , 1000m , 2000m ) Impact : - Higher limits allow better performance under load but consume more cluster resources - Limits prevent MongoDB from starving other workloads of CPU - Total CPU limit = this value \u00d7 number of replicas - Setting too low can cause performance degradation Related variables : - mongodb_cpu_requests : Should be set lower than limits for burst capacity - mongodb_replicas : Total CPU = limits \u00d7 replicas - mongodb_mem_limits : Balance CPU and memory allocation Note : Ensure cluster has sufficient CPU capacity for all replicas. Monitor actual CPU usage to right-size limits. mongodb_mem_limits \u00a4 Maximum memory allocated to each MongoDB container. Optional Environment Variable: MONGODB_MEM_LIMITS Default Value: 1Gi Purpose : Sets the upper bound on memory usage for MongoDB containers. MongoDB uses memory for caching data and indexes, so adequate memory is critical for performance. When to use : - Increase significantly for production workloads (4Gi-8Gi recommended) - Increase for large datasets to improve cache hit rates - Default (1Gi) is only suitable for development/testing - Set higher than mongodb_mem_requests to allow burst capacity Valid values : Memory size (e.g., 1Gi , 2Gi , 4Gi , 8Gi , 16Gi ) Impact : - Higher limits improve performance through better caching - Limits prevent MongoDB from consuming all cluster memory - Total memory limit = this value \u00d7 number of replicas - Setting too low causes frequent cache evictions and poor performance Related variables : - mongodb_mem_requests : Should be set lower than limits - mongodb_replicas : Total memory = limits \u00d7 replicas - mongodb_cpu_limits : Balance CPU and memory allocation Note : MongoDB performance heavily depends on available memory. Production deployments typically need 4Gi-8Gi per replica. Monitor memory usage and adjust accordingly. mongodb_cpu_requests \u00a4 Guaranteed CPU cores reserved for each MongoDB container. Optional Environment Variable: MONGODB_CPU_REQUESTS Default Value: 500m Purpose : Defines the minimum CPU resources guaranteed to MongoDB containers. Kubernetes scheduler uses this to place pods on nodes with sufficient available CPU. When to use : - Increase for production workloads requiring consistent performance - Set to match expected baseline CPU usage - Use default (500m) for development or light workloads - Set lower than mongodb_cpu_limits to allow burst capacity Valid values : CPU units as decimal (e.g., 0.5 , 1 , 2 ) or millicores (e.g., 500m , 1000m , 2000m ) Impact : - Higher requests guarantee more CPU but may limit pod scheduling if cluster capacity is constrained - Requests ensure MongoDB has minimum CPU even under cluster load - Total CPU request = this value \u00d7 number of replicas - Affects pod Quality of Service (QoS) class Related variables : - mongodb_cpu_limits : Requests should be lower than limits - mongodb_replicas : Total CPU requests = this value \u00d7 replicas - mongodb_mem_requests : Balance CPU and memory requests Note : Set requests based on baseline usage, not peak. The difference between requests and limits provides burst capacity. mongodb_mem_requests \u00a4 Guaranteed memory reserved for each MongoDB container. Optional Environment Variable: MONGODB_MEM_REQUESTS Default Value: 1Gi Purpose : Defines the minimum memory resources guaranteed to MongoDB containers. Kubernetes scheduler uses this to place pods on nodes with sufficient available memory. When to use : - Increase for production workloads (2Gi-4Gi recommended) - Set to match expected baseline memory usage - Default (1Gi) is only suitable for development/testing - Set lower than mongodb_mem_limits to allow burst capacity Valid values : Memory size (e.g., 1Gi , 2Gi , 4Gi , 8Gi ) Impact : - Higher requests guarantee more memory but may limit pod scheduling if cluster capacity is constrained - Requests ensure MongoDB has minimum memory even under cluster load - Total memory request = this value \u00d7 number of replicas - Affects pod Quality of Service (QoS) class Related variables : - mongodb_mem_limits : Requests should be lower than limits - mongodb_replicas : Total memory requests = this value \u00d7 replicas - mongodb_cpu_requests : Balance CPU and memory requests Note : MongoDB needs adequate memory for good performance. Production deployments typically need 2Gi-4Gi requests. Set based on baseline usage, not peak. mongodb_replicas \u00a4 Number of MongoDB replica set members to deploy. Optional Environment Variable: MONGODB_REPLICAS Default Value: 3 Purpose : Determines the size of the MongoDB replica set, which affects high availability, read scalability, and resource consumption. Replica sets provide data redundancy and automatic failover. When to use : - Use default (3) for production deployments with high availability requirements - Set to 1 only for Single Node OpenShift (SNO) clusters or development environments - Use 5 or more for critical production workloads requiring higher availability - Odd numbers (1, 3, 5) are recommended for proper election quorum Valid values : Positive integers, typically 1, 3, or 5 Impact : - More replicas = higher availability but more resource consumption - Each replica requires its own data and log PVCs - Total resources = (CPU + memory + storage) \u00d7 replicas - Affects election behavior and write acknowledgment Related variables : - mongodb_storage_capacity_data : Total data storage = capacity \u00d7 replicas - mongodb_storage_capacity_logs : Total log storage = capacity \u00d7 replicas - mongodb_cpu_limits and mongodb_mem_limits : Total resources = limits \u00d7 replicas Note : Set to 1 for SNO clusters. Production deployments should use 3 or more for high availability. Changing replica count after deployment requires careful planning. custom_labels \u00a4 Comma-separated list of key=value labels to apply to MongoDB resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Purpose : Adds Kubernetes labels to MongoDB resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,app=mongodb ) Impact : Labels are applied to MongoDB resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect MongoDB functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management. mongodb_v5_upgrade \u00a4 Confirmation flag to upgrade MongoDB from version 4.x to version 5. Optional Environment Variable: MONGODB_V5_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades. Upgrading MongoDB major versions requires careful planning and testing. When to use : - Set to true only when intentionally upgrading from MongoDB 4.2 or 4.4 to version 5 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to a 5.x version, triggers MongoDB upgrade from 4.x to 5.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to a 5.x version for upgrade to proceed - Other upgrade flags: mongodb_v6_upgrade , mongodb_v7_upgrade , mongodb_v8_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 5.0 release notes for breaking changes and new requirements (e.g., AVX instruction set). mongodb_v6_upgrade \u00a4 Confirmation flag to upgrade MongoDB from version 5 to version 6. Optional Environment Variable: MONGODB_V6_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades from version 5 to 6. When to use : - Set to true only when intentionally upgrading from MongoDB 5 to version 6 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to a 6.x version, triggers MongoDB upgrade from 5.x to 6.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to a 6.x version for upgrade to proceed - Other upgrade flags: mongodb_v5_upgrade , mongodb_v7_upgrade , mongodb_v8_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 6.0 release notes for breaking changes. mongodb_v7_upgrade \u00a4 Confirmation flag to upgrade MongoDB from version 6 to version 7. Optional Environment Variable: MONGODB_V7_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades from version 6 to 7. When to use : - Set to true only when intentionally upgrading from MongoDB 6 to version 7 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to a 7.x version, triggers MongoDB upgrade from 6.x to 7.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to a 7.x version for upgrade to proceed - Other upgrade flags: mongodb_v5_upgrade , mongodb_v6_upgrade , mongodb_v8_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 7.0 release notes for breaking changes. mongodb_v8_upgrade \u00a4 Confirmation flag to upgrade MongoDB from version 7 to version 8. Optional Environment Variable: MONGODB_V8_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades from version 7 to 8. When to use : - Set to true only when intentionally upgrading from MongoDB 7 to version 8 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to an 8.x version, triggers MongoDB upgrade from 7.x to 8.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to an 8.x version for upgrade to proceed - Other upgrade flags: mongodb_v5_upgrade , mongodb_v6_upgrade , mongodb_v7_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 8.0 release notes for breaking changes. masbr_confirm_cluster \u00a4 Enables cluster confirmation prompt before executing backup or restore operations. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false Purpose : Provides a safety check to confirm you're connected to the correct cluster before performing backup or restore operations, preventing accidental operations on wrong clusters. When to use : - Set to true in environments with multiple clusters to prevent mistakes - Set to true for production environments as an additional safety measure - Leave as false for automated pipelines where confirmation isn't possible Valid values : true , false Impact : When true , the role will prompt for confirmation of the cluster before proceeding with backup/restore. This adds a manual step but prevents costly mistakes. Related variables : Used with mongodb_action when set to backup or restore . masbr_copy_timeout_sec \u00a4 Timeout in seconds for backup/restore file transfer operations. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) Purpose : Sets the maximum time allowed for copying backup files to/from storage. Prevents operations from hanging indefinitely on slow networks or large datasets. When to use : - Increase for very large databases or slow network connections - Decrease for smaller databases to fail faster on issues - Use default (12 hours) for most deployments Valid values : Positive integer representing seconds (e.g., 3600 = 1 hour, 43200 = 12 hours, 86400 = 24 hours) Impact : Operations exceeding this timeout will fail. Setting too low causes failures on legitimate long-running transfers. Setting too high delays detection of stuck operations. Related variables : Used with mongodb_action when set to backup or restore . Note : Consider database size and network speed when setting timeout. Monitor actual transfer times to optimize this value. masbr_job_timezone \u00a4 Time zone for scheduled backup job execution. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None (uses UTC) Purpose : Specifies the time zone for scheduled backup CronJobs, ensuring backups run at the intended local time rather than UTC. When to use : - Set when scheduling backups to run at specific local times - Use for compliance with backup windows in specific time zones - Leave unset to use UTC (recommended for global deployments) Valid values : Any valid tz database time zone (e.g., America/New_York , Europe/London , Asia/Tokyo ) Impact : Affects when scheduled backups execute. Incorrect time zone can cause backups to run during business hours or miss backup windows. Related variables : - masbr_backup_schedule : Defines the cron schedule - Only applies when masbr_backup_schedule is set Note : When not set, CronJobs use UTC time zone. Consider daylight saving time changes when scheduling backups. masbr_storage_local_folder \u00a4 Local filesystem path where backup files will be stored or retrieved from. Required when mongodb_action is backup or restore Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None Purpose : Specifies the directory for storing MongoDB backup files. This location must have sufficient space and appropriate permissions for backup operations. When to use : - Always required for backup and restore operations - Use a path with adequate storage space for full database backups - Consider using network-attached storage for backup retention - Ensure path is accessible and has proper permissions Valid values : Any valid local filesystem path (e.g., /backup/mongodb , /mnt/nfs/backups , /tmp/masbr ) Impact : Backup files are written to this location. Insufficient space causes backup failures. Path must be accessible during restore operations. Related variables : - masbr_backup_type : Determines if full or incremental backups are stored here - masbr_restore_from_version : Specifies which backup version to restore from this location Note : Ensure adequate disk space (at least 2x database size for full backups). Implement backup retention policies to manage storage usage. masbr_backup_type \u00a4 Type of backup to create: full or incremental. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full Purpose : Determines whether to create a complete database backup or an incremental backup containing only changes since the last full backup. Incremental backups save time and storage. When to use : - Use full for initial backups or periodic complete backups - Use incr for frequent backups between full backups to save time and space - Implement a strategy like weekly full + daily incremental backups Valid values : full , incr Impact : - full : Creates complete backup, takes longer, uses more storage - incr : Creates incremental backup, faster, uses less storage, requires base full backup Related variables : - masbr_backup_from_version : Required when incr is used to specify base full backup - mongodb_action : Must be set to backup Note : Incremental backups require a full backup as base. Restore operations may need to apply multiple incremental backups sequentially. masbr_backup_from_version \u00a4 Timestamp of the full backup to use as base for incremental backup. Optional Environment Variable: MASBR_BACKUP_FROM_VERSION Default: None (automatically uses latest full backup) Purpose : Specifies which full backup serves as the base for an incremental backup. This links the incremental backup to a specific full backup version. When to use : - Set when creating incremental backups and you want to specify a particular full backup - Leave unset to automatically use the most recent full backup - Only valid when masbr_backup_type=incr Valid values : Timestamp in format YYYYMMDDHHMMSS (e.g., 20240621021316 ) Impact : Incremental backup will contain only changes since the specified full backup. Incorrect version can cause backup chain issues. Related variables : - masbr_backup_type : Must be set to incr - masbr_storage_local_folder : Location where full backup exists Note : If not specified, role automatically finds the latest full backup. Ensure the specified full backup exists in the storage location. masbr_backup_schedule \u00a4 Cron expression for scheduling automated backups. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None (creates on-demand backup) Purpose : Defines when automated backups should run using standard cron syntax. Enables regular, unattended backup operations. When to use : - Set to schedule regular automated backups (e.g., daily, weekly) - Leave unset for manual, on-demand backups - Consider backup windows and system load when scheduling Valid values : Standard cron expression (e.g., 0 2 * * * for daily at 2 AM, 0 2 * * 0 for weekly on Sunday at 2 AM) Impact : When set, creates a Kubernetes CronJob that automatically runs backups on schedule. Without this, backups only run when role is manually executed. Related variables : - masbr_job_timezone : Specifies time zone for schedule - masbr_backup_type : Determines if scheduled backups are full or incremental Note : Test cron expressions before deploying. Consider backup duration when scheduling to avoid overlapping backup jobs. masbr_restore_from_version \u00a4 Timestamp of the backup version to restore. Required when mongodb_action=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Purpose : Specifies which backup version to restore from. This allows point-in-time recovery to a specific backup. When to use : - Always required when performing restore operations - Use to restore to a specific point in time - Verify backup version exists before attempting restore Valid values : Timestamp in format YYYYMMDDHHMMSS (e.g., 20240621021316 ) Impact : Restores MongoDB to the state captured in the specified backup. All data after this backup will be lost. This is a destructive operation. Related variables : - mongodb_action : Must be set to restore - masbr_storage_local_folder : Location where backup exists Note : Verify backup version before restoring . List available backups in storage location first. Restore is destructive and cannot be undone without another backup. Role Variables - IBM Cloud \u00a4 ibm_mongo_name \u00a4 Name for the IBM Cloud Databases for MongoDB instance. Required when mongodb_provider=ibm Environment Variable: IBM_MONGO_NAME Default Value: mongo-${MAS_INSTANCE_ID} Purpose : Identifies the MongoDB database instance in IBM Cloud. This name is used for resource identification, billing, and management within IBM Cloud. When to use : - Always required when using IBM Cloud as the MongoDB provider - Use default naming convention for consistency across MAS instances - Customize only if organizational naming standards require it Valid values : Valid IBM Cloud resource name (alphanumeric, hyphens allowed, must start with letter) Impact : This name appears in IBM Cloud console, billing reports, and resource lists. Changing it after creation requires recreating the database instance. Related variables : - mas_instance_id : Default name includes this value - mongodb_provider : Must be set to ibm Note : Choose a descriptive name that identifies the MAS instance and environment. The name cannot be changed after creation. ibm_mongo_admin_password \u00a4 Administrator password for the IBM Cloud MongoDB instance. Optional Environment Variable: IBM_MONGO_ADMIN_PASSWORD Default Value: Auto-generated 20-character string Purpose : Sets the password for the MongoDB administrator user. If not provided, a secure random password is automatically generated. When to use : - Set explicitly if you need to know the password in advance - Leave unset to use auto-generated secure password (recommended) - Set if integrating with external password management systems Valid values : String meeting IBM Cloud password requirements (minimum length, complexity) Impact : This password is used for administrative access to the MongoDB instance. Auto-generated passwords are stored in Kubernetes secrets. Related variables : - ibm_mongo_admin_credentials_secret_name : Secret where credentials are stored Note : Auto-generated passwords are more secure. If setting manually, ensure password meets security requirements and is stored securely. ibm_mongo_admin_credentials_secret_name \u00a4 Name of the Kubernetes secret containing MongoDB admin credentials. Optional Environment Variable: IBM_MONGO_ADMIN_CREDENTIALS_SECRET_NAME Default Value: <mongo-name>-admin-credentials Purpose : Specifies the Kubernetes secret name where MongoDB administrator credentials are stored. This secret is created automatically by the role. When to use : - Customize if organizational standards require specific secret naming - Use default for standard deployments - Reference this secret name in other automation Valid values : Valid Kubernetes secret name Impact : The secret contains admin username and password for MongoDB access. Other roles and applications reference this secret for database connectivity. Related variables : - ibm_mongo_name : Default secret name includes this value - ibm_mongo_admin_password : Password stored in this secret ibm_mongo_service_credentials_secret_name \u00a4 Name of the Kubernetes secret containing MongoDB service credentials. Optional Environment Variable: IBM_MONGO_SERVICE_CREDENTIALS_SECRET_NAME Default Value: <mongo-name>-service-credentials Purpose : Specifies the Kubernetes secret name where MongoDB service-level credentials are stored. These credentials are used by MAS applications to connect to MongoDB. When to use : - Customize if organizational standards require specific secret naming - Use default for standard deployments - Reference this secret name when configuring MAS applications Valid values : Valid Kubernetes secret name Impact : The secret contains connection strings and credentials for application-level MongoDB access. MAS applications use this secret to connect to the database. Related variables : - ibm_mongo_name : Default secret name includes this value ibm_mongo_resourcegroup \u00a4 IBM Cloud resource group for MongoDB instance placement. Required when mongodb_provider=ibm Environment Variable: IBM_MONGO_RESOURCEGROUP Default Value: Default Purpose : Specifies which IBM Cloud resource group will contain the MongoDB instance. Resource groups organize IBM Cloud resources for access control and billing. When to use : - Always required when using IBM Cloud provider - Use Default for simple deployments - Specify custom resource group for organizational resource management - Align with IBM Cloud IAM and billing structure Valid values : Name of an existing IBM Cloud resource group in your account Impact : Determines access control, billing allocation, and resource organization. Users need appropriate IAM permissions for the specified resource group. Related variables : - ibmcloud_apikey : API key must have access to the specified resource group Note : Ensure the resource group exists and your API key has permissions to create resources in it. ibm_mongo_region \u00a4 IBM Cloud region where MongoDB instance will be deployed. Required when mongodb_provider=ibm Environment Variable: IBM_MONGO_REGION Default Value: us-east Purpose : Specifies the geographic region for MongoDB deployment. Region selection affects latency, data residency, and availability. When to use : - Always required when using IBM Cloud provider - Choose region closest to your OpenShift cluster for lowest latency - Consider data residency requirements for compliance - Use default ( us-east ) if no specific requirements Valid values : Valid IBM Cloud region (e.g., us-east , us-south , eu-gb , eu-de , jp-tok , au-syd ) Impact : Affects network latency between OpenShift and MongoDB, data residency compliance, and regional pricing. Cannot be changed after creation. Related variables : - ibmcloud_apikey : API key must have access to the specified region Note : Choose region carefully as it cannot be changed. Consider deploying MongoDB in the same region as your OpenShift cluster for best performance. ibmcloud_apikey \u00a4 IBM Cloud API key for authentication and resource management. Required when mongodb_provider=ibm Environment Variable: IBMCLOUD_APIKEY Default Value: None Purpose : Provides authentication credentials for creating and managing IBM Cloud resources. The API key must have sufficient permissions to create and manage Databases for MongoDB instances. When to use : - Always required when using IBM Cloud provider - Must have permissions for the target resource group and region - Should be stored securely (e.g., in Ansible Vault or external secret management) Valid values : Valid IBM Cloud API key string Impact : This key is used to authenticate all IBM Cloud API calls. Insufficient permissions will cause deployment failures. Related variables : - ibm_mongo_resourcegroup : API key must have access to this resource group - ibm_mongo_region : API key must have access to this region Note : Never commit API keys to source control . Use secure secret management. Ensure the API key has appropriate IAM permissions for Databases for MongoDB service. ibm_mongo_plan \u00a4 IBM Cloud service plan for MongoDB instance. Optional Environment Variable: IBM_MONGO_PLAN Default Value: standard Purpose : Specifies the IBM Cloud service plan tier for MongoDB. Different plans offer different performance, availability, and pricing characteristics. When to use : - Use standard (default) for most production deployments - Consider enterprise plan for critical workloads requiring higher SLA - Review IBM Cloud pricing and plan features before selecting Valid values : standard , enterprise (check IBM Cloud documentation for current plan options) Impact : Affects pricing, performance characteristics, SLA, and available features. Plan cannot be changed after creation without recreating the instance. Related variables : - ibm_mongo_memory , ibm_mongo_disk , ibm_mongo_cpu : Resource allocations vary by plan Note : Review IBM Cloud Databases for MongoDB plan documentation for current offerings and pricing. ibm_mongo_service \u00a4 IBM Cloud service type identifier for MongoDB. Read-only Value: databases-for-mongodb Purpose : Identifies the IBM Cloud service type. This is a fixed value used internally by the role. When to use : This is set automatically by the role and should not be modified. Valid values : databases-for-mongodb Impact : Used in IBM Cloud API calls to specify the service type. Note : This is a constant value and does not need to be set by users. ibm_mongo_service_endpoints \u00a4 Network endpoint type for MongoDB connectivity. Optional Environment Variable: IBM_MONGO_SERVICE_ENDPOINTS Default Value: public Purpose : Determines whether MongoDB is accessible via public internet or private network only. Private endpoints provide better security and performance for cluster-to-database communication. When to use : - Use public for simple deployments or when OpenShift cluster lacks private network connectivity - Use private for production deployments with private network connectivity (recommended) - Private endpoints require IBM Cloud private network configuration Valid values : public , private Impact : - public : MongoDB accessible over internet (requires firewall rules) - private : MongoDB accessible only via IBM Cloud private network (more secure, lower latency) Related variables : Network configuration must support the chosen endpoint type Note : Private endpoints are recommended for production. Ensure your OpenShift cluster can reach IBM Cloud private network if using private . ibm_mongo_version \u00a4 MongoDB version to deploy in IBM Cloud. Optional Environment Variable: IBM_MONGO_VERSION Default Value: 4.2 Purpose : Specifies which MongoDB version to deploy. Version selection affects available features, performance, and compatibility. When to use : - Use default ( 4.2 ) for compatibility with older MAS versions - Specify newer version (e.g., 5.0 , 6.0 ) for new deployments - Check MAS compatibility matrix before selecting version Valid values : MongoDB versions supported by IBM Cloud Databases (e.g., 4.2 , 4.4 , 5.0 , 6.0 ) Impact : Affects available MongoDB features, performance characteristics, and MAS compatibility. Version cannot be easily downgraded. Related variables : - Check MAS compatibility requirements before selecting version Note : Verify version compatibility with your MAS version. Newer MongoDB versions may require MAS updates. ibm_mongo_memory \u00a4 Memory allocation per MongoDB member in MB. Optional Environment Variable: IBM_MONGO_MEMORY Default Value: 3840 (3.75 GB) Purpose : Specifies memory allocation for each MongoDB replica set member. Memory affects caching performance and query execution. When to use : - Use default (3840 MB) for development or small deployments - Increase for production workloads (8192 MB or higher recommended) - Increase for large datasets to improve cache hit rates Valid values : Integer in MB, minimum varies by plan (typically 1024 MB minimum) Impact : Higher memory improves performance through better caching but increases costs. Total cost = memory \u00d7 number of members. Related variables : - ibm_mongo_plan : Available memory ranges vary by plan - ibm_mongo_disk : Balance memory and disk allocation Note : IBM Cloud charges based on allocated memory. Production deployments typically need 8GB+ per member. ibm_mongo_disk \u00a4 Disk storage allocation per MongoDB member in MB. Optional Environment Variable: IBM_MONGO_DISK Default Value: 30720 (30 GB) Purpose : Specifies disk storage for each MongoDB replica set member. Storage holds databases, indexes, and operational logs. When to use : - Use default (30 GB) for development or small datasets - Increase for production workloads based on data volume - Plan for data growth and backup storage needs Valid values : Integer in MB, minimum varies by plan (typically 5120 MB minimum) Impact : Affects storage capacity and costs. Disk can be expanded but not shrunk. Total storage = disk \u00d7 number of members. Related variables : - ibm_mongo_plan : Available disk ranges vary by plan - ibm_mongo_memory : Balance memory and disk allocation Note : Plan for data growth. Disk can be expanded online but cannot be reduced. Monitor storage usage to avoid running out of space. ibm_mongo_cpu \u00a4 Dedicated CPU cores per MongoDB member. Optional Environment Variable: IBM_MONGO_CPU Default Value: 0 (shared CPU) Purpose : Specifies dedicated CPU cores for each MongoDB member. Dedicated CPUs provide consistent performance but increase costs. When to use : - Use 0 (default) for shared CPU, suitable for development and light workloads - Set to 3 or higher for production workloads requiring consistent performance - Dedicated CPUs recommended for production environments Valid values : 0 (shared), or integer \u2265 3 for dedicated CPUs Impact : - 0 : Shared CPU, lower cost, variable performance - \u22653 : Dedicated CPUs, higher cost, consistent performance Related variables : - ibm_mongo_plan : CPU options vary by plan - ibm_mongo_memory : Balance CPU and memory allocation Note : Dedicated CPUs significantly increase costs but provide predictable performance. Production workloads typically need dedicated CPUs. ibm_mongo_backup_id \u00a4 IBM Cloud backup CRN (Cloud Resource Name) for restore operations. Required when is_restore=true Environment Variable: IBM_MONGO_BACKUP_ID Default Value: None Purpose : Specifies the IBM Cloud backup resource to restore from. The CRN uniquely identifies a specific backup in IBM Cloud. When to use : - Required only when restoring from an IBM Cloud backup - Obtain CRN from IBM Cloud console or CLI - Leave unset for new deployments Valid values : Valid IBM Cloud CRN for a MongoDB backup (format: crn:v1:... ) Impact : Restores MongoDB to the state captured in the specified backup. All current data will be replaced. Related variables : - is_restore : Must be set to true - restored_mongodb_service_name : Name for the restored service Note : Verify backup CRN before restoring . Restore is destructive and replaces all current data. Test restore procedures in non-production first. is_restore \u00a4 Flag to enable restore from IBM Cloud backup. Optional Environment Variable: IS_RESTORE Default Value: false Purpose : Controls whether to create a new MongoDB instance or restore from an existing backup. Acts as a safety flag to prevent accidental restores. When to use : - Set to true only when intentionally restoring from backup - Leave as false (default) for new deployments - Must be explicitly set to perform restore Valid values : true , false Impact : When true , creates MongoDB instance from backup instead of fresh deployment. Requires ibm_mongo_backup_id and restored_mongodb_service_name . Related variables : - ibm_mongo_backup_id : Required when true - restored_mongodb_service_name : Required when true Note : Always verify backup details before setting to true . Restore operations cannot be undone without another backup. restored_mongodb_service_name \u00a4 Name for the MongoDB service when restoring from backup. Required when is_restore=true Environment Variable: RESTORED_MONGODB_SERVICE_NAME Default Value: None Purpose : Specifies the name for the new MongoDB service created from backup. This allows restoring to a different service name than the original. When to use : - Required only when is_restore=true - Can be same as or different from original service name - Use different name to restore alongside existing instance for testing Valid values : Valid IBM Cloud resource name Impact : The restored MongoDB instance will have this name in IBM Cloud. Choose carefully as it affects resource identification and billing. Related variables : - is_restore : Must be set to true - ibm_mongo_backup_id : Backup to restore from - ibm_mongo_name : Original service name (can be different) Note : Using a different name allows side-by-side comparison of restored and current instances before switching over. Role Variables - AWS DocumentDB \u00a4 aws_access_key_id \u00a4 AWS account access key ID for authentication. Required when mongodb_provider=aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None Purpose : Provides AWS authentication credentials for creating and managing DocumentDB resources. The access key must have sufficient IAM permissions for DocumentDB, VPC, and related services. When to use : - Always required when using AWS DocumentDB provider - Must have IAM permissions for DocumentDB, EC2 (VPC/subnets/security groups) - Should be stored securely (e.g., in Ansible Vault or external secret management) Valid values : Valid AWS access key ID string Impact : This key is used to authenticate all AWS API calls. Insufficient permissions will cause deployment failures. Related variables : - aws_secret_access_key : Must be provided together - aws_region : Access key must have permissions in the target region Note : Never commit AWS credentials to source control . Use secure secret management. Ensure the IAM user/role has appropriate permissions for DocumentDB and VPC operations. aws_secret_access_key \u00a4 AWS account secret access key for authentication. Required when mongodb_provider=aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None Purpose : Provides the secret component of AWS authentication credentials. Works together with aws_access_key_id to authenticate AWS API requests. When to use : - Always required when using AWS DocumentDB provider - Must correspond to the provided aws_access_key_id - Should be stored securely Valid values : Valid AWS secret access key string Impact : Used with access key ID to authenticate AWS API calls. Invalid or mismatched credentials will cause authentication failures. Related variables : - aws_access_key_id : Must be provided together Note : Store securely and never commit to source control . Rotate credentials regularly following AWS security best practices. aws_region \u00a4 AWS region where DocumentDB cluster will be deployed. Required when mongodb_provider=aws Environment Variable: AWS_REGION Default Value: us-east-2 Purpose : Specifies the geographic AWS region for DocumentDB deployment. Region selection affects latency, data residency, availability zones, and pricing. When to use : - Always required when using AWS DocumentDB provider - Choose region closest to your OpenShift cluster for lowest latency - Consider data residency requirements for compliance - Use default ( us-east-2 ) if no specific requirements Valid values : Valid AWS region code (e.g., us-east-1 , us-east-2 , us-west-2 , eu-west-1 , ap-southeast-1 ) Impact : Affects network latency, data residency compliance, available availability zones, and regional pricing. Cannot be changed after creation. Related variables : - vpc_id : VPC must exist in the specified region - aws_access_key_id : Credentials must have permissions in this region Note : Choose region carefully as it cannot be changed. Deploy DocumentDB in the same region as your OpenShift cluster for best performance. vpc_id \u00a4 AWS VPC ID where DocumentDB resources will be created. Required when mongodb_provider=aws Environment Variable: VPC_ID Default Value: None Purpose : Specifies the AWS Virtual Private Cloud where DocumentDB cluster, subnets, and security groups will be created. The VPC provides network isolation and connectivity. When to use : - Always required when using AWS DocumentDB provider - Use the same VPC as your OpenShift cluster for direct connectivity - VPC must exist in the specified aws_region Valid values : Valid AWS VPC ID (format: vpc-xxxxxxxxxxxxxxxxx ) Impact : Determines network connectivity and security boundaries. DocumentDB will only be accessible from resources within this VPC or connected networks. Related variables : - aws_region : VPC must exist in this region - docdb_cidr_az1 , docdb_cidr_az2 , docdb_cidr_az3 : Subnets created within this VPC - docdb_ingress_cidr , docdb_egress_cidr : Should match VPC CIDR ranges Note : Ensure the VPC has sufficient available IP addresses and appropriate routing for DocumentDB connectivity. docdb_cluster_name \u00a4 Name for the AWS DocumentDB cluster. Required when mongodb_provider=aws Environment Variable: DOCDB_CLUSTER_NAME Default Value: None Purpose : Identifies the DocumentDB cluster in AWS. This name is used for resource identification, tagging, and as a prefix for related resources. When to use : - Always required when using AWS DocumentDB provider - Choose a descriptive name that identifies the MAS instance and environment - Name is used as prefix for subnet groups and security groups Valid values : Valid AWS DocumentDB cluster identifier (lowercase, alphanumeric, hyphens, must start with letter) Impact : This name appears in AWS console, CloudWatch metrics, and billing. Related resources (subnet group, security group) are named based on this value. Related variables : - docdb_subnet_group_name : Defaults to docdb-{cluster_name} - docdb_security_group_name : Defaults to docdb-{cluster_name} - docdb_admin_credentials_secret_name : Defaults to {cluster_name}-admin-credentials Note : Choose a meaningful name as it cannot be easily changed. The name must be unique within your AWS account and region. docdb_subnet_group_name \u00a4 Name for the DocumentDB subnet group. Optional Default Value: docdb-{{ docdb_cluster_name }} Purpose : Specifies the name for the DocumentDB subnet group that defines which subnets the cluster can use. The role creates this subnet group automatically. When to use : - Use default naming for standard deployments - Customize only if organizational naming standards require it Valid values : Valid AWS subnet group name Impact : The subnet group associates the DocumentDB cluster with specific subnets across availability zones. Related variables : - docdb_cluster_name : Default name includes this value - docdb_cidr_az1 , docdb_cidr_az2 , docdb_cidr_az3 : Subnets included in this group Note : This is automatically created by the role. Default naming is recommended for consistency. docdb_security_group_name \u00a4 Name for the DocumentDB security group. Optional Default Value: docdb-{{ docdb_cluster_name }} Purpose : Specifies the name for the security group that controls network access to the DocumentDB cluster. The role creates this security group automatically. When to use : - Use default naming for standard deployments - Customize only if organizational naming standards require it Valid values : Valid AWS security group name Impact : The security group defines firewall rules for DocumentDB access based on docdb_ingress_cidr and docdb_egress_cidr . Related variables : - docdb_cluster_name : Default name includes this value - docdb_ingress_cidr : Allowed source CIDR for inbound traffic - docdb_egress_cidr : Allowed destination CIDR for outbound traffic Note : This is automatically created by the role. Default naming is recommended for consistency. docdb_admin_credentials_secret_name \u00a4 Name of the Kubernetes secret containing DocumentDB admin credentials. Optional Default Value: {{ docdb_cluster_name }}-admin-credentials Purpose : Specifies the Kubernetes secret name where DocumentDB administrator credentials are stored. This secret is created automatically by the role. When to use : - Use default naming for standard deployments - Customize only if organizational naming standards require it Valid values : Valid Kubernetes secret name Impact : The secret contains admin username and password for DocumentDB access. MAS and other applications reference this secret for database connectivity. Related variables : - docdb_cluster_name : Default name includes this value - docdb_master_username : Username stored in this secret Note : This secret is created in the MAS core namespace. Default naming is recommended for consistency. docdb_engine_version \u00a4 DocumentDB engine version to deploy. Optional Environment Variable: DOCDB_ENGINE_VERSION Default Value: 5.0.0 Purpose : Specifies the DocumentDB engine version. MAS requires DocumentDB 5.0.0 for MongoDB compatibility. When to use : - Use default ( 5.0.0 ) for MAS deployments (required) - Do not change unless specifically required by MAS version Valid values : 5.0.0 (only version supported by MAS) Impact : Determines MongoDB compatibility and available features. MAS is only certified with DocumentDB 5.0.0. Related variables : None Note : MAS only supports DocumentDB 5.0.0 . Do not change this value unless MAS documentation explicitly supports other versions. docdb_master_username \u00a4 Master username for DocumentDB cluster administration. Optional Environment Variable: DOCDB_MASTER_USERNAME Default Value: docdbadmin Purpose : Specifies the master administrator username for the DocumentDB cluster. This user has full administrative privileges. When to use : - Use default ( docdbadmin ) for standard deployments - Customize if organizational security policies require specific usernames Valid values : Valid DocumentDB username (alphanumeric, must start with letter, 1-63 characters) Impact : This username is used for administrative access and is stored in the Kubernetes secret specified by docdb_admin_credentials_secret_name . Related variables : - docdb_admin_credentials_secret_name : Secret where credentials are stored Note : Choose carefully as the master username cannot be changed after cluster creation. docdb_instance_class \u00a4 AWS instance class for DocumentDB instances. Optional Environment Variable: DOCDB_INSTANCE_CLASS Default Value: db.t3.medium Purpose : Specifies the compute and memory capacity for each DocumentDB instance. Instance class affects performance, availability, and cost. When to use : - Use db.t3.medium (default) for development or small deployments - Use db.r5.large or larger for production workloads - Consider db.r6g instances for better price/performance (ARM-based) Valid values : Valid DocumentDB instance class (e.g., db.t3.medium , db.r5.large , db.r5.xlarge , db.r6g.large ) Impact : Affects CPU, memory, network performance, and cost. Larger instances provide better performance but cost more. Related variables : - docdb_instance_number : Total cost = instance class cost \u00d7 number of instances Note : Production deployments typically need db.r5.large or larger. Review AWS DocumentDB pricing and instance specifications. docdb_instance_number \u00a4 Number of DocumentDB instances in the cluster. Optional Environment Variable: DOCDB_INSTANCE_NUMBER Default Value: 3 Purpose : Determines the number of instances in the DocumentDB cluster. More instances provide higher availability and read scalability. When to use : - Use default ( 3 ) for production deployments with high availability - Use 1 only for development or testing (no high availability) - Use 5 or more for critical workloads requiring higher availability Valid values : Integer from 1 to 16 Impact : - More instances = higher availability and read capacity but higher cost - Instances are distributed across availability zones for fault tolerance - Total cost = instance class cost \u00d7 number of instances Related variables : - docdb_instance_class : Determines per-instance cost and performance - docdb_cidr_az1 , docdb_cidr_az2 , docdb_cidr_az3 : Instances distributed across these AZs Note : Production deployments should use 3 or more instances for high availability. Single instance has no failover capability. docdb_instance_identifier_prefix \u00a4 Prefix for DocumentDB instance identifiers. Required when mongodb_provider=aws Environment Variable: DOCDB_INSTANCE_IDENTIFIER_PREFIX Default Value: None Purpose : Specifies the prefix used to name individual DocumentDB instances. Instance names are formed as {prefix}-{number} . When to use : - Always required when using AWS DocumentDB provider - Use a descriptive prefix that identifies the cluster and environment - Typically matches or relates to docdb_cluster_name Valid values : Valid AWS instance identifier prefix (lowercase, alphanumeric, hyphens) Impact : Instance names appear in AWS console and CloudWatch metrics. Choose a meaningful prefix for easy identification. Related variables : - docdb_cluster_name : Typically related to cluster name Note : Instance identifiers are formed as {prefix}-1 , {prefix}-2 , etc. Choose a clear, descriptive prefix. docdb_ingress_cidr \u00a4 CIDR block allowed to connect to DocumentDB cluster. Required when mongodb_provider=aws Environment Variable: DOCDB_INGRESS_CIDR Default Value: None Purpose : Specifies the IPv4 CIDR range from which incoming connections to DocumentDB are allowed. This is used in security group ingress rules. When to use : - Always required when using AWS DocumentDB provider - Typically set to the CIDR of your OpenShift cluster's VPC - Can be set to specific subnet CIDRs for tighter security Valid values : Valid IPv4 CIDR notation (e.g., 10.0.0.0/16 , 172.31.0.0/16 ) Impact : Only traffic from this CIDR range can connect to DocumentDB. Too restrictive blocks legitimate traffic; too permissive reduces security. Related variables : - vpc_id : Should match VPC CIDR or subnet CIDRs within the VPC - docdb_egress_cidr : Typically set to same value Note : Set to your OpenShift cluster's VPC CIDR for proper connectivity. Verify CIDR ranges before deployment. docdb_egress_cidr \u00a4 CIDR block for outbound connections from DocumentDB cluster. Required when mongodb_provider=aws Environment Variable: DOCDB_EGRESS_CIDR Default Value: None Purpose : Specifies the IPv4 CIDR range to which DocumentDB can send outbound connections. This is used in security group egress rules. When to use : - Always required when using AWS DocumentDB provider - Typically set to the same value as docdb_ingress_cidr - Set to VPC CIDR for standard deployments Valid values : Valid IPv4 CIDR notation (e.g., 10.0.0.0/16 , 172.31.0.0/16 ) Impact : DocumentDB can only send traffic to this CIDR range. Affects ability to respond to client connections. Related variables : - docdb_ingress_cidr : Typically set to same value - vpc_id : Should match VPC CIDR Note : Usually set to the same value as docdb_ingress_cidr . Verify CIDR ranges match your network configuration. docdb_cidr_az1 \u00a4 CIDR block for DocumentDB subnet in availability zone 1. Required when mongodb_provider=aws Environment Variable: DOCDB_CIDR_AZ1 Default Value: None Purpose : Specifies the IPv4 CIDR for the subnet in the first availability zone. If the subnet exists with tag Name: {{ docdb_cluster_name }} , it's used; otherwise, a new subnet is created. When to use : - Always required when using AWS DocumentDB provider - Must be within the VPC CIDR range - Must not overlap with other subnets in the VPC Valid values : Valid IPv4 CIDR notation within VPC range (e.g., 10.0.1.0/24 ) Impact : Defines the IP address range for DocumentDB instances in AZ1. Subnet size affects number of available IP addresses. Related variables : - vpc_id : CIDR must be within this VPC's range - docdb_cidr_az2 , docdb_cidr_az3 : Must not overlap with these subnets Note : Plan subnet sizes carefully. Each DocumentDB instance needs an IP address. Use /24 or larger subnets for flexibility. docdb_cidr_az2 \u00a4 CIDR block for DocumentDB subnet in availability zone 2. Required when mongodb_provider=aws Environment Variable: DOCDB_CIDR_AZ2 Default Value: None Purpose : Specifies the IPv4 CIDR for the subnet in the second availability zone. If the subnet exists with tag Name: {{ docdb_cluster_name }} , it's used; otherwise, a new subnet is created. When to use : - Always required when using AWS DocumentDB provider - Must be within the VPC CIDR range - Must not overlap with other subnets in the VPC Valid values : Valid IPv4 CIDR notation within VPC range (e.g., 10.0.2.0/24 ) Impact : Defines the IP address range for DocumentDB instances in AZ2. Required for multi-AZ high availability. Related variables : - vpc_id : CIDR must be within this VPC's range - docdb_cidr_az1 , docdb_cidr_az3 : Must not overlap with these subnets Note : Use different availability zones for AZ1, AZ2, and AZ3 to ensure high availability across zones. docdb_cidr_az3 \u00a4 CIDR block for DocumentDB subnet in availability zone 3. Required when mongodb_provider=aws Environment Variable: DOCDB_CIDR_AZ3 Default Value: None Purpose : Specifies the IPv4 CIDR for the subnet in the third availability zone. If the subnet exists with tag Name: {{ docdb_cluster_name }} , it's used; otherwise, a new subnet is created. When to use : - Always required when using AWS DocumentDB provider - Must be within the VPC CIDR range - Must not overlap with other subnets in the VPC Valid values : Valid IPv4 CIDR notation within VPC range (e.g., 10.0.3.0/24 ) Impact : Defines the IP address range for DocumentDB instances in AZ3. Provides third availability zone for maximum fault tolerance. Related variables : - vpc_id : CIDR must be within this VPC's range - docdb_cidr_az1 , docdb_cidr_az2 : Must not overlap with these subnets Note : Three availability zones provide best fault tolerance. Ensure subnets are in different AZs for proper distribution. AWS DocumentDB Secret Rotation Variables \u00a4 The following variables are used for rotating DocumentDB credentials. These are typically used with mongodb_action=rotate-secret . docdb_mongo_instance_name \u00a4 DocumentDB instance name for secret rotation. Required when rotating secrets Environment Variable: DOCDB_MONGO_INSTANCE_NAME Default Value: None Purpose : Identifies the specific DocumentDB instance for credential rotation operations. When to use : - Required when performing secret rotation ( mongodb_action=rotate-secret ) - Must match an existing DocumentDB instance name Valid values : Valid DocumentDB instance identifier Impact : Specifies which DocumentDB instance's credentials will be rotated. Related variables : - docdb_cluster_name : Instance belongs to this cluster - docdb_host : Host address of this instance Note : Verify instance name before rotation to avoid affecting wrong instance. docdb_host \u00a4 DocumentDB instance host address for connection. Required when rotating secrets Environment Variable: DOCDB_HOST Default Value: None Purpose : Specifies the host address of a DocumentDB instance for establishing connection during secret rotation. When to use : - Required when performing secret rotation - Use any one host address from the DocumentDB cluster - Obtain from AWS console or DocumentDB cluster endpoint Valid values : Valid DocumentDB instance hostname or endpoint Impact : Used to connect to DocumentDB for credential rotation operations. Related variables : - docdb_port : Port for this host - docdb_mongo_instance_name : Instance identifier Note : Any instance host from the cluster can be used for rotation operations. docdb_port \u00a4 DocumentDB instance port number. Required when rotating secrets Environment Variable: DOCDB_PORT Default Value: None (typically 27017 ) Purpose : Specifies the port number for connecting to the DocumentDB instance during secret rotation. When to use : - Required when performing secret rotation - Typically 27017 (default DocumentDB port) Valid values : Valid port number (typically 27017 ) Impact : Used with docdb_host to establish connection for credential rotation. Related variables : - docdb_host : Host address for this port Note : DocumentDB uses port 27017 by default unless customized during cluster creation. docdb_instance_username \u00a4 Username for which password is being rotated. Required when rotating secrets Environment Variable: DOCDB_INSTANCE_USERNAME Default Value: None Purpose : Specifies the DocumentDB username whose password will be changed during rotation. When to use : - Required when performing secret rotation - Typically the application user or admin user - Must be an existing DocumentDB user Valid values : Valid DocumentDB username Impact : This user's password will be changed. Applications using this username must be updated with the new password. Related variables : - docdb_instance_password_old : Current password for this user - docdb_master_username : Master user for performing rotation Note : Ensure applications can handle password rotation. Consider using connection pooling with reconnection logic. docdb_instance_password_old \u00a4 Current password for the user being rotated. Required when rotating secrets Environment Variable: DOCDB_PASSWORD_OLD Default Value: None Purpose : Provides the current password for authentication before rotation. Used to verify current credentials. When to use : - Required when performing secret rotation - Must be the current valid password Valid values : Current password string Impact : Used to authenticate before changing password. Incorrect password will cause rotation to fail. Related variables : - docdb_instance_username : User for this password Note : Store securely. After rotation, this password will no longer be valid. docdb_master_password \u00a4 DocumentDB master user password for administrative operations. Required when rotating secrets Environment Variable: DOCDB_MASTER_PASSWORD Default Value: None Purpose : Provides master user credentials for performing password rotation operations. Master user has privileges to change other users' passwords. When to use : - Required when performing secret rotation - Must be the current master password Valid values : Valid master password string Impact : Used to authenticate as master user to perform credential rotation. Related variables : - docdb_master_username : Master username for this password Note : Store master credentials securely . Never commit to source control. docdb_master_username \u00a4 DocumentDB master username for administrative operations. Required when rotating secrets Environment Variable: DOCDB_MASTER_USERNAME Default Value: None Purpose : Specifies the master username for performing password rotation operations. Master user has administrative privileges. When to use : - Required when performing secret rotation - Typically docdbadmin or the value set during cluster creation Valid values : Valid DocumentDB master username Impact : Used with docdb_master_password to authenticate for credential rotation. Related variables : - docdb_master_password : Password for this master user Note : This should match the master username set during DocumentDB cluster creation. AWS DocumentDB destroy-data action Variables \u00a4 mas_instance_id \u00a4 The specified MAS instance ID Required Environment Variable: MAS_INSTANCE_ID Default Value: None mongo_username \u00a4 Mongo Username Environment Variable: MONGO_USERNAME Default Value: None mongo_password \u00a4 Mongo password Environment Variable: MONGO_PASSWORD Default Value: None config \u00a4 Mongo Config, please refer to the below example playbook section for details Required Environment Variable: CONFIG Default Value: None certificates \u00a4 Mongo Certificates, please refer to the below example playbook section for details Required Environment Variable: CERTIFICATES Default Value: None Example Playbook \u00a4 Install (CE Operator) \u00a4 - hosts: localhost any_errors_fatal: true vars: mongodb_storage_class: ibmc-block-gold mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.mongodb Backup (CE Operator) \u00a4 - hosts: localhost any_errors_fatal: true vars: mongodb_action: backup mas_instance_id: masinst1 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.mongodb Restore (CE Operator) \u00a4 - hosts: localhost any_errors_fatal: true vars: mongodb_action: restore mas_instance_id: masinst1 masbr_restore_from_version: 20240621021316 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.mongodb Install (IBM Cloud) \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: ibm ibmcloud_apikey: apikey**** ibmcloud_resource_group: mas-test roles: - ibm.mas_devops.mongodb Install (AWS DocumentDB) \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: aws mongodb_action: provision docdb_size: ~/docdb-config.yml docdb_cluster_name: test-db docdb_ingress_cidr: 10.0.0.0/16 docdb_egress_cidr: 10.0.0.0/16 docdb_cidr_az1: 10.0.0.0/26 docdb_cidr_az2: 10.0.0.64/26 docdb_cidr_az3: 10.0.0.128/26 docdb_instance_identifier_prefix: test-db-instance vpc_id: test-vpc-id aws_access_key_id: aws-key aws_secret_access_key: aws-access-key roles: - ibm.mas_devops.mongodb AWS DocumentDb Secret Rotation \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: aws mongodb_action: docdb_secret_rotate docdb_mongo_instance_name: test-db-instance db_host: aws.test1.host7283-***** db_port: 27017 docdb_master_username: admin docdb_master_password: pass*** docdb_instance_password_old: oldpass**** docdb_instance_username: testuser aws_access_key_id: aws-key aws_secret_access_key: aws-access-key roles: - ibm.mas_devops.mongodb AWS DocumentDb destroy-data action \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mongodb_provider: aws mongodb_action: destroy-data mongo_username: pqradmin mongo_password: xyzabc config: configDb: admin authMechanism: DEFAULT retryWrites: false hosts: - host: abc-0.pqr.databases.appdomain.cloud port: 32250 - host: abc-1.pqr.databases.appdomain.cloud port: 32250 - host: abc-2.pqr.databases.appdomain.cloud port: 32250 certificates: - alias: ca crt: | -----BEGIN CERTIFICATE----- MIIDDzCCAfegAwIBAgIJANEH58y2/kzHMA0GCSqGSIb3DQEBCwUAMB4xHDAaBgNV BAMME0lCTSBDbG91ZCBEYXRhYmFzZXMwHhcNMTgwNjI1MTQyOTAwWhcNMjgwNjIy MTQyOTAwWjAeMRwwGgYDVQQDDBNJQk0gQ2xvdWQgRGF0YWJhc2VzMIIBIjANBgkq 1eKI2FLzYKpoKBe5rcnrM7nHgNc/nCdEs5JecHb1dHv1QfPm6pzIxwIDAQABo1Aw TjAdBgNVHQ4EFgQUK3+XZo1wyKs+DEoYXbHruwSpXjgwHwYDVR0jBBgwFoAUK3+X Zo1wyKs+DEoYXbHruwSpXjgwDAYDVR0TBAUwAwEB/zANBgkqhkiG9w0BAQsFAAOC doqqgGIZ2nxCkp5/FXxF/TMb55vteTQwfgBy60jVVkbF7eVOWCv0KaNHPF5hrqbN i+3XjJ7/peF3xMvTMoy35DcT3E2ZeSVjouZs15O90kI3k2daS2OHJABW0vSj4nLz +PQzp/B9cQmOO8dCe049Q3oaUA== -----END CERTIFICATE----- roles: - ibm.mas_devops.mongodb Run Role Playbook \u00a4 export MONGODB_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=masinst1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook ibm.mas_devops.run_role Troubleshooting \u00a4 Important Please be cautious while performing any of the troubleshooting steps outlined below. It is important to understand that the MongoDB Community operator persists data within Persistent Volume Claims. These claims should not be removed inadvertent deletion of the mongoce namespace could result in data loss. MongoDB Replica Set Pods Will Not Start \u00a4 MongoDB 5 has introduced new platform specific requirements. Please consult the Platform Support Notes for detailed information. It is of particular importance to confirm that the AVX instruction set is exposed or available to the MongoDB workloads. This can easily be determined by entering any running pod on the same OpenShift cluster where MongoDB replica set members are failing to start. Once inside of a running pod the following command can be executed to confirm if the AVX instruction set is available: cat /proc/cpuinfo | grep flags | grep avx If avx is not found in the available flags then either the physical processor hosting the OpenShift cluster does not provide the AVX instruction set or the virtual host configuration is not exposing the AVX instruction set. If the latter is suspected the virtual hosting documentation should be referenced for details on how to expose the AVX instruction set. LDAP Authentication \u00a4 If authenticating via LDAP with PLAIN specified for authMechanism then configDb must be set to $external in the MongoCfg. The field configDb in the MongoCfg refers to the authentication database. CA Certificate Renewal \u00a4 Warning If the MongoDB CA Certificate expires the MongoDB replica set will become unusable. Replica set members will not be able to communicate with each other and client applications (i.e. Maximo Application Suite components) will not be to connect. In order to renew the CA Certificate used by the MongoDB replica set the following steps must be taken: Delete the CA Certificate resource Delete the MongoDB server Certificate resource Delete the Secrets resources associated with both the CA Certificate and Server Certificate Delete the Secret resource which contains the MongoDB configuration parameters Delete the ConfigMap resources which contains the CA certificate Delete the Secret resource which contains the sever certificate and private key The following steps illustrate the process required to renew the CA Certificate, sever certificate and reconfigure the MongoDB replica set with the new CA and server certificates. The first step is to stop the Mongo replica set and MongoDb CE Operator pod. oc project mongoce oc delete deployment mongodb-kubernetes-operator Important Make sure the MongoDB Community operator pod has terminated before proceeding. oc delete statefulset mas-mongo-ce Important Make sure all pods in the mongoce namespace have terminated before proceeding Remove expired CA Certificate and Server Certificate resources. Clean up MongoDB Community configuration and then run the mongodb role. oc delete certificate mongo-ca-crt oc delete certificate mongo-server oc delete secret mongo-ca-secret oc delete secret mongo-server-cert oc delete secret mas-mongo-ce-config oc delete configmap mas-mongo-ce-cert-map oc delete secret mas-mongo-ce-server-certificate-key export ROLE_NAME=mongodb ansible-playbook ibm.mas_devops.run_role Once the mongodb role has completed the MongoDb CE Operator pod and Mongo replica set should be configured. After the CA and server Certificates have been renewed you must ensure that that MongoCfg Suite CR is updated with the new CA Certificate. First obtain the CA Certificate from the Secret resource mongo-ca-secret . Then edit the Suite MongoCfg CR in the Maximo Application Suite core namespace. This is done by updating the appropriate certificate under .spec.certificates in the MongoCfg CR: spec: certificates: - alias: ca crt: | -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- If an IBM Suite Licensing Service (SLS) is also connecting to the MongoDB replica set the LicenseService CR must also be updated to reflect the new MongoDB CA. This can be added to the .spec.mongo.certificates section of the LicenseService CR. mongo: certificates: - alias: mongoca crt: | -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- Once the CA certificate has been updated for the MongoCfg and LicenseService CRs several pods in the core and SLS namespaces might need to be restarted to pick up the changes. This would include but is not limited to coreidp, coreapi, api-licensing. License \u00a4 EPL-2.0","title":"mongodb"},{"location":"roles/mongodb/#mongodb","text":"This role currently supports provisioning of mongodb in three different providers: - community - aws (documentdb) - ibm If the selected provider is community then the MongoDB Community Kubernetes Operator will be configured and deployed into the specified namespace. By default a three member MongoDB replica set will be created. The cluster will bind six PVCs, these provide persistence for the data and system logs across the three nodes. Currently there is no support built-in for customizing the cluster beyond this configuration. Tip The role will generate a yaml file containing the definition of a Secret and MongoCfg resource that can be used to configure the deployed instance as the MAS system MongoDb. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/mongocfg-mongoce-system.yaml or used in conjunction with the suite_config role.","title":"mongodb"},{"location":"roles/mongodb/#prerequisites","text":"To run this role with providers as ibm or aws you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role when provider is either ibm or aws . To run the docdb_secret_rotate MONGODB_ACTION when the provider is aws you must have already installed the Mongo Shell . This role will install a GrafanaDashboard used for monitoring the MongoDB instance when the provided is community and you have run the grafana role previously. If you did not run the grafana role then the GrafanaDashboard won't be installed.","title":"Prerequisites"},{"location":"roles/mongodb/#role-variables","text":"","title":"Role Variables"},{"location":"roles/mongodb/#common-variables","text":"","title":"Common Variables"},{"location":"roles/mongodb/#mas_instance_id","text":"Unique identifier for the MAS instance that will use this MongoDB deployment. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None Purpose : Identifies which MAS instance this MongoDB configuration targets. Used to generate the MongoCfg resource that connects MAS to the MongoDB instance. When to use : - Set when you want to automatically generate MongoCfg for MAS integration - Must be set together with mas_config_dir for MongoCfg generation - Leave unset if manually managing MongoDB configuration for MAS Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : When set with mas_config_dir , generates a MongoCfg YAML file at $MAS_CONFIG_DIR/mongocfg-mongoce-system.yaml . Without this, no MAS configuration file is created. Related variables : Must be set together with mas_config_dir for MongoCfg generation.","title":"mas_instance_id"},{"location":"roles/mongodb/#mas_config_dir","text":"Local directory path where the generated MongoCfg resource file will be saved. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None Purpose : Specifies where to save the MongoCfg YAML file that configures MAS to connect to this MongoDB instance. This file can be applied manually or used with the suite_config role for automated MAS configuration. When to use : - Set when you want to automatically generate MongoCfg for MAS integration - Must be set together with mas_instance_id for MongoCfg generation - Use the same directory across multiple dependency roles (mongodb, db2, sls) to collect all configurations - Leave unset if manually managing MongoDB configuration for MAS Valid values : Any valid local filesystem path (e.g., /home/user/masconfig , ~/masconfig , ./config ) Impact : When set with mas_instance_id , creates file mongocfg-mongoce-system.yaml in this directory. The file contains Secret and MongoCfg resources ready to apply to MAS. Related variables : - Must be set together with mas_instance_id for MongoCfg generation - Used by suite_config role to apply configurations","title":"mas_config_dir"},{"location":"roles/mongodb/#mongodb_provider","text":"Selects which MongoDB deployment option to use for MAS database requirements. Optional Environment Variable: MONGODB_PROVIDER Default Value: community Purpose : Determines the MongoDB infrastructure provider, which affects deployment architecture, management approach, operational requirements, and cost model. Each provider offers different trade-offs between control, convenience, and cost. When to use : - Use community for self-managed deployments on OpenShift with full control - Use ibm for managed IBM Cloud Databases for MongoDB service - Use aws for managed AWS DocumentDB service (MongoDB-compatible) - Consider operational expertise, cloud platform, and management preferences Valid values : - community - MongoDB Community Edition Operator (self-managed on OpenShift) - ibm - IBM Cloud Databases for MongoDB (managed service) - aws - AWS DocumentDB (managed service, MongoDB-compatible) Impact : - community : Requires cluster storage, manual backup management, and operational overhead - ibm : Requires IBM Cloud account, API key, and incurs IBM Cloud service charges - aws : Requires AWS account, VPC configuration, and incurs AWS service charges Related variables : - When community : Requires mongodb_storage_class and related storage/resource variables - When ibm : Requires ibmcloud_apikey , ibm_mongo_region , ibm_mongo_resourcegroup - When aws : Requires aws_access_key_id , aws_secret_access_key , vpc_id , docdb_* variables - Affects which mongodb_action values are supported Note : Provider cannot be changed after initial deployment. Migration between providers requires backup and restore procedures.","title":"mongodb_provider"},{"location":"roles/mongodb/#mongodb_action","text":"Specifies which operation to perform on the MongoDB instance. Optional Environment Variable: MONGODB_ACTION Default Value: install Purpose : Controls what action the role executes against the MongoDB instance. Different providers support different sets of actions based on their capabilities and management model. When to use : - Use install for initial deployment or updates - Use uninstall to remove MongoDB instance (use with caution) - Use backup to create MongoDB backups (community/ibm only) - Use restore to restore from backup (community/ibm only) - Use docdb_secret_rotate to rotate DocumentDB credentials (aws only) - Use destroy-data to delete all data from MongoDB (aws only) - Use create-mongo-service-credentials to generate service credentials (ibm only) Valid values (provider-specific): - community : install , uninstall , backup , restore - aws : install , uninstall , docdb_secret_rotate , destroy-data - ibm : install , uninstall , backup , restore , create-mongo-service-credentials Impact : The action determines what the role will do. Destructive actions like uninstall and destroy-data will permanently delete data. Backup/restore actions require additional variables to be set. Related variables : - mongodb_provider determines which actions are available - Backup actions require masbr_* variables - Restore actions require masbr_restore_from_version - AWS secret rotation requires docdb_* credential variables Note : Always backup data before performing destructive operations. Some actions are irreversible.","title":"mongodb_action"},{"location":"roles/mongodb/#ce-operator-variables","text":"","title":"CE Operator Variables"},{"location":"roles/mongodb/#mongodb_namespace","text":"OpenShift namespace where the MongoDB Community Operator and MongoDB cluster will be deployed. Optional Environment Variable: MONGODB_NAMESPACE Default Value: mongoce Purpose : Defines the Kubernetes namespace for MongoDB resources, providing isolation and organization for the MongoDB deployment within the cluster. When to use : - Use default mongoce for standard deployments - Change only if you need multiple MongoDB instances or have namespace naming requirements - Ensure namespace doesn't conflict with existing deployments Valid values : Any valid Kubernetes namespace name (lowercase alphanumeric and hyphens) Impact : All MongoDB resources (operator, replica set pods, PVCs, secrets, services) will be created in this namespace. Changing this after deployment requires reinstallation. Related variables : Used in MongoCfg generation to reference MongoDB service endpoints.","title":"mongodb_namespace"},{"location":"roles/mongodb/#mongodb_version","text":"Specifies the MongoDB version to deploy. Optional Environment Variable: MONGODB_VERSION Default Value: Automatically defined by the mongo version specified in the latest MAS case bundle available Purpose : Controls which MongoDB version is deployed, ensuring compatibility with MAS requirements and enabling version-specific features. The default aligns with the tested and supported version for your MAS release. When to use : - Leave as default for standard deployments (recommended) - Override only when specific version requirements exist - Use for testing compatibility with newer MongoDB versions - Never use to downgrade an existing MongoDB instance Valid values : 7.0.12 , 7.0.22 , 7.0.23 , 8.0.13 , 8.0.17 (check MAS compatibility matrix for supported versions) Impact : Determines MongoDB feature set, performance characteristics, and compatibility. Changing versions may require data migration or compatibility testing. Related variables : - Version upgrades require corresponding mongodb_v*_upgrade flags - Must be compatible with MAS version requirements Note : Never downgrade MongoDB versions . Always create scheduled backups before version changes. Use mongodb_v5_upgrade , mongodb_v6_upgrade , mongodb_v7_upgrade , or mongodb_v8_upgrade flags when upgrading between major versions.","title":"mongodb_version"},{"location":"roles/mongodb/#mongodb_override_spec","text":"Forces the role to use environment variables instead of preserving existing MongoDB spec settings. Optional Environment Variable: MONGODB_OVERRIDE_SPEC Default Value: false Purpose : Controls whether the role preserves existing MongoDB configuration during upgrades/reinstalls or applies new values from environment variables. This prevents accidental configuration changes during routine operations. When to use : - Leave as false (default) to preserve existing settings during upgrades - Set to true only when intentionally changing MongoDB configuration - Use with caution - requires setting all environment variables to match desired state Valid values : true , false Impact : - When false : Existing CPU, memory, storage, and replica settings are preserved during reinstall/upgrade - When true : All settings are taken from environment variables; unset variables revert to defaults Related variables : When set to true , affects these settings: - mongodb_cpu_limits - mongodb_mem_limits - mongodb_cpu_requests - mongodb_mem_requests - mongodb_storage_class - mongodb_storage_capacity_data - mongodb_storage_capacity_logs - mongodb_replicas Note : Check existing MongoDB installation before enabling . If environment variables don't match current spec, resources may be reset to defaults, potentially causing disruption. Unknown settings are not preserved.","title":"mongodb_override_spec"},{"location":"roles/mongodb/#mongodb_storage_class","text":"Name of the Kubernetes storage class for MongoDB persistent volumes. Required when mongodb_provider=community Environment Variable: MONGODB_STORAGE_CLASS Default Value: None Purpose : Specifies which storage class provides persistent volumes for MongoDB data and logs. The storage class determines performance characteristics, availability, and cost of MongoDB storage. When to use : - Always required for Community Edition deployments - Choose based on performance requirements (SSD vs HDD) - Consider backup and snapshot capabilities of the storage class - Verify storage class exists in your cluster before deployment Valid values : Any valid storage class name in your cluster that supports ReadWriteOnce (RWO) access mode Impact : Affects MongoDB performance, reliability, and cost. Six PVCs will be created (data + logs for each of 3 replicas by default). Storage class cannot be changed after deployment without data migration. Related variables : - mongodb_storage_capacity_data - Size of data PVCs - mongodb_storage_capacity_logs - Size of log PVCs - mongodb_replicas - Number of replica sets (affects total PVC count) Note : Storage class must support ReadWriteOnce (RWO) access mode. Verify with oc get storageclass before deployment.","title":"mongodb_storage_class"},{"location":"roles/mongodb/#mongodb_storage_capacity_data","text":"Size of the persistent volume claim (PVC) for MongoDB data storage on each replica set member. Optional Environment Variable: MONGODB_STORAGE_CAPACITY_DATA Default Value: 20Gi Purpose : Determines disk space allocated for storing MongoDB databases, collections, and indexes on each replica. Proper sizing prevents storage exhaustion and ensures adequate space for data growth. When to use : - Increase from default for production environments with large data volumes - Increase for environments with high data growth rates - Use default (20Gi) for development, testing, or small deployments - Consider backup strategy when sizing (larger volumes take longer to backup) Valid values : Any valid Kubernetes storage size (e.g., 20Gi , 100Gi , 500Gi , 1Ti ) Impact : - Larger values consume more cluster storage resources - Cannot be decreased after deployment (PVC expansion only) - Total storage = this value \u00d7 number of replicas - Affects backup and restore duration Related variables : - mongodb_replicas : Total storage = capacity \u00d7 replicas - mongodb_storage_class : Must support volume expansion if you plan to increase size later - mongodb_storage_capacity_logs : Consider balancing data and log storage Note : PVCs can be expanded but not shrunk. Plan for growth when setting initial size. Monitor storage usage to avoid running out of space.","title":"mongodb_storage_capacity_data"},{"location":"roles/mongodb/#mongodb_storage_capacity_logs","text":"Size of the persistent volume claim (PVC) for MongoDB log storage on each replica set member. Optional Environment Variable: MONGODB_STORAGE_CAPACITY_LOGS Default Value: 20Gi Purpose : Determines disk space allocated for MongoDB operational logs on each replica. Logs are essential for troubleshooting, auditing, and monitoring MongoDB operations. When to use : - Increase for production environments with verbose logging requirements - Increase if log retention policies require longer history - Use default (20Gi) for standard deployments - Consider log rotation and retention policies when sizing Valid values : Any valid Kubernetes storage size (e.g., 10Gi , 20Gi , 50Gi ) Impact : - Larger values consume more cluster storage resources - Cannot be decreased after deployment (PVC expansion only) - Total log storage = this value \u00d7 number of replicas - Insufficient log space can cause MongoDB operational issues Related variables : - mongodb_replicas : Total log storage = capacity \u00d7 replicas - mongodb_storage_class : Must support volume expansion - mongodb_storage_capacity_data : Consider balancing data and log storage Note : Monitor log usage and implement log rotation to prevent filling log volumes. PVCs can be expanded but not shrunk.","title":"mongodb_storage_capacity_logs"},{"location":"roles/mongodb/#mongodb_cpu_limits","text":"Maximum CPU cores allocated to each MongoDB container. Optional Environment Variable: MONGODB_CPU_LIMITS Default Value: 1 Purpose : Sets the upper bound on CPU usage for MongoDB containers, preventing any single MongoDB instance from consuming excessive cluster CPU resources. When to use : - Increase for production workloads with high query volumes - Increase for large datasets requiring more processing power - Use default (1 core) for development or light workloads - Set higher than mongodb_cpu_requests to allow burst capacity Valid values : CPU units as decimal (e.g., 0.5 , 1 , 2 , 4 ) or millicores (e.g., 500m , 1000m , 2000m ) Impact : - Higher limits allow better performance under load but consume more cluster resources - Limits prevent MongoDB from starving other workloads of CPU - Total CPU limit = this value \u00d7 number of replicas - Setting too low can cause performance degradation Related variables : - mongodb_cpu_requests : Should be set lower than limits for burst capacity - mongodb_replicas : Total CPU = limits \u00d7 replicas - mongodb_mem_limits : Balance CPU and memory allocation Note : Ensure cluster has sufficient CPU capacity for all replicas. Monitor actual CPU usage to right-size limits.","title":"mongodb_cpu_limits"},{"location":"roles/mongodb/#mongodb_mem_limits","text":"Maximum memory allocated to each MongoDB container. Optional Environment Variable: MONGODB_MEM_LIMITS Default Value: 1Gi Purpose : Sets the upper bound on memory usage for MongoDB containers. MongoDB uses memory for caching data and indexes, so adequate memory is critical for performance. When to use : - Increase significantly for production workloads (4Gi-8Gi recommended) - Increase for large datasets to improve cache hit rates - Default (1Gi) is only suitable for development/testing - Set higher than mongodb_mem_requests to allow burst capacity Valid values : Memory size (e.g., 1Gi , 2Gi , 4Gi , 8Gi , 16Gi ) Impact : - Higher limits improve performance through better caching - Limits prevent MongoDB from consuming all cluster memory - Total memory limit = this value \u00d7 number of replicas - Setting too low causes frequent cache evictions and poor performance Related variables : - mongodb_mem_requests : Should be set lower than limits - mongodb_replicas : Total memory = limits \u00d7 replicas - mongodb_cpu_limits : Balance CPU and memory allocation Note : MongoDB performance heavily depends on available memory. Production deployments typically need 4Gi-8Gi per replica. Monitor memory usage and adjust accordingly.","title":"mongodb_mem_limits"},{"location":"roles/mongodb/#mongodb_cpu_requests","text":"Guaranteed CPU cores reserved for each MongoDB container. Optional Environment Variable: MONGODB_CPU_REQUESTS Default Value: 500m Purpose : Defines the minimum CPU resources guaranteed to MongoDB containers. Kubernetes scheduler uses this to place pods on nodes with sufficient available CPU. When to use : - Increase for production workloads requiring consistent performance - Set to match expected baseline CPU usage - Use default (500m) for development or light workloads - Set lower than mongodb_cpu_limits to allow burst capacity Valid values : CPU units as decimal (e.g., 0.5 , 1 , 2 ) or millicores (e.g., 500m , 1000m , 2000m ) Impact : - Higher requests guarantee more CPU but may limit pod scheduling if cluster capacity is constrained - Requests ensure MongoDB has minimum CPU even under cluster load - Total CPU request = this value \u00d7 number of replicas - Affects pod Quality of Service (QoS) class Related variables : - mongodb_cpu_limits : Requests should be lower than limits - mongodb_replicas : Total CPU requests = this value \u00d7 replicas - mongodb_mem_requests : Balance CPU and memory requests Note : Set requests based on baseline usage, not peak. The difference between requests and limits provides burst capacity.","title":"mongodb_cpu_requests"},{"location":"roles/mongodb/#mongodb_mem_requests","text":"Guaranteed memory reserved for each MongoDB container. Optional Environment Variable: MONGODB_MEM_REQUESTS Default Value: 1Gi Purpose : Defines the minimum memory resources guaranteed to MongoDB containers. Kubernetes scheduler uses this to place pods on nodes with sufficient available memory. When to use : - Increase for production workloads (2Gi-4Gi recommended) - Set to match expected baseline memory usage - Default (1Gi) is only suitable for development/testing - Set lower than mongodb_mem_limits to allow burst capacity Valid values : Memory size (e.g., 1Gi , 2Gi , 4Gi , 8Gi ) Impact : - Higher requests guarantee more memory but may limit pod scheduling if cluster capacity is constrained - Requests ensure MongoDB has minimum memory even under cluster load - Total memory request = this value \u00d7 number of replicas - Affects pod Quality of Service (QoS) class Related variables : - mongodb_mem_limits : Requests should be lower than limits - mongodb_replicas : Total memory requests = this value \u00d7 replicas - mongodb_cpu_requests : Balance CPU and memory requests Note : MongoDB needs adequate memory for good performance. Production deployments typically need 2Gi-4Gi requests. Set based on baseline usage, not peak.","title":"mongodb_mem_requests"},{"location":"roles/mongodb/#mongodb_replicas","text":"Number of MongoDB replica set members to deploy. Optional Environment Variable: MONGODB_REPLICAS Default Value: 3 Purpose : Determines the size of the MongoDB replica set, which affects high availability, read scalability, and resource consumption. Replica sets provide data redundancy and automatic failover. When to use : - Use default (3) for production deployments with high availability requirements - Set to 1 only for Single Node OpenShift (SNO) clusters or development environments - Use 5 or more for critical production workloads requiring higher availability - Odd numbers (1, 3, 5) are recommended for proper election quorum Valid values : Positive integers, typically 1, 3, or 5 Impact : - More replicas = higher availability but more resource consumption - Each replica requires its own data and log PVCs - Total resources = (CPU + memory + storage) \u00d7 replicas - Affects election behavior and write acknowledgment Related variables : - mongodb_storage_capacity_data : Total data storage = capacity \u00d7 replicas - mongodb_storage_capacity_logs : Total log storage = capacity \u00d7 replicas - mongodb_cpu_limits and mongodb_mem_limits : Total resources = limits \u00d7 replicas Note : Set to 1 for SNO clusters. Production deployments should use 3 or more for high availability. Changing replica count after deployment requires careful planning.","title":"mongodb_replicas"},{"location":"roles/mongodb/#custom_labels","text":"Comma-separated list of key=value labels to apply to MongoDB resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Purpose : Adds Kubernetes labels to MongoDB resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,app=mongodb ) Impact : Labels are applied to MongoDB resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect MongoDB functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management.","title":"custom_labels"},{"location":"roles/mongodb/#mongodb_v5_upgrade","text":"Confirmation flag to upgrade MongoDB from version 4.x to version 5. Optional Environment Variable: MONGODB_V5_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades. Upgrading MongoDB major versions requires careful planning and testing. When to use : - Set to true only when intentionally upgrading from MongoDB 4.2 or 4.4 to version 5 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to a 5.x version, triggers MongoDB upgrade from 4.x to 5.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to a 5.x version for upgrade to proceed - Other upgrade flags: mongodb_v6_upgrade , mongodb_v7_upgrade , mongodb_v8_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 5.0 release notes for breaking changes and new requirements (e.g., AVX instruction set).","title":"mongodb_v5_upgrade"},{"location":"roles/mongodb/#mongodb_v6_upgrade","text":"Confirmation flag to upgrade MongoDB from version 5 to version 6. Optional Environment Variable: MONGODB_V6_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades from version 5 to 6. When to use : - Set to true only when intentionally upgrading from MongoDB 5 to version 6 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to a 6.x version, triggers MongoDB upgrade from 5.x to 6.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to a 6.x version for upgrade to proceed - Other upgrade flags: mongodb_v5_upgrade , mongodb_v7_upgrade , mongodb_v8_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 6.0 release notes for breaking changes.","title":"mongodb_v6_upgrade"},{"location":"roles/mongodb/#mongodb_v7_upgrade","text":"Confirmation flag to upgrade MongoDB from version 6 to version 7. Optional Environment Variable: MONGODB_V7_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades from version 6 to 7. When to use : - Set to true only when intentionally upgrading from MongoDB 6 to version 7 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to a 7.x version, triggers MongoDB upgrade from 6.x to 7.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to a 7.x version for upgrade to proceed - Other upgrade flags: mongodb_v5_upgrade , mongodb_v6_upgrade , mongodb_v8_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 7.0 release notes for breaking changes.","title":"mongodb_v7_upgrade"},{"location":"roles/mongodb/#mongodb_v8_upgrade","text":"Confirmation flag to upgrade MongoDB from version 7 to version 8. Optional Environment Variable: MONGODB_V8_UPGRADE Default Value: false Purpose : Acts as a safety confirmation to prevent accidental MongoDB major version upgrades from version 7 to 8. When to use : - Set to true only when intentionally upgrading from MongoDB 7 to version 8 - Must be explicitly set to perform the upgrade - Leave as false for all other operations Valid values : true , false Impact : When true and mongodb_version is set to an 8.x version, triggers MongoDB upgrade from 7.x to 8.x. This is a one-way operation that cannot be reversed without restoring from backup. Related variables : - mongodb_version : Must be set to an 8.x version for upgrade to proceed - Other upgrade flags: mongodb_v5_upgrade , mongodb_v6_upgrade , mongodb_v7_upgrade Note : Always backup before upgrading . Test upgrades in non-production environments first. Review MongoDB 8.0 release notes for breaking changes.","title":"mongodb_v8_upgrade"},{"location":"roles/mongodb/#masbr_confirm_cluster","text":"Enables cluster confirmation prompt before executing backup or restore operations. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false Purpose : Provides a safety check to confirm you're connected to the correct cluster before performing backup or restore operations, preventing accidental operations on wrong clusters. When to use : - Set to true in environments with multiple clusters to prevent mistakes - Set to true for production environments as an additional safety measure - Leave as false for automated pipelines where confirmation isn't possible Valid values : true , false Impact : When true , the role will prompt for confirmation of the cluster before proceeding with backup/restore. This adds a manual step but prevents costly mistakes. Related variables : Used with mongodb_action when set to backup or restore .","title":"masbr_confirm_cluster"},{"location":"roles/mongodb/#masbr_copy_timeout_sec","text":"Timeout in seconds for backup/restore file transfer operations. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) Purpose : Sets the maximum time allowed for copying backup files to/from storage. Prevents operations from hanging indefinitely on slow networks or large datasets. When to use : - Increase for very large databases or slow network connections - Decrease for smaller databases to fail faster on issues - Use default (12 hours) for most deployments Valid values : Positive integer representing seconds (e.g., 3600 = 1 hour, 43200 = 12 hours, 86400 = 24 hours) Impact : Operations exceeding this timeout will fail. Setting too low causes failures on legitimate long-running transfers. Setting too high delays detection of stuck operations. Related variables : Used with mongodb_action when set to backup or restore . Note : Consider database size and network speed when setting timeout. Monitor actual transfer times to optimize this value.","title":"masbr_copy_timeout_sec"},{"location":"roles/mongodb/#masbr_job_timezone","text":"Time zone for scheduled backup job execution. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None (uses UTC) Purpose : Specifies the time zone for scheduled backup CronJobs, ensuring backups run at the intended local time rather than UTC. When to use : - Set when scheduling backups to run at specific local times - Use for compliance with backup windows in specific time zones - Leave unset to use UTC (recommended for global deployments) Valid values : Any valid tz database time zone (e.g., America/New_York , Europe/London , Asia/Tokyo ) Impact : Affects when scheduled backups execute. Incorrect time zone can cause backups to run during business hours or miss backup windows. Related variables : - masbr_backup_schedule : Defines the cron schedule - Only applies when masbr_backup_schedule is set Note : When not set, CronJobs use UTC time zone. Consider daylight saving time changes when scheduling backups.","title":"masbr_job_timezone"},{"location":"roles/mongodb/#masbr_storage_local_folder","text":"Local filesystem path where backup files will be stored or retrieved from. Required when mongodb_action is backup or restore Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None Purpose : Specifies the directory for storing MongoDB backup files. This location must have sufficient space and appropriate permissions for backup operations. When to use : - Always required for backup and restore operations - Use a path with adequate storage space for full database backups - Consider using network-attached storage for backup retention - Ensure path is accessible and has proper permissions Valid values : Any valid local filesystem path (e.g., /backup/mongodb , /mnt/nfs/backups , /tmp/masbr ) Impact : Backup files are written to this location. Insufficient space causes backup failures. Path must be accessible during restore operations. Related variables : - masbr_backup_type : Determines if full or incremental backups are stored here - masbr_restore_from_version : Specifies which backup version to restore from this location Note : Ensure adequate disk space (at least 2x database size for full backups). Implement backup retention policies to manage storage usage.","title":"masbr_storage_local_folder"},{"location":"roles/mongodb/#masbr_backup_type","text":"Type of backup to create: full or incremental. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full Purpose : Determines whether to create a complete database backup or an incremental backup containing only changes since the last full backup. Incremental backups save time and storage. When to use : - Use full for initial backups or periodic complete backups - Use incr for frequent backups between full backups to save time and space - Implement a strategy like weekly full + daily incremental backups Valid values : full , incr Impact : - full : Creates complete backup, takes longer, uses more storage - incr : Creates incremental backup, faster, uses less storage, requires base full backup Related variables : - masbr_backup_from_version : Required when incr is used to specify base full backup - mongodb_action : Must be set to backup Note : Incremental backups require a full backup as base. Restore operations may need to apply multiple incremental backups sequentially.","title":"masbr_backup_type"},{"location":"roles/mongodb/#masbr_backup_from_version","text":"Timestamp of the full backup to use as base for incremental backup. Optional Environment Variable: MASBR_BACKUP_FROM_VERSION Default: None (automatically uses latest full backup) Purpose : Specifies which full backup serves as the base for an incremental backup. This links the incremental backup to a specific full backup version. When to use : - Set when creating incremental backups and you want to specify a particular full backup - Leave unset to automatically use the most recent full backup - Only valid when masbr_backup_type=incr Valid values : Timestamp in format YYYYMMDDHHMMSS (e.g., 20240621021316 ) Impact : Incremental backup will contain only changes since the specified full backup. Incorrect version can cause backup chain issues. Related variables : - masbr_backup_type : Must be set to incr - masbr_storage_local_folder : Location where full backup exists Note : If not specified, role automatically finds the latest full backup. Ensure the specified full backup exists in the storage location.","title":"masbr_backup_from_version"},{"location":"roles/mongodb/#masbr_backup_schedule","text":"Cron expression for scheduling automated backups. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None (creates on-demand backup) Purpose : Defines when automated backups should run using standard cron syntax. Enables regular, unattended backup operations. When to use : - Set to schedule regular automated backups (e.g., daily, weekly) - Leave unset for manual, on-demand backups - Consider backup windows and system load when scheduling Valid values : Standard cron expression (e.g., 0 2 * * * for daily at 2 AM, 0 2 * * 0 for weekly on Sunday at 2 AM) Impact : When set, creates a Kubernetes CronJob that automatically runs backups on schedule. Without this, backups only run when role is manually executed. Related variables : - masbr_job_timezone : Specifies time zone for schedule - masbr_backup_type : Determines if scheduled backups are full or incremental Note : Test cron expressions before deploying. Consider backup duration when scheduling to avoid overlapping backup jobs.","title":"masbr_backup_schedule"},{"location":"roles/mongodb/#masbr_restore_from_version","text":"Timestamp of the backup version to restore. Required when mongodb_action=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Purpose : Specifies which backup version to restore from. This allows point-in-time recovery to a specific backup. When to use : - Always required when performing restore operations - Use to restore to a specific point in time - Verify backup version exists before attempting restore Valid values : Timestamp in format YYYYMMDDHHMMSS (e.g., 20240621021316 ) Impact : Restores MongoDB to the state captured in the specified backup. All data after this backup will be lost. This is a destructive operation. Related variables : - mongodb_action : Must be set to restore - masbr_storage_local_folder : Location where backup exists Note : Verify backup version before restoring . List available backups in storage location first. Restore is destructive and cannot be undone without another backup.","title":"masbr_restore_from_version"},{"location":"roles/mongodb/#role-variables-ibm-cloud","text":"","title":"Role Variables - IBM Cloud"},{"location":"roles/mongodb/#ibm_mongo_name","text":"Name for the IBM Cloud Databases for MongoDB instance. Required when mongodb_provider=ibm Environment Variable: IBM_MONGO_NAME Default Value: mongo-${MAS_INSTANCE_ID} Purpose : Identifies the MongoDB database instance in IBM Cloud. This name is used for resource identification, billing, and management within IBM Cloud. When to use : - Always required when using IBM Cloud as the MongoDB provider - Use default naming convention for consistency across MAS instances - Customize only if organizational naming standards require it Valid values : Valid IBM Cloud resource name (alphanumeric, hyphens allowed, must start with letter) Impact : This name appears in IBM Cloud console, billing reports, and resource lists. Changing it after creation requires recreating the database instance. Related variables : - mas_instance_id : Default name includes this value - mongodb_provider : Must be set to ibm Note : Choose a descriptive name that identifies the MAS instance and environment. The name cannot be changed after creation.","title":"ibm_mongo_name"},{"location":"roles/mongodb/#ibm_mongo_admin_password","text":"Administrator password for the IBM Cloud MongoDB instance. Optional Environment Variable: IBM_MONGO_ADMIN_PASSWORD Default Value: Auto-generated 20-character string Purpose : Sets the password for the MongoDB administrator user. If not provided, a secure random password is automatically generated. When to use : - Set explicitly if you need to know the password in advance - Leave unset to use auto-generated secure password (recommended) - Set if integrating with external password management systems Valid values : String meeting IBM Cloud password requirements (minimum length, complexity) Impact : This password is used for administrative access to the MongoDB instance. Auto-generated passwords are stored in Kubernetes secrets. Related variables : - ibm_mongo_admin_credentials_secret_name : Secret where credentials are stored Note : Auto-generated passwords are more secure. If setting manually, ensure password meets security requirements and is stored securely.","title":"ibm_mongo_admin_password"},{"location":"roles/mongodb/#ibm_mongo_admin_credentials_secret_name","text":"Name of the Kubernetes secret containing MongoDB admin credentials. Optional Environment Variable: IBM_MONGO_ADMIN_CREDENTIALS_SECRET_NAME Default Value: <mongo-name>-admin-credentials Purpose : Specifies the Kubernetes secret name where MongoDB administrator credentials are stored. This secret is created automatically by the role. When to use : - Customize if organizational standards require specific secret naming - Use default for standard deployments - Reference this secret name in other automation Valid values : Valid Kubernetes secret name Impact : The secret contains admin username and password for MongoDB access. Other roles and applications reference this secret for database connectivity. Related variables : - ibm_mongo_name : Default secret name includes this value - ibm_mongo_admin_password : Password stored in this secret","title":"ibm_mongo_admin_credentials_secret_name"},{"location":"roles/mongodb/#ibm_mongo_service_credentials_secret_name","text":"Name of the Kubernetes secret containing MongoDB service credentials. Optional Environment Variable: IBM_MONGO_SERVICE_CREDENTIALS_SECRET_NAME Default Value: <mongo-name>-service-credentials Purpose : Specifies the Kubernetes secret name where MongoDB service-level credentials are stored. These credentials are used by MAS applications to connect to MongoDB. When to use : - Customize if organizational standards require specific secret naming - Use default for standard deployments - Reference this secret name when configuring MAS applications Valid values : Valid Kubernetes secret name Impact : The secret contains connection strings and credentials for application-level MongoDB access. MAS applications use this secret to connect to the database. Related variables : - ibm_mongo_name : Default secret name includes this value","title":"ibm_mongo_service_credentials_secret_name"},{"location":"roles/mongodb/#ibm_mongo_resourcegroup","text":"IBM Cloud resource group for MongoDB instance placement. Required when mongodb_provider=ibm Environment Variable: IBM_MONGO_RESOURCEGROUP Default Value: Default Purpose : Specifies which IBM Cloud resource group will contain the MongoDB instance. Resource groups organize IBM Cloud resources for access control and billing. When to use : - Always required when using IBM Cloud provider - Use Default for simple deployments - Specify custom resource group for organizational resource management - Align with IBM Cloud IAM and billing structure Valid values : Name of an existing IBM Cloud resource group in your account Impact : Determines access control, billing allocation, and resource organization. Users need appropriate IAM permissions for the specified resource group. Related variables : - ibmcloud_apikey : API key must have access to the specified resource group Note : Ensure the resource group exists and your API key has permissions to create resources in it.","title":"ibm_mongo_resourcegroup"},{"location":"roles/mongodb/#ibm_mongo_region","text":"IBM Cloud region where MongoDB instance will be deployed. Required when mongodb_provider=ibm Environment Variable: IBM_MONGO_REGION Default Value: us-east Purpose : Specifies the geographic region for MongoDB deployment. Region selection affects latency, data residency, and availability. When to use : - Always required when using IBM Cloud provider - Choose region closest to your OpenShift cluster for lowest latency - Consider data residency requirements for compliance - Use default ( us-east ) if no specific requirements Valid values : Valid IBM Cloud region (e.g., us-east , us-south , eu-gb , eu-de , jp-tok , au-syd ) Impact : Affects network latency between OpenShift and MongoDB, data residency compliance, and regional pricing. Cannot be changed after creation. Related variables : - ibmcloud_apikey : API key must have access to the specified region Note : Choose region carefully as it cannot be changed. Consider deploying MongoDB in the same region as your OpenShift cluster for best performance.","title":"ibm_mongo_region"},{"location":"roles/mongodb/#ibmcloud_apikey","text":"IBM Cloud API key for authentication and resource management. Required when mongodb_provider=ibm Environment Variable: IBMCLOUD_APIKEY Default Value: None Purpose : Provides authentication credentials for creating and managing IBM Cloud resources. The API key must have sufficient permissions to create and manage Databases for MongoDB instances. When to use : - Always required when using IBM Cloud provider - Must have permissions for the target resource group and region - Should be stored securely (e.g., in Ansible Vault or external secret management) Valid values : Valid IBM Cloud API key string Impact : This key is used to authenticate all IBM Cloud API calls. Insufficient permissions will cause deployment failures. Related variables : - ibm_mongo_resourcegroup : API key must have access to this resource group - ibm_mongo_region : API key must have access to this region Note : Never commit API keys to source control . Use secure secret management. Ensure the API key has appropriate IAM permissions for Databases for MongoDB service.","title":"ibmcloud_apikey"},{"location":"roles/mongodb/#ibm_mongo_plan","text":"IBM Cloud service plan for MongoDB instance. Optional Environment Variable: IBM_MONGO_PLAN Default Value: standard Purpose : Specifies the IBM Cloud service plan tier for MongoDB. Different plans offer different performance, availability, and pricing characteristics. When to use : - Use standard (default) for most production deployments - Consider enterprise plan for critical workloads requiring higher SLA - Review IBM Cloud pricing and plan features before selecting Valid values : standard , enterprise (check IBM Cloud documentation for current plan options) Impact : Affects pricing, performance characteristics, SLA, and available features. Plan cannot be changed after creation without recreating the instance. Related variables : - ibm_mongo_memory , ibm_mongo_disk , ibm_mongo_cpu : Resource allocations vary by plan Note : Review IBM Cloud Databases for MongoDB plan documentation for current offerings and pricing.","title":"ibm_mongo_plan"},{"location":"roles/mongodb/#ibm_mongo_service","text":"IBM Cloud service type identifier for MongoDB. Read-only Value: databases-for-mongodb Purpose : Identifies the IBM Cloud service type. This is a fixed value used internally by the role. When to use : This is set automatically by the role and should not be modified. Valid values : databases-for-mongodb Impact : Used in IBM Cloud API calls to specify the service type. Note : This is a constant value and does not need to be set by users.","title":"ibm_mongo_service"},{"location":"roles/mongodb/#ibm_mongo_service_endpoints","text":"Network endpoint type for MongoDB connectivity. Optional Environment Variable: IBM_MONGO_SERVICE_ENDPOINTS Default Value: public Purpose : Determines whether MongoDB is accessible via public internet or private network only. Private endpoints provide better security and performance for cluster-to-database communication. When to use : - Use public for simple deployments or when OpenShift cluster lacks private network connectivity - Use private for production deployments with private network connectivity (recommended) - Private endpoints require IBM Cloud private network configuration Valid values : public , private Impact : - public : MongoDB accessible over internet (requires firewall rules) - private : MongoDB accessible only via IBM Cloud private network (more secure, lower latency) Related variables : Network configuration must support the chosen endpoint type Note : Private endpoints are recommended for production. Ensure your OpenShift cluster can reach IBM Cloud private network if using private .","title":"ibm_mongo_service_endpoints"},{"location":"roles/mongodb/#ibm_mongo_version","text":"MongoDB version to deploy in IBM Cloud. Optional Environment Variable: IBM_MONGO_VERSION Default Value: 4.2 Purpose : Specifies which MongoDB version to deploy. Version selection affects available features, performance, and compatibility. When to use : - Use default ( 4.2 ) for compatibility with older MAS versions - Specify newer version (e.g., 5.0 , 6.0 ) for new deployments - Check MAS compatibility matrix before selecting version Valid values : MongoDB versions supported by IBM Cloud Databases (e.g., 4.2 , 4.4 , 5.0 , 6.0 ) Impact : Affects available MongoDB features, performance characteristics, and MAS compatibility. Version cannot be easily downgraded. Related variables : - Check MAS compatibility requirements before selecting version Note : Verify version compatibility with your MAS version. Newer MongoDB versions may require MAS updates.","title":"ibm_mongo_version"},{"location":"roles/mongodb/#ibm_mongo_memory","text":"Memory allocation per MongoDB member in MB. Optional Environment Variable: IBM_MONGO_MEMORY Default Value: 3840 (3.75 GB) Purpose : Specifies memory allocation for each MongoDB replica set member. Memory affects caching performance and query execution. When to use : - Use default (3840 MB) for development or small deployments - Increase for production workloads (8192 MB or higher recommended) - Increase for large datasets to improve cache hit rates Valid values : Integer in MB, minimum varies by plan (typically 1024 MB minimum) Impact : Higher memory improves performance through better caching but increases costs. Total cost = memory \u00d7 number of members. Related variables : - ibm_mongo_plan : Available memory ranges vary by plan - ibm_mongo_disk : Balance memory and disk allocation Note : IBM Cloud charges based on allocated memory. Production deployments typically need 8GB+ per member.","title":"ibm_mongo_memory"},{"location":"roles/mongodb/#ibm_mongo_disk","text":"Disk storage allocation per MongoDB member in MB. Optional Environment Variable: IBM_MONGO_DISK Default Value: 30720 (30 GB) Purpose : Specifies disk storage for each MongoDB replica set member. Storage holds databases, indexes, and operational logs. When to use : - Use default (30 GB) for development or small datasets - Increase for production workloads based on data volume - Plan for data growth and backup storage needs Valid values : Integer in MB, minimum varies by plan (typically 5120 MB minimum) Impact : Affects storage capacity and costs. Disk can be expanded but not shrunk. Total storage = disk \u00d7 number of members. Related variables : - ibm_mongo_plan : Available disk ranges vary by plan - ibm_mongo_memory : Balance memory and disk allocation Note : Plan for data growth. Disk can be expanded online but cannot be reduced. Monitor storage usage to avoid running out of space.","title":"ibm_mongo_disk"},{"location":"roles/mongodb/#ibm_mongo_cpu","text":"Dedicated CPU cores per MongoDB member. Optional Environment Variable: IBM_MONGO_CPU Default Value: 0 (shared CPU) Purpose : Specifies dedicated CPU cores for each MongoDB member. Dedicated CPUs provide consistent performance but increase costs. When to use : - Use 0 (default) for shared CPU, suitable for development and light workloads - Set to 3 or higher for production workloads requiring consistent performance - Dedicated CPUs recommended for production environments Valid values : 0 (shared), or integer \u2265 3 for dedicated CPUs Impact : - 0 : Shared CPU, lower cost, variable performance - \u22653 : Dedicated CPUs, higher cost, consistent performance Related variables : - ibm_mongo_plan : CPU options vary by plan - ibm_mongo_memory : Balance CPU and memory allocation Note : Dedicated CPUs significantly increase costs but provide predictable performance. Production workloads typically need dedicated CPUs.","title":"ibm_mongo_cpu"},{"location":"roles/mongodb/#ibm_mongo_backup_id","text":"IBM Cloud backup CRN (Cloud Resource Name) for restore operations. Required when is_restore=true Environment Variable: IBM_MONGO_BACKUP_ID Default Value: None Purpose : Specifies the IBM Cloud backup resource to restore from. The CRN uniquely identifies a specific backup in IBM Cloud. When to use : - Required only when restoring from an IBM Cloud backup - Obtain CRN from IBM Cloud console or CLI - Leave unset for new deployments Valid values : Valid IBM Cloud CRN for a MongoDB backup (format: crn:v1:... ) Impact : Restores MongoDB to the state captured in the specified backup. All current data will be replaced. Related variables : - is_restore : Must be set to true - restored_mongodb_service_name : Name for the restored service Note : Verify backup CRN before restoring . Restore is destructive and replaces all current data. Test restore procedures in non-production first.","title":"ibm_mongo_backup_id"},{"location":"roles/mongodb/#is_restore","text":"Flag to enable restore from IBM Cloud backup. Optional Environment Variable: IS_RESTORE Default Value: false Purpose : Controls whether to create a new MongoDB instance or restore from an existing backup. Acts as a safety flag to prevent accidental restores. When to use : - Set to true only when intentionally restoring from backup - Leave as false (default) for new deployments - Must be explicitly set to perform restore Valid values : true , false Impact : When true , creates MongoDB instance from backup instead of fresh deployment. Requires ibm_mongo_backup_id and restored_mongodb_service_name . Related variables : - ibm_mongo_backup_id : Required when true - restored_mongodb_service_name : Required when true Note : Always verify backup details before setting to true . Restore operations cannot be undone without another backup.","title":"is_restore"},{"location":"roles/mongodb/#restored_mongodb_service_name","text":"Name for the MongoDB service when restoring from backup. Required when is_restore=true Environment Variable: RESTORED_MONGODB_SERVICE_NAME Default Value: None Purpose : Specifies the name for the new MongoDB service created from backup. This allows restoring to a different service name than the original. When to use : - Required only when is_restore=true - Can be same as or different from original service name - Use different name to restore alongside existing instance for testing Valid values : Valid IBM Cloud resource name Impact : The restored MongoDB instance will have this name in IBM Cloud. Choose carefully as it affects resource identification and billing. Related variables : - is_restore : Must be set to true - ibm_mongo_backup_id : Backup to restore from - ibm_mongo_name : Original service name (can be different) Note : Using a different name allows side-by-side comparison of restored and current instances before switching over.","title":"restored_mongodb_service_name"},{"location":"roles/mongodb/#role-variables-aws-documentdb","text":"","title":"Role Variables - AWS DocumentDB"},{"location":"roles/mongodb/#aws_access_key_id","text":"AWS account access key ID for authentication. Required when mongodb_provider=aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None Purpose : Provides AWS authentication credentials for creating and managing DocumentDB resources. The access key must have sufficient IAM permissions for DocumentDB, VPC, and related services. When to use : - Always required when using AWS DocumentDB provider - Must have IAM permissions for DocumentDB, EC2 (VPC/subnets/security groups) - Should be stored securely (e.g., in Ansible Vault or external secret management) Valid values : Valid AWS access key ID string Impact : This key is used to authenticate all AWS API calls. Insufficient permissions will cause deployment failures. Related variables : - aws_secret_access_key : Must be provided together - aws_region : Access key must have permissions in the target region Note : Never commit AWS credentials to source control . Use secure secret management. Ensure the IAM user/role has appropriate permissions for DocumentDB and VPC operations.","title":"aws_access_key_id"},{"location":"roles/mongodb/#aws_secret_access_key","text":"AWS account secret access key for authentication. Required when mongodb_provider=aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None Purpose : Provides the secret component of AWS authentication credentials. Works together with aws_access_key_id to authenticate AWS API requests. When to use : - Always required when using AWS DocumentDB provider - Must correspond to the provided aws_access_key_id - Should be stored securely Valid values : Valid AWS secret access key string Impact : Used with access key ID to authenticate AWS API calls. Invalid or mismatched credentials will cause authentication failures. Related variables : - aws_access_key_id : Must be provided together Note : Store securely and never commit to source control . Rotate credentials regularly following AWS security best practices.","title":"aws_secret_access_key"},{"location":"roles/mongodb/#aws_region","text":"AWS region where DocumentDB cluster will be deployed. Required when mongodb_provider=aws Environment Variable: AWS_REGION Default Value: us-east-2 Purpose : Specifies the geographic AWS region for DocumentDB deployment. Region selection affects latency, data residency, availability zones, and pricing. When to use : - Always required when using AWS DocumentDB provider - Choose region closest to your OpenShift cluster for lowest latency - Consider data residency requirements for compliance - Use default ( us-east-2 ) if no specific requirements Valid values : Valid AWS region code (e.g., us-east-1 , us-east-2 , us-west-2 , eu-west-1 , ap-southeast-1 ) Impact : Affects network latency, data residency compliance, available availability zones, and regional pricing. Cannot be changed after creation. Related variables : - vpc_id : VPC must exist in the specified region - aws_access_key_id : Credentials must have permissions in this region Note : Choose region carefully as it cannot be changed. Deploy DocumentDB in the same region as your OpenShift cluster for best performance.","title":"aws_region"},{"location":"roles/mongodb/#vpc_id","text":"AWS VPC ID where DocumentDB resources will be created. Required when mongodb_provider=aws Environment Variable: VPC_ID Default Value: None Purpose : Specifies the AWS Virtual Private Cloud where DocumentDB cluster, subnets, and security groups will be created. The VPC provides network isolation and connectivity. When to use : - Always required when using AWS DocumentDB provider - Use the same VPC as your OpenShift cluster for direct connectivity - VPC must exist in the specified aws_region Valid values : Valid AWS VPC ID (format: vpc-xxxxxxxxxxxxxxxxx ) Impact : Determines network connectivity and security boundaries. DocumentDB will only be accessible from resources within this VPC or connected networks. Related variables : - aws_region : VPC must exist in this region - docdb_cidr_az1 , docdb_cidr_az2 , docdb_cidr_az3 : Subnets created within this VPC - docdb_ingress_cidr , docdb_egress_cidr : Should match VPC CIDR ranges Note : Ensure the VPC has sufficient available IP addresses and appropriate routing for DocumentDB connectivity.","title":"vpc_id"},{"location":"roles/mongodb/#docdb_cluster_name","text":"Name for the AWS DocumentDB cluster. Required when mongodb_provider=aws Environment Variable: DOCDB_CLUSTER_NAME Default Value: None Purpose : Identifies the DocumentDB cluster in AWS. This name is used for resource identification, tagging, and as a prefix for related resources. When to use : - Always required when using AWS DocumentDB provider - Choose a descriptive name that identifies the MAS instance and environment - Name is used as prefix for subnet groups and security groups Valid values : Valid AWS DocumentDB cluster identifier (lowercase, alphanumeric, hyphens, must start with letter) Impact : This name appears in AWS console, CloudWatch metrics, and billing. Related resources (subnet group, security group) are named based on this value. Related variables : - docdb_subnet_group_name : Defaults to docdb-{cluster_name} - docdb_security_group_name : Defaults to docdb-{cluster_name} - docdb_admin_credentials_secret_name : Defaults to {cluster_name}-admin-credentials Note : Choose a meaningful name as it cannot be easily changed. The name must be unique within your AWS account and region.","title":"docdb_cluster_name"},{"location":"roles/mongodb/#docdb_subnet_group_name","text":"Name for the DocumentDB subnet group. Optional Default Value: docdb-{{ docdb_cluster_name }} Purpose : Specifies the name for the DocumentDB subnet group that defines which subnets the cluster can use. The role creates this subnet group automatically. When to use : - Use default naming for standard deployments - Customize only if organizational naming standards require it Valid values : Valid AWS subnet group name Impact : The subnet group associates the DocumentDB cluster with specific subnets across availability zones. Related variables : - docdb_cluster_name : Default name includes this value - docdb_cidr_az1 , docdb_cidr_az2 , docdb_cidr_az3 : Subnets included in this group Note : This is automatically created by the role. Default naming is recommended for consistency.","title":"docdb_subnet_group_name"},{"location":"roles/mongodb/#docdb_security_group_name","text":"Name for the DocumentDB security group. Optional Default Value: docdb-{{ docdb_cluster_name }} Purpose : Specifies the name for the security group that controls network access to the DocumentDB cluster. The role creates this security group automatically. When to use : - Use default naming for standard deployments - Customize only if organizational naming standards require it Valid values : Valid AWS security group name Impact : The security group defines firewall rules for DocumentDB access based on docdb_ingress_cidr and docdb_egress_cidr . Related variables : - docdb_cluster_name : Default name includes this value - docdb_ingress_cidr : Allowed source CIDR for inbound traffic - docdb_egress_cidr : Allowed destination CIDR for outbound traffic Note : This is automatically created by the role. Default naming is recommended for consistency.","title":"docdb_security_group_name"},{"location":"roles/mongodb/#docdb_admin_credentials_secret_name","text":"Name of the Kubernetes secret containing DocumentDB admin credentials. Optional Default Value: {{ docdb_cluster_name }}-admin-credentials Purpose : Specifies the Kubernetes secret name where DocumentDB administrator credentials are stored. This secret is created automatically by the role. When to use : - Use default naming for standard deployments - Customize only if organizational naming standards require it Valid values : Valid Kubernetes secret name Impact : The secret contains admin username and password for DocumentDB access. MAS and other applications reference this secret for database connectivity. Related variables : - docdb_cluster_name : Default name includes this value - docdb_master_username : Username stored in this secret Note : This secret is created in the MAS core namespace. Default naming is recommended for consistency.","title":"docdb_admin_credentials_secret_name"},{"location":"roles/mongodb/#docdb_engine_version","text":"DocumentDB engine version to deploy. Optional Environment Variable: DOCDB_ENGINE_VERSION Default Value: 5.0.0 Purpose : Specifies the DocumentDB engine version. MAS requires DocumentDB 5.0.0 for MongoDB compatibility. When to use : - Use default ( 5.0.0 ) for MAS deployments (required) - Do not change unless specifically required by MAS version Valid values : 5.0.0 (only version supported by MAS) Impact : Determines MongoDB compatibility and available features. MAS is only certified with DocumentDB 5.0.0. Related variables : None Note : MAS only supports DocumentDB 5.0.0 . Do not change this value unless MAS documentation explicitly supports other versions.","title":"docdb_engine_version"},{"location":"roles/mongodb/#docdb_master_username","text":"Master username for DocumentDB cluster administration. Optional Environment Variable: DOCDB_MASTER_USERNAME Default Value: docdbadmin Purpose : Specifies the master administrator username for the DocumentDB cluster. This user has full administrative privileges. When to use : - Use default ( docdbadmin ) for standard deployments - Customize if organizational security policies require specific usernames Valid values : Valid DocumentDB username (alphanumeric, must start with letter, 1-63 characters) Impact : This username is used for administrative access and is stored in the Kubernetes secret specified by docdb_admin_credentials_secret_name . Related variables : - docdb_admin_credentials_secret_name : Secret where credentials are stored Note : Choose carefully as the master username cannot be changed after cluster creation.","title":"docdb_master_username"},{"location":"roles/mongodb/#docdb_instance_class","text":"AWS instance class for DocumentDB instances. Optional Environment Variable: DOCDB_INSTANCE_CLASS Default Value: db.t3.medium Purpose : Specifies the compute and memory capacity for each DocumentDB instance. Instance class affects performance, availability, and cost. When to use : - Use db.t3.medium (default) for development or small deployments - Use db.r5.large or larger for production workloads - Consider db.r6g instances for better price/performance (ARM-based) Valid values : Valid DocumentDB instance class (e.g., db.t3.medium , db.r5.large , db.r5.xlarge , db.r6g.large ) Impact : Affects CPU, memory, network performance, and cost. Larger instances provide better performance but cost more. Related variables : - docdb_instance_number : Total cost = instance class cost \u00d7 number of instances Note : Production deployments typically need db.r5.large or larger. Review AWS DocumentDB pricing and instance specifications.","title":"docdb_instance_class"},{"location":"roles/mongodb/#docdb_instance_number","text":"Number of DocumentDB instances in the cluster. Optional Environment Variable: DOCDB_INSTANCE_NUMBER Default Value: 3 Purpose : Determines the number of instances in the DocumentDB cluster. More instances provide higher availability and read scalability. When to use : - Use default ( 3 ) for production deployments with high availability - Use 1 only for development or testing (no high availability) - Use 5 or more for critical workloads requiring higher availability Valid values : Integer from 1 to 16 Impact : - More instances = higher availability and read capacity but higher cost - Instances are distributed across availability zones for fault tolerance - Total cost = instance class cost \u00d7 number of instances Related variables : - docdb_instance_class : Determines per-instance cost and performance - docdb_cidr_az1 , docdb_cidr_az2 , docdb_cidr_az3 : Instances distributed across these AZs Note : Production deployments should use 3 or more instances for high availability. Single instance has no failover capability.","title":"docdb_instance_number"},{"location":"roles/mongodb/#docdb_instance_identifier_prefix","text":"Prefix for DocumentDB instance identifiers. Required when mongodb_provider=aws Environment Variable: DOCDB_INSTANCE_IDENTIFIER_PREFIX Default Value: None Purpose : Specifies the prefix used to name individual DocumentDB instances. Instance names are formed as {prefix}-{number} . When to use : - Always required when using AWS DocumentDB provider - Use a descriptive prefix that identifies the cluster and environment - Typically matches or relates to docdb_cluster_name Valid values : Valid AWS instance identifier prefix (lowercase, alphanumeric, hyphens) Impact : Instance names appear in AWS console and CloudWatch metrics. Choose a meaningful prefix for easy identification. Related variables : - docdb_cluster_name : Typically related to cluster name Note : Instance identifiers are formed as {prefix}-1 , {prefix}-2 , etc. Choose a clear, descriptive prefix.","title":"docdb_instance_identifier_prefix"},{"location":"roles/mongodb/#docdb_ingress_cidr","text":"CIDR block allowed to connect to DocumentDB cluster. Required when mongodb_provider=aws Environment Variable: DOCDB_INGRESS_CIDR Default Value: None Purpose : Specifies the IPv4 CIDR range from which incoming connections to DocumentDB are allowed. This is used in security group ingress rules. When to use : - Always required when using AWS DocumentDB provider - Typically set to the CIDR of your OpenShift cluster's VPC - Can be set to specific subnet CIDRs for tighter security Valid values : Valid IPv4 CIDR notation (e.g., 10.0.0.0/16 , 172.31.0.0/16 ) Impact : Only traffic from this CIDR range can connect to DocumentDB. Too restrictive blocks legitimate traffic; too permissive reduces security. Related variables : - vpc_id : Should match VPC CIDR or subnet CIDRs within the VPC - docdb_egress_cidr : Typically set to same value Note : Set to your OpenShift cluster's VPC CIDR for proper connectivity. Verify CIDR ranges before deployment.","title":"docdb_ingress_cidr"},{"location":"roles/mongodb/#docdb_egress_cidr","text":"CIDR block for outbound connections from DocumentDB cluster. Required when mongodb_provider=aws Environment Variable: DOCDB_EGRESS_CIDR Default Value: None Purpose : Specifies the IPv4 CIDR range to which DocumentDB can send outbound connections. This is used in security group egress rules. When to use : - Always required when using AWS DocumentDB provider - Typically set to the same value as docdb_ingress_cidr - Set to VPC CIDR for standard deployments Valid values : Valid IPv4 CIDR notation (e.g., 10.0.0.0/16 , 172.31.0.0/16 ) Impact : DocumentDB can only send traffic to this CIDR range. Affects ability to respond to client connections. Related variables : - docdb_ingress_cidr : Typically set to same value - vpc_id : Should match VPC CIDR Note : Usually set to the same value as docdb_ingress_cidr . Verify CIDR ranges match your network configuration.","title":"docdb_egress_cidr"},{"location":"roles/mongodb/#docdb_cidr_az1","text":"CIDR block for DocumentDB subnet in availability zone 1. Required when mongodb_provider=aws Environment Variable: DOCDB_CIDR_AZ1 Default Value: None Purpose : Specifies the IPv4 CIDR for the subnet in the first availability zone. If the subnet exists with tag Name: {{ docdb_cluster_name }} , it's used; otherwise, a new subnet is created. When to use : - Always required when using AWS DocumentDB provider - Must be within the VPC CIDR range - Must not overlap with other subnets in the VPC Valid values : Valid IPv4 CIDR notation within VPC range (e.g., 10.0.1.0/24 ) Impact : Defines the IP address range for DocumentDB instances in AZ1. Subnet size affects number of available IP addresses. Related variables : - vpc_id : CIDR must be within this VPC's range - docdb_cidr_az2 , docdb_cidr_az3 : Must not overlap with these subnets Note : Plan subnet sizes carefully. Each DocumentDB instance needs an IP address. Use /24 or larger subnets for flexibility.","title":"docdb_cidr_az1"},{"location":"roles/mongodb/#docdb_cidr_az2","text":"CIDR block for DocumentDB subnet in availability zone 2. Required when mongodb_provider=aws Environment Variable: DOCDB_CIDR_AZ2 Default Value: None Purpose : Specifies the IPv4 CIDR for the subnet in the second availability zone. If the subnet exists with tag Name: {{ docdb_cluster_name }} , it's used; otherwise, a new subnet is created. When to use : - Always required when using AWS DocumentDB provider - Must be within the VPC CIDR range - Must not overlap with other subnets in the VPC Valid values : Valid IPv4 CIDR notation within VPC range (e.g., 10.0.2.0/24 ) Impact : Defines the IP address range for DocumentDB instances in AZ2. Required for multi-AZ high availability. Related variables : - vpc_id : CIDR must be within this VPC's range - docdb_cidr_az1 , docdb_cidr_az3 : Must not overlap with these subnets Note : Use different availability zones for AZ1, AZ2, and AZ3 to ensure high availability across zones.","title":"docdb_cidr_az2"},{"location":"roles/mongodb/#docdb_cidr_az3","text":"CIDR block for DocumentDB subnet in availability zone 3. Required when mongodb_provider=aws Environment Variable: DOCDB_CIDR_AZ3 Default Value: None Purpose : Specifies the IPv4 CIDR for the subnet in the third availability zone. If the subnet exists with tag Name: {{ docdb_cluster_name }} , it's used; otherwise, a new subnet is created. When to use : - Always required when using AWS DocumentDB provider - Must be within the VPC CIDR range - Must not overlap with other subnets in the VPC Valid values : Valid IPv4 CIDR notation within VPC range (e.g., 10.0.3.0/24 ) Impact : Defines the IP address range for DocumentDB instances in AZ3. Provides third availability zone for maximum fault tolerance. Related variables : - vpc_id : CIDR must be within this VPC's range - docdb_cidr_az1 , docdb_cidr_az2 : Must not overlap with these subnets Note : Three availability zones provide best fault tolerance. Ensure subnets are in different AZs for proper distribution.","title":"docdb_cidr_az3"},{"location":"roles/mongodb/#aws-documentdb-secret-rotation-variables","text":"The following variables are used for rotating DocumentDB credentials. These are typically used with mongodb_action=rotate-secret .","title":"AWS DocumentDB Secret Rotation Variables"},{"location":"roles/mongodb/#docdb_mongo_instance_name","text":"DocumentDB instance name for secret rotation. Required when rotating secrets Environment Variable: DOCDB_MONGO_INSTANCE_NAME Default Value: None Purpose : Identifies the specific DocumentDB instance for credential rotation operations. When to use : - Required when performing secret rotation ( mongodb_action=rotate-secret ) - Must match an existing DocumentDB instance name Valid values : Valid DocumentDB instance identifier Impact : Specifies which DocumentDB instance's credentials will be rotated. Related variables : - docdb_cluster_name : Instance belongs to this cluster - docdb_host : Host address of this instance Note : Verify instance name before rotation to avoid affecting wrong instance.","title":"docdb_mongo_instance_name"},{"location":"roles/mongodb/#docdb_host","text":"DocumentDB instance host address for connection. Required when rotating secrets Environment Variable: DOCDB_HOST Default Value: None Purpose : Specifies the host address of a DocumentDB instance for establishing connection during secret rotation. When to use : - Required when performing secret rotation - Use any one host address from the DocumentDB cluster - Obtain from AWS console or DocumentDB cluster endpoint Valid values : Valid DocumentDB instance hostname or endpoint Impact : Used to connect to DocumentDB for credential rotation operations. Related variables : - docdb_port : Port for this host - docdb_mongo_instance_name : Instance identifier Note : Any instance host from the cluster can be used for rotation operations.","title":"docdb_host"},{"location":"roles/mongodb/#docdb_port","text":"DocumentDB instance port number. Required when rotating secrets Environment Variable: DOCDB_PORT Default Value: None (typically 27017 ) Purpose : Specifies the port number for connecting to the DocumentDB instance during secret rotation. When to use : - Required when performing secret rotation - Typically 27017 (default DocumentDB port) Valid values : Valid port number (typically 27017 ) Impact : Used with docdb_host to establish connection for credential rotation. Related variables : - docdb_host : Host address for this port Note : DocumentDB uses port 27017 by default unless customized during cluster creation.","title":"docdb_port"},{"location":"roles/mongodb/#docdb_instance_username","text":"Username for which password is being rotated. Required when rotating secrets Environment Variable: DOCDB_INSTANCE_USERNAME Default Value: None Purpose : Specifies the DocumentDB username whose password will be changed during rotation. When to use : - Required when performing secret rotation - Typically the application user or admin user - Must be an existing DocumentDB user Valid values : Valid DocumentDB username Impact : This user's password will be changed. Applications using this username must be updated with the new password. Related variables : - docdb_instance_password_old : Current password for this user - docdb_master_username : Master user for performing rotation Note : Ensure applications can handle password rotation. Consider using connection pooling with reconnection logic.","title":"docdb_instance_username"},{"location":"roles/mongodb/#docdb_instance_password_old","text":"Current password for the user being rotated. Required when rotating secrets Environment Variable: DOCDB_PASSWORD_OLD Default Value: None Purpose : Provides the current password for authentication before rotation. Used to verify current credentials. When to use : - Required when performing secret rotation - Must be the current valid password Valid values : Current password string Impact : Used to authenticate before changing password. Incorrect password will cause rotation to fail. Related variables : - docdb_instance_username : User for this password Note : Store securely. After rotation, this password will no longer be valid.","title":"docdb_instance_password_old"},{"location":"roles/mongodb/#docdb_master_password","text":"DocumentDB master user password for administrative operations. Required when rotating secrets Environment Variable: DOCDB_MASTER_PASSWORD Default Value: None Purpose : Provides master user credentials for performing password rotation operations. Master user has privileges to change other users' passwords. When to use : - Required when performing secret rotation - Must be the current master password Valid values : Valid master password string Impact : Used to authenticate as master user to perform credential rotation. Related variables : - docdb_master_username : Master username for this password Note : Store master credentials securely . Never commit to source control.","title":"docdb_master_password"},{"location":"roles/mongodb/#docdb_master_username_1","text":"DocumentDB master username for administrative operations. Required when rotating secrets Environment Variable: DOCDB_MASTER_USERNAME Default Value: None Purpose : Specifies the master username for performing password rotation operations. Master user has administrative privileges. When to use : - Required when performing secret rotation - Typically docdbadmin or the value set during cluster creation Valid values : Valid DocumentDB master username Impact : Used with docdb_master_password to authenticate for credential rotation. Related variables : - docdb_master_password : Password for this master user Note : This should match the master username set during DocumentDB cluster creation.","title":"docdb_master_username"},{"location":"roles/mongodb/#aws-documentdb-destroy-data-action-variables","text":"","title":"AWS DocumentDB destroy-data action Variables"},{"location":"roles/mongodb/#mas_instance_id_1","text":"The specified MAS instance ID Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/mongodb/#mongo_username","text":"Mongo Username Environment Variable: MONGO_USERNAME Default Value: None","title":"mongo_username"},{"location":"roles/mongodb/#mongo_password","text":"Mongo password Environment Variable: MONGO_PASSWORD Default Value: None","title":"mongo_password"},{"location":"roles/mongodb/#config","text":"Mongo Config, please refer to the below example playbook section for details Required Environment Variable: CONFIG Default Value: None","title":"config"},{"location":"roles/mongodb/#certificates","text":"Mongo Certificates, please refer to the below example playbook section for details Required Environment Variable: CERTIFICATES Default Value: None","title":"certificates"},{"location":"roles/mongodb/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/mongodb/#install-ce-operator","text":"- hosts: localhost any_errors_fatal: true vars: mongodb_storage_class: ibmc-block-gold mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.mongodb","title":"Install (CE Operator)"},{"location":"roles/mongodb/#backup-ce-operator","text":"- hosts: localhost any_errors_fatal: true vars: mongodb_action: backup mas_instance_id: masinst1 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.mongodb","title":"Backup (CE Operator)"},{"location":"roles/mongodb/#restore-ce-operator","text":"- hosts: localhost any_errors_fatal: true vars: mongodb_action: restore mas_instance_id: masinst1 masbr_restore_from_version: 20240621021316 masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.mongodb","title":"Restore (CE Operator)"},{"location":"roles/mongodb/#install-ibm-cloud","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: ibm ibmcloud_apikey: apikey**** ibmcloud_resource_group: mas-test roles: - ibm.mas_devops.mongodb","title":"Install (IBM Cloud)"},{"location":"roles/mongodb/#install-aws-documentdb","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: aws mongodb_action: provision docdb_size: ~/docdb-config.yml docdb_cluster_name: test-db docdb_ingress_cidr: 10.0.0.0/16 docdb_egress_cidr: 10.0.0.0/16 docdb_cidr_az1: 10.0.0.0/26 docdb_cidr_az2: 10.0.0.64/26 docdb_cidr_az3: 10.0.0.128/26 docdb_instance_identifier_prefix: test-db-instance vpc_id: test-vpc-id aws_access_key_id: aws-key aws_secret_access_key: aws-access-key roles: - ibm.mas_devops.mongodb","title":"Install (AWS DocumentDB)"},{"location":"roles/mongodb/#aws-documentdb-secret-rotation","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: aws mongodb_action: docdb_secret_rotate docdb_mongo_instance_name: test-db-instance db_host: aws.test1.host7283-***** db_port: 27017 docdb_master_username: admin docdb_master_password: pass*** docdb_instance_password_old: oldpass**** docdb_instance_username: testuser aws_access_key_id: aws-key aws_secret_access_key: aws-access-key roles: - ibm.mas_devops.mongodb","title":"AWS DocumentDb Secret Rotation"},{"location":"roles/mongodb/#aws-documentdb-destroy-data-action","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mongodb_provider: aws mongodb_action: destroy-data mongo_username: pqradmin mongo_password: xyzabc config: configDb: admin authMechanism: DEFAULT retryWrites: false hosts: - host: abc-0.pqr.databases.appdomain.cloud port: 32250 - host: abc-1.pqr.databases.appdomain.cloud port: 32250 - host: abc-2.pqr.databases.appdomain.cloud port: 32250 certificates: - alias: ca crt: | -----BEGIN CERTIFICATE----- MIIDDzCCAfegAwIBAgIJANEH58y2/kzHMA0GCSqGSIb3DQEBCwUAMB4xHDAaBgNV BAMME0lCTSBDbG91ZCBEYXRhYmFzZXMwHhcNMTgwNjI1MTQyOTAwWhcNMjgwNjIy MTQyOTAwWjAeMRwwGgYDVQQDDBNJQk0gQ2xvdWQgRGF0YWJhc2VzMIIBIjANBgkq 1eKI2FLzYKpoKBe5rcnrM7nHgNc/nCdEs5JecHb1dHv1QfPm6pzIxwIDAQABo1Aw TjAdBgNVHQ4EFgQUK3+XZo1wyKs+DEoYXbHruwSpXjgwHwYDVR0jBBgwFoAUK3+X Zo1wyKs+DEoYXbHruwSpXjgwDAYDVR0TBAUwAwEB/zANBgkqhkiG9w0BAQsFAAOC doqqgGIZ2nxCkp5/FXxF/TMb55vteTQwfgBy60jVVkbF7eVOWCv0KaNHPF5hrqbN i+3XjJ7/peF3xMvTMoy35DcT3E2ZeSVjouZs15O90kI3k2daS2OHJABW0vSj4nLz +PQzp/B9cQmOO8dCe049Q3oaUA== -----END CERTIFICATE----- roles: - ibm.mas_devops.mongodb","title":"AWS DocumentDb destroy-data action"},{"location":"roles/mongodb/#run-role-playbook","text":"export MONGODB_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=masinst1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/mongodb/#troubleshooting","text":"Important Please be cautious while performing any of the troubleshooting steps outlined below. It is important to understand that the MongoDB Community operator persists data within Persistent Volume Claims. These claims should not be removed inadvertent deletion of the mongoce namespace could result in data loss.","title":"Troubleshooting"},{"location":"roles/mongodb/#mongodb-replica-set-pods-will-not-start","text":"MongoDB 5 has introduced new platform specific requirements. Please consult the Platform Support Notes for detailed information. It is of particular importance to confirm that the AVX instruction set is exposed or available to the MongoDB workloads. This can easily be determined by entering any running pod on the same OpenShift cluster where MongoDB replica set members are failing to start. Once inside of a running pod the following command can be executed to confirm if the AVX instruction set is available: cat /proc/cpuinfo | grep flags | grep avx If avx is not found in the available flags then either the physical processor hosting the OpenShift cluster does not provide the AVX instruction set or the virtual host configuration is not exposing the AVX instruction set. If the latter is suspected the virtual hosting documentation should be referenced for details on how to expose the AVX instruction set.","title":"MongoDB Replica Set Pods Will Not Start"},{"location":"roles/mongodb/#ldap-authentication","text":"If authenticating via LDAP with PLAIN specified for authMechanism then configDb must be set to $external in the MongoCfg. The field configDb in the MongoCfg refers to the authentication database.","title":"LDAP Authentication"},{"location":"roles/mongodb/#ca-certificate-renewal","text":"Warning If the MongoDB CA Certificate expires the MongoDB replica set will become unusable. Replica set members will not be able to communicate with each other and client applications (i.e. Maximo Application Suite components) will not be to connect. In order to renew the CA Certificate used by the MongoDB replica set the following steps must be taken: Delete the CA Certificate resource Delete the MongoDB server Certificate resource Delete the Secrets resources associated with both the CA Certificate and Server Certificate Delete the Secret resource which contains the MongoDB configuration parameters Delete the ConfigMap resources which contains the CA certificate Delete the Secret resource which contains the sever certificate and private key The following steps illustrate the process required to renew the CA Certificate, sever certificate and reconfigure the MongoDB replica set with the new CA and server certificates. The first step is to stop the Mongo replica set and MongoDb CE Operator pod. oc project mongoce oc delete deployment mongodb-kubernetes-operator Important Make sure the MongoDB Community operator pod has terminated before proceeding. oc delete statefulset mas-mongo-ce Important Make sure all pods in the mongoce namespace have terminated before proceeding Remove expired CA Certificate and Server Certificate resources. Clean up MongoDB Community configuration and then run the mongodb role. oc delete certificate mongo-ca-crt oc delete certificate mongo-server oc delete secret mongo-ca-secret oc delete secret mongo-server-cert oc delete secret mas-mongo-ce-config oc delete configmap mas-mongo-ce-cert-map oc delete secret mas-mongo-ce-server-certificate-key export ROLE_NAME=mongodb ansible-playbook ibm.mas_devops.run_role Once the mongodb role has completed the MongoDb CE Operator pod and Mongo replica set should be configured. After the CA and server Certificates have been renewed you must ensure that that MongoCfg Suite CR is updated with the new CA Certificate. First obtain the CA Certificate from the Secret resource mongo-ca-secret . Then edit the Suite MongoCfg CR in the Maximo Application Suite core namespace. This is done by updating the appropriate certificate under .spec.certificates in the MongoCfg CR: spec: certificates: - alias: ca crt: | -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- If an IBM Suite Licensing Service (SLS) is also connecting to the MongoDB replica set the LicenseService CR must also be updated to reflect the new MongoDB CA. This can be added to the .spec.mongo.certificates section of the LicenseService CR. mongo: certificates: - alias: mongoca crt: | -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- Once the CA certificate has been updated for the MongoCfg and LicenseService CRs several pods in the core and SLS namespaces might need to be restarted to pick up the changes. This would include but is not limited to coreidp, coreapi, api-licensing.","title":"CA Certificate Renewal"},{"location":"roles/mongodb/#license","text":"EPL-2.0","title":"License"},{"location":"roles/nvidia_gpu/","text":"nvidia_gpu \u00a4 This role installs and configures the NVIDIA GPU Operator on OpenShift clusters to enable GPU workloads. The GPU Operator manages the lifecycle of NVIDIA software components required to run GPU-accelerated applications. The role automatically installs the Node Feature Discovery (NFD) Operator as a prerequisite, then deploys the NVIDIA GPU Operator and creates a ClusterPolicy to configure GPU support across the cluster. This is required for applications like Maximo Visual Inspection that need GPU acceleration. Prerequisites \u00a4 OpenShift cluster with GPU-enabled worker nodes Cluster administrator access GPU-capable hardware (NVIDIA GPUs) available in the cluster Role Variables \u00a4 GPU Operator Variables \u00a4 gpu_namespace \u00a4 The namespace where the NVIDIA GPU Operator will be installed. Optional Environment Variable: GPU_NAMESPACE Default Value: nvidia-gpu-operator gpu_channel \u00a4 The subscription channel for the GPU Operator. This determines which version of the operator will be installed. Optional Environment Variable: GPU_CHANNEL Default Value: v24.9 gpu_driver_version \u00a4 Specific NVIDIA driver version to install. If not specified, the latest compatible driver version will be used. Optional Environment Variable: GPU_DRIVER_VERSION Default Value: None (uses latest) gpu_driver_repository_path \u00a4 The container registry path for NVIDIA driver images. Optional Environment Variable: GPU_DRIVER_REPOSITORY_PATH Default Value: nvcr.io/nvidia Node Feature Discovery Variables \u00a4 nfd_namespace \u00a4 The namespace where the Node Feature Discovery Operator will be installed. NFD is required by the GPU Operator to detect GPU hardware. Optional Environment Variable: NFD_NAMESPACE Default Value: openshift-nfd nfd_channel \u00a4 The subscription channel for the Node Feature Discovery Operator. Optional Environment Variable: NFD_CHANNEL Default Value: stable Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: gpu_namespace: nvidia-gpu-operator gpu_channel: v24.9 nfd_namespace: openshift-nfd roles: - ibm.mas_devops.nvidia_gpu Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export GPU_NAMESPACE=nvidia-gpu-operator export GPU_CHANNEL=v24.9 export NFD_NAMESPACE=openshift-nfd ROLE_NAME=nvidia_gpu ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"nvidia_gpu"},{"location":"roles/nvidia_gpu/#nvidia_gpu","text":"This role installs and configures the NVIDIA GPU Operator on OpenShift clusters to enable GPU workloads. The GPU Operator manages the lifecycle of NVIDIA software components required to run GPU-accelerated applications. The role automatically installs the Node Feature Discovery (NFD) Operator as a prerequisite, then deploys the NVIDIA GPU Operator and creates a ClusterPolicy to configure GPU support across the cluster. This is required for applications like Maximo Visual Inspection that need GPU acceleration.","title":"nvidia_gpu"},{"location":"roles/nvidia_gpu/#prerequisites","text":"OpenShift cluster with GPU-enabled worker nodes Cluster administrator access GPU-capable hardware (NVIDIA GPUs) available in the cluster","title":"Prerequisites"},{"location":"roles/nvidia_gpu/#role-variables","text":"","title":"Role Variables"},{"location":"roles/nvidia_gpu/#gpu-operator-variables","text":"","title":"GPU Operator Variables"},{"location":"roles/nvidia_gpu/#gpu_namespace","text":"The namespace where the NVIDIA GPU Operator will be installed. Optional Environment Variable: GPU_NAMESPACE Default Value: nvidia-gpu-operator","title":"gpu_namespace"},{"location":"roles/nvidia_gpu/#gpu_channel","text":"The subscription channel for the GPU Operator. This determines which version of the operator will be installed. Optional Environment Variable: GPU_CHANNEL Default Value: v24.9","title":"gpu_channel"},{"location":"roles/nvidia_gpu/#gpu_driver_version","text":"Specific NVIDIA driver version to install. If not specified, the latest compatible driver version will be used. Optional Environment Variable: GPU_DRIVER_VERSION Default Value: None (uses latest)","title":"gpu_driver_version"},{"location":"roles/nvidia_gpu/#gpu_driver_repository_path","text":"The container registry path for NVIDIA driver images. Optional Environment Variable: GPU_DRIVER_REPOSITORY_PATH Default Value: nvcr.io/nvidia","title":"gpu_driver_repository_path"},{"location":"roles/nvidia_gpu/#node-feature-discovery-variables","text":"","title":"Node Feature Discovery Variables"},{"location":"roles/nvidia_gpu/#nfd_namespace","text":"The namespace where the Node Feature Discovery Operator will be installed. NFD is required by the GPU Operator to detect GPU hardware. Optional Environment Variable: NFD_NAMESPACE Default Value: openshift-nfd","title":"nfd_namespace"},{"location":"roles/nvidia_gpu/#nfd_channel","text":"The subscription channel for the Node Feature Discovery Operator. Optional Environment Variable: NFD_CHANNEL Default Value: stable","title":"nfd_channel"},{"location":"roles/nvidia_gpu/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: gpu_namespace: nvidia-gpu-operator gpu_channel: v24.9 nfd_namespace: openshift-nfd roles: - ibm.mas_devops.nvidia_gpu","title":"Example Playbook"},{"location":"roles/nvidia_gpu/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export GPU_NAMESPACE=nvidia-gpu-operator export GPU_CHANNEL=v24.9 export NFD_NAMESPACE=openshift-nfd ROLE_NAME=nvidia_gpu ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/nvidia_gpu/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_cluster_monitoring/","text":"ocp_cluster_monitoring \u00a4 Configures the OpenShift Container Platform Cluster Monitoring enabling two settings: OpenShift user defined project monitoring is enabled ( openshift-monitoring namespace) OpenShift monitoring stack is configured to use persistent storage ( openshift-monitoring namespace) This role is version-aware and will automatically apply the appropriate configuration template based on the detected OpenShift version: - For OpenShift 4.18 and higher: Uses a simplified configuration template compatible with newer versions - For OpenShift versions below 4.18: Uses the traditional configuration template Role Variables \u00a4 cluster_monitoring_action \u00a4 Inform the role whether to perform an install or an uninstall of the cluster monitoring stack. Optional Environment Variable: CLUSTER_MONITORING_ACTION Default: install prometheus_retention_period \u00a4 Adjust the retention period for Prometheus metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_RETENTION_PERIOD Default Value: 15d prometheus_storage_class \u00a4 Declare the storage class for Prometheus' metrics data persistent volume. Storage class must support ReadWriteOnce(RWO) access mode. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: PROMETHEUS_STORAGE_CLASS Default Value: ibmc-block-gold , ocs-storagecluster-ceph-rbd , or managed-premium (if available) prometheus_storage_size \u00a4 Adjust the size of the volume used to store metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_STORAGE_SIZE Default Value: 20Gi prometheus_alertmgr_storage_class \u00a4 Declare the storage class for AlertManager's persistent volume. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available) Note : Storage class must support ReadWriteMany(RWX) access mode. prometheus_alertmgr_storage_size \u00a4 Adjust the size of the volume used by AlertManager, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_SIZE Default Value: 20Gi prometheus_userworkload_retention_period \u00a4 Adjust the retention period for User Workload Prometheus metrics, this parameter applies only to the User Workload Prometheus instance. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_RETENTION_PERIOD Default Value: 15d prometheus_userworkload_storage_class \u00a4 Declare the storage class for User Workload Prometheus' metrics data persistent volume. Storage class must support ReadWriteOnce(RWO) access mode. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_CLASS Default Value: PROMETHEUS_STORAGE_CLASS prometheus_userworkload_storage_size \u00a4 Adjust the size of the volume used to store User Workload metrics. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_SIZE Default Value: 20Gi Example Playbook \u00a4 - hosts: localhost vars: prometheus_storage_class: \"ibmc-block-gold\" prometheus_alertmgr_storage_class: \"ibmc-file-gold-gid\" roles: - ibm.mas_devops.ocp_cluster_monitoring License \u00a4 EPL-2.0","title":"ocp_cluster_monitoring"},{"location":"roles/ocp_cluster_monitoring/#ocp_cluster_monitoring","text":"Configures the OpenShift Container Platform Cluster Monitoring enabling two settings: OpenShift user defined project monitoring is enabled ( openshift-monitoring namespace) OpenShift monitoring stack is configured to use persistent storage ( openshift-monitoring namespace) This role is version-aware and will automatically apply the appropriate configuration template based on the detected OpenShift version: - For OpenShift 4.18 and higher: Uses a simplified configuration template compatible with newer versions - For OpenShift versions below 4.18: Uses the traditional configuration template","title":"ocp_cluster_monitoring"},{"location":"roles/ocp_cluster_monitoring/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_cluster_monitoring/#cluster_monitoring_action","text":"Inform the role whether to perform an install or an uninstall of the cluster monitoring stack. Optional Environment Variable: CLUSTER_MONITORING_ACTION Default: install","title":"cluster_monitoring_action"},{"location":"roles/ocp_cluster_monitoring/#prometheus_retention_period","text":"Adjust the retention period for Prometheus metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_RETENTION_PERIOD Default Value: 15d","title":"prometheus_retention_period"},{"location":"roles/ocp_cluster_monitoring/#prometheus_storage_class","text":"Declare the storage class for Prometheus' metrics data persistent volume. Storage class must support ReadWriteOnce(RWO) access mode. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: PROMETHEUS_STORAGE_CLASS Default Value: ibmc-block-gold , ocs-storagecluster-ceph-rbd , or managed-premium (if available)","title":"prometheus_storage_class"},{"location":"roles/ocp_cluster_monitoring/#prometheus_storage_size","text":"Adjust the size of the volume used to store metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_STORAGE_SIZE Default Value: 20Gi","title":"prometheus_storage_size"},{"location":"roles/ocp_cluster_monitoring/#prometheus_alertmgr_storage_class","text":"Declare the storage class for AlertManager's persistent volume. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available) Note : Storage class must support ReadWriteMany(RWX) access mode.","title":"prometheus_alertmgr_storage_class"},{"location":"roles/ocp_cluster_monitoring/#prometheus_alertmgr_storage_size","text":"Adjust the size of the volume used by AlertManager, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_SIZE Default Value: 20Gi","title":"prometheus_alertmgr_storage_size"},{"location":"roles/ocp_cluster_monitoring/#prometheus_userworkload_retention_period","text":"Adjust the retention period for User Workload Prometheus metrics, this parameter applies only to the User Workload Prometheus instance. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_RETENTION_PERIOD Default Value: 15d","title":"prometheus_userworkload_retention_period"},{"location":"roles/ocp_cluster_monitoring/#prometheus_userworkload_storage_class","text":"Declare the storage class for User Workload Prometheus' metrics data persistent volume. Storage class must support ReadWriteOnce(RWO) access mode. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_CLASS Default Value: PROMETHEUS_STORAGE_CLASS","title":"prometheus_userworkload_storage_class"},{"location":"roles/ocp_cluster_monitoring/#prometheus_userworkload_storage_size","text":"Adjust the size of the volume used to store User Workload metrics. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_SIZE Default Value: 20Gi","title":"prometheus_userworkload_storage_size"},{"location":"roles/ocp_cluster_monitoring/#example-playbook","text":"- hosts: localhost vars: prometheus_storage_class: \"ibmc-block-gold\" prometheus_alertmgr_storage_class: \"ibmc-file-gold-gid\" roles: - ibm.mas_devops.ocp_cluster_monitoring","title":"Example Playbook"},{"location":"roles/ocp_cluster_monitoring/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_config/","text":"ocp_config \u00a4 This role can perform the following configuration: Tune the IngressController to avoid request failures due to timeout for long running requests Update APIServer and IngressController to set a custom tlsSecurityProfile to accommodate ciphers supported by IBM Java Semeru runtime. This is required for allowing the Java applications using Semeru runtime to run in FIPS mode. The following cipers will be enabled: TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 ECDHE-ECDSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-GCM-SHA256 ECDHE-ECDSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-GCM-SHA384 ECDHE-ECDSA-CHACHA20-POLY1305 ECDHE-RSA-CHACHA20-POLY1305 DHE-RSA-AES128-GCM-SHA256 DHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-SHA Disable the default Red Hat CatalogSources : certified-operators community-operators redhat-operators Role Variables - API Server \u00a4 ocp_update_ciphers_for_semeru \u00a4 Set to True if you want to configure the API Server and Ingress Controller to use a custom set of ciphers that are compatible with IBM Java Semeru in FIPS mode. Optional Environment Variable: OCP_UPDATE_CIPHERS_FOR_SEMERU Default Value: False Role Variables - Ingress Controller \u00a4 ocp_ingress_update_timeouts \u00a4 Set to True if you want to customize the Ingress's client and server timeout values Optional Environment Variable: OCP_INGRESS_UPDATE_TIMEOUTS Default Value: False ocp_ingress_client_timeout \u00a4 Specifies how long a connection is held open while waiting for a client response Optional Environment Variable: OCP_INGRESS_CLIENT_TIMEOUT Default Value: 30s ocp_ingress_server_timeout \u00a4 Specifies how long a connection is held open while waiting for a server response Optional Environment Variable: OCP_INGRESS_SERVER_TIMEOUT Default Value: 30s Role Variables - OperatorHub \u00a4 ocp_operatorhub_disable_redhat_sources \u00a4 Set to True if you want to disable the default Red Hat catalog sources Optional Environment Variable: OCP_OPERATORHUB_DISABLE_REDHAT_SOURCES Default Value: False Note Setting this to False will not enable the default catalog sources if they are currently disabled, it will just instruct this role to take no action. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: ocp_update_ciphers_for_semeru: True ocp_ingress_update_timeouts: True ocp_ingress_client_timeout: 30s ocp_ingress_server_timeout: 30s ocp_operatorhub_disable_redhat_sources: True roles: - ibm.mas_devops.ocp_config License \u00a4 EPL-2.0","title":"ocp_config"},{"location":"roles/ocp_config/#ocp_config","text":"This role can perform the following configuration: Tune the IngressController to avoid request failures due to timeout for long running requests Update APIServer and IngressController to set a custom tlsSecurityProfile to accommodate ciphers supported by IBM Java Semeru runtime. This is required for allowing the Java applications using Semeru runtime to run in FIPS mode. The following cipers will be enabled: TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 ECDHE-ECDSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-GCM-SHA256 ECDHE-ECDSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-GCM-SHA384 ECDHE-ECDSA-CHACHA20-POLY1305 ECDHE-RSA-CHACHA20-POLY1305 DHE-RSA-AES128-GCM-SHA256 DHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-SHA Disable the default Red Hat CatalogSources : certified-operators community-operators redhat-operators","title":"ocp_config"},{"location":"roles/ocp_config/#role-variables-api-server","text":"","title":"Role Variables - API Server"},{"location":"roles/ocp_config/#ocp_update_ciphers_for_semeru","text":"Set to True if you want to configure the API Server and Ingress Controller to use a custom set of ciphers that are compatible with IBM Java Semeru in FIPS mode. Optional Environment Variable: OCP_UPDATE_CIPHERS_FOR_SEMERU Default Value: False","title":"ocp_update_ciphers_for_semeru"},{"location":"roles/ocp_config/#role-variables-ingress-controller","text":"","title":"Role Variables - Ingress Controller"},{"location":"roles/ocp_config/#ocp_ingress_update_timeouts","text":"Set to True if you want to customize the Ingress's client and server timeout values Optional Environment Variable: OCP_INGRESS_UPDATE_TIMEOUTS Default Value: False","title":"ocp_ingress_update_timeouts"},{"location":"roles/ocp_config/#ocp_ingress_client_timeout","text":"Specifies how long a connection is held open while waiting for a client response Optional Environment Variable: OCP_INGRESS_CLIENT_TIMEOUT Default Value: 30s","title":"ocp_ingress_client_timeout"},{"location":"roles/ocp_config/#ocp_ingress_server_timeout","text":"Specifies how long a connection is held open while waiting for a server response Optional Environment Variable: OCP_INGRESS_SERVER_TIMEOUT Default Value: 30s","title":"ocp_ingress_server_timeout"},{"location":"roles/ocp_config/#role-variables-operatorhub","text":"","title":"Role Variables - OperatorHub"},{"location":"roles/ocp_config/#ocp_operatorhub_disable_redhat_sources","text":"Set to True if you want to disable the default Red Hat catalog sources Optional Environment Variable: OCP_OPERATORHUB_DISABLE_REDHAT_SOURCES Default Value: False Note Setting this to False will not enable the default catalog sources if they are currently disabled, it will just instruct this role to take no action.","title":"ocp_operatorhub_disable_redhat_sources"},{"location":"roles/ocp_config/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: ocp_update_ciphers_for_semeru: True ocp_ingress_update_timeouts: True ocp_ingress_client_timeout: 30s ocp_ingress_server_timeout: 30s ocp_operatorhub_disable_redhat_sources: True roles: - ibm.mas_devops.ocp_config","title":"Example Playbook"},{"location":"roles/ocp_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_deprovision/","text":"ocp_deprovision \u00a4 Deprovision OCP cluster in Fyre, IBM Cloud, & ROSA. Role Variables \u00a4 cluster_type \u00a4 Required. Specify the cluster type, supported values are roks and quickburn . Required Environment Variable: CLUSTER_TYPE Default Value: None cluster_name \u00a4 Required. Specify the name of the cluster Required Environment Variable: CLUSTER_NAME Default Value: None Role Variables - ROKS \u00a4 ibmcloud_apikey \u00a4 The APIKey to be used by ibmcloud login comand. Required if cluster_type = roks Environment Variable: IBMCLOUD_APIKEY Default Value: None Role Variables - ROSA \u00a4 rosa_token \u00a4 The Token used to authenticate with the ROSA service. Required if cluster_type = rosa Environment Variable: ROSA_TOKEN Default Value: None Role Variables - FYRE \u00a4 fyre_username \u00a4 Username to authenticate with the Fyre API. Required if cluster_type = quickburn . Environment Variable: FYRE_USERNAME Default Value: None fyre_apikey \u00a4 API key to authenticate with the Fyre API. Required if cluster_type = quickburn . Environment Variable: FYRE_APIKEY Default Value: None fyre_site \u00a4 Site in Fyre where cluster had been provisioned Optional Environment Variable: FYRE_SITE Default Value: svl Role Variables - IPI \u00a4 The following variables are only used when cluster_type = ipi . ipi_install_dir \u00a4 The directory that is used to store the openshift-install executable, its configuration, & generated log files. Optional when cluster_type = aws-ipi Environment Variable: IPI_DIR Default Value: ~/openshift-install ipi_platform \u00a4 Platform the cluster was created on, any platform supported by openshift-install . Values allowed: aws and gcp . Required. Environment Variable: IPI_PLATFORM Default Value: None Role Variables - AWS \u00a4 The following variables are only used when cluster_type = ipi and ipi_platform = aws . aws_access_key_id \u00a4 AWS access key associated with an IAM user or role. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None aws_secret_access_key \u00a4 AWS secret access key associated with an IAM user or role. Make sure the access key has permissions to delete instances. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None Role Variables - GCP \u00a4 The following variables are only used when cluster_type = ipi and ipi_platform = gcp . gcp_service_account_file \u00a4 GCP service account file path. Make sure the service account has permissions to create instances. Required when cluster_type = ipi and ipi_platform = gcp Environment Variable: GOOGLE_APPLICATION_CREDENTIALS Default Value: None Example Playbook \u00a4 - hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_deprovision License \u00a4 EPL-2.0","title":"ocp_deprovision"},{"location":"roles/ocp_deprovision/#ocp_deprovision","text":"Deprovision OCP cluster in Fyre, IBM Cloud, & ROSA.","title":"ocp_deprovision"},{"location":"roles/ocp_deprovision/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_deprovision/#cluster_type","text":"Required. Specify the cluster type, supported values are roks and quickburn . Required Environment Variable: CLUSTER_TYPE Default Value: None","title":"cluster_type"},{"location":"roles/ocp_deprovision/#cluster_name","text":"Required. Specify the name of the cluster Required Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/ocp_deprovision/#role-variables-roks","text":"","title":"Role Variables - ROKS"},{"location":"roles/ocp_deprovision/#ibmcloud_apikey","text":"The APIKey to be used by ibmcloud login comand. Required if cluster_type = roks Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/ocp_deprovision/#role-variables-rosa","text":"","title":"Role Variables - ROSA"},{"location":"roles/ocp_deprovision/#rosa_token","text":"The Token used to authenticate with the ROSA service. Required if cluster_type = rosa Environment Variable: ROSA_TOKEN Default Value: None","title":"rosa_token"},{"location":"roles/ocp_deprovision/#role-variables-fyre","text":"","title":"Role Variables - FYRE"},{"location":"roles/ocp_deprovision/#fyre_username","text":"Username to authenticate with the Fyre API. Required if cluster_type = quickburn . Environment Variable: FYRE_USERNAME Default Value: None","title":"fyre_username"},{"location":"roles/ocp_deprovision/#fyre_apikey","text":"API key to authenticate with the Fyre API. Required if cluster_type = quickburn . Environment Variable: FYRE_APIKEY Default Value: None","title":"fyre_apikey"},{"location":"roles/ocp_deprovision/#fyre_site","text":"Site in Fyre where cluster had been provisioned Optional Environment Variable: FYRE_SITE Default Value: svl","title":"fyre_site"},{"location":"roles/ocp_deprovision/#role-variables-ipi","text":"The following variables are only used when cluster_type = ipi .","title":"Role Variables - IPI"},{"location":"roles/ocp_deprovision/#ipi_install_dir","text":"The directory that is used to store the openshift-install executable, its configuration, & generated log files. Optional when cluster_type = aws-ipi Environment Variable: IPI_DIR Default Value: ~/openshift-install","title":"ipi_install_dir"},{"location":"roles/ocp_deprovision/#ipi_platform","text":"Platform the cluster was created on, any platform supported by openshift-install . Values allowed: aws and gcp . Required. Environment Variable: IPI_PLATFORM Default Value: None","title":"ipi_platform"},{"location":"roles/ocp_deprovision/#role-variables-aws","text":"The following variables are only used when cluster_type = ipi and ipi_platform = aws .","title":"Role Variables - AWS"},{"location":"roles/ocp_deprovision/#aws_access_key_id","text":"AWS access key associated with an IAM user or role. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None","title":"aws_access_key_id"},{"location":"roles/ocp_deprovision/#aws_secret_access_key","text":"AWS secret access key associated with an IAM user or role. Make sure the access key has permissions to delete instances. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None","title":"aws_secret_access_key"},{"location":"roles/ocp_deprovision/#role-variables-gcp","text":"The following variables are only used when cluster_type = ipi and ipi_platform = gcp .","title":"Role Variables - GCP"},{"location":"roles/ocp_deprovision/#gcp_service_account_file","text":"GCP service account file path. Make sure the service account has permissions to create instances. Required when cluster_type = ipi and ipi_platform = gcp Environment Variable: GOOGLE_APPLICATION_CREDENTIALS Default Value: None","title":"gcp_service_account_file"},{"location":"roles/ocp_deprovision/#example-playbook","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_deprovision","title":"Example Playbook"},{"location":"roles/ocp_deprovision/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_efs/","text":"ocp_efs \u00a4 This role provides support to install aws-efs on aws using aws cli and connect that to ROSA using oc CLI. This role create a new inbound rule in the security group of the ec2 instance where rosa is installed and then creates a new EFS instance and adds access points and network mounts to access EFS from ROSA. Role Variables \u00a4 aws_access_key_id \u00a4 The AWS access key will be used to login to aws cli. Required Environment Variable: AWS_ACCESS_KEY_ID Default: None aws_secret_access_key \u00a4 The AWS access secret key will be used to login to aws cli. Required Environment Variable: AWS_SECRET_ACCESS_KEY Default: None AWS_region \u00a4 The aws region where you wish to provision the EFS instance. Required Environment Variable: AWS_DEFAULT_REGION Default: eu-west-2 cluster_name \u00a4 The name of the cluster we are going to attach the EFS storage to. Required Environment Variable: CLUSTER_NAME Default: None efs_unique_id \u00a4 Any unique identifier like mas instance id which will be used as EFS storage class name. If this parameter is not set, then cluster_name will be used Optional Environment Variable: EFS_UNIQUE_ID Default: None creation_token_prefix \u00a4 CreationTokens associated for AWS resources are built by concatenating creation_token_prefix and efs_unique_id. Optional Environment Variable: CREATION_TOKEN_PREFIX Default: 'mas_devops.' create_storage_class \u00a4 If true, a StorageClass for the EFS instance named efs<efs_unique_id> will be automatically created in the cluster. Optional Environment Variable: CREATE_STORAGE_CLASS . Unset implies true , otherwise Ansible's bool filter is used to interpret the value as a boolean. Default: true License \u00a4 EPL-2.0","title":"ocp_efs"},{"location":"roles/ocp_efs/#ocp_efs","text":"This role provides support to install aws-efs on aws using aws cli and connect that to ROSA using oc CLI. This role create a new inbound rule in the security group of the ec2 instance where rosa is installed and then creates a new EFS instance and adds access points and network mounts to access EFS from ROSA.","title":"ocp_efs"},{"location":"roles/ocp_efs/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_efs/#aws_access_key_id","text":"The AWS access key will be used to login to aws cli. Required Environment Variable: AWS_ACCESS_KEY_ID Default: None","title":"aws_access_key_id"},{"location":"roles/ocp_efs/#aws_secret_access_key","text":"The AWS access secret key will be used to login to aws cli. Required Environment Variable: AWS_SECRET_ACCESS_KEY Default: None","title":"aws_secret_access_key"},{"location":"roles/ocp_efs/#aws_region","text":"The aws region where you wish to provision the EFS instance. Required Environment Variable: AWS_DEFAULT_REGION Default: eu-west-2","title":"AWS_region"},{"location":"roles/ocp_efs/#cluster_name","text":"The name of the cluster we are going to attach the EFS storage to. Required Environment Variable: CLUSTER_NAME Default: None","title":"cluster_name"},{"location":"roles/ocp_efs/#efs_unique_id","text":"Any unique identifier like mas instance id which will be used as EFS storage class name. If this parameter is not set, then cluster_name will be used Optional Environment Variable: EFS_UNIQUE_ID Default: None","title":"efs_unique_id"},{"location":"roles/ocp_efs/#creation_token_prefix","text":"CreationTokens associated for AWS resources are built by concatenating creation_token_prefix and efs_unique_id. Optional Environment Variable: CREATION_TOKEN_PREFIX Default: 'mas_devops.'","title":"creation_token_prefix"},{"location":"roles/ocp_efs/#create_storage_class","text":"If true, a StorageClass for the EFS instance named efs<efs_unique_id> will be automatically created in the cluster. Optional Environment Variable: CREATE_STORAGE_CLASS . Unset implies true , otherwise Ansible's bool filter is used to interpret the value as a boolean. Default: true","title":"create_storage_class"},{"location":"roles/ocp_efs/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_github_oauth/","text":"ocp_github_oauth \u00a4 This role provides to support to configure cluster oauth using GitHub. Warning Make sure you have configured the oauth app in GitHub organization before use this role. When configuring make sure to use ibmgithub as the oauth id. Requires organization admin permission to perform this action. Role Variables \u00a4 oauth.github_client_secret_value Secret value provided by the GitHub oauth app configuration. ouath.github_client_id_value Client ID value provided by the GitHub oauth app configuration. oauth.github_hostname can be used to target public GitHub or an enterprise account (e.g. github.ibm.com) oauth.groups List of groups to be created and its cluster role bindings oauth.groups.name Defines the name of the group oauth.groups.users List of users to be added to the group oauth.groups.groups_cluster_rolebindings List of cluster role bindings to be created for the group oauth.organizations List of GitHub organizations where the authentication will be performed Example Playbook \u00a4 TODO: Add example License \u00a4 EPL-2.0","title":"ocp_github_oauth"},{"location":"roles/ocp_github_oauth/#ocp_github_oauth","text":"This role provides to support to configure cluster oauth using GitHub. Warning Make sure you have configured the oauth app in GitHub organization before use this role. When configuring make sure to use ibmgithub as the oauth id. Requires organization admin permission to perform this action.","title":"ocp_github_oauth"},{"location":"roles/ocp_github_oauth/#role-variables","text":"oauth.github_client_secret_value Secret value provided by the GitHub oauth app configuration. ouath.github_client_id_value Client ID value provided by the GitHub oauth app configuration. oauth.github_hostname can be used to target public GitHub or an enterprise account (e.g. github.ibm.com) oauth.groups List of groups to be created and its cluster role bindings oauth.groups.name Defines the name of the group oauth.groups.users List of users to be added to the group oauth.groups.groups_cluster_rolebindings List of cluster role bindings to be created for the group oauth.organizations List of GitHub organizations where the authentication will be performed","title":"Role Variables"},{"location":"roles/ocp_github_oauth/#example-playbook","text":"TODO: Add example","title":"Example Playbook"},{"location":"roles/ocp_github_oauth/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_idms/","text":"ocp_idms \u00a4 Installs an ImageDigestMirrorSet (IDMS)for IBM Maximo Application Suite's Maximo Operator Catalog. Also install IDMS suitable for the Red Hat Operator Catalogs created by mirror_ocp . If there are legacy ImageContentSourcePolicies installed by previous versions of this role, they will be deleted. If PRODUCT_FAMILY is aiservice then it will install an ImageTagMirrorSet for OpenDataHub Warning This role doesn't work on IBMCloud ROKS. IBM Cloud RedHat OpenShift Service does not implement support for ImageDigestMirrorSet . If you want to use image mirroring you must manually configure each worker node individually using the IBM Cloud command line tool. IBM Maximo Operator Catalog Content \u00a4 All content used in the MAS install is sourced from three registries: icr.io , cp.icr.io , & quay.io : icr.io/cpopen All IBM operators icr.io/ibm-truststore-mgr IBM truststore manager worker image icr.io/ibm-sls IBM SLS content icr.io/db2u IBM Db2 Universal operator content cp.icr.io/cp All IBM entitled container images quay.io/opencloudio IBM common services quay.io/mongodb MongoDb Community Edition Operator & associated container images quay.io/amlen Eclipse Amlen - Message Broker for IoT/Mobile/Web quay.io/ibmmas Non-product IBM Maximo Application Suite images (e.g. MAS CLI) Red Hat Operator Catalog Content \u00a4 All content from the subset of the Red Hat operator catalogs supported by mirror_ocp is sourced from eight registries: icr.io , docker.io , quay.io , gcr.io , ghcr.io , nvcr.io , registry.connect.redhat.com , and registry.redhat.io : icr.io/cpopen docker.io/grafana quay.io/community-operator-pipeline-prod quay.io/operator-pipeline-prod quay.io/openshift-community-operators quay.io/strimzi quay.io/rh-marketplace gcr.io/kubebuilder ghcr.io/grafana ghcr.io/open-telemetry nvcr.io/nvidia registry.connect.redhat.com/crunchydata registry.connect.redhat.com/nvidia registry.connect.redhat.com/turbonomic registry.connect.redhat.com/rh-marketplace registry.redhat.io/openshift4 registry.redhat.io/source-to-image registry.redhat.io/odf4 registry.redhat.io/cert-manager registry.redhat.io/rhceph registry.redhat.io/amq-streams registry.redhat.io/ubi8 registry.redhat.io/openshift-pipelines registry.redhat.io/openshift-serverless-1 registry.redhat.io/lvms4 Note A content source policy for this content is only configured when setup_redhat_catalogs is set to True . If you are managing the Red Hat Operator Catalogs yourself the content therein may well be different depending how you have configured mirroring. Role Variables \u00a4 product_family \u00a4 Creates the ImageDigestMirrorSet for the respective product family Required Environment Variable: PRODUCT_FAMILY Default: mas Values: mas | aiservice setup_redhat_release \u00a4 Instruct the role to setup ImageDigestMirrorSet for the mirrored release content generated by mirror_ocp . This will create an additional policy named ibm-mas-redhat-release . Required Environment Variable: SETUP_REDHAT_RELEASE Default: False setup_redhat_catalogs \u00a4 Instruct the role to setup CatalogSources and ImageDigestMirrorSet for the mirror catalogs generated by mirror_ocp . This will create an additional policy named ibm-mas-redhat-catalogs . Required Environment Variable: SETUP_REDHAT_CATALOGS Default: False Role Variables - Target Registry \u00a4 registry_private_host \u00a4 The private hostname for the target registry Required Environment Variable: REGISTRY_PRIVATE_HOST Default: None registry_private_port \u00a4 The private port number for the target registry Optional Environment Variable: REGISTRY_PRIVATE_PORT Default: None registry_private_ca_file \u00a4 The CA certificate presented by the registry on it's private endpoint. Required Environment Variable: REGISTRY_PRIVATE_CA_FILE Default: None registry_username \u00a4 The username for the target registry. Required Environment Variable: REGISTRY_USERNAME Default: None registry_password \u00a4 The password for the target registry. Required Environment Variable: REGISTRY_PASSWORD Default: None registry_prefix \u00a4 Optional additional path prefixed to all image repositories related to the IBM Maximo Operator Catalog in the target registry. We recommend the use of the catalog datestamp for this prefix to organize your registry, e.g. mas-241107 , mas-241205 . This should match the value used when you mirrored the images with mirror_images . Optional Environment Variable: REGISTRY_PREFIX Default: None registry_prefix_redhat \u00a4 Optional additional path prefixed to all image repositories related to the Red Hat Release and Operator Catalogs in the target registry. We recommend the use of the OpenShift release for this prefix to organize your registry, e.g. ocp-412 , ocp-414 . This should match the value used when you mirrored the images with mirror_ocp . Required Environment Variable: REGISTRY_PREFIX_REDHAT Default: None redhat_catalogs_prefix \u00a4 An optional prefix to apply to the catalog sources names for the 3 Red Hat catalogs supported by this role. For example, setting a value of ibm-mas will result in three catalog sources named ibm-mas-certified-operator-index , ibm-mas-community-operator-index , ibm-mas-redhat-operator-index being created. Optional Environment Variable: REDHAT_CATALOGS_PREFIX Default: None machine_config_multiupdate \u00a4 An optional value that if present will set the max unavailable nodes for the worker Machine Config Pool. This is only recommended to be set during setup of an environment where nodes will be lightly loaded and draining multiple worker nodes in parallel is possible Optional Environment Variable: MACHINE_CONFIG_MULTIUPDATE Default: false Example Playbook \u00a4 - hosts: localhost vars: registry_private_host: myocp-5f1320191125833da1cac8216c06779e-0000.us-south.containers.appdomain.cloud registry_private_port: 32500 registry_private_ca_file: ~/registry-ca.crt registry_username: admin registry_password: 8934jk77s862! # Not a real password, don't worry security folks setup_redhat_catalogs: true roles: - ibm.mas_devops.ocp_contentsourcepolicy License \u00a4 EPL-2.0","title":"ocp_idms"},{"location":"roles/ocp_idms/#ocp_idms","text":"Installs an ImageDigestMirrorSet (IDMS)for IBM Maximo Application Suite's Maximo Operator Catalog. Also install IDMS suitable for the Red Hat Operator Catalogs created by mirror_ocp . If there are legacy ImageContentSourcePolicies installed by previous versions of this role, they will be deleted. If PRODUCT_FAMILY is aiservice then it will install an ImageTagMirrorSet for OpenDataHub Warning This role doesn't work on IBMCloud ROKS. IBM Cloud RedHat OpenShift Service does not implement support for ImageDigestMirrorSet . If you want to use image mirroring you must manually configure each worker node individually using the IBM Cloud command line tool.","title":"ocp_idms"},{"location":"roles/ocp_idms/#ibm-maximo-operator-catalog-content","text":"All content used in the MAS install is sourced from three registries: icr.io , cp.icr.io , & quay.io : icr.io/cpopen All IBM operators icr.io/ibm-truststore-mgr IBM truststore manager worker image icr.io/ibm-sls IBM SLS content icr.io/db2u IBM Db2 Universal operator content cp.icr.io/cp All IBM entitled container images quay.io/opencloudio IBM common services quay.io/mongodb MongoDb Community Edition Operator & associated container images quay.io/amlen Eclipse Amlen - Message Broker for IoT/Mobile/Web quay.io/ibmmas Non-product IBM Maximo Application Suite images (e.g. MAS CLI)","title":"IBM Maximo Operator Catalog Content"},{"location":"roles/ocp_idms/#red-hat-operator-catalog-content","text":"All content from the subset of the Red Hat operator catalogs supported by mirror_ocp is sourced from eight registries: icr.io , docker.io , quay.io , gcr.io , ghcr.io , nvcr.io , registry.connect.redhat.com , and registry.redhat.io : icr.io/cpopen docker.io/grafana quay.io/community-operator-pipeline-prod quay.io/operator-pipeline-prod quay.io/openshift-community-operators quay.io/strimzi quay.io/rh-marketplace gcr.io/kubebuilder ghcr.io/grafana ghcr.io/open-telemetry nvcr.io/nvidia registry.connect.redhat.com/crunchydata registry.connect.redhat.com/nvidia registry.connect.redhat.com/turbonomic registry.connect.redhat.com/rh-marketplace registry.redhat.io/openshift4 registry.redhat.io/source-to-image registry.redhat.io/odf4 registry.redhat.io/cert-manager registry.redhat.io/rhceph registry.redhat.io/amq-streams registry.redhat.io/ubi8 registry.redhat.io/openshift-pipelines registry.redhat.io/openshift-serverless-1 registry.redhat.io/lvms4 Note A content source policy for this content is only configured when setup_redhat_catalogs is set to True . If you are managing the Red Hat Operator Catalogs yourself the content therein may well be different depending how you have configured mirroring.","title":"Red Hat Operator Catalog Content"},{"location":"roles/ocp_idms/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_idms/#product_family","text":"Creates the ImageDigestMirrorSet for the respective product family Required Environment Variable: PRODUCT_FAMILY Default: mas Values: mas | aiservice","title":"product_family"},{"location":"roles/ocp_idms/#setup_redhat_release","text":"Instruct the role to setup ImageDigestMirrorSet for the mirrored release content generated by mirror_ocp . This will create an additional policy named ibm-mas-redhat-release . Required Environment Variable: SETUP_REDHAT_RELEASE Default: False","title":"setup_redhat_release"},{"location":"roles/ocp_idms/#setup_redhat_catalogs","text":"Instruct the role to setup CatalogSources and ImageDigestMirrorSet for the mirror catalogs generated by mirror_ocp . This will create an additional policy named ibm-mas-redhat-catalogs . Required Environment Variable: SETUP_REDHAT_CATALOGS Default: False","title":"setup_redhat_catalogs"},{"location":"roles/ocp_idms/#role-variables-target-registry","text":"","title":"Role Variables - Target Registry"},{"location":"roles/ocp_idms/#registry_private_host","text":"The private hostname for the target registry Required Environment Variable: REGISTRY_PRIVATE_HOST Default: None","title":"registry_private_host"},{"location":"roles/ocp_idms/#registry_private_port","text":"The private port number for the target registry Optional Environment Variable: REGISTRY_PRIVATE_PORT Default: None","title":"registry_private_port"},{"location":"roles/ocp_idms/#registry_private_ca_file","text":"The CA certificate presented by the registry on it's private endpoint. Required Environment Variable: REGISTRY_PRIVATE_CA_FILE Default: None","title":"registry_private_ca_file"},{"location":"roles/ocp_idms/#registry_username","text":"The username for the target registry. Required Environment Variable: REGISTRY_USERNAME Default: None","title":"registry_username"},{"location":"roles/ocp_idms/#registry_password","text":"The password for the target registry. Required Environment Variable: REGISTRY_PASSWORD Default: None","title":"registry_password"},{"location":"roles/ocp_idms/#registry_prefix","text":"Optional additional path prefixed to all image repositories related to the IBM Maximo Operator Catalog in the target registry. We recommend the use of the catalog datestamp for this prefix to organize your registry, e.g. mas-241107 , mas-241205 . This should match the value used when you mirrored the images with mirror_images . Optional Environment Variable: REGISTRY_PREFIX Default: None","title":"registry_prefix"},{"location":"roles/ocp_idms/#registry_prefix_redhat","text":"Optional additional path prefixed to all image repositories related to the Red Hat Release and Operator Catalogs in the target registry. We recommend the use of the OpenShift release for this prefix to organize your registry, e.g. ocp-412 , ocp-414 . This should match the value used when you mirrored the images with mirror_ocp . Required Environment Variable: REGISTRY_PREFIX_REDHAT Default: None","title":"registry_prefix_redhat"},{"location":"roles/ocp_idms/#redhat_catalogs_prefix","text":"An optional prefix to apply to the catalog sources names for the 3 Red Hat catalogs supported by this role. For example, setting a value of ibm-mas will result in three catalog sources named ibm-mas-certified-operator-index , ibm-mas-community-operator-index , ibm-mas-redhat-operator-index being created. Optional Environment Variable: REDHAT_CATALOGS_PREFIX Default: None","title":"redhat_catalogs_prefix"},{"location":"roles/ocp_idms/#machine_config_multiupdate","text":"An optional value that if present will set the max unavailable nodes for the worker Machine Config Pool. This is only recommended to be set during setup of an environment where nodes will be lightly loaded and draining multiple worker nodes in parallel is possible Optional Environment Variable: MACHINE_CONFIG_MULTIUPDATE Default: false","title":"machine_config_multiupdate"},{"location":"roles/ocp_idms/#example-playbook","text":"- hosts: localhost vars: registry_private_host: myocp-5f1320191125833da1cac8216c06779e-0000.us-south.containers.appdomain.cloud registry_private_port: 32500 registry_private_ca_file: ~/registry-ca.crt registry_username: admin registry_password: 8934jk77s862! # Not a real password, don't worry security folks setup_redhat_catalogs: true roles: - ibm.mas_devops.ocp_contentsourcepolicy","title":"Example Playbook"},{"location":"roles/ocp_idms/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_login/","text":"ocp_login \u00a4 This role provides support to login to a cluster using the oc CLI by looking up cluster information from the infrastructure provider's APIs, it also supports setting ocp_server and ocp_token directly to support login to any Kubernetes cluster. Role Variables \u00a4 cluster_name \u00a4 Cluster name for credential lookup. Required (unless ocp_server and ocp_token are set) Environment Variable: CLUSTER_NAME Default: None Purpose : Identifies the cluster to login to by name. Used to automatically lookup login credentials from the infrastructure provider's API. When to use : - Use with cluster_type for automatic credential lookup - Alternative to manually providing ocp_server and ocp_token - Recommended for managed clusters (ROKS, ROSA, Fyre) Valid values : String matching your cluster name in the provider's system Impact : Combined with cluster_type , automatically retrieves cluster server URL and login token from provider API. Related variables : - cluster_type : Required with this variable (roks, fyre, rosa) - ocp_server / ocp_token : Alternative manual login method Note : Requires provider-specific credentials (e.g., ibmcloud_apikey for ROKS, rosa_token for ROSA, fyre_apikey for Fyre). cluster_type \u00a4 Infrastructure provider type for cluster. Required (unless ocp_server and ocp_token are set) Environment Variable: CLUSTER_TYPE Default: None Purpose : Specifies the infrastructure provider type to determine which API to use for credential lookup. When to use : - Use with cluster_name for automatic credential lookup - Required for managed cluster login - Each type requires specific provider credentials Valid values : roks , fyre , rosa - roks : IBM Cloud Red Hat OpenShift Kubernetes Service - fyre : IBM DevIT Fyre clusters - rosa : AWS Red Hat OpenShift Service on AWS Impact : Determines which provider API is called to retrieve cluster credentials. Each type requires different authentication variables. Related variables : - cluster_name : Cluster to lookup - ibmcloud_apikey : Required for roks - fyre_username / fyre_apikey : Required for fyre - rosa_token : Required for rosa Note : Alternative to using ocp_server and ocp_token for direct login. ocp_server \u00a4 OpenShift server URL for direct login. Required (unless cluster_name and cluster_type are set) Environment Variable: OCP_SERVER Default: None Purpose : Specifies the OpenShift API server URL for direct cluster login without provider API lookup. When to use : - Use with ocp_token for direct login - When cluster is not managed by supported providers - For custom or on-premises clusters - Alternative to cluster_name / cluster_type approach Valid values : Full OpenShift API server URL (e.g., https://api.cluster.example.com:6443 ) Impact : Used directly for oc login command. Bypasses provider API credential lookup. Related variables : - ocp_token : Required with this variable - cluster_name / cluster_type : Alternative automatic lookup method Note : Both ocp_server and ocp_token must be provided together for direct login. This method works with any Kubernetes cluster. ocp_token \u00a4 Authentication token for direct login. Required (unless cluster_name and cluster_type are set) Environment Variable: OCP_TOKEN Default: None Purpose : Provides the authentication token for direct cluster login without provider API lookup. When to use : - Use with ocp_server for direct login - When cluster is not managed by supported providers - For service account tokens or manually obtained tokens - Alternative to cluster_name / cluster_type approach Valid values : Valid OpenShift authentication token string Impact : Used directly for oc login --token command. Bypasses provider API credential lookup. Related variables : - ocp_server : Required with this variable - cluster_name / cluster_type : Alternative automatic lookup method Note : SECURITY - Both ocp_server and ocp_token must be provided together. Token should be kept secure and not committed to version control. This method works with any Kubernetes cluster. Role Variables - IBMCloud ROKS \u00a4 ibmcloud_apikey \u00a4 IBM Cloud API key for ROKS authentication. Required (when cluster_type=roks ) Environment Variable: IBMCLOUD_APIKEY Default: None Purpose : Provides IBM Cloud API key for authenticating with IBM Cloud to retrieve ROKS cluster credentials. When to use : - Required when cluster_type=roks - Used to authenticate with IBM Cloud API - Enables automatic cluster credential lookup Valid values : Valid IBM Cloud API key string Impact : Used to authenticate with IBM Cloud and retrieve cluster server URL and login token for ROKS clusters. Related variables : - cluster_type : Must be roks - cluster_name : ROKS cluster name to lookup - ibmcloud_endpoint : Optional API endpoint override Note : SECURITY - API key should be kept secure and not committed to version control. Obtain from IBM Cloud IAM. ibmcloud_endpoint \u00a4 IBM Cloud API endpoint URL. Optional Environment Variable: IBMCLOUD_ENDPOINT Default: https://cloud.ibm.com Purpose : Overrides the default IBM Cloud API endpoint for ROKS cluster credential lookup. When to use : - Use default for public IBM Cloud - Override for private or regional endpoints - Only applies when cluster_type=roks Valid values : Valid IBM Cloud API endpoint URL Impact : Determines which IBM Cloud API endpoint is used for authentication and cluster lookup. Related variables : - cluster_type : Must be roks - ibmcloud_apikey : Required for authentication Note : The default public endpoint works for most deployments. Override only for specific regional or private cloud requirements. Role Variables - IBM DevIT Fyre \u00a4 fyre_username \u00a4 IBM DevIT Fyre username. Required (when cluster_type=fyre ) Environment Variable: FYRE_USERNAME Default: None Purpose : Provides Fyre username for authenticating with IBM DevIT Fyre to retrieve cluster credentials. When to use : - Required when cluster_type=fyre - Used with fyre_apikey for Fyre authentication - Enables automatic cluster credential lookup Valid values : Valid Fyre username string Impact : Used to authenticate with Fyre API and retrieve cluster server URL and login token. Related variables : - cluster_type : Must be fyre - fyre_apikey : Required for authentication - cluster_name : Fyre cluster name to lookup - fyre_site : Fyre site location Note : Fyre is IBM's internal development and test infrastructure. Requires IBM internal credentials. fyre_apikey \u00a4 IBM DevIT Fyre API key. Required (when cluster_type=fyre ) Environment Variable: FYRE_APIKEY Default: None Purpose : Provides Fyre API key for authenticating with IBM DevIT Fyre to retrieve cluster credentials. When to use : - Required when cluster_type=fyre - Used with fyre_username for Fyre authentication - Enables automatic cluster credential lookup Valid values : Valid Fyre API key string Impact : Used to authenticate with Fyre API and retrieve cluster server URL and login token. Related variables : - cluster_type : Must be fyre - fyre_username : Required for authentication - cluster_name : Fyre cluster name to lookup - fyre_site : Fyre site location Note : SECURITY - API key should be kept secure. Fyre is IBM's internal development and test infrastructure. fyre_site \u00a4 Fyre site location for cluster. Optional Environment Variable: FYRE_SITE Default: svl Purpose : Specifies which Fyre site the cluster was provisioned in for proper API routing. When to use : - Use default ( svl ) for most Fyre clusters - Override for clusters in other sites (e.g., rtp ) - Only applies when cluster_type=fyre Valid values : Fyre site codes (e.g., svl , rtp ) Impact : Determines which Fyre site API endpoint is used for cluster lookup. Related variables : - cluster_type : Must be fyre - enable_ipv6 : Required for RTP site Note : SVL (San Jose Valley) is the default site. RTP (Research Triangle Park) requires IPv6 enablement. enable_ipv6 \u00a4 Enable IPv6 for Fyre RTP site. Optional Environment Variable: ENABLE_IPV6 Default: false Purpose : Enables IPv6 networking for Fyre clusters at the RTP (Research Triangle Park) site. When to use : - Set to true only for Fyre RTP site clusters - Leave as false for all other sites (including SVL) - Only applies when cluster_type=fyre Valid values : true , false Impact : Configures network settings for IPv6 connectivity to RTP site clusters. Related variables : - cluster_type : Must be fyre - fyre_site : Should be rtp when this is true Note : Only required for Fyre RTP site. SVL and other sites use IPv4. Role Variables - AWS ROSA \u00a4 rosa_token \u00a4 AWS ROSA authentication token. Required (when cluster_type=rosa ) Environment Variable: ROSA_TOKEN Default: None Purpose : Provides ROSA (Red Hat OpenShift Service on AWS) authentication token for retrieving cluster credentials. When to use : - Required when cluster_type=rosa - Used to authenticate with ROSA API - Enables automatic cluster credential lookup Valid values : Valid ROSA authentication token string Impact : Used to authenticate with ROSA API and retrieve cluster server URL and login credentials. Related variables : - cluster_type : Must be rosa - cluster_name : ROSA cluster name to lookup - rosa_cluster_admin_password : Required for cluster-admin login Note : SECURITY - Token should be kept secure and not committed to version control. Obtain from Red Hat Hybrid Cloud Console. rosa_cluster_admin_password \u00a4 ROSA cluster-admin account password. Required (when cluster_type=rosa ) Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default: None Purpose : Provides the password for the cluster-admin account to login to ROSA clusters. When to use : - Required when cluster_type=rosa - Password created during cluster provisioning - Used for cluster-admin level access Valid values : Valid cluster-admin password string Impact : Used to authenticate as cluster-admin user on ROSA clusters. Related variables : - cluster_type : Must be rosa - rosa_token : Required for ROSA API authentication - cluster_name : ROSA cluster name to login to Note : SECURITY - Password should be kept secure and not committed to version control. This is the cluster-admin account password set during ROSA cluster provisioning. Example Playbooks \u00a4 Direct Login \u00a4 - hosts: localhost vars: ocp_server: xxxxx ocp_token: xxxxx roles: - ibm.mas_devops.ocp_login IBMCloud ROKS \u00a4 - hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx ibmcloud_resourcegroup: mygroup roles: - ibm.mas_devops.ocp_login AWS ROSA \u00a4 - hosts: localhost vars: cluster_name: mycluster cluster_type: rosa rosa_token: xxxxx rosa_cluster_admin_password: xxxxx roles: - ibm.mas_devops.ocp_login IBM DevIT Fyre \u00a4 - hosts: localhost vars: cluster_name: mycluster cluster_type: fyre fyre_username: xxxxx fyre_password: xxxxx roles: - ibm.mas_devops.ocp_login License \u00a4 EPL-2.0","title":"ocp_login"},{"location":"roles/ocp_login/#ocp_login","text":"This role provides support to login to a cluster using the oc CLI by looking up cluster information from the infrastructure provider's APIs, it also supports setting ocp_server and ocp_token directly to support login to any Kubernetes cluster.","title":"ocp_login"},{"location":"roles/ocp_login/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_login/#cluster_name","text":"Cluster name for credential lookup. Required (unless ocp_server and ocp_token are set) Environment Variable: CLUSTER_NAME Default: None Purpose : Identifies the cluster to login to by name. Used to automatically lookup login credentials from the infrastructure provider's API. When to use : - Use with cluster_type for automatic credential lookup - Alternative to manually providing ocp_server and ocp_token - Recommended for managed clusters (ROKS, ROSA, Fyre) Valid values : String matching your cluster name in the provider's system Impact : Combined with cluster_type , automatically retrieves cluster server URL and login token from provider API. Related variables : - cluster_type : Required with this variable (roks, fyre, rosa) - ocp_server / ocp_token : Alternative manual login method Note : Requires provider-specific credentials (e.g., ibmcloud_apikey for ROKS, rosa_token for ROSA, fyre_apikey for Fyre).","title":"cluster_name"},{"location":"roles/ocp_login/#cluster_type","text":"Infrastructure provider type for cluster. Required (unless ocp_server and ocp_token are set) Environment Variable: CLUSTER_TYPE Default: None Purpose : Specifies the infrastructure provider type to determine which API to use for credential lookup. When to use : - Use with cluster_name for automatic credential lookup - Required for managed cluster login - Each type requires specific provider credentials Valid values : roks , fyre , rosa - roks : IBM Cloud Red Hat OpenShift Kubernetes Service - fyre : IBM DevIT Fyre clusters - rosa : AWS Red Hat OpenShift Service on AWS Impact : Determines which provider API is called to retrieve cluster credentials. Each type requires different authentication variables. Related variables : - cluster_name : Cluster to lookup - ibmcloud_apikey : Required for roks - fyre_username / fyre_apikey : Required for fyre - rosa_token : Required for rosa Note : Alternative to using ocp_server and ocp_token for direct login.","title":"cluster_type"},{"location":"roles/ocp_login/#ocp_server","text":"OpenShift server URL for direct login. Required (unless cluster_name and cluster_type are set) Environment Variable: OCP_SERVER Default: None Purpose : Specifies the OpenShift API server URL for direct cluster login without provider API lookup. When to use : - Use with ocp_token for direct login - When cluster is not managed by supported providers - For custom or on-premises clusters - Alternative to cluster_name / cluster_type approach Valid values : Full OpenShift API server URL (e.g., https://api.cluster.example.com:6443 ) Impact : Used directly for oc login command. Bypasses provider API credential lookup. Related variables : - ocp_token : Required with this variable - cluster_name / cluster_type : Alternative automatic lookup method Note : Both ocp_server and ocp_token must be provided together for direct login. This method works with any Kubernetes cluster.","title":"ocp_server"},{"location":"roles/ocp_login/#ocp_token","text":"Authentication token for direct login. Required (unless cluster_name and cluster_type are set) Environment Variable: OCP_TOKEN Default: None Purpose : Provides the authentication token for direct cluster login without provider API lookup. When to use : - Use with ocp_server for direct login - When cluster is not managed by supported providers - For service account tokens or manually obtained tokens - Alternative to cluster_name / cluster_type approach Valid values : Valid OpenShift authentication token string Impact : Used directly for oc login --token command. Bypasses provider API credential lookup. Related variables : - ocp_server : Required with this variable - cluster_name / cluster_type : Alternative automatic lookup method Note : SECURITY - Both ocp_server and ocp_token must be provided together. Token should be kept secure and not committed to version control. This method works with any Kubernetes cluster.","title":"ocp_token"},{"location":"roles/ocp_login/#role-variables-ibmcloud-roks","text":"","title":"Role Variables - IBMCloud ROKS"},{"location":"roles/ocp_login/#ibmcloud_apikey","text":"IBM Cloud API key for ROKS authentication. Required (when cluster_type=roks ) Environment Variable: IBMCLOUD_APIKEY Default: None Purpose : Provides IBM Cloud API key for authenticating with IBM Cloud to retrieve ROKS cluster credentials. When to use : - Required when cluster_type=roks - Used to authenticate with IBM Cloud API - Enables automatic cluster credential lookup Valid values : Valid IBM Cloud API key string Impact : Used to authenticate with IBM Cloud and retrieve cluster server URL and login token for ROKS clusters. Related variables : - cluster_type : Must be roks - cluster_name : ROKS cluster name to lookup - ibmcloud_endpoint : Optional API endpoint override Note : SECURITY - API key should be kept secure and not committed to version control. Obtain from IBM Cloud IAM.","title":"ibmcloud_apikey"},{"location":"roles/ocp_login/#ibmcloud_endpoint","text":"IBM Cloud API endpoint URL. Optional Environment Variable: IBMCLOUD_ENDPOINT Default: https://cloud.ibm.com Purpose : Overrides the default IBM Cloud API endpoint for ROKS cluster credential lookup. When to use : - Use default for public IBM Cloud - Override for private or regional endpoints - Only applies when cluster_type=roks Valid values : Valid IBM Cloud API endpoint URL Impact : Determines which IBM Cloud API endpoint is used for authentication and cluster lookup. Related variables : - cluster_type : Must be roks - ibmcloud_apikey : Required for authentication Note : The default public endpoint works for most deployments. Override only for specific regional or private cloud requirements.","title":"ibmcloud_endpoint"},{"location":"roles/ocp_login/#role-variables-ibm-devit-fyre","text":"","title":"Role Variables - IBM DevIT Fyre"},{"location":"roles/ocp_login/#fyre_username","text":"IBM DevIT Fyre username. Required (when cluster_type=fyre ) Environment Variable: FYRE_USERNAME Default: None Purpose : Provides Fyre username for authenticating with IBM DevIT Fyre to retrieve cluster credentials. When to use : - Required when cluster_type=fyre - Used with fyre_apikey for Fyre authentication - Enables automatic cluster credential lookup Valid values : Valid Fyre username string Impact : Used to authenticate with Fyre API and retrieve cluster server URL and login token. Related variables : - cluster_type : Must be fyre - fyre_apikey : Required for authentication - cluster_name : Fyre cluster name to lookup - fyre_site : Fyre site location Note : Fyre is IBM's internal development and test infrastructure. Requires IBM internal credentials.","title":"fyre_username"},{"location":"roles/ocp_login/#fyre_apikey","text":"IBM DevIT Fyre API key. Required (when cluster_type=fyre ) Environment Variable: FYRE_APIKEY Default: None Purpose : Provides Fyre API key for authenticating with IBM DevIT Fyre to retrieve cluster credentials. When to use : - Required when cluster_type=fyre - Used with fyre_username for Fyre authentication - Enables automatic cluster credential lookup Valid values : Valid Fyre API key string Impact : Used to authenticate with Fyre API and retrieve cluster server URL and login token. Related variables : - cluster_type : Must be fyre - fyre_username : Required for authentication - cluster_name : Fyre cluster name to lookup - fyre_site : Fyre site location Note : SECURITY - API key should be kept secure. Fyre is IBM's internal development and test infrastructure.","title":"fyre_apikey"},{"location":"roles/ocp_login/#fyre_site","text":"Fyre site location for cluster. Optional Environment Variable: FYRE_SITE Default: svl Purpose : Specifies which Fyre site the cluster was provisioned in for proper API routing. When to use : - Use default ( svl ) for most Fyre clusters - Override for clusters in other sites (e.g., rtp ) - Only applies when cluster_type=fyre Valid values : Fyre site codes (e.g., svl , rtp ) Impact : Determines which Fyre site API endpoint is used for cluster lookup. Related variables : - cluster_type : Must be fyre - enable_ipv6 : Required for RTP site Note : SVL (San Jose Valley) is the default site. RTP (Research Triangle Park) requires IPv6 enablement.","title":"fyre_site"},{"location":"roles/ocp_login/#enable_ipv6","text":"Enable IPv6 for Fyre RTP site. Optional Environment Variable: ENABLE_IPV6 Default: false Purpose : Enables IPv6 networking for Fyre clusters at the RTP (Research Triangle Park) site. When to use : - Set to true only for Fyre RTP site clusters - Leave as false for all other sites (including SVL) - Only applies when cluster_type=fyre Valid values : true , false Impact : Configures network settings for IPv6 connectivity to RTP site clusters. Related variables : - cluster_type : Must be fyre - fyre_site : Should be rtp when this is true Note : Only required for Fyre RTP site. SVL and other sites use IPv4.","title":"enable_ipv6"},{"location":"roles/ocp_login/#role-variables-aws-rosa","text":"","title":"Role Variables - AWS ROSA"},{"location":"roles/ocp_login/#rosa_token","text":"AWS ROSA authentication token. Required (when cluster_type=rosa ) Environment Variable: ROSA_TOKEN Default: None Purpose : Provides ROSA (Red Hat OpenShift Service on AWS) authentication token for retrieving cluster credentials. When to use : - Required when cluster_type=rosa - Used to authenticate with ROSA API - Enables automatic cluster credential lookup Valid values : Valid ROSA authentication token string Impact : Used to authenticate with ROSA API and retrieve cluster server URL and login credentials. Related variables : - cluster_type : Must be rosa - cluster_name : ROSA cluster name to lookup - rosa_cluster_admin_password : Required for cluster-admin login Note : SECURITY - Token should be kept secure and not committed to version control. Obtain from Red Hat Hybrid Cloud Console.","title":"rosa_token"},{"location":"roles/ocp_login/#rosa_cluster_admin_password","text":"ROSA cluster-admin account password. Required (when cluster_type=rosa ) Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default: None Purpose : Provides the password for the cluster-admin account to login to ROSA clusters. When to use : - Required when cluster_type=rosa - Password created during cluster provisioning - Used for cluster-admin level access Valid values : Valid cluster-admin password string Impact : Used to authenticate as cluster-admin user on ROSA clusters. Related variables : - cluster_type : Must be rosa - rosa_token : Required for ROSA API authentication - cluster_name : ROSA cluster name to login to Note : SECURITY - Password should be kept secure and not committed to version control. This is the cluster-admin account password set during ROSA cluster provisioning.","title":"rosa_cluster_admin_password"},{"location":"roles/ocp_login/#example-playbooks","text":"","title":"Example Playbooks"},{"location":"roles/ocp_login/#direct-login","text":"- hosts: localhost vars: ocp_server: xxxxx ocp_token: xxxxx roles: - ibm.mas_devops.ocp_login","title":"Direct Login"},{"location":"roles/ocp_login/#ibmcloud-roks","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx ibmcloud_resourcegroup: mygroup roles: - ibm.mas_devops.ocp_login","title":"IBMCloud ROKS"},{"location":"roles/ocp_login/#aws-rosa","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: rosa rosa_token: xxxxx rosa_cluster_admin_password: xxxxx roles: - ibm.mas_devops.ocp_login","title":"AWS ROSA"},{"location":"roles/ocp_login/#ibm-devit-fyre","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: fyre fyre_username: xxxxx fyre_password: xxxxx roles: - ibm.mas_devops.ocp_login","title":"IBM DevIT Fyre"},{"location":"roles/ocp_login/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_node_config/","text":"ocp_node_config \u00a4 Use the following command to verify the effect of any labels and taints that you apply to a node: oc get pods --all-namespaces -o wide --field-selector spec.nodeName=xxx Role Variables - Node Selection \u00a4 Use either ocp_node_name or ocp_node_index to identify the node to be modified. If you specify both then the index will take priority over the name, if you specify neither then the role will fail to execute. ocp_node_name \u00a4 The name of the node to work with Optional Environment Variable: OCP_NODE_NAME Default: None ocp_node_index \u00a4 The index (in the list of nodes) of the node to work with. Note that the index starts at 0 (so use ocp_node_index=0 if you want to work with the first node). Optional Environment Variable: OCP_NODE_INDEX Default: None Role Variables - Node Labels \u00a4 ocp_node_label_keys \u00a4 A comma-seperated list of labels to add to the selected node Optional Environment Variable: OCP_NODE_LABEL_KEYS Default: None ocp_node_label_values \u00a4 A comma-seperated list of values for the labels being created Optional Environment Variable: OCP_NODE_LABEL_VALUES Default: None Role Variables - Node Taints \u00a4 ocp_node_taint_keys \u00a4 A comma-seperated list of taints to add to the selected node Optional Environment Variable: OCP_NODE_TAINT_KEYS Default: None ocp_node_taint_values \u00a4 A comma-seperated list of values for the taints being created Optional Environment Variable: OCP_NODE_TAINT_VALUES Default: None ocp_node_taint_effects \u00a4 A comma-seperated list of taint effects to set: NoSchedule : New pods will not be scheduled onto the node PreferNoSchedule : New pods try not to be scheduled onto the node NoExecute : New pods are not schedules onto the node, existing pods are removed Optional Environment Variable: OCP_NODE_TAINT_EFFECTS Default: None Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # Turn this worker node into a dedicated Db2 worker node ocp_node_name: \"10.172.168.89\" # Add the label that will be applied to all dedicated Db2 nodes # this will be used to direct Db2 workloads to these nodes ocp_node_label_keys: workload ocp_node_label_values: db2 # Set a taint preventing anything other than Db2 workloads for masinst1 running # on this node specific Db2 node ocp_node_taint_keys: dedicatedDb2Node ocp_node_taint_values: masinst1 ocp_node_taint_effects: NoExecute roles: - ibm.mas_devops.ocp_node_config License \u00a4 EPL-2.0","title":"ocp_node_config"},{"location":"roles/ocp_node_config/#ocp_node_config","text":"Use the following command to verify the effect of any labels and taints that you apply to a node: oc get pods --all-namespaces -o wide --field-selector spec.nodeName=xxx","title":"ocp_node_config"},{"location":"roles/ocp_node_config/#role-variables-node-selection","text":"Use either ocp_node_name or ocp_node_index to identify the node to be modified. If you specify both then the index will take priority over the name, if you specify neither then the role will fail to execute.","title":"Role Variables - Node Selection"},{"location":"roles/ocp_node_config/#ocp_node_name","text":"The name of the node to work with Optional Environment Variable: OCP_NODE_NAME Default: None","title":"ocp_node_name"},{"location":"roles/ocp_node_config/#ocp_node_index","text":"The index (in the list of nodes) of the node to work with. Note that the index starts at 0 (so use ocp_node_index=0 if you want to work with the first node). Optional Environment Variable: OCP_NODE_INDEX Default: None","title":"ocp_node_index"},{"location":"roles/ocp_node_config/#role-variables-node-labels","text":"","title":"Role Variables - Node Labels"},{"location":"roles/ocp_node_config/#ocp_node_label_keys","text":"A comma-seperated list of labels to add to the selected node Optional Environment Variable: OCP_NODE_LABEL_KEYS Default: None","title":"ocp_node_label_keys"},{"location":"roles/ocp_node_config/#ocp_node_label_values","text":"A comma-seperated list of values for the labels being created Optional Environment Variable: OCP_NODE_LABEL_VALUES Default: None","title":"ocp_node_label_values"},{"location":"roles/ocp_node_config/#role-variables-node-taints","text":"","title":"Role Variables - Node Taints"},{"location":"roles/ocp_node_config/#ocp_node_taint_keys","text":"A comma-seperated list of taints to add to the selected node Optional Environment Variable: OCP_NODE_TAINT_KEYS Default: None","title":"ocp_node_taint_keys"},{"location":"roles/ocp_node_config/#ocp_node_taint_values","text":"A comma-seperated list of values for the taints being created Optional Environment Variable: OCP_NODE_TAINT_VALUES Default: None","title":"ocp_node_taint_values"},{"location":"roles/ocp_node_config/#ocp_node_taint_effects","text":"A comma-seperated list of taint effects to set: NoSchedule : New pods will not be scheduled onto the node PreferNoSchedule : New pods try not to be scheduled onto the node NoExecute : New pods are not schedules onto the node, existing pods are removed Optional Environment Variable: OCP_NODE_TAINT_EFFECTS Default: None","title":"ocp_node_taint_effects"},{"location":"roles/ocp_node_config/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Turn this worker node into a dedicated Db2 worker node ocp_node_name: \"10.172.168.89\" # Add the label that will be applied to all dedicated Db2 nodes # this will be used to direct Db2 workloads to these nodes ocp_node_label_keys: workload ocp_node_label_values: db2 # Set a taint preventing anything other than Db2 workloads for masinst1 running # on this node specific Db2 node ocp_node_taint_keys: dedicatedDb2Node ocp_node_taint_values: masinst1 ocp_node_taint_effects: NoExecute roles: - ibm.mas_devops.ocp_node_config","title":"Example Playbook"},{"location":"roles/ocp_node_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_provision/","text":"ocp_provision \u00a4 Provision OCP cluster on IBM Cloud ROKS, ROSA, or DevIT Fyre. Fyre clusters will be automatically reconfigured to enable NFS storage. By default this is made available via the nfs-client storage class and supports both ReadWriteOnce and ReadWriteMany access modes. The image-registry-storage PVC used by the OpenShift image registry component will also be reconfigured to use this storage class. Role Variables \u00a4 cluster_type \u00a4 Infrastructure provider type for cluster provisioning. Required Environment Variable: CLUSTER_TYPE Default: None Purpose : Specifies which infrastructure provider to use for provisioning the OpenShift cluster. Determines provisioning method and required variables. When to use : - Always required for cluster provisioning - Each type requires different provider-specific variables - Determines available features (e.g., GPU support for ROKS) Valid values : fyre , roks , rosa , ipi - fyre : IBM DevIT Fyre clusters (internal development) - roks : IBM Cloud Red Hat OpenShift Kubernetes Service - rosa : AWS Red Hat OpenShift Service on AWS - ipi : Installer-Provisioned Infrastructure (bare metal/on-premises) Impact : Determines provisioning workflow and which provider-specific variables are required. Each type has different capabilities and configuration options. Related variables : - cluster_name : Name for the new cluster - ocp_version : OpenShift version to install - Provider-specific variables (ibmcloud_apikey, rosa_token, fyre_apikey, etc.) Note : Fyre clusters automatically configure NFS storage. ROKS requires version format like 4.19_openshift . cluster_name \u00a4 Name for the new cluster. Required Environment Variable: CLUSTER_NAME Default: None Purpose : Specifies the name for the OpenShift cluster to be provisioned. Used as the cluster identifier in the provider's system. When to use : - Always required for cluster provisioning - Must be unique within the provider's account/region - Used for cluster identification and resource naming Valid values : String following provider naming conventions (typically lowercase alphanumeric with hyphens) Impact : Determines the cluster name in the provider's system. Used for DNS, resource naming, and cluster identification. Related variables : - cluster_type : Provider where cluster will be created - ocp_version : OpenShift version for the cluster Note : Name must follow provider-specific naming rules. Some providers have length limits or character restrictions. ocp_version \u00a4 OpenShift version to install. Required Environment Variable: OCP_VERSION Default: None Purpose : Specifies which version of OpenShift Container Platform to install on the provisioned cluster. When to use : - Always required for cluster provisioning - Use specific version for production (e.g., 4.19.14 ) - Use default for latest MAS-supported version - Use rotate for testing (version changes by day of week) Valid values : - Specific version: 4.19 , 4.19.14 - Alias: default (newest MAS-supported version) - Alias: rotate (predetermined version by day, for testing) - ROKS format : Must append _openshift (e.g., 4.19_openshift , 4.19.14_openshift ) Impact : Determines OpenShift version installed. Version must be compatible with MAS and available from the provider. Related variables : - cluster_type : ROKS requires _openshift suffix Note : IMPORTANT - For ROKS ( cluster_type=roks ), version MUST include _openshift suffix. The default alias selects the newest MAS-supported version. The rotate alias is for testing only. ocp_storage_provider \u00a4 Storage provider configuration for Fyre clusters. Optional Environment Variable: OCP_STORAGE_PROVIDER Default: None Purpose : Configures NFS storage for Fyre clusters, creating an nfs-client storage class and reconfiguring the image registry. When to use : - Set to nfs for Fyre clusters to enable NFS storage - Only applies when cluster_type=fyre - Leave unset for other cluster types (ROKS, ROSA, IPI) Valid values : nfs (for Fyre clusters only) Impact : - nfs : Creates nfs-client storage class connected to infrastructure node, reconfigures image registry PVC to use NFS - Unset: No storage configuration changes Related variables : - cluster_type : Must be fyre for this to have effect Note : Only functional for Fyre clusters. When enabled, the existing image registry PVC is deleted and recreated with NFS storage. NFS storage class supports both ReadWriteOnce (RWO) and ReadWriteMany (RWX) access modes. Role Variables - GPU Node Support \u00a4 ocp_provision_gpu \u00a4 Enable GPU worker nodes during provisioning. Optional Environment Variable: OCP_PROVISION_GPU Default: false Purpose : Controls whether GPU-enabled worker nodes are provisioned with the cluster. Required for GPU-intensive applications like MAS Visual Inspection (MVI). When to use : - Set to true for MAS Visual Inspection deployments - Set to true for other GPU-intensive workloads - Leave as false (default) for standard deployments - Currently only supported for ROKS clusters Valid values : true , false Impact : - true : Provisions GPU worker pool with specified number of GPU nodes - false : No GPU nodes provisioned (standard cluster) Related variables : - gpu_workerpool_name : Name of GPU worker pool - gpu_workers : Number of GPU nodes to provision - cluster_type : Must be roks for GPU support Note : GPU support is currently only available for ROKS clusters. GPU nodes use mg4c.32x384.2xp100-GPU flavor with P100 GPUs. gpu_workerpool_name \u00a4 Name for GPU worker pool. Optional Environment Variable: GPU_WORKERPOOL_NAME Default: gpu Purpose : Specifies the name for the GPU worker pool to be created or modified in the cluster. When to use : - Use default ( gpu ) for new GPU deployments - Set to existing pool name to modify rather than create new - Only applies when ocp_provision_gpu=true Valid values : String following worker pool naming conventions Impact : Determines GPU worker pool name. Using an existing name modifies that pool; using a new name creates a new pool. Related variables : - ocp_provision_gpu : Must be true for this to apply - gpu_workers : Number of nodes in this pool - cluster_type : Must be roks Note : If a GPU worker pool already exists with this name, it will be modified rather than creating a duplicate. Use the existing name to avoid multiple GPU pools. gpu_workers \u00a4 The number of GPU worker nodes that will be deploy in the cluster. The node created will have mg4c.32x384.2xp100-GPU flavor. This variable depends on ocp_provision_gpu and is currently supported on ROKS clusters only. Optional Environment Variable: GPU_WORKERS Default Value: 1 compute_node_count \u00a4 The number of compute nodes (i.e. worker nodes) allocate to the OCP cluster. Optional Environment Variable: COMPUTE_NODE_COUNT Default Value: 3 controlplane_node_count \u00a4 The number of control plane nodes (i.e. master nodes) allocate to the OCP cluster. Optional Environment Variable: CONTROLPLANE_NODE_COUNT Default Value: 3 gpu_workerpool_name \u00a4 The name of the gpu worker pool to added to or modify in the cluster. If already existing, use the existing name to avoid recreating another gpu worker pool unless that is the goal. Environment Variable: GPU_WORKERPOOL_NAME Default Value: gpu Role Variables - ROKS \u00a4 The following variables are only used when cluster_type = roks . ibmcloud_apikey \u00a4 The APIKey to be used by ibmcloud login comand. Required if cluster_type = roks Environment Variable: IBMCLOUD_APIKEY Default Value: None ibmcloud_endpoint \u00a4 Override the default IBMCloud API endpoint. Optional Environment Variable: IBMCLOUD_ENDPOINT Default Value: https://cloud.ibm.com ibmcloud_resourcegroup \u00a4 The resource group to create the cluster inside. Optional Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default roks_zone \u00a4 IBM Cloud zone where the cluster should be provisioned. Optional Environment Variable: ROKS_ZONE Default Value: dal10 roks_flavor \u00a4 Worker node flavor Optional Environment Variable: ROKS_FLAVOR Default Value: b3c.16x64.300gb roks_workers \u00a4 Number of worker nodes for the roks cluster Optional Environment Variable: ROKS_WORKERS Default Value: 3 roks_flags \u00a4 Can be used to specify additional parameters for the cluster creation Optional Environment Variable: ROKS_FLAGS Default Value: None Role Variables - ROSA \u00a4 The following variables are only used when cluster_type = rosa . rosa_token \u00a4 Token to authenticate to the ROSA service. To obtain your API token login to the OpenShift cluster manager . Required if cluster_type = rosa . Environment Variable: ROSA_TOKEN Default Value: None rosa_cluster_admin_password \u00a4 Password to set up for the cluster-admin user account on the OCP instance. You will need this to log onto the cluster after it is provisioned. If this is not set then a password is auto-generated. Optional if cluster_type = rosa . Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default Value: None rosa_compute_nodes \u00a4 Number of compute nodes to deploy in the cluster. Optional Environment Variable: ROSA_COMPUTE_NODES Default Value: 3 rosa_compute_machine_type \u00a4 Worker nodes machine Optional Environment Variable: ROSA_COMPUTE_MACHINE_TYPE Default Value: m5.4xlarge rosa_config_dir \u00a4 Config directory to hold the rosa-{{cluster_name}}-details.yaml file that contains the api endpoint and cluster-admin details Optional Environment Variable: ROSA_CONFIG_DIR Default Value: None Role Variables - FYRE \u00a4 The following variables are only used when cluster_type = fyre . fyre_username \u00a4 Username to authenticate with Fyre API. Required if cluster_type = fyre . Environment Variable: FYRE_USERNAME Default Value: None fyre_apikey \u00a4 API key to authenticate with Fyre API. Required if cluster_type = fyre . Environment Variable: FYRE_APIKEY Default Value: None fyre_quota_type \u00a4 Type of quota to draw from when provisioning the cluster, valid options are quick_burn and product_group . Required if cluster_type = fyre . Environment Variable: FYRE_QUOTA_TYPE Default Value: quick_burn fyre_product_id \u00a4 The Product ID that the cluster will be associated with for accounting purposes. Required if cluster_type = fyre . Environment Variable: FYRE_PRODUCT_ID Default Value: None fyre_site \u00a4 Provide a site in Fyre where cluster will be provisioned Optional Environment Variable: FYRE_SITE Default Value: svl fyre_cluster_description \u00a4 Provide a description for the cluster. Optional Environment Variable: FYRE_CLUSTER_DESCRIPTION Default Value: None ocp_fips_enabled \u00a4 Set to true to provision a FIPS enabled cluster. Optional Environment Variable: OCP_FIPS_ENABLED Default Value: false fyre_cluster_size \u00a4 The name of one of Fyre's pre-defined cluster sizes to use for the new cluster. Required when cluster_type = fyre and fyre_quota_type = quick_burn . Environment Variable: FYRE_CLUSTER_SIZE Default Value: medium fyre_worker_count \u00a4 The number of worker nodes to provision in the cluster. Required when cluster_type = fyre and fyre_quota_type = product_group . Environment Variable: FYRE_WORKER_COUNT Default Value: 2 fyre_worker_cpu \u00a4 The amount of CPU to assign to each worker node (maximum value supported by FYRE 16). Required when cluster_type = fyre and fyre_quota_type = product_group . Environment Variable: FYRE_WORKER_CPU Default Value: 8 fyre_worker_memory \u00a4 The amount of memory to assign to each worker node (maximum value supported by FYRE 64). Required when cluster_type = fyre and fyre_quota_type = product_group . Environment Variable: FYRE_WORKER_MEMORY Default Value: 32 fyre_worker_additional_disks \u00a4 The size of additional disks in Gb added to each worker node, defined in a comma-seperated list, e.g. 400,400 will add two 400gb disks to each worker node. By default no additional disks will be attached. Optional Environment Variable: FYRE_WORKER_ADDITIONAL_DISKS Default Value: None fyre_nfs_image_registry_size \u00a4 Defines the image registry storage size when configured to use NFS. The size allocated cannot be superior of storage available in the Fyre Infrastructure node. Optional Environment Variable: FYRE_NFS_IMAGE_REGISTRY_SIZE Default: 100Gi enable_ipv6 \u00a4 Enable IPv6. This is for Fyre at RTP site only. Environment Variable: ENABLE_IPV6 Default: False Role Variables - IPI \u00a4 These variables are only used when cluster_type = ipi . Note IPI stands for Installer Provisioned Infrastructure . OpenShift offers two possible deployment methods: IPI and UPI (User Provisioned Infrastructure). The difference is the degree of automation and customization. IPI will not only deploy OpenShift but also all infrastructure components and configurations. ipi_platform \u00a4 Platform to create the cluster on. Technically, any platform supported by openshift-install should work here, but currently we have only specifically tested on aws and gcp , where aws is the default value. Optional when cluster_type = ipi Environment Variable: IPI_PLATFORM Default Value: aws ipi_region \u00a4 Platform region where OCP cluster will be created. Optional when cluster_type = ipi Environment Variable: IPI_REGION Default Value: us-east-1 ipi_base_domain \u00a4 Specify the base domain of the cluster that will be provisioned. Required when cluster_type = ipi Environment Variable: IPI_BASE_DOMAIN Default Value: None ipi_pull_secret_file \u00a4 Location of the file containing your Redhat OpenShift pull secret. This file can be obtained from the Red Hat Hybrid Cloud Console Required when cluster_type = ipi Environment Variable: IPI_PULL_SECRET_FILE Default Value: None ipi_dir \u00a4 The working directory that is used to perform the installation, it will contain the openshift-install executable, its configuration files, & any generated logs. Optional when cluster_type = ipi Environment Variable: IPI_DIR Default Value: ~/openshift-install sshKey \u00a4 Public SSH key value. It will be set in the OCP cluster nodes. Can be used to SSH into the OCP cluster nodes using a bastion. Optional when cluster_type = ipi Environment Variable: SSH_PUB_KEY ipi_controlplane_type \u00a4 Control plane node type. Optional when cluster_type = ipi Environment Variable: IPI_CONTROLPLANE_TYPE Default Value: m5.4xlarge ipi_controlplane_replicas \u00a4 The number of master nodes to provision to form the control plane of your cluster. Optional when cluster_type = ipi Environment Variable: IPI_CONTROLPLANE_REPLICAS Default Value: 3 ipi_compute_type \u00a4 Compute node type. Optional when cluster_type = ipi Environment Variable: IPI_COMPUTE_TYPE Default Value: m5.4xlarge ipi_compute_replicas \u00a4 The number of worker nodes to provsision in the cluster, providing your compute resource. Optional when cluster_type = ipi Environment Variable: IPI_COMPUTE_REPLICAS Default Value: 3 ipi_rootvolume_size \u00a4 The size of root volume in GiB. Optional when cluster_type = ipi Environment variable: IPI_ROOTVOLUME_SIZE Role Variables - AWS \u00a4 The following variables are only used when cluster_type = ipi and ipi_platform = aws . aws_access_key_id \u00a4 AWS access key associated with an IAM user or role. Make sure the access key has permissions to create instances. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None aws_secret_access_key \u00a4 AWS secret access key associated with an IAM user or role. Required when cluster_type = aws-ipi and ipi_platform = aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None Role Variables - GCP \u00a4 The following variables are only used when cluster_type = ipi and ipi_platform = gcp . gcp_service_account_file \u00a4 GCP service account file path. Make sure the service account has permissions to create instances. Required when cluster_type = ipi and ipi_platform = gcp Environment Variable: GOOGLE_APPLICATION_CREDENTIALS Default Value: None ipi_gcp_projectid \u00a4 GCP project id in which the cluster will be deployed. Required when cluster_type = ipi and ipi_platform = gcp Environment Variable: GOOGLE_PROJECTID Default Value: None Example Playbook \u00a4 - hosts: localhost vars: cluster_type: roks cluster_name: mycluster ocp_version: 4.10 ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_provision License \u00a4 EPL-2.0","title":"ocp_provision"},{"location":"roles/ocp_provision/#ocp_provision","text":"Provision OCP cluster on IBM Cloud ROKS, ROSA, or DevIT Fyre. Fyre clusters will be automatically reconfigured to enable NFS storage. By default this is made available via the nfs-client storage class and supports both ReadWriteOnce and ReadWriteMany access modes. The image-registry-storage PVC used by the OpenShift image registry component will also be reconfigured to use this storage class.","title":"ocp_provision"},{"location":"roles/ocp_provision/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_provision/#cluster_type","text":"Infrastructure provider type for cluster provisioning. Required Environment Variable: CLUSTER_TYPE Default: None Purpose : Specifies which infrastructure provider to use for provisioning the OpenShift cluster. Determines provisioning method and required variables. When to use : - Always required for cluster provisioning - Each type requires different provider-specific variables - Determines available features (e.g., GPU support for ROKS) Valid values : fyre , roks , rosa , ipi - fyre : IBM DevIT Fyre clusters (internal development) - roks : IBM Cloud Red Hat OpenShift Kubernetes Service - rosa : AWS Red Hat OpenShift Service on AWS - ipi : Installer-Provisioned Infrastructure (bare metal/on-premises) Impact : Determines provisioning workflow and which provider-specific variables are required. Each type has different capabilities and configuration options. Related variables : - cluster_name : Name for the new cluster - ocp_version : OpenShift version to install - Provider-specific variables (ibmcloud_apikey, rosa_token, fyre_apikey, etc.) Note : Fyre clusters automatically configure NFS storage. ROKS requires version format like 4.19_openshift .","title":"cluster_type"},{"location":"roles/ocp_provision/#cluster_name","text":"Name for the new cluster. Required Environment Variable: CLUSTER_NAME Default: None Purpose : Specifies the name for the OpenShift cluster to be provisioned. Used as the cluster identifier in the provider's system. When to use : - Always required for cluster provisioning - Must be unique within the provider's account/region - Used for cluster identification and resource naming Valid values : String following provider naming conventions (typically lowercase alphanumeric with hyphens) Impact : Determines the cluster name in the provider's system. Used for DNS, resource naming, and cluster identification. Related variables : - cluster_type : Provider where cluster will be created - ocp_version : OpenShift version for the cluster Note : Name must follow provider-specific naming rules. Some providers have length limits or character restrictions.","title":"cluster_name"},{"location":"roles/ocp_provision/#ocp_version","text":"OpenShift version to install. Required Environment Variable: OCP_VERSION Default: None Purpose : Specifies which version of OpenShift Container Platform to install on the provisioned cluster. When to use : - Always required for cluster provisioning - Use specific version for production (e.g., 4.19.14 ) - Use default for latest MAS-supported version - Use rotate for testing (version changes by day of week) Valid values : - Specific version: 4.19 , 4.19.14 - Alias: default (newest MAS-supported version) - Alias: rotate (predetermined version by day, for testing) - ROKS format : Must append _openshift (e.g., 4.19_openshift , 4.19.14_openshift ) Impact : Determines OpenShift version installed. Version must be compatible with MAS and available from the provider. Related variables : - cluster_type : ROKS requires _openshift suffix Note : IMPORTANT - For ROKS ( cluster_type=roks ), version MUST include _openshift suffix. The default alias selects the newest MAS-supported version. The rotate alias is for testing only.","title":"ocp_version"},{"location":"roles/ocp_provision/#ocp_storage_provider","text":"Storage provider configuration for Fyre clusters. Optional Environment Variable: OCP_STORAGE_PROVIDER Default: None Purpose : Configures NFS storage for Fyre clusters, creating an nfs-client storage class and reconfiguring the image registry. When to use : - Set to nfs for Fyre clusters to enable NFS storage - Only applies when cluster_type=fyre - Leave unset for other cluster types (ROKS, ROSA, IPI) Valid values : nfs (for Fyre clusters only) Impact : - nfs : Creates nfs-client storage class connected to infrastructure node, reconfigures image registry PVC to use NFS - Unset: No storage configuration changes Related variables : - cluster_type : Must be fyre for this to have effect Note : Only functional for Fyre clusters. When enabled, the existing image registry PVC is deleted and recreated with NFS storage. NFS storage class supports both ReadWriteOnce (RWO) and ReadWriteMany (RWX) access modes.","title":"ocp_storage_provider"},{"location":"roles/ocp_provision/#role-variables-gpu-node-support","text":"","title":"Role Variables - GPU Node Support"},{"location":"roles/ocp_provision/#ocp_provision_gpu","text":"Enable GPU worker nodes during provisioning. Optional Environment Variable: OCP_PROVISION_GPU Default: false Purpose : Controls whether GPU-enabled worker nodes are provisioned with the cluster. Required for GPU-intensive applications like MAS Visual Inspection (MVI). When to use : - Set to true for MAS Visual Inspection deployments - Set to true for other GPU-intensive workloads - Leave as false (default) for standard deployments - Currently only supported for ROKS clusters Valid values : true , false Impact : - true : Provisions GPU worker pool with specified number of GPU nodes - false : No GPU nodes provisioned (standard cluster) Related variables : - gpu_workerpool_name : Name of GPU worker pool - gpu_workers : Number of GPU nodes to provision - cluster_type : Must be roks for GPU support Note : GPU support is currently only available for ROKS clusters. GPU nodes use mg4c.32x384.2xp100-GPU flavor with P100 GPUs.","title":"ocp_provision_gpu"},{"location":"roles/ocp_provision/#gpu_workerpool_name","text":"Name for GPU worker pool. Optional Environment Variable: GPU_WORKERPOOL_NAME Default: gpu Purpose : Specifies the name for the GPU worker pool to be created or modified in the cluster. When to use : - Use default ( gpu ) for new GPU deployments - Set to existing pool name to modify rather than create new - Only applies when ocp_provision_gpu=true Valid values : String following worker pool naming conventions Impact : Determines GPU worker pool name. Using an existing name modifies that pool; using a new name creates a new pool. Related variables : - ocp_provision_gpu : Must be true for this to apply - gpu_workers : Number of nodes in this pool - cluster_type : Must be roks Note : If a GPU worker pool already exists with this name, it will be modified rather than creating a duplicate. Use the existing name to avoid multiple GPU pools.","title":"gpu_workerpool_name"},{"location":"roles/ocp_provision/#gpu_workers","text":"The number of GPU worker nodes that will be deploy in the cluster. The node created will have mg4c.32x384.2xp100-GPU flavor. This variable depends on ocp_provision_gpu and is currently supported on ROKS clusters only. Optional Environment Variable: GPU_WORKERS Default Value: 1","title":"gpu_workers"},{"location":"roles/ocp_provision/#compute_node_count","text":"The number of compute nodes (i.e. worker nodes) allocate to the OCP cluster. Optional Environment Variable: COMPUTE_NODE_COUNT Default Value: 3","title":"compute_node_count"},{"location":"roles/ocp_provision/#controlplane_node_count","text":"The number of control plane nodes (i.e. master nodes) allocate to the OCP cluster. Optional Environment Variable: CONTROLPLANE_NODE_COUNT Default Value: 3","title":"controlplane_node_count"},{"location":"roles/ocp_provision/#gpu_workerpool_name_1","text":"The name of the gpu worker pool to added to or modify in the cluster. If already existing, use the existing name to avoid recreating another gpu worker pool unless that is the goal. Environment Variable: GPU_WORKERPOOL_NAME Default Value: gpu","title":"gpu_workerpool_name"},{"location":"roles/ocp_provision/#role-variables-roks","text":"The following variables are only used when cluster_type = roks .","title":"Role Variables - ROKS"},{"location":"roles/ocp_provision/#ibmcloud_apikey","text":"The APIKey to be used by ibmcloud login comand. Required if cluster_type = roks Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/ocp_provision/#ibmcloud_endpoint","text":"Override the default IBMCloud API endpoint. Optional Environment Variable: IBMCLOUD_ENDPOINT Default Value: https://cloud.ibm.com","title":"ibmcloud_endpoint"},{"location":"roles/ocp_provision/#ibmcloud_resourcegroup","text":"The resource group to create the cluster inside. Optional Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/ocp_provision/#roks_zone","text":"IBM Cloud zone where the cluster should be provisioned. Optional Environment Variable: ROKS_ZONE Default Value: dal10","title":"roks_zone"},{"location":"roles/ocp_provision/#roks_flavor","text":"Worker node flavor Optional Environment Variable: ROKS_FLAVOR Default Value: b3c.16x64.300gb","title":"roks_flavor"},{"location":"roles/ocp_provision/#roks_workers","text":"Number of worker nodes for the roks cluster Optional Environment Variable: ROKS_WORKERS Default Value: 3","title":"roks_workers"},{"location":"roles/ocp_provision/#roks_flags","text":"Can be used to specify additional parameters for the cluster creation Optional Environment Variable: ROKS_FLAGS Default Value: None","title":"roks_flags"},{"location":"roles/ocp_provision/#role-variables-rosa","text":"The following variables are only used when cluster_type = rosa .","title":"Role Variables - ROSA"},{"location":"roles/ocp_provision/#rosa_token","text":"Token to authenticate to the ROSA service. To obtain your API token login to the OpenShift cluster manager . Required if cluster_type = rosa . Environment Variable: ROSA_TOKEN Default Value: None","title":"rosa_token"},{"location":"roles/ocp_provision/#rosa_cluster_admin_password","text":"Password to set up for the cluster-admin user account on the OCP instance. You will need this to log onto the cluster after it is provisioned. If this is not set then a password is auto-generated. Optional if cluster_type = rosa . Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default Value: None","title":"rosa_cluster_admin_password"},{"location":"roles/ocp_provision/#rosa_compute_nodes","text":"Number of compute nodes to deploy in the cluster. Optional Environment Variable: ROSA_COMPUTE_NODES Default Value: 3","title":"rosa_compute_nodes"},{"location":"roles/ocp_provision/#rosa_compute_machine_type","text":"Worker nodes machine Optional Environment Variable: ROSA_COMPUTE_MACHINE_TYPE Default Value: m5.4xlarge","title":"rosa_compute_machine_type"},{"location":"roles/ocp_provision/#rosa_config_dir","text":"Config directory to hold the rosa-{{cluster_name}}-details.yaml file that contains the api endpoint and cluster-admin details Optional Environment Variable: ROSA_CONFIG_DIR Default Value: None","title":"rosa_config_dir"},{"location":"roles/ocp_provision/#role-variables-fyre","text":"The following variables are only used when cluster_type = fyre .","title":"Role Variables - FYRE"},{"location":"roles/ocp_provision/#fyre_username","text":"Username to authenticate with Fyre API. Required if cluster_type = fyre . Environment Variable: FYRE_USERNAME Default Value: None","title":"fyre_username"},{"location":"roles/ocp_provision/#fyre_apikey","text":"API key to authenticate with Fyre API. Required if cluster_type = fyre . Environment Variable: FYRE_APIKEY Default Value: None","title":"fyre_apikey"},{"location":"roles/ocp_provision/#fyre_quota_type","text":"Type of quota to draw from when provisioning the cluster, valid options are quick_burn and product_group . Required if cluster_type = fyre . Environment Variable: FYRE_QUOTA_TYPE Default Value: quick_burn","title":"fyre_quota_type"},{"location":"roles/ocp_provision/#fyre_product_id","text":"The Product ID that the cluster will be associated with for accounting purposes. Required if cluster_type = fyre . Environment Variable: FYRE_PRODUCT_ID Default Value: None","title":"fyre_product_id"},{"location":"roles/ocp_provision/#fyre_site","text":"Provide a site in Fyre where cluster will be provisioned Optional Environment Variable: FYRE_SITE Default Value: svl","title":"fyre_site"},{"location":"roles/ocp_provision/#fyre_cluster_description","text":"Provide a description for the cluster. Optional Environment Variable: FYRE_CLUSTER_DESCRIPTION Default Value: None","title":"fyre_cluster_description"},{"location":"roles/ocp_provision/#ocp_fips_enabled","text":"Set to true to provision a FIPS enabled cluster. Optional Environment Variable: OCP_FIPS_ENABLED Default Value: false","title":"ocp_fips_enabled"},{"location":"roles/ocp_provision/#fyre_cluster_size","text":"The name of one of Fyre's pre-defined cluster sizes to use for the new cluster. Required when cluster_type = fyre and fyre_quota_type = quick_burn . Environment Variable: FYRE_CLUSTER_SIZE Default Value: medium","title":"fyre_cluster_size"},{"location":"roles/ocp_provision/#fyre_worker_count","text":"The number of worker nodes to provision in the cluster. Required when cluster_type = fyre and fyre_quota_type = product_group . Environment Variable: FYRE_WORKER_COUNT Default Value: 2","title":"fyre_worker_count"},{"location":"roles/ocp_provision/#fyre_worker_cpu","text":"The amount of CPU to assign to each worker node (maximum value supported by FYRE 16). Required when cluster_type = fyre and fyre_quota_type = product_group . Environment Variable: FYRE_WORKER_CPU Default Value: 8","title":"fyre_worker_cpu"},{"location":"roles/ocp_provision/#fyre_worker_memory","text":"The amount of memory to assign to each worker node (maximum value supported by FYRE 64). Required when cluster_type = fyre and fyre_quota_type = product_group . Environment Variable: FYRE_WORKER_MEMORY Default Value: 32","title":"fyre_worker_memory"},{"location":"roles/ocp_provision/#fyre_worker_additional_disks","text":"The size of additional disks in Gb added to each worker node, defined in a comma-seperated list, e.g. 400,400 will add two 400gb disks to each worker node. By default no additional disks will be attached. Optional Environment Variable: FYRE_WORKER_ADDITIONAL_DISKS Default Value: None","title":"fyre_worker_additional_disks"},{"location":"roles/ocp_provision/#fyre_nfs_image_registry_size","text":"Defines the image registry storage size when configured to use NFS. The size allocated cannot be superior of storage available in the Fyre Infrastructure node. Optional Environment Variable: FYRE_NFS_IMAGE_REGISTRY_SIZE Default: 100Gi","title":"fyre_nfs_image_registry_size"},{"location":"roles/ocp_provision/#enable_ipv6","text":"Enable IPv6. This is for Fyre at RTP site only. Environment Variable: ENABLE_IPV6 Default: False","title":"enable_ipv6"},{"location":"roles/ocp_provision/#role-variables-ipi","text":"These variables are only used when cluster_type = ipi . Note IPI stands for Installer Provisioned Infrastructure . OpenShift offers two possible deployment methods: IPI and UPI (User Provisioned Infrastructure). The difference is the degree of automation and customization. IPI will not only deploy OpenShift but also all infrastructure components and configurations.","title":"Role Variables - IPI"},{"location":"roles/ocp_provision/#ipi_platform","text":"Platform to create the cluster on. Technically, any platform supported by openshift-install should work here, but currently we have only specifically tested on aws and gcp , where aws is the default value. Optional when cluster_type = ipi Environment Variable: IPI_PLATFORM Default Value: aws","title":"ipi_platform"},{"location":"roles/ocp_provision/#ipi_region","text":"Platform region where OCP cluster will be created. Optional when cluster_type = ipi Environment Variable: IPI_REGION Default Value: us-east-1","title":"ipi_region"},{"location":"roles/ocp_provision/#ipi_base_domain","text":"Specify the base domain of the cluster that will be provisioned. Required when cluster_type = ipi Environment Variable: IPI_BASE_DOMAIN Default Value: None","title":"ipi_base_domain"},{"location":"roles/ocp_provision/#ipi_pull_secret_file","text":"Location of the file containing your Redhat OpenShift pull secret. This file can be obtained from the Red Hat Hybrid Cloud Console Required when cluster_type = ipi Environment Variable: IPI_PULL_SECRET_FILE Default Value: None","title":"ipi_pull_secret_file"},{"location":"roles/ocp_provision/#ipi_dir","text":"The working directory that is used to perform the installation, it will contain the openshift-install executable, its configuration files, & any generated logs. Optional when cluster_type = ipi Environment Variable: IPI_DIR Default Value: ~/openshift-install","title":"ipi_dir"},{"location":"roles/ocp_provision/#sshkey","text":"Public SSH key value. It will be set in the OCP cluster nodes. Can be used to SSH into the OCP cluster nodes using a bastion. Optional when cluster_type = ipi Environment Variable: SSH_PUB_KEY","title":"sshKey"},{"location":"roles/ocp_provision/#ipi_controlplane_type","text":"Control plane node type. Optional when cluster_type = ipi Environment Variable: IPI_CONTROLPLANE_TYPE Default Value: m5.4xlarge","title":"ipi_controlplane_type"},{"location":"roles/ocp_provision/#ipi_controlplane_replicas","text":"The number of master nodes to provision to form the control plane of your cluster. Optional when cluster_type = ipi Environment Variable: IPI_CONTROLPLANE_REPLICAS Default Value: 3","title":"ipi_controlplane_replicas"},{"location":"roles/ocp_provision/#ipi_compute_type","text":"Compute node type. Optional when cluster_type = ipi Environment Variable: IPI_COMPUTE_TYPE Default Value: m5.4xlarge","title":"ipi_compute_type"},{"location":"roles/ocp_provision/#ipi_compute_replicas","text":"The number of worker nodes to provsision in the cluster, providing your compute resource. Optional when cluster_type = ipi Environment Variable: IPI_COMPUTE_REPLICAS Default Value: 3","title":"ipi_compute_replicas"},{"location":"roles/ocp_provision/#ipi_rootvolume_size","text":"The size of root volume in GiB. Optional when cluster_type = ipi Environment variable: IPI_ROOTVOLUME_SIZE","title":"ipi_rootvolume_size"},{"location":"roles/ocp_provision/#role-variables-aws","text":"The following variables are only used when cluster_type = ipi and ipi_platform = aws .","title":"Role Variables - AWS"},{"location":"roles/ocp_provision/#aws_access_key_id","text":"AWS access key associated with an IAM user or role. Make sure the access key has permissions to create instances. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None","title":"aws_access_key_id"},{"location":"roles/ocp_provision/#aws_secret_access_key","text":"AWS secret access key associated with an IAM user or role. Required when cluster_type = aws-ipi and ipi_platform = aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None","title":"aws_secret_access_key"},{"location":"roles/ocp_provision/#role-variables-gcp","text":"The following variables are only used when cluster_type = ipi and ipi_platform = gcp .","title":"Role Variables - GCP"},{"location":"roles/ocp_provision/#gcp_service_account_file","text":"GCP service account file path. Make sure the service account has permissions to create instances. Required when cluster_type = ipi and ipi_platform = gcp Environment Variable: GOOGLE_APPLICATION_CREDENTIALS Default Value: None","title":"gcp_service_account_file"},{"location":"roles/ocp_provision/#ipi_gcp_projectid","text":"GCP project id in which the cluster will be deployed. Required when cluster_type = ipi and ipi_platform = gcp Environment Variable: GOOGLE_PROJECTID Default Value: None","title":"ipi_gcp_projectid"},{"location":"roles/ocp_provision/#example-playbook","text":"- hosts: localhost vars: cluster_type: roks cluster_name: mycluster ocp_version: 4.10 ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_provision","title":"Example Playbook"},{"location":"roles/ocp_provision/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_roks_upgrade_registry_storage/","text":"ocp_roks_upgrade_registry_storage \u00a4 This role will use IBMCloud APIs to upgrade the capacity of the volume backing the OCP cluster's image registry. The volume will be increased from the default capacity of 100GB to 400GB. This is needed if you intend to install all of the services available in CloudPak for Data because the 100GB volume is not large enough. Role Variables \u00a4 ibmcloud_apikey \u00a4 The APIKey to be used to modify the storage volume associated with the image registry. Environment Variable: IBMCLOUD_APIKEY Default Value: None Example Playbook \u00a4 - hosts: localhost roles: - ibm.mas_devops.ocp_roks_tuning License \u00a4 EPL-2.0","title":"ocp_roks_upgrade_registry_storage"},{"location":"roles/ocp_roks_upgrade_registry_storage/#ocp_roks_upgrade_registry_storage","text":"This role will use IBMCloud APIs to upgrade the capacity of the volume backing the OCP cluster's image registry. The volume will be increased from the default capacity of 100GB to 400GB. This is needed if you intend to install all of the services available in CloudPak for Data because the 100GB volume is not large enough.","title":"ocp_roks_upgrade_registry_storage"},{"location":"roles/ocp_roks_upgrade_registry_storage/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_roks_upgrade_registry_storage/#ibmcloud_apikey","text":"The APIKey to be used to modify the storage volume associated with the image registry. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/ocp_roks_upgrade_registry_storage/#example-playbook","text":"- hosts: localhost roles: - ibm.mas_devops.ocp_roks_tuning","title":"Example Playbook"},{"location":"roles/ocp_roks_upgrade_registry_storage/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_simulate_disconnected_network/","text":"ocp_simulate_disconnected_network \u00a4 Our goal is to modify the hosts file on each node (worker and master) to add a bogus entry that breaks DNS resolution for all the registries that we are going to mirror. This will simulate the cluster running in an air gap configuration, although network access will be possible elsewhere, the cluster will be unable to access the docker registries Important The host file (on Fyre) will look something like this, this role is (for now anyway) mainly focused on simulating an air gap cluster in Fyre, it may work on other cluster providers but that can not be guaranteed and may require modifications depending on the specific way OpenShift is set up. oc get nodes oc debug node/node1 sh-4.4# more /host/etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 172.30.55.8 image-registry.openshift-image-registry.svc image-registry.openshift-image-registry.svc.cluster.local # openshift-generated-node-resolver The default exclusions are: quay.io registry.redhat.io registry.connect.redhat.com gcr.io nvcr.io icr.io cp.icr.io docker-na-public.artifactory.swg-devops.com docker-na-proxy-svl.artifactory.swg-devops.com docker-na-proxy-rtp.artifactory.swg-devops.com These can be changed by setting airgap_network_exclusions explicitly.","title":"ocp_simulate_disconnected_network"},{"location":"roles/ocp_simulate_disconnected_network/#ocp_simulate_disconnected_network","text":"Our goal is to modify the hosts file on each node (worker and master) to add a bogus entry that breaks DNS resolution for all the registries that we are going to mirror. This will simulate the cluster running in an air gap configuration, although network access will be possible elsewhere, the cluster will be unable to access the docker registries Important The host file (on Fyre) will look something like this, this role is (for now anyway) mainly focused on simulating an air gap cluster in Fyre, it may work on other cluster providers but that can not be guaranteed and may require modifications depending on the specific way OpenShift is set up. oc get nodes oc debug node/node1 sh-4.4# more /host/etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 172.30.55.8 image-registry.openshift-image-registry.svc image-registry.openshift-image-registry.svc.cluster.local # openshift-generated-node-resolver The default exclusions are: quay.io registry.redhat.io registry.connect.redhat.com gcr.io nvcr.io icr.io cp.icr.io docker-na-public.artifactory.swg-devops.com docker-na-proxy-svl.artifactory.swg-devops.com docker-na-proxy-rtp.artifactory.swg-devops.com These can be changed by setting airgap_network_exclusions explicitly.","title":"ocp_simulate_disconnected_network"},{"location":"roles/ocp_upgrade/","text":"ocp_upgrade \u00a4 This role supports the upgrade of the Openshift Cluster version for master and worker nodes in IBM Cloud provider. Role Variables \u00a4 cluster_type \u00a4 Required. Specify the cluster type, only IBM Cloud Openshift Clusters are supported by this role at the moment. If you provide a different cluster type than roks , this role will fail. Environment Variable: CLUSTER_TYPE Default Value: None cluster_name \u00a4 Required. Specify the name of the cluster to be upgraded. Environment Variable: CLUSTER_NAME Default Value: None ocp_version_upgrade \u00a4 Required. Specify the target version of the Openshift to be upgraded. Environment Variable: OCP_VERSION_UPGRADE Default Value: None Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: cluster_name: my-ocp-cluster cluster_type: roks ocp_version_upgrade: 4.10_openshift roles: - ibm.mas_devops.ocp_upgrade License \u00a4 EPL-2.0","title":"ocp_upgrade"},{"location":"roles/ocp_upgrade/#ocp_upgrade","text":"This role supports the upgrade of the Openshift Cluster version for master and worker nodes in IBM Cloud provider.","title":"ocp_upgrade"},{"location":"roles/ocp_upgrade/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_upgrade/#cluster_type","text":"Required. Specify the cluster type, only IBM Cloud Openshift Clusters are supported by this role at the moment. If you provide a different cluster type than roks , this role will fail. Environment Variable: CLUSTER_TYPE Default Value: None","title":"cluster_type"},{"location":"roles/ocp_upgrade/#cluster_name","text":"Required. Specify the name of the cluster to be upgraded. Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/ocp_upgrade/#ocp_version_upgrade","text":"Required. Specify the target version of the Openshift to be upgraded. Environment Variable: OCP_VERSION_UPGRADE Default Value: None","title":"ocp_version_upgrade"},{"location":"roles/ocp_upgrade/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: cluster_name: my-ocp-cluster cluster_type: roks ocp_version_upgrade: 4.10_openshift roles: - ibm.mas_devops.ocp_upgrade","title":"Example Playbook"},{"location":"roles/ocp_upgrade/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_verify/","text":"ocp_verify \u00a4 This role will verify that the target OCP cluster is ready to be setup for MAS. For example, in IBMCloud ROKS we have seen delays of over an hour before the Red Hat Operator catalog is ready to use. This will cause attempts to install anything from that CatalogSource to fail as the timeouts built into the roles in this collection are designed to catch problems with an install, rather than a half-provisioned cluster that is not properly ready to use yet. Role Variables \u00a4 verify_cluster \u00a4 Enable cluster health verification. Optional Environment Variable: VERIFY_CLUSTER Default: true Purpose : Verifies that the OCP cluster is healthy and ready to use by checking the ClusterVersion resource Ready condition. When to use : - Leave as true (default) for comprehensive cluster verification - Set to false only to skip cluster health checks - Recommended to keep enabled for production deployments Valid values : true , false Impact : - true : Verifies cluster Ready condition (fails if not ready within 1 hour) - false : Skips cluster health verification Related variables : - Other verify_* variables control additional verification checks Note : This check ensures the cluster is fully provisioned and ready. In some environments (e.g., IBMCloud ROKS), clusters may take time to become fully ready. The 1-hour timeout accommodates typical provisioning delays. verify_catalogsources \u00a4 Enable catalog source health verification. Optional Environment Variable: VERIFY_CATALOGSOURCES Default: true Purpose : Verifies that all installed OCP catalog sources are healthy and ready to provide operators. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip catalog source checks - Critical for ensuring operator installation will succeed Valid values : true , false Impact : - true : Verifies all CatalogSources report lastObservedState as READY (fails if not ready within 30 minutes) - false : Skips catalog source verification Related variables : - verify_cluster : Cluster-level health check - verify_subscriptions : Operator subscription verification Note : Catalog sources must be ready before operators can be installed. In some environments (e.g., IBMCloud ROKS), catalog sources may take time to sync. The 30-minute timeout accommodates typical delays. This check prevents operator installation failures due to unavailable catalogs. verify_subscriptions \u00a4 Enable operator subscription verification. Optional Environment Variable: VERIFY_SUBSCRIPTIONS Default: true Purpose : Verifies that all operator subscriptions are up to date and at their latest known version. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip subscription checks - Important for ensuring operators are properly installed Valid values : true , false Impact : - true : Verifies all Subscriptions report state as AtLatestKnown (fails if not ready within 5 hours) - false : Skips subscription verification Related variables : - verify_catalogsources : Catalog source health (prerequisite) - verify_workloads : Workload deployment verification Note : Subscriptions must be at latest known version before operators are fully functional. The 5-hour timeout accommodates operator installation and upgrade processes. This check ensures operators are properly installed and ready to manage resources. verify_workloads \u00a4 Enable workload deployment verification. Optional Environment Variable: VERIFY_WORKLOADS Default: true Purpose : Verifies that all deployments and statefulsets are fully rolled out with all replicas updated and available. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip workload checks - Critical for ensuring cluster workloads are healthy Valid values : true , false Impact : - true : Verifies all Deployments and StatefulSets have updatedReplicas and availableReplicas equal to replicas (fails if not ready within 10 hours) - false : Skips workload verification Related variables : - verify_subscriptions : Operator subscription verification (prerequisite) - verify_cluster : Cluster-level health check Note : Workloads must be fully deployed before the cluster is ready for MAS installation. The 10-hour timeout accommodates large-scale deployments and rolling updates. This check ensures all pods are running and ready. verify_ingress \u00a4 Enable ingress TLS certificate verification. Optional Environment Variable: VERIFY_INGRESS Default: true Purpose : Verifies that the cluster ingress TLS certificate can be obtained. This certificate is required by multiple roles in the collection. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip ingress certificate checks - Required for roles that need cluster ingress certificate Valid values : true , false Impact : - true : Verifies ingress TLS certificate can be retrieved - false : Skips ingress certificate verification Related variables : - cluster_name : Cluster name for certificate lookup - ocp_ingress_tls_secret_name : Secret name containing certificate Note : Many roles in this collection require the cluster ingress certificate. This check ensures the certificate is accessible before proceeding with MAS installation. cluster_name \u00a4 Cluster name for ingress certificate lookup. Optional (only used when verify_ingress=true ) Environment Variable: CLUSTER_NAME Default: None Purpose : Specifies the cluster name used to determine the default router certificate name in certain cluster configurations. When to use : - Only required when verify_ingress=true - Set when cluster setup requires cluster name for certificate lookup - Leave unset if certificate can be found without cluster name Valid values : String matching your cluster name Impact : Used to construct the ingress TLS secret name in cluster configurations where the secret name includes the cluster name. Related variables : - verify_ingress : Must be true for this variable to be used - ocp_ingress_tls_secret_name : Alternative way to specify certificate secret Note : Only needed in specific cluster configurations. Most clusters use the default secret name and don't require this variable. ocp_ingress_tls_secret_name \u00a4 Ingress TLS secret name. Optional (only used when verify_ingress=true ) Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default: router-certs-default Purpose : Specifies the name of the Kubernetes secret containing the cluster's default router certificate. When to use : - Only applies when verify_ingress=true - Use default ( router-certs-default ) for most clusters - Override only if your cluster uses a different secret name Valid values : String matching the secret name in openshift-ingress namespace Impact : Determines which secret is checked for the ingress TLS certificate. Incorrect name will cause verification to fail. Related variables : - verify_ingress : Must be true for this variable to be used - cluster_name : Alternative way to determine secret name Note : The default router-certs-default works for most OCP clusters. Only override if your cluster uses a custom ingress certificate secret name. Example Playbook \u00a4 - hosts: localhost vars: verify_cluster: True verify_catalogsources: True verify_subscriptions: True verify_workloads: True verify_ingress: True roles: - ibm.mas_devops.ocp_verify License \u00a4 EPL-2.0","title":"ocp_verify"},{"location":"roles/ocp_verify/#ocp_verify","text":"This role will verify that the target OCP cluster is ready to be setup for MAS. For example, in IBMCloud ROKS we have seen delays of over an hour before the Red Hat Operator catalog is ready to use. This will cause attempts to install anything from that CatalogSource to fail as the timeouts built into the roles in this collection are designed to catch problems with an install, rather than a half-provisioned cluster that is not properly ready to use yet.","title":"ocp_verify"},{"location":"roles/ocp_verify/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_verify/#verify_cluster","text":"Enable cluster health verification. Optional Environment Variable: VERIFY_CLUSTER Default: true Purpose : Verifies that the OCP cluster is healthy and ready to use by checking the ClusterVersion resource Ready condition. When to use : - Leave as true (default) for comprehensive cluster verification - Set to false only to skip cluster health checks - Recommended to keep enabled for production deployments Valid values : true , false Impact : - true : Verifies cluster Ready condition (fails if not ready within 1 hour) - false : Skips cluster health verification Related variables : - Other verify_* variables control additional verification checks Note : This check ensures the cluster is fully provisioned and ready. In some environments (e.g., IBMCloud ROKS), clusters may take time to become fully ready. The 1-hour timeout accommodates typical provisioning delays.","title":"verify_cluster"},{"location":"roles/ocp_verify/#verify_catalogsources","text":"Enable catalog source health verification. Optional Environment Variable: VERIFY_CATALOGSOURCES Default: true Purpose : Verifies that all installed OCP catalog sources are healthy and ready to provide operators. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip catalog source checks - Critical for ensuring operator installation will succeed Valid values : true , false Impact : - true : Verifies all CatalogSources report lastObservedState as READY (fails if not ready within 30 minutes) - false : Skips catalog source verification Related variables : - verify_cluster : Cluster-level health check - verify_subscriptions : Operator subscription verification Note : Catalog sources must be ready before operators can be installed. In some environments (e.g., IBMCloud ROKS), catalog sources may take time to sync. The 30-minute timeout accommodates typical delays. This check prevents operator installation failures due to unavailable catalogs.","title":"verify_catalogsources"},{"location":"roles/ocp_verify/#verify_subscriptions","text":"Enable operator subscription verification. Optional Environment Variable: VERIFY_SUBSCRIPTIONS Default: true Purpose : Verifies that all operator subscriptions are up to date and at their latest known version. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip subscription checks - Important for ensuring operators are properly installed Valid values : true , false Impact : - true : Verifies all Subscriptions report state as AtLatestKnown (fails if not ready within 5 hours) - false : Skips subscription verification Related variables : - verify_catalogsources : Catalog source health (prerequisite) - verify_workloads : Workload deployment verification Note : Subscriptions must be at latest known version before operators are fully functional. The 5-hour timeout accommodates operator installation and upgrade processes. This check ensures operators are properly installed and ready to manage resources.","title":"verify_subscriptions"},{"location":"roles/ocp_verify/#verify_workloads","text":"Enable workload deployment verification. Optional Environment Variable: VERIFY_WORKLOADS Default: true Purpose : Verifies that all deployments and statefulsets are fully rolled out with all replicas updated and available. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip workload checks - Critical for ensuring cluster workloads are healthy Valid values : true , false Impact : - true : Verifies all Deployments and StatefulSets have updatedReplicas and availableReplicas equal to replicas (fails if not ready within 10 hours) - false : Skips workload verification Related variables : - verify_subscriptions : Operator subscription verification (prerequisite) - verify_cluster : Cluster-level health check Note : Workloads must be fully deployed before the cluster is ready for MAS installation. The 10-hour timeout accommodates large-scale deployments and rolling updates. This check ensures all pods are running and ready.","title":"verify_workloads"},{"location":"roles/ocp_verify/#verify_ingress","text":"Enable ingress TLS certificate verification. Optional Environment Variable: VERIFY_INGRESS Default: true Purpose : Verifies that the cluster ingress TLS certificate can be obtained. This certificate is required by multiple roles in the collection. When to use : - Leave as true (default) for comprehensive verification - Set to false only to skip ingress certificate checks - Required for roles that need cluster ingress certificate Valid values : true , false Impact : - true : Verifies ingress TLS certificate can be retrieved - false : Skips ingress certificate verification Related variables : - cluster_name : Cluster name for certificate lookup - ocp_ingress_tls_secret_name : Secret name containing certificate Note : Many roles in this collection require the cluster ingress certificate. This check ensures the certificate is accessible before proceeding with MAS installation.","title":"verify_ingress"},{"location":"roles/ocp_verify/#cluster_name","text":"Cluster name for ingress certificate lookup. Optional (only used when verify_ingress=true ) Environment Variable: CLUSTER_NAME Default: None Purpose : Specifies the cluster name used to determine the default router certificate name in certain cluster configurations. When to use : - Only required when verify_ingress=true - Set when cluster setup requires cluster name for certificate lookup - Leave unset if certificate can be found without cluster name Valid values : String matching your cluster name Impact : Used to construct the ingress TLS secret name in cluster configurations where the secret name includes the cluster name. Related variables : - verify_ingress : Must be true for this variable to be used - ocp_ingress_tls_secret_name : Alternative way to specify certificate secret Note : Only needed in specific cluster configurations. Most clusters use the default secret name and don't require this variable.","title":"cluster_name"},{"location":"roles/ocp_verify/#ocp_ingress_tls_secret_name","text":"Ingress TLS secret name. Optional (only used when verify_ingress=true ) Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default: router-certs-default Purpose : Specifies the name of the Kubernetes secret containing the cluster's default router certificate. When to use : - Only applies when verify_ingress=true - Use default ( router-certs-default ) for most clusters - Override only if your cluster uses a different secret name Valid values : String matching the secret name in openshift-ingress namespace Impact : Determines which secret is checked for the ingress TLS certificate. Incorrect name will cause verification to fail. Related variables : - verify_ingress : Must be true for this variable to be used - cluster_name : Alternative way to determine secret name Note : The default router-certs-default works for most OCP clusters. Only override if your cluster uses a custom ingress certificate secret name.","title":"ocp_ingress_tls_secret_name"},{"location":"roles/ocp_verify/#example-playbook","text":"- hosts: localhost vars: verify_cluster: True verify_catalogsources: True verify_subscriptions: True verify_workloads: True verify_ingress: True roles: - ibm.mas_devops.ocp_verify","title":"Example Playbook"},{"location":"roles/ocp_verify/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocs/","text":"ocs \u00a4 This role provides support to install/update OpenShift Data Foundation Operator (ODF) (formerly called Openshift Container Storage (OCS)) and the Local Storage Operator (LSO). This role is not used by default when setting up IBM Cloud ROKS clusters because they are automatically provisioned with their own storage plugin already. The OCP Version is autodetected and if it is found to be at version 4.10 or less then the ocs operator is used, else the odf operator is used. Unfortunately, starting from OCP 4.8 IBM/Red Hat have decided to stop supporting OCS on IBMCloud ROKS. So this role is of limited value to users of ROKS going forward. If you attempt to install OpenShift Container Storage on ROKS via a Subscription channel you will be met by the following error as the admission webhook has been coded to prevent use of the OCS operator on IBM Cloud ROKS: Failed to apply object: \"admission webhook \"validate.managed.openshift.io\" denied the request: Installing OpenShift Data Foundation on IBM Cloud by using OperatorHub is not supported. You can install OpenShift Data Foundation by using the IBM Cloud add-on. For more information, see https://cloud.ibm.com/docs/openshift?topic=openshift-ocs-storage-prep. Role Variables \u00a4 lso_device_path \u00a4 Set this value with your actual local disk filepath to be used in the LocalVolume for block storage using the localblock storageclass. Environment Variable: LSO_DEVICE_PATH Default Value: /dev/vdb ocs_action \u00a4 Set this value to the required action for install or upgrade . If set to install, the local storage and OCF/ODF storage operators will be installed with their storage cluster. If set to upgrade, the operators and storage cluster will be updated based on the ocp version in the cluster. Environment Variable: OCS_ACTION Default Value: install Example Playbook \u00a4 --- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.ocs License \u00a4 EPL-2.0","title":"ocs"},{"location":"roles/ocs/#ocs","text":"This role provides support to install/update OpenShift Data Foundation Operator (ODF) (formerly called Openshift Container Storage (OCS)) and the Local Storage Operator (LSO). This role is not used by default when setting up IBM Cloud ROKS clusters because they are automatically provisioned with their own storage plugin already. The OCP Version is autodetected and if it is found to be at version 4.10 or less then the ocs operator is used, else the odf operator is used. Unfortunately, starting from OCP 4.8 IBM/Red Hat have decided to stop supporting OCS on IBMCloud ROKS. So this role is of limited value to users of ROKS going forward. If you attempt to install OpenShift Container Storage on ROKS via a Subscription channel you will be met by the following error as the admission webhook has been coded to prevent use of the OCS operator on IBM Cloud ROKS: Failed to apply object: \"admission webhook \"validate.managed.openshift.io\" denied the request: Installing OpenShift Data Foundation on IBM Cloud by using OperatorHub is not supported. You can install OpenShift Data Foundation by using the IBM Cloud add-on. For more information, see https://cloud.ibm.com/docs/openshift?topic=openshift-ocs-storage-prep.","title":"ocs"},{"location":"roles/ocs/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocs/#lso_device_path","text":"Set this value with your actual local disk filepath to be used in the LocalVolume for block storage using the localblock storageclass. Environment Variable: LSO_DEVICE_PATH Default Value: /dev/vdb","title":"lso_device_path"},{"location":"roles/ocs/#ocs_action","text":"Set this value to the required action for install or upgrade . If set to install, the local storage and OCF/ODF storage operators will be installed with their storage cluster. If set to upgrade, the operators and storage cluster will be updated based on the ocp version in the cluster. Environment Variable: OCS_ACTION Default Value: install","title":"ocs_action"},{"location":"roles/ocs/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.ocs","title":"Example Playbook"},{"location":"roles/ocs/#license","text":"EPL-2.0","title":"License"},{"location":"roles/registry/","text":"registry \u00a4 Create a Docker Registry running on RedHat OpenShift cluster. The registry will be backed by persistant storage, and accessible via either a clusterIP or loadbalancer service. This role can also be used to delete a docker registry on a cluster for a clean start. See usage below for more information. Usage \u00a4 If you set up the registry with a loadbalancer service you will be able to push to the registry via the cluster's hostname, but before you can use the registry you will need to install the registry's CA certificate and restart the Docker daemon so that your client trusts the new registry: CACERT=$(oc -n airgap-registry get secret airgap-registry-certificate -o jsonpath='{.data.ca\\.crt}' | base64 -d) DOMAIN=$(oc get ingress.config cluster -o jsonpath='{.spec.domain}') sudo mkdir -p /etc/docker/certs.d/$DOMAIN\\:32500/ sudo echo \"$CACERT\" > /etc/docker/certs.d/$DOMAIN\\:32500/ca.crt sudo service docker restart You can now use the registry as normal: DOMAIN=$(oc get ingress.config cluster -o jsonpath='{.spec.domain}') docker pull registry.access.redhat.com/ubi8/ubi-minimal docker tag registry.access.redhat.com/ubi8/ubi-minimal $DOMAIN:32500/ubi8/ubi-minimal docker push $DOMAIN:32500/ubi8/ubi-minimal If you set up the registry with a clusterip service you will only be able to push to the registry after using port forwarding: oc -n airgap-registry port-forward deployment/airgap-registry 9000:5000 docker pull registry.access.redhat.com/ubi8/ubi-minimal docker tag registry.access.redhat.com/ubi8/ubi-minimal localhost:9000/ubi8/ubi-minimal docker push localhost:9000/ubi8/ubi-minimal However, you will still need to set up Docker trust for the \"local\" registry: CACERT=$(oc -n airgap-registry get secret airgap-registry-certificate -o jsonpath='{.data.ca\\.crt}' | base64 -d) sudo mkdir -p /etc/docker/certs.d/$DOMAIN\\:32500/ sudo mkdir /etc/docker/certs.d/localhost\\:9000 sudo echo \"$CACERT\" > /etc/docker/certs.d/localhost\\:9000/ca.crt sudo service docker restart Usage for tear-down action \u00a4 This role can also be used to permanently delete a mirror registry from a given cluster by setting the registry_action to tear-down and specifying the corresponding registry_namespace , if not using the default value. Note that the tear-down action deletes the registry completely including the PVC storage and the registry namespace. To start up the registry again, the role needs to be run again with the registry_action on default or setup . Images previously stored in the registry before the tear-down will no longer be available and will need to be mirrored again once the registry setup has completed. Take precaution when using this function and expect that images can no longer be accessed from the registry that has been torn down. Note: Recreating the registry will also create a new ca cert for the new registry. An appropriate time to use this tear-down function is when the registry has too many images that are not being used or when there has been a shift to support newer versions but images of older versions are clogging the registry. The tear-down function frees the disk space and allows for a new registry to be setup. Role Variables \u00a4 registry_action \u00a4 The action to perform with this role. Can be set to tear-down to remove an existing registry and its namespace. Default is setup Optional Environment Variable: REGISTRY_ACTION Default Value: setup registry_namespace \u00a4 The namespace where the registry to run Optional Environment Variable: REGISTRY_NAMESPACE Default Value: airgap-registry registry_storage_class \u00a4 Required : The name of the storage class to configure the MongoDb operator to use for persistent storage in the MongoDb cluster. Storage class must support ReadWriteOnce(RWO) access mode. Required , unless running in IBM Cloud ROKS, where the storage class will default to ibmc-block-gold . Environment Variable: REGISTRY_STORAGE_CLASS Default Value: None registry_storage_capacity \u00a4 The size of the PVC that will be created for data storage in the cluster. Optional Environment Variable: REGISTRY_STORAGE_CAPACITY Default Value: 100Gi registry_service_type \u00a4 The type of service to set up in front of the registry, either loadbalancer or clusterip . Using loadbalancer will allow you to access the registry from outside of your cluster via the cluster domain on port 32500 . If you have other loadbalancers on the cluster that already claim port 32500 this role can not be usedbecause currently the loadbalancer port can not be customised. Optional Environment Variable: REGISTRY_SERVICE_TYPE Default Value: loadbalancer Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: registry_storage_class: ibmc-block-gold registry_storage_capacity: 500Gb registry_service_type: loadbalancer roles: - ibm.mas_devops.registry License \u00a4 EPL-2.0","title":"registry"},{"location":"roles/registry/#registry","text":"Create a Docker Registry running on RedHat OpenShift cluster. The registry will be backed by persistant storage, and accessible via either a clusterIP or loadbalancer service. This role can also be used to delete a docker registry on a cluster for a clean start. See usage below for more information.","title":"registry"},{"location":"roles/registry/#usage","text":"If you set up the registry with a loadbalancer service you will be able to push to the registry via the cluster's hostname, but before you can use the registry you will need to install the registry's CA certificate and restart the Docker daemon so that your client trusts the new registry: CACERT=$(oc -n airgap-registry get secret airgap-registry-certificate -o jsonpath='{.data.ca\\.crt}' | base64 -d) DOMAIN=$(oc get ingress.config cluster -o jsonpath='{.spec.domain}') sudo mkdir -p /etc/docker/certs.d/$DOMAIN\\:32500/ sudo echo \"$CACERT\" > /etc/docker/certs.d/$DOMAIN\\:32500/ca.crt sudo service docker restart You can now use the registry as normal: DOMAIN=$(oc get ingress.config cluster -o jsonpath='{.spec.domain}') docker pull registry.access.redhat.com/ubi8/ubi-minimal docker tag registry.access.redhat.com/ubi8/ubi-minimal $DOMAIN:32500/ubi8/ubi-minimal docker push $DOMAIN:32500/ubi8/ubi-minimal If you set up the registry with a clusterip service you will only be able to push to the registry after using port forwarding: oc -n airgap-registry port-forward deployment/airgap-registry 9000:5000 docker pull registry.access.redhat.com/ubi8/ubi-minimal docker tag registry.access.redhat.com/ubi8/ubi-minimal localhost:9000/ubi8/ubi-minimal docker push localhost:9000/ubi8/ubi-minimal However, you will still need to set up Docker trust for the \"local\" registry: CACERT=$(oc -n airgap-registry get secret airgap-registry-certificate -o jsonpath='{.data.ca\\.crt}' | base64 -d) sudo mkdir -p /etc/docker/certs.d/$DOMAIN\\:32500/ sudo mkdir /etc/docker/certs.d/localhost\\:9000 sudo echo \"$CACERT\" > /etc/docker/certs.d/localhost\\:9000/ca.crt sudo service docker restart","title":"Usage"},{"location":"roles/registry/#usage-for-tear-down-action","text":"This role can also be used to permanently delete a mirror registry from a given cluster by setting the registry_action to tear-down and specifying the corresponding registry_namespace , if not using the default value. Note that the tear-down action deletes the registry completely including the PVC storage and the registry namespace. To start up the registry again, the role needs to be run again with the registry_action on default or setup . Images previously stored in the registry before the tear-down will no longer be available and will need to be mirrored again once the registry setup has completed. Take precaution when using this function and expect that images can no longer be accessed from the registry that has been torn down. Note: Recreating the registry will also create a new ca cert for the new registry. An appropriate time to use this tear-down function is when the registry has too many images that are not being used or when there has been a shift to support newer versions but images of older versions are clogging the registry. The tear-down function frees the disk space and allows for a new registry to be setup.","title":"Usage for tear-down action"},{"location":"roles/registry/#role-variables","text":"","title":"Role Variables"},{"location":"roles/registry/#registry_action","text":"The action to perform with this role. Can be set to tear-down to remove an existing registry and its namespace. Default is setup Optional Environment Variable: REGISTRY_ACTION Default Value: setup","title":"registry_action"},{"location":"roles/registry/#registry_namespace","text":"The namespace where the registry to run Optional Environment Variable: REGISTRY_NAMESPACE Default Value: airgap-registry","title":"registry_namespace"},{"location":"roles/registry/#registry_storage_class","text":"Required : The name of the storage class to configure the MongoDb operator to use for persistent storage in the MongoDb cluster. Storage class must support ReadWriteOnce(RWO) access mode. Required , unless running in IBM Cloud ROKS, where the storage class will default to ibmc-block-gold . Environment Variable: REGISTRY_STORAGE_CLASS Default Value: None","title":"registry_storage_class"},{"location":"roles/registry/#registry_storage_capacity","text":"The size of the PVC that will be created for data storage in the cluster. Optional Environment Variable: REGISTRY_STORAGE_CAPACITY Default Value: 100Gi","title":"registry_storage_capacity"},{"location":"roles/registry/#registry_service_type","text":"The type of service to set up in front of the registry, either loadbalancer or clusterip . Using loadbalancer will allow you to access the registry from outside of your cluster via the cluster domain on port 32500 . If you have other loadbalancers on the cluster that already claim port 32500 this role can not be usedbecause currently the loadbalancer port can not be customised. Optional Environment Variable: REGISTRY_SERVICE_TYPE Default Value: loadbalancer","title":"registry_service_type"},{"location":"roles/registry/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: registry_storage_class: ibmc-block-gold registry_storage_capacity: 500Gb registry_service_type: loadbalancer roles: - ibm.mas_devops.registry","title":"Example Playbook"},{"location":"roles/registry/#license","text":"EPL-2.0","title":"License"},{"location":"roles/sls/","text":"sls \u00a4 Install IBM Suite License Service and generate a configuration that can be directly applied to IBM Maximo Application Suite. The role assumes that you have already installed the Certificate Manager in the target cluster. This action is performed by the cert_manager role if you want to use this collection to install the cert-manager operator. Role Variables \u00a4 General Variables \u00a4 sls_action \u00a4 Specifies which operation to perform on the Suite License Service (SLS) instance. Optional Environment Variable: SLS_ACTION Default: install Purpose : Controls what action the role executes against the SLS instance. This allows the same role to handle installation, configuration generation, and removal of SLS. When to use : - Use install for initial SLS deployment or updates - Use gencfg to generate SLS configuration for MAS without installing SLS (when using existing SLS) - Use uninstall to remove SLS instance (use with caution) Valid values : install , gencfg , uninstall Impact : - install : Deploys or updates SLS operator and instance - gencfg : Only generates SLSCfg resource for MAS integration - uninstall : Removes SLS instance and operator (destructive operation) Related variables : When using gencfg , requires sls_url to be set to point to existing SLS instance. Note : Always backup license data before using uninstall . The gencfg action is useful when SLS is shared across multiple MAS instances. Role Variables - Installation \u00a4 If sls_url is set then the role will skip the installation of an SLS instance and simply generate the SLSCfg resource for the SLS instance defined. artifactory_username \u00a4 Username for authenticating to IBM Artifactory to access development builds of SLS. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None Purpose : Provides authentication credentials to pull development/pre-release SLS operator images from IBM's Artifactory registry. Required only when installing development builds for testing or early access. When to use : - Required when sls_catalog_source is set to ibm-sls-operators (development catalog) - Not needed for production installations using ibm-operator-catalog - Use your IBM w3Id username for development builds Valid values : Valid IBM Artifactory username (typically your w3Id) Impact : Without valid credentials, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_token : Must be set together with this username - sls_catalog_source : Determines if Artifactory credentials are needed Note : Only required for development/pre-release builds. Production installations use ibm_entitlement_key instead (for SLS 3.7.0 and earlier). artifactory_token \u00a4 API token for authenticating to IBM Artifactory to access development builds of SLS. Optional Environment Variable: ARTIFACTORY_TOKEN Default: None Purpose : Provides the API token/password credential to authenticate with IBM's Artifactory registry when pulling development/pre-release SLS operator images. When to use : - Required when sls_catalog_source is set to ibm-sls-operators (development catalog) - Not needed for production installations using ibm-operator-catalog - Use your IBM Artifactory API key for development builds Valid values : Valid IBM Artifactory API token Impact : Without a valid token, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_username : Must be set together with this token - sls_catalog_source : Determines if Artifactory credentials are needed Note : Only required for development/pre-release builds. Keep this token secure and do not commit to source control. sls_catalog_source \u00a4 Specifies the OpenShift operator catalog source containing the SLS operator subscription. Optional Environment Variable: SLS_CATALOG_SOURCE Default: ibm-operator-catalog Purpose : Controls which operator catalog is used to locate and install the SLS operator. This determines where OpenShift looks for the operator images and metadata during installation. When to use : - Use default ibm-operator-catalog for production installations with official IBM releases - Use ibm-sls-operators for development builds from Artifactory - Only change if directed to use a custom catalog for specific testing or airgap scenarios Valid values : Any valid CatalogSource name present in the openshift-marketplace namespace (typically ibm-operator-catalog or ibm-sls-operators ) Impact : Changing this value affects which operator versions are available for installation. An invalid catalog source will cause the subscription to fail. Development catalogs require additional authentication via artifactory_username and artifactory_token . Related variables : - sls_channel : Works together to determine the specific operator version installed - artifactory_username and artifactory_token : Required when using ibm-sls-operators Note : For development catalogs, you must also provide Artifactory credentials. sls_channel \u00a4 Specifies the SLS operator subscription channel, which determines the version stream you'll receive updates from. Optional Environment Variable: SLS_CHANNEL Default: 3.x Purpose : Controls which version of SLS will be installed and which updates will be automatically applied. The channel corresponds to major version releases and determines the feature set and compatibility level of your SLS installation. When to use : - Use default 3.x for latest SLS 3.x releases (recommended for most deployments) - Use specific older channels only if required for compatibility with older MAS versions - Consult the MAS compatibility matrix before selecting a channel Valid values : 3.x (check the IBM Operator Catalog for currently available channels) Impact : The channel determines which SLS version is installed and which automatic updates are applied. Changing channels after installation will trigger an upgrade to the latest version in that channel. Related variables : Works with sls_catalog_source to determine available channels. Note : SLS 3.8.0+ no longer requires IBM entitlement keys as images moved to public registry. Review the SLS release notes before changing this value. sls_namespace \u00a4 OpenShift namespace where the SLS operator and instance will be deployed. Optional Environment Variable: SLS_NAMESPACE Default: ibm-sls Purpose : Defines the Kubernetes namespace for SLS resources, providing isolation and organization for the SLS deployment within the cluster. When to use : - Use default ibm-sls for standard deployments - Change only if you need multiple SLS instances or have namespace naming requirements - Ensure namespace doesn't conflict with existing deployments Valid values : Any valid Kubernetes namespace name (lowercase alphanumeric and hyphens) Impact : All SLS resources (operator, pods, secrets, services, ConfigMaps) will be created in this namespace. Changing this after deployment requires reinstallation. Related variables : Used in SLSCfg generation to reference SLS service endpoints. Note : The namespace will be created if it doesn't exist. Ensure you have permissions to create namespaces in the cluster. sls_icr_cpopen \u00a4 Container registry source for SLS operator and component images. Optional Environment Variable: SLS_ICR_CPOPEN Default: icr.io/cpopen Purpose : Specifies the container registry from which SLS pulls all operator and component images. From SLS 3.8.0 onwards, this is the primary registry variable as images moved to the public IBM Container Registry. When to use : - Use default icr.io/cpopen for production installations (SLS 3.8.0+) - Override only for development images or airgap/mirror scenarios - No authentication required for default public registry Valid values : Any valid container registry URL (e.g., icr.io/cpopen , my-registry.com/sls ) Impact : All SLS images will be pulled from this registry. An incorrect or inaccessible registry will cause image pull failures. The default public registry requires no entitlement key (SLS 3.8.0+). Related variables : - sls_icr_cp : Deprecated in SLS 3.8.0, only needed for SLS 3.7.0 and earlier - ibm_entitlement_key : Not required when using default public registry (SLS 3.8.0+) Note : SLS 3.8.0+ uses public registry - no entitlement key needed. For SLS 3.7.0 and earlier, use sls_icr_cp and provide ibm_entitlement_key . sls_instance_name \u00a4 Unique identifier for this SLS installation within the cluster. Optional Environment Variable: SLS_INSTANCE_NAME Default: sls Purpose : Provides a unique identifier that distinguishes this SLS installation from others that may exist in the same cluster. This ID is used to generate resource names throughout the installation. When to use : - Use default sls for standard single-SLS deployments - Change only if you need multiple SLS instances in the same cluster - Must be unique within the cluster if running multiple SLS instances Valid values : Lowercase alphanumeric string (e.g., sls , sls-prod , sls-dev ) Impact : This ID becomes part of resource names and is embedded in many SLS resources. It cannot be changed after installation without reinstalling. Related variables : Used in SLSCfg generation to identify the SLS instance. Note : Choose carefully as this cannot be changed after installation. Most deployments use the default sls value. sls_icr_cp \u00a4 Container registry source for SLS images (SLS 3.7.0 and earlier only). Optional Environment Variable: SLS_ICR_CP Default: cp.icr.io/cp Purpose : Specifies the container registry for SLS images in versions 3.7.0 and earlier. This registry required IBM entitlement key authentication. When to use : - Only required for SLS versions 3.7.0 and earlier - Not needed for SLS 3.8.0+ (images moved to public registry) - Override only for development images or airgap scenarios with older SLS versions Valid values : Any valid container registry URL (e.g., cp.icr.io/cp , my-registry.com/sls ) Impact : For SLS 3.7.0 and earlier, all SLS images will be pulled from this registry. Requires ibm_entitlement_key for authentication. Related variables : - sls_icr_cpopen : Use this instead for SLS 3.8.0+ - ibm_entitlement_key : Required when using this registry (SLS 3.7.0 and earlier) Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry ( icr.io/cpopen ). This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, use sls_icr_cpopen instead (no entitlement key needed). ibm_entitlement_key \u00a4 IBM entitlement key for accessing IBM Container Registry images (SLS 3.7.0 and earlier only). Required for SLS 3.7.0 and earlier (unless sls_url is provided) Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Authenticates access to IBM's entitled container registry to pull SLS operator and component images. This key is tied to your IBM Cloud account and product entitlements. When to use : - Required for SLS versions 3.7.0 and earlier - Not required for SLS 3.8.0 and later (images moved to public registry) - Obtain from IBM Container Library Valid values : IBM entitlement key string (typically starts with \"eyJ...\") Impact : Invalid or expired keys will cause image pull failures for SLS 3.7.0 and earlier. The key is stored in a Kubernetes secret in the SLS namespace. Related variables : - sls_entitlement_username : Username paired with this key (default: cp ) - sls_icr_cp : Registry requiring this authentication (SLS 3.7.0 and earlier) Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry ( icr.io/cpopen ). This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, no entitlement key is needed. sls_entitlement_username \u00a4 Username for authenticating to IBM Container Registry (SLS 3.7.0 and earlier only). Optional Environment Variable: SLS_ENTITLEMENT_USERNAME Default: cp Purpose : Provides the username credential for authenticating with IBM's entitled container registry when pulling SLS images for versions 3.7.0 and earlier. When to use : - Only needed for SLS versions 3.7.0 and earlier - Not required for SLS 3.8.0+ (images moved to public registry) - Usually can be left at default cp for production installations Valid values : cp (for production installations with IBM entitlement key) Impact : Used together with ibm_entitlement_key to create image pull secrets for the SLS namespace. Incorrect username will cause image pull authentication failures. Related variables : - ibm_entitlement_key : Must be set together with this username (SLS 3.7.0 and earlier) - sls_entitlement_key : Alternative entitlement key specific for SLS Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry. This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, no authentication is needed. sls_entitlement_key \u00a4 SLS-specific IBM entitlement key override (SLS 3.7.0 and earlier only). Optional Environment Variable: SLS_ENTITLEMENT_KEY Default: None (falls back to ibm_entitlement_key ) Purpose : Provides an SLS-specific IBM entitlement key that overrides the global ibm_entitlement_key variable. Primarily used in development scenarios where different keys are needed for different components. When to use : - Only needed for SLS versions 3.7.0 and earlier - Use when you need a different entitlement key specifically for SLS - Leave unset to use the global ibm_entitlement_key value - Not required for SLS 3.8.0+ (images moved to public registry) Valid values : IBM entitlement key string (typically starts with \"eyJ...\") Impact : When set, this key is used instead of ibm_entitlement_key for SLS image authentication. Invalid keys will cause image pull failures. Related variables : - ibm_entitlement_key : Global entitlement key that this variable overrides - sls_entitlement_username : Username paired with this key Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry. This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, no entitlement key is needed. Role Variables - Configuration \u00a4 sls_domain \u00a4 Custom domain name for external access to SLS via OpenShift routes. Optional Environment Variable: SLS_DOMAIN Default: None (SLS is only accessible within the cluster) Purpose : Enables external access to SLS through a custom domain route. This is essential when SLS needs to serve multiple MAS instances deployed in separate OpenShift clusters. When to use : - Required when SLS serves MAS instances in different OpenShift clusters - Required for multi-cluster SLS deployments - Leave unset for single-cluster deployments where SLS and MAS are in the same cluster Valid values : Any valid DNS domain name (e.g., sls.mycompany.com , license.example.org ) Impact : When set, SLS creates an external route accessible at the specified domain. You must ensure DNS is properly configured to resolve this domain to your cluster. When not set, SLS is only accessible via internal cluster services. Related variables : Affects how MAS instances connect to SLS (internal service vs external route). Note : Ensure proper DNS configuration and SSL certificates for the domain. External access requires appropriate network security controls. sls_auth_enforce \u00a4 Controls whether SLS enforces client authentication via mutual TLS (mTLS). Optional Environment Variable: SLS_AUTH_ENFORCE Default: True Purpose : Determines whether SLS requires clients to authenticate using mTLS certificates generated through the client registration flow. This provides secure authentication for SLS API access. When to use : - Use True (default) for production environments to enforce secure authentication - Set to False only in development/testing environments where simplified access is needed - Leave as default for standard secure deployments Valid values : True , False Impact : - When True : Clients must register with SLS and use mTLS certificates for API calls (secure) - When False : Authentication is not enforced, allowing unauthenticated API access (insecure) Related variables : Works with sls_registration_open to control client registration and access. Note : Setting to False removes authentication requirements and should only be used in non-production environments. Production deployments should always enforce authentication. sls_mongo_retrywrites \u00a4 Controls whether SLS uses MongoDB retryable writes feature. Optional Environment Variable: SLS_MONGO_RETRYWRITES Default: true Purpose : Determines whether SLS enables MongoDB's retryable writes feature, which automatically retries certain write operations that fail due to transient network errors or replica set elections. When to use : - Use true (default) when using MongoDB Community Edition or IBM Cloud Databases for MongoDB - Set to false when using AWS DocumentDB (which doesn't support retryable writes) - Set to false for any MongoDB-compatible database that doesn't support this feature Valid values : true , false Impact : - When true : Enables automatic retry of failed write operations (improves reliability) - When false : Disables retryable writes (required for DocumentDB and similar services) Related variables : - sls_mongodb_cfg_file or sls_mongodb.* : Determines which MongoDB service is used - Must align with the capabilities of your MongoDB provider Note : AWS DocumentDB does not support retryable writes. Set to false when using DocumentDB to avoid connection errors. sls_compliance_enforce \u00a4 Controls whether SLS enforces license token compliance. Optional Environment Variable: SLS_COMPLIANCE_ENFORCE Default: True Purpose : Determines whether SLS blocks license checkout requests when insufficient tokens are available. This controls whether license limits are strictly enforced or merely tracked. When to use : - Use True (default) for production to enforce license compliance and prevent over-consumption - Set to False only in development/testing environments or during grace periods - Leave as default for standard compliant deployments Valid values : True , False Impact : - When True : License checkout requests are denied when insufficient tokens are available (enforces compliance) - When False : License checkout requests succeed even without available tokens (tracks but doesn't enforce) Related variables : Works with license entitlement configuration to control token availability. Note : Setting to False allows operation beyond licensed capacity and may result in license compliance issues. Use only in non-production environments or with explicit approval. sls_registration_open \u00a4 Controls whether SLS allows new client self-registration. Optional Environment Variable: SLS_REGISTRATION_OPEN Default: True Purpose : Determines whether clients can register themselves with SLS to obtain certificates for API access. This controls the initial onboarding process for new SLS clients. When to use : - Use True (default) to allow MAS instances and other clients to self-register - Set to False after all clients are registered to prevent unauthorized registrations - Leave as default for standard deployments where clients need to register Valid values : True , False Impact : - When True : Clients can register themselves and obtain mTLS certificates for SLS API access - When False : New client registrations are blocked; only pre-registered clients can access SLS Related variables : Works with sls_auth_enforce to control authentication and registration flow. Note : For production environments, consider setting to False after all legitimate clients are registered to prevent unauthorized access. sls_mongodb_cfg_file \u00a4 Local file path to a MongoCfg YAML file for SLS MongoDB configuration. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: SLS_MONGODB_CFG_FILE Default: None Purpose : Provides MongoDB connection details to SLS by referencing a MongoCfg file generated by the mongodb role. This simplifies configuration by reusing existing MongoDB setup information. When to use : - Use when you've deployed MongoDB using the mongodb role (which generates this file) - Preferred method as it ensures consistency with MongoDB deployment - Alternative to manually specifying sls_mongodb.* variables Valid values : Any valid local filesystem path to a MongoCfg YAML file (e.g., /home/user/masconfig/mongocfg-mongoce-system.yaml ) Impact : The role extracts MongoDB connection details (hosts, certificates, credentials) from this file to configure SLS. Invalid or missing file will cause installation to fail. Related variables : - sls_mongodb.* : Alternative method to specify MongoDB connection details - mas_config_dir : Directory where mongodb role generates this file Note : Either this variable or the sls_mongodb object must be provided. Using this file is recommended for consistency with MongoDB deployment. sls_mongodb.hosts \u00a4 List of MongoDB host:port pairs for SLS database connection. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Specifies the MongoDB replica set members that SLS will connect to for license data storage. Multiple hosts provide high availability and failover capability. When to use : - Use when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Required for all MongoDB deployment types (Community, IBM Cloud, AWS DocumentDB) - Provide all replica set members for high availability Valid values : Array of strings in format [\"host1:port1\", \"host2:port2\", \"host3:port3\"] (e.g., [\"mongo-0.mongo:27017\", \"mongo-1.mongo:27017\", \"mongo-2.mongo:27017\"] ) Impact : SLS uses these hosts to establish database connections. Incorrect hosts will cause SLS installation to fail. Multiple hosts enable automatic failover. Related variables : - sls_mongodb.certificates : TLS certificates for secure connection - sls_mongodb.username and sls_mongodb.password : Authentication credentials - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file . sls_mongodb.certificates \u00a4 List of TLS/SSL certificates for secure MongoDB connections. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Provides the TLS/SSL certificates needed to establish secure, encrypted connections between SLS and MongoDB. These certificates verify the MongoDB server's identity and enable encrypted communication. When to use : - Required when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Needed for all MongoDB deployments using TLS/SSL (recommended for production) - Obtain from your MongoDB deployment (CA certificate or server certificate) Valid values : Array of certificate strings in PEM format (e.g., [\"-----BEGIN CERTIFICATE-----\\n...\\n-----END CERTIFICATE-----\"] ) Impact : Without valid certificates, SLS cannot establish secure connections to MongoDB. Invalid certificates will cause connection failures. Related variables : - sls_mongodb.hosts : MongoDB servers these certificates authenticate - sls_mongodb.username and sls_mongodb.password : Used with certificates for authentication - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file . Certificates must match the MongoDB deployment's TLS configuration. sls_mongodb.username \u00a4 MongoDB username for SLS database authentication. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Provides the username credential for SLS to authenticate with MongoDB. This user must have appropriate permissions to read and write to the SLS database. When to use : - Required when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Must correspond to a valid MongoDB user with SLS database permissions - Obtain from your MongoDB deployment configuration Valid values : Valid MongoDB username string Impact : SLS uses this username to authenticate with MongoDB. Invalid username will cause authentication failures and prevent SLS from accessing the database. Related variables : - sls_mongodb.password : Password for this username - sls_mongodb.hosts : MongoDB servers to authenticate against - sls_mongodb.certificates : TLS certificates for secure authentication - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file . Ensure the user has appropriate database permissions. sls_mongodb.password \u00a4 MongoDB password for SLS database authentication. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Provides the password credential for SLS to authenticate with MongoDB. Used together with the username to establish secure database connections. When to use : - Required when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Must correspond to the password for the specified MongoDB username - Obtain from your MongoDB deployment configuration Valid values : Valid MongoDB password string Impact : SLS uses this password to authenticate with MongoDB. Invalid password will cause authentication failures and prevent SLS from accessing the database. Related variables : - sls_mongodb.username : Username for this password - sls_mongodb.hosts : MongoDB servers to authenticate against - sls_mongodb.certificates : TLS certificates for secure authentication - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file . Keep this password secure and do not commit to source control. mas_pod_templates_dir \u00a4 Local directory path containing pod template customization files for SLS. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Allows customization of pod specifications for SLS workloads, enabling control over resource limits, node affinity, tolerations, and other Kubernetes pod settings. This is essential for production deployments with specific infrastructure requirements. When to use : - Use to set custom resource limits (CPU, memory) for SLS pods - Use to configure node affinity or anti-affinity rules - Use to add tolerations for tainted nodes - Use to apply custom security contexts or service accounts - Leave unset for default pod configurations Valid values : Any valid local filesystem path containing ibm-sls-licenseservice.yml Impact : The pod template file will be applied to the LicenseService Custom Resource under spec.podTemplates . Invalid templates can cause pod scheduling failures. Related variables : None Note : This role looks for a file named ibm-sls-licenseservice.yml in the specified directory. The file content should be the YAML block to insert under podTemplates: {object} . For examples, see the BestEfforts reference configuration . For full documentation, refer to Customizing Pod Templates . Bootstrap Variables (SLS 3.7.0 and higher) \u00a4 entitlement_file \u00a4 Local file path to the IBM license entitlement file for bootstrapping SLS. Optional Environment Variable: SLS_ENTITLEMENT_FILE Default: None Purpose : Provides the license entitlement file that defines the MAS product licenses and token allocations available in SLS. This file is obtained from IBM and contains your license entitlements. When to use : - Use to automatically bootstrap SLS with license entitlements during installation - Leave unset if you plan to upload license entitlements manually after SLS installation - Required for fully automated SLS deployments Valid values : Any valid local filesystem path to an IBM license entitlement file (e.g., /home/user/licenses/entitlement.dat ) Impact : When provided, SLS is automatically configured with the license entitlements from this file. Without this, you must manually upload license entitlements through the SLS UI or API after installation. Related variables : None Note : Application Support: SLS 3.7.0 and higher . The license file is obtained from IBM and contains your MAS product entitlements. Keep this file secure as it represents your license rights. Role Variables - Bootstrap [Partly deprecated in SLS 3.7.0] \u00a4 bootstrap.license_file \u00a4 Deprecated in SLS 3.7.0 Defines the License File to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on. Note: this variable used to be called bootstrap.entitlement_file and defaulted to {{mas_config_dir}}/entitlement.lic , this is no longer the case and SLS_LICENSE_FILE has to be set in order to bootstrap. This is now deprecated in SLS 3.7.0. Use this only for versions up to 3.6.0. Optional Environment Variable: SLS_LICENSE_FILE Default: None bootstrap.license_id \u00a4 Deprecated in SLS 3.7.0 Defines the License Id to be used to bootstrap SLS. This must be set when bootstrap.license_file is also set and should match the licenseId from the license file. Don't set if you wish to setup entitlement later on. Note: this is now deprecated in SLS 3.7.0. Use this only for versions up to 3.6.0. Optional unless bootstrap.license_file is set Environment Variable: SLS_LICENSE_ID Default: None bootstrap.registration_key \u00a4 Defines the Registration Key to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on Optional Environment Variable: SLS_REGISTRATION_KEY Default: None SLSCfg Variables \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the SlsCfg configuration will target. Optional, if this or mas_config_dir are not set then the role will not generate a SlsCfg template Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated SlsCfg resource definition. This can be used to manually configure a MAS instance to connect to SLS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a SlsCfg template. Optional (if this or mas_config_dir are not set then the role will not generate a SlsCfg template) Environment Variable: MAS_CONFIG_DIR Default Value: None sls_url \u00a4 The URL of the LicenseService to be called when the Maximo Application Suite is registered with SLS. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_URL Default Value: None mas_license_sync_frequency \u00a4 The sync frequency of user license sync cronjob between Maximo Application Suite and SLS. Optional Environment Variable: MAS_LICENSE_SYNC_FREQUENCY Default Value: */30 * * * * sls_tls_crt \u00a4 The TLS CA certificate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. Takes precedence over sls_tls_crt_local_file_path . Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_TLS_CERT Default Value: None sls_tls_crt_local_file_path \u00a4 The path on the local system to a file containing the TLS CA certificate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. This variable is only used if sls_tls_crt has not been set. Optional (used to instruct the role to set up MAS for an existing SLS instance) Environment Variable: SLS_TLS_CERT_LOCAL_FILE_PATH Default Value: None sls_registration_key \u00a4 The Registration key of the LicenseService instance to be used when the Maximo Application Suite is registered with SLS. Optional Environment Variable: SLS_REGISTRATION_KEY Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None mas_pod_templates_dir \u00a4 Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-slscfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the SlsCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Example Playbook \u00a4 Install and generate a configuration [up to SLS 3.6.0] \u00a4 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxx mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" bootstrap: license_id: \"aa78dd65ef10\" license_file: \"/etc/mas/entitlement.lic\" registration_key: xxxx roles: - ibm.mas_devops.sls Install and upload license file [SLS 3.7.0] \u00a4 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxx mas_instance_id: inst1 sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" entitlement_file: \"/etc/mas/entitlement.lic\" roles: - ibm.mas_devops.sls Install and upload license file [from SLS 3.8.0] \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" entitlement_file: \"/etc/mas/entitlement.lic\" roles: - ibm.mas_devops.sls Generate a configuration for an existing installation \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_action: gencfg sls_tls_crt_local_file_path: \"/home/me/sls.crt\" slscfg_url: \"https://xxx\" slscfg_registration_key: \"xxx\" roles: - ibm.mas_devops.sls Run Role Playbook \u00a4 export SLS_MONGODB_CFG_FILE=/etc/mas/mongodb.yml export SLS_ENTITLEMENT_FILE=/etc/mas/entitlement.lic export MAS_INSTANCE_ID=inst1 ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"sls"},{"location":"roles/sls/#sls","text":"Install IBM Suite License Service and generate a configuration that can be directly applied to IBM Maximo Application Suite. The role assumes that you have already installed the Certificate Manager in the target cluster. This action is performed by the cert_manager role if you want to use this collection to install the cert-manager operator.","title":"sls"},{"location":"roles/sls/#role-variables","text":"","title":"Role Variables"},{"location":"roles/sls/#general-variables","text":"","title":"General Variables"},{"location":"roles/sls/#sls_action","text":"Specifies which operation to perform on the Suite License Service (SLS) instance. Optional Environment Variable: SLS_ACTION Default: install Purpose : Controls what action the role executes against the SLS instance. This allows the same role to handle installation, configuration generation, and removal of SLS. When to use : - Use install for initial SLS deployment or updates - Use gencfg to generate SLS configuration for MAS without installing SLS (when using existing SLS) - Use uninstall to remove SLS instance (use with caution) Valid values : install , gencfg , uninstall Impact : - install : Deploys or updates SLS operator and instance - gencfg : Only generates SLSCfg resource for MAS integration - uninstall : Removes SLS instance and operator (destructive operation) Related variables : When using gencfg , requires sls_url to be set to point to existing SLS instance. Note : Always backup license data before using uninstall . The gencfg action is useful when SLS is shared across multiple MAS instances.","title":"sls_action"},{"location":"roles/sls/#role-variables-installation","text":"If sls_url is set then the role will skip the installation of an SLS instance and simply generate the SLSCfg resource for the SLS instance defined.","title":"Role Variables - Installation"},{"location":"roles/sls/#artifactory_username","text":"Username for authenticating to IBM Artifactory to access development builds of SLS. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None Purpose : Provides authentication credentials to pull development/pre-release SLS operator images from IBM's Artifactory registry. Required only when installing development builds for testing or early access. When to use : - Required when sls_catalog_source is set to ibm-sls-operators (development catalog) - Not needed for production installations using ibm-operator-catalog - Use your IBM w3Id username for development builds Valid values : Valid IBM Artifactory username (typically your w3Id) Impact : Without valid credentials, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_token : Must be set together with this username - sls_catalog_source : Determines if Artifactory credentials are needed Note : Only required for development/pre-release builds. Production installations use ibm_entitlement_key instead (for SLS 3.7.0 and earlier).","title":"artifactory_username"},{"location":"roles/sls/#artifactory_token","text":"API token for authenticating to IBM Artifactory to access development builds of SLS. Optional Environment Variable: ARTIFACTORY_TOKEN Default: None Purpose : Provides the API token/password credential to authenticate with IBM's Artifactory registry when pulling development/pre-release SLS operator images. When to use : - Required when sls_catalog_source is set to ibm-sls-operators (development catalog) - Not needed for production installations using ibm-operator-catalog - Use your IBM Artifactory API key for development builds Valid values : Valid IBM Artifactory API token Impact : Without a valid token, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_username : Must be set together with this token - sls_catalog_source : Determines if Artifactory credentials are needed Note : Only required for development/pre-release builds. Keep this token secure and do not commit to source control.","title":"artifactory_token"},{"location":"roles/sls/#sls_catalog_source","text":"Specifies the OpenShift operator catalog source containing the SLS operator subscription. Optional Environment Variable: SLS_CATALOG_SOURCE Default: ibm-operator-catalog Purpose : Controls which operator catalog is used to locate and install the SLS operator. This determines where OpenShift looks for the operator images and metadata during installation. When to use : - Use default ibm-operator-catalog for production installations with official IBM releases - Use ibm-sls-operators for development builds from Artifactory - Only change if directed to use a custom catalog for specific testing or airgap scenarios Valid values : Any valid CatalogSource name present in the openshift-marketplace namespace (typically ibm-operator-catalog or ibm-sls-operators ) Impact : Changing this value affects which operator versions are available for installation. An invalid catalog source will cause the subscription to fail. Development catalogs require additional authentication via artifactory_username and artifactory_token . Related variables : - sls_channel : Works together to determine the specific operator version installed - artifactory_username and artifactory_token : Required when using ibm-sls-operators Note : For development catalogs, you must also provide Artifactory credentials.","title":"sls_catalog_source"},{"location":"roles/sls/#sls_channel","text":"Specifies the SLS operator subscription channel, which determines the version stream you'll receive updates from. Optional Environment Variable: SLS_CHANNEL Default: 3.x Purpose : Controls which version of SLS will be installed and which updates will be automatically applied. The channel corresponds to major version releases and determines the feature set and compatibility level of your SLS installation. When to use : - Use default 3.x for latest SLS 3.x releases (recommended for most deployments) - Use specific older channels only if required for compatibility with older MAS versions - Consult the MAS compatibility matrix before selecting a channel Valid values : 3.x (check the IBM Operator Catalog for currently available channels) Impact : The channel determines which SLS version is installed and which automatic updates are applied. Changing channels after installation will trigger an upgrade to the latest version in that channel. Related variables : Works with sls_catalog_source to determine available channels. Note : SLS 3.8.0+ no longer requires IBM entitlement keys as images moved to public registry. Review the SLS release notes before changing this value.","title":"sls_channel"},{"location":"roles/sls/#sls_namespace","text":"OpenShift namespace where the SLS operator and instance will be deployed. Optional Environment Variable: SLS_NAMESPACE Default: ibm-sls Purpose : Defines the Kubernetes namespace for SLS resources, providing isolation and organization for the SLS deployment within the cluster. When to use : - Use default ibm-sls for standard deployments - Change only if you need multiple SLS instances or have namespace naming requirements - Ensure namespace doesn't conflict with existing deployments Valid values : Any valid Kubernetes namespace name (lowercase alphanumeric and hyphens) Impact : All SLS resources (operator, pods, secrets, services, ConfigMaps) will be created in this namespace. Changing this after deployment requires reinstallation. Related variables : Used in SLSCfg generation to reference SLS service endpoints. Note : The namespace will be created if it doesn't exist. Ensure you have permissions to create namespaces in the cluster.","title":"sls_namespace"},{"location":"roles/sls/#sls_icr_cpopen","text":"Container registry source for SLS operator and component images. Optional Environment Variable: SLS_ICR_CPOPEN Default: icr.io/cpopen Purpose : Specifies the container registry from which SLS pulls all operator and component images. From SLS 3.8.0 onwards, this is the primary registry variable as images moved to the public IBM Container Registry. When to use : - Use default icr.io/cpopen for production installations (SLS 3.8.0+) - Override only for development images or airgap/mirror scenarios - No authentication required for default public registry Valid values : Any valid container registry URL (e.g., icr.io/cpopen , my-registry.com/sls ) Impact : All SLS images will be pulled from this registry. An incorrect or inaccessible registry will cause image pull failures. The default public registry requires no entitlement key (SLS 3.8.0+). Related variables : - sls_icr_cp : Deprecated in SLS 3.8.0, only needed for SLS 3.7.0 and earlier - ibm_entitlement_key : Not required when using default public registry (SLS 3.8.0+) Note : SLS 3.8.0+ uses public registry - no entitlement key needed. For SLS 3.7.0 and earlier, use sls_icr_cp and provide ibm_entitlement_key .","title":"sls_icr_cpopen"},{"location":"roles/sls/#sls_instance_name","text":"Unique identifier for this SLS installation within the cluster. Optional Environment Variable: SLS_INSTANCE_NAME Default: sls Purpose : Provides a unique identifier that distinguishes this SLS installation from others that may exist in the same cluster. This ID is used to generate resource names throughout the installation. When to use : - Use default sls for standard single-SLS deployments - Change only if you need multiple SLS instances in the same cluster - Must be unique within the cluster if running multiple SLS instances Valid values : Lowercase alphanumeric string (e.g., sls , sls-prod , sls-dev ) Impact : This ID becomes part of resource names and is embedded in many SLS resources. It cannot be changed after installation without reinstalling. Related variables : Used in SLSCfg generation to identify the SLS instance. Note : Choose carefully as this cannot be changed after installation. Most deployments use the default sls value.","title":"sls_instance_name"},{"location":"roles/sls/#sls_icr_cp","text":"Container registry source for SLS images (SLS 3.7.0 and earlier only). Optional Environment Variable: SLS_ICR_CP Default: cp.icr.io/cp Purpose : Specifies the container registry for SLS images in versions 3.7.0 and earlier. This registry required IBM entitlement key authentication. When to use : - Only required for SLS versions 3.7.0 and earlier - Not needed for SLS 3.8.0+ (images moved to public registry) - Override only for development images or airgap scenarios with older SLS versions Valid values : Any valid container registry URL (e.g., cp.icr.io/cp , my-registry.com/sls ) Impact : For SLS 3.7.0 and earlier, all SLS images will be pulled from this registry. Requires ibm_entitlement_key for authentication. Related variables : - sls_icr_cpopen : Use this instead for SLS 3.8.0+ - ibm_entitlement_key : Required when using this registry (SLS 3.7.0 and earlier) Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry ( icr.io/cpopen ). This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, use sls_icr_cpopen instead (no entitlement key needed).","title":"sls_icr_cp"},{"location":"roles/sls/#ibm_entitlement_key","text":"IBM entitlement key for accessing IBM Container Registry images (SLS 3.7.0 and earlier only). Required for SLS 3.7.0 and earlier (unless sls_url is provided) Environment Variable: IBM_ENTITLEMENT_KEY Default: None Purpose : Authenticates access to IBM's entitled container registry to pull SLS operator and component images. This key is tied to your IBM Cloud account and product entitlements. When to use : - Required for SLS versions 3.7.0 and earlier - Not required for SLS 3.8.0 and later (images moved to public registry) - Obtain from IBM Container Library Valid values : IBM entitlement key string (typically starts with \"eyJ...\") Impact : Invalid or expired keys will cause image pull failures for SLS 3.7.0 and earlier. The key is stored in a Kubernetes secret in the SLS namespace. Related variables : - sls_entitlement_username : Username paired with this key (default: cp ) - sls_icr_cp : Registry requiring this authentication (SLS 3.7.0 and earlier) Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry ( icr.io/cpopen ). This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, no entitlement key is needed.","title":"ibm_entitlement_key"},{"location":"roles/sls/#sls_entitlement_username","text":"Username for authenticating to IBM Container Registry (SLS 3.7.0 and earlier only). Optional Environment Variable: SLS_ENTITLEMENT_USERNAME Default: cp Purpose : Provides the username credential for authenticating with IBM's entitled container registry when pulling SLS images for versions 3.7.0 and earlier. When to use : - Only needed for SLS versions 3.7.0 and earlier - Not required for SLS 3.8.0+ (images moved to public registry) - Usually can be left at default cp for production installations Valid values : cp (for production installations with IBM entitlement key) Impact : Used together with ibm_entitlement_key to create image pull secrets for the SLS namespace. Incorrect username will cause image pull authentication failures. Related variables : - ibm_entitlement_key : Must be set together with this username (SLS 3.7.0 and earlier) - sls_entitlement_key : Alternative entitlement key specific for SLS Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry. This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, no authentication is needed.","title":"sls_entitlement_username"},{"location":"roles/sls/#sls_entitlement_key","text":"SLS-specific IBM entitlement key override (SLS 3.7.0 and earlier only). Optional Environment Variable: SLS_ENTITLEMENT_KEY Default: None (falls back to ibm_entitlement_key ) Purpose : Provides an SLS-specific IBM entitlement key that overrides the global ibm_entitlement_key variable. Primarily used in development scenarios where different keys are needed for different components. When to use : - Only needed for SLS versions 3.7.0 and earlier - Use when you need a different entitlement key specifically for SLS - Leave unset to use the global ibm_entitlement_key value - Not required for SLS 3.8.0+ (images moved to public registry) Valid values : IBM entitlement key string (typically starts with \"eyJ...\") Impact : When set, this key is used instead of ibm_entitlement_key for SLS image authentication. Invalid keys will cause image pull failures. Related variables : - ibm_entitlement_key : Global entitlement key that this variable overrides - sls_entitlement_username : Username paired with this key Note : DEPRECATED in SLS 3.8.0 - SLS images moved to public registry. This variable is only required for SLS versions up to 3.7.0. For SLS 3.8.0+, no entitlement key is needed.","title":"sls_entitlement_key"},{"location":"roles/sls/#role-variables-configuration","text":"","title":"Role Variables - Configuration"},{"location":"roles/sls/#sls_domain","text":"Custom domain name for external access to SLS via OpenShift routes. Optional Environment Variable: SLS_DOMAIN Default: None (SLS is only accessible within the cluster) Purpose : Enables external access to SLS through a custom domain route. This is essential when SLS needs to serve multiple MAS instances deployed in separate OpenShift clusters. When to use : - Required when SLS serves MAS instances in different OpenShift clusters - Required for multi-cluster SLS deployments - Leave unset for single-cluster deployments where SLS and MAS are in the same cluster Valid values : Any valid DNS domain name (e.g., sls.mycompany.com , license.example.org ) Impact : When set, SLS creates an external route accessible at the specified domain. You must ensure DNS is properly configured to resolve this domain to your cluster. When not set, SLS is only accessible via internal cluster services. Related variables : Affects how MAS instances connect to SLS (internal service vs external route). Note : Ensure proper DNS configuration and SSL certificates for the domain. External access requires appropriate network security controls.","title":"sls_domain"},{"location":"roles/sls/#sls_auth_enforce","text":"Controls whether SLS enforces client authentication via mutual TLS (mTLS). Optional Environment Variable: SLS_AUTH_ENFORCE Default: True Purpose : Determines whether SLS requires clients to authenticate using mTLS certificates generated through the client registration flow. This provides secure authentication for SLS API access. When to use : - Use True (default) for production environments to enforce secure authentication - Set to False only in development/testing environments where simplified access is needed - Leave as default for standard secure deployments Valid values : True , False Impact : - When True : Clients must register with SLS and use mTLS certificates for API calls (secure) - When False : Authentication is not enforced, allowing unauthenticated API access (insecure) Related variables : Works with sls_registration_open to control client registration and access. Note : Setting to False removes authentication requirements and should only be used in non-production environments. Production deployments should always enforce authentication.","title":"sls_auth_enforce"},{"location":"roles/sls/#sls_mongo_retrywrites","text":"Controls whether SLS uses MongoDB retryable writes feature. Optional Environment Variable: SLS_MONGO_RETRYWRITES Default: true Purpose : Determines whether SLS enables MongoDB's retryable writes feature, which automatically retries certain write operations that fail due to transient network errors or replica set elections. When to use : - Use true (default) when using MongoDB Community Edition or IBM Cloud Databases for MongoDB - Set to false when using AWS DocumentDB (which doesn't support retryable writes) - Set to false for any MongoDB-compatible database that doesn't support this feature Valid values : true , false Impact : - When true : Enables automatic retry of failed write operations (improves reliability) - When false : Disables retryable writes (required for DocumentDB and similar services) Related variables : - sls_mongodb_cfg_file or sls_mongodb.* : Determines which MongoDB service is used - Must align with the capabilities of your MongoDB provider Note : AWS DocumentDB does not support retryable writes. Set to false when using DocumentDB to avoid connection errors.","title":"sls_mongo_retrywrites"},{"location":"roles/sls/#sls_compliance_enforce","text":"Controls whether SLS enforces license token compliance. Optional Environment Variable: SLS_COMPLIANCE_ENFORCE Default: True Purpose : Determines whether SLS blocks license checkout requests when insufficient tokens are available. This controls whether license limits are strictly enforced or merely tracked. When to use : - Use True (default) for production to enforce license compliance and prevent over-consumption - Set to False only in development/testing environments or during grace periods - Leave as default for standard compliant deployments Valid values : True , False Impact : - When True : License checkout requests are denied when insufficient tokens are available (enforces compliance) - When False : License checkout requests succeed even without available tokens (tracks but doesn't enforce) Related variables : Works with license entitlement configuration to control token availability. Note : Setting to False allows operation beyond licensed capacity and may result in license compliance issues. Use only in non-production environments or with explicit approval.","title":"sls_compliance_enforce"},{"location":"roles/sls/#sls_registration_open","text":"Controls whether SLS allows new client self-registration. Optional Environment Variable: SLS_REGISTRATION_OPEN Default: True Purpose : Determines whether clients can register themselves with SLS to obtain certificates for API access. This controls the initial onboarding process for new SLS clients. When to use : - Use True (default) to allow MAS instances and other clients to self-register - Set to False after all clients are registered to prevent unauthorized registrations - Leave as default for standard deployments where clients need to register Valid values : True , False Impact : - When True : Clients can register themselves and obtain mTLS certificates for SLS API access - When False : New client registrations are blocked; only pre-registered clients can access SLS Related variables : Works with sls_auth_enforce to control authentication and registration flow. Note : For production environments, consider setting to False after all legitimate clients are registered to prevent unauthorized access.","title":"sls_registration_open"},{"location":"roles/sls/#sls_mongodb_cfg_file","text":"Local file path to a MongoCfg YAML file for SLS MongoDB configuration. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: SLS_MONGODB_CFG_FILE Default: None Purpose : Provides MongoDB connection details to SLS by referencing a MongoCfg file generated by the mongodb role. This simplifies configuration by reusing existing MongoDB setup information. When to use : - Use when you've deployed MongoDB using the mongodb role (which generates this file) - Preferred method as it ensures consistency with MongoDB deployment - Alternative to manually specifying sls_mongodb.* variables Valid values : Any valid local filesystem path to a MongoCfg YAML file (e.g., /home/user/masconfig/mongocfg-mongoce-system.yaml ) Impact : The role extracts MongoDB connection details (hosts, certificates, credentials) from this file to configure SLS. Invalid or missing file will cause installation to fail. Related variables : - sls_mongodb.* : Alternative method to specify MongoDB connection details - mas_config_dir : Directory where mongodb role generates this file Note : Either this variable or the sls_mongodb object must be provided. Using this file is recommended for consistency with MongoDB deployment.","title":"sls_mongodb_cfg_file"},{"location":"roles/sls/#sls_mongodbhosts","text":"List of MongoDB host:port pairs for SLS database connection. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Specifies the MongoDB replica set members that SLS will connect to for license data storage. Multiple hosts provide high availability and failover capability. When to use : - Use when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Required for all MongoDB deployment types (Community, IBM Cloud, AWS DocumentDB) - Provide all replica set members for high availability Valid values : Array of strings in format [\"host1:port1\", \"host2:port2\", \"host3:port3\"] (e.g., [\"mongo-0.mongo:27017\", \"mongo-1.mongo:27017\", \"mongo-2.mongo:27017\"] ) Impact : SLS uses these hosts to establish database connections. Incorrect hosts will cause SLS installation to fail. Multiple hosts enable automatic failover. Related variables : - sls_mongodb.certificates : TLS certificates for secure connection - sls_mongodb.username and sls_mongodb.password : Authentication credentials - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file .","title":"sls_mongodb.hosts"},{"location":"roles/sls/#sls_mongodbcertificates","text":"List of TLS/SSL certificates for secure MongoDB connections. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Provides the TLS/SSL certificates needed to establish secure, encrypted connections between SLS and MongoDB. These certificates verify the MongoDB server's identity and enable encrypted communication. When to use : - Required when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Needed for all MongoDB deployments using TLS/SSL (recommended for production) - Obtain from your MongoDB deployment (CA certificate or server certificate) Valid values : Array of certificate strings in PEM format (e.g., [\"-----BEGIN CERTIFICATE-----\\n...\\n-----END CERTIFICATE-----\"] ) Impact : Without valid certificates, SLS cannot establish secure connections to MongoDB. Invalid certificates will cause connection failures. Related variables : - sls_mongodb.hosts : MongoDB servers these certificates authenticate - sls_mongodb.username and sls_mongodb.password : Used with certificates for authentication - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file . Certificates must match the MongoDB deployment's TLS configuration.","title":"sls_mongodb.certificates"},{"location":"roles/sls/#sls_mongodbusername","text":"MongoDB username for SLS database authentication. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Provides the username credential for SLS to authenticate with MongoDB. This user must have appropriate permissions to read and write to the SLS database. When to use : - Required when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Must correspond to a valid MongoDB user with SLS database permissions - Obtain from your MongoDB deployment configuration Valid values : Valid MongoDB username string Impact : SLS uses this username to authenticate with MongoDB. Invalid username will cause authentication failures and prevent SLS from accessing the database. Related variables : - sls_mongodb.password : Password for this username - sls_mongodb.hosts : MongoDB servers to authenticate against - sls_mongodb.certificates : TLS certificates for secure authentication - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file . Ensure the user has appropriate database permissions.","title":"sls_mongodb.username"},{"location":"roles/sls/#sls_mongodbpassword","text":"MongoDB password for SLS database authentication. Required (Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS) Environment Variable: None Default: None Purpose : Provides the password credential for SLS to authenticate with MongoDB. Used together with the username to establish secure database connections. When to use : - Required when manually configuring MongoDB connection (not using sls_mongodb_cfg_file ) - Must correspond to the password for the specified MongoDB username - Obtain from your MongoDB deployment configuration Valid values : Valid MongoDB password string Impact : SLS uses this password to authenticate with MongoDB. Invalid password will cause authentication failures and prevent SLS from accessing the database. Related variables : - sls_mongodb.username : Username for this password - sls_mongodb.hosts : MongoDB servers to authenticate against - sls_mongodb.certificates : TLS certificates for secure authentication - sls_mongodb_cfg_file : Alternative method to provide this information Note : Part of the sls_mongodb object. All sls_mongodb.* variables must be provided together if not using sls_mongodb_cfg_file . Keep this password secure and do not commit to source control.","title":"sls_mongodb.password"},{"location":"roles/sls/#mas_pod_templates_dir","text":"Local directory path containing pod template customization files for SLS. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Allows customization of pod specifications for SLS workloads, enabling control over resource limits, node affinity, tolerations, and other Kubernetes pod settings. This is essential for production deployments with specific infrastructure requirements. When to use : - Use to set custom resource limits (CPU, memory) for SLS pods - Use to configure node affinity or anti-affinity rules - Use to add tolerations for tainted nodes - Use to apply custom security contexts or service accounts - Leave unset for default pod configurations Valid values : Any valid local filesystem path containing ibm-sls-licenseservice.yml Impact : The pod template file will be applied to the LicenseService Custom Resource under spec.podTemplates . Invalid templates can cause pod scheduling failures. Related variables : None Note : This role looks for a file named ibm-sls-licenseservice.yml in the specified directory. The file content should be the YAML block to insert under podTemplates: {object} . For examples, see the BestEfforts reference configuration . For full documentation, refer to Customizing Pod Templates .","title":"mas_pod_templates_dir"},{"location":"roles/sls/#bootstrap-variables-sls-370-and-higher","text":"","title":"Bootstrap Variables (SLS 3.7.0 and higher)"},{"location":"roles/sls/#entitlement_file","text":"Local file path to the IBM license entitlement file for bootstrapping SLS. Optional Environment Variable: SLS_ENTITLEMENT_FILE Default: None Purpose : Provides the license entitlement file that defines the MAS product licenses and token allocations available in SLS. This file is obtained from IBM and contains your license entitlements. When to use : - Use to automatically bootstrap SLS with license entitlements during installation - Leave unset if you plan to upload license entitlements manually after SLS installation - Required for fully automated SLS deployments Valid values : Any valid local filesystem path to an IBM license entitlement file (e.g., /home/user/licenses/entitlement.dat ) Impact : When provided, SLS is automatically configured with the license entitlements from this file. Without this, you must manually upload license entitlements through the SLS UI or API after installation. Related variables : None Note : Application Support: SLS 3.7.0 and higher . The license file is obtained from IBM and contains your MAS product entitlements. Keep this file secure as it represents your license rights.","title":"entitlement_file"},{"location":"roles/sls/#role-variables-bootstrap-partly-deprecated-in-sls-370","text":"","title":"Role Variables - Bootstrap [Partly deprecated in SLS 3.7.0]"},{"location":"roles/sls/#bootstraplicense_file","text":"Deprecated in SLS 3.7.0 Defines the License File to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on. Note: this variable used to be called bootstrap.entitlement_file and defaulted to {{mas_config_dir}}/entitlement.lic , this is no longer the case and SLS_LICENSE_FILE has to be set in order to bootstrap. This is now deprecated in SLS 3.7.0. Use this only for versions up to 3.6.0. Optional Environment Variable: SLS_LICENSE_FILE Default: None","title":"bootstrap.license_file"},{"location":"roles/sls/#bootstraplicense_id","text":"Deprecated in SLS 3.7.0 Defines the License Id to be used to bootstrap SLS. This must be set when bootstrap.license_file is also set and should match the licenseId from the license file. Don't set if you wish to setup entitlement later on. Note: this is now deprecated in SLS 3.7.0. Use this only for versions up to 3.6.0. Optional unless bootstrap.license_file is set Environment Variable: SLS_LICENSE_ID Default: None","title":"bootstrap.license_id"},{"location":"roles/sls/#bootstrapregistration_key","text":"Defines the Registration Key to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on Optional Environment Variable: SLS_REGISTRATION_KEY Default: None","title":"bootstrap.registration_key"},{"location":"roles/sls/#slscfg-variables","text":"","title":"SLSCfg Variables"},{"location":"roles/sls/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the SlsCfg configuration will target. Optional, if this or mas_config_dir are not set then the role will not generate a SlsCfg template Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/sls/#mas_config_dir","text":"Local directory to save the generated SlsCfg resource definition. This can be used to manually configure a MAS instance to connect to SLS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a SlsCfg template. Optional (if this or mas_config_dir are not set then the role will not generate a SlsCfg template) Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/sls/#sls_url","text":"The URL of the LicenseService to be called when the Maximo Application Suite is registered with SLS. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_URL Default Value: None","title":"sls_url"},{"location":"roles/sls/#mas_license_sync_frequency","text":"The sync frequency of user license sync cronjob between Maximo Application Suite and SLS. Optional Environment Variable: MAS_LICENSE_SYNC_FREQUENCY Default Value: */30 * * * *","title":"mas_license_sync_frequency"},{"location":"roles/sls/#sls_tls_crt","text":"The TLS CA certificate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. Takes precedence over sls_tls_crt_local_file_path . Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_TLS_CERT Default Value: None","title":"sls_tls_crt"},{"location":"roles/sls/#sls_tls_crt_local_file_path","text":"The path on the local system to a file containing the TLS CA certificate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. This variable is only used if sls_tls_crt has not been set. Optional (used to instruct the role to set up MAS for an existing SLS instance) Environment Variable: SLS_TLS_CERT_LOCAL_FILE_PATH Default Value: None","title":"sls_tls_crt_local_file_path"},{"location":"roles/sls/#sls_registration_key","text":"The Registration key of the LicenseService instance to be used when the Maximo Application Suite is registered with SLS. Optional Environment Variable: SLS_REGISTRATION_KEY Default Value: None","title":"sls_registration_key"},{"location":"roles/sls/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/sls/#mas_pod_templates_dir_1","text":"Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-slscfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the SlsCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None","title":"mas_pod_templates_dir"},{"location":"roles/sls/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/sls/#install-and-generate-a-configuration-up-to-sls-360","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxx mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" bootstrap: license_id: \"aa78dd65ef10\" license_file: \"/etc/mas/entitlement.lic\" registration_key: xxxx roles: - ibm.mas_devops.sls","title":"Install and generate a configuration [up to SLS 3.6.0]"},{"location":"roles/sls/#install-and-upload-license-file-sls-370","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxx mas_instance_id: inst1 sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" entitlement_file: \"/etc/mas/entitlement.lic\" roles: - ibm.mas_devops.sls","title":"Install and upload license file [SLS 3.7.0]"},{"location":"roles/sls/#install-and-upload-license-file-from-sls-380","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" entitlement_file: \"/etc/mas/entitlement.lic\" roles: - ibm.mas_devops.sls","title":"Install and upload license file [from SLS 3.8.0]"},{"location":"roles/sls/#generate-a-configuration-for-an-existing-installation","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_action: gencfg sls_tls_crt_local_file_path: \"/home/me/sls.crt\" slscfg_url: \"https://xxx\" slscfg_registration_key: \"xxx\" roles: - ibm.mas_devops.sls","title":"Generate a configuration for an existing installation"},{"location":"roles/sls/#run-role-playbook","text":"export SLS_MONGODB_CFG_FILE=/etc/mas/mongodb.yml export SLS_ENTITLEMENT_FILE=/etc/mas/entitlement.lic export MAS_INSTANCE_ID=inst1 ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/sls/#license","text":"EPL-2.0","title":"License"},{"location":"roles/smtp/","text":"smtp \u00a4 Generate an SMTP configuration that can be directly applied to IBM Maximo Application Suite. The role supports the Twilio SendGrid email provider. Twilio SendGrid Prior to running this role, you must create an account with SendGrid. The SendGrid account needs to support creating subusers. Tip The role will generate a yaml file containing the definition of a Secret and SmtpCfg resource that can be used to configure the smtp email provider for MAS. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/smtp-$MAS_INSTANCE_ID.yml\" or used in conjunction with the suite_config role. This role will create a subuser that must be validated. An email with a validation link will be sent to the primary email address. You need to validate the subuser using this link. If validation fails, you can resend the email using the SendGrid admin UI. Role Variables \u00a4 smtp_type \u00a4 Required. Specify the smtp provider. Currently the supported value is sendgrid . Environment Variable: SMTP_TYPE Default: None mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the SmtpCfg configuration will target. Required. If this or mas_config_dir are not set then the role will not generate a SmtpCfg template Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated SmtpCfg resource definition. This can be used to manually configure a MAS instance to connect to smtp provider, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a SmtpCfg template. Required. if this or mas_config_dir are not set then the role will not generate a SmtpCfg template Environment Variable: MAS_CONFIG_DIR Default Value: None mas_pod_templates_dir \u00a4 Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-smtpcfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the SmtpCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None sendgrid_primary_username \u00a4 Required. Username of the existing SendGrid account. Environment Variable: SMTP_PRIMARY_USERNAME Default: None sendgrid_primary_email \u00a4 Required. Email of the existing SendGrid account. Environment Variable: SMTP_PRIMARY_EMAIL Default: None sendgrid_subuser_email \u00a4 Required. Email of the SendGrid subuser. This role creates a SendGrid subuser for sending emails subusers Environment Variable: SMTP_SUBUSER_EMAIL Default: None sendgrid_defaultrecipientemail \u00a4 Required. Default destination email address. Environment Variable: SENDGRID_DEFAULTRECIPIENTEMAIL Default: None sendgrid_primary_apikey \u00a4 Required. Apikey of the existing SendGrid account. Environment Variable: SENDGRID_PRIMARY_APIKEY Default: None sendgrid_ips \u00a4 Required. ips of the existing SendGrid account. The primary SendGrid account has one or more IP Addresses associated with it. Specify the list of SendGrid IP Addresses to associate with the subuser. Environment Variable: SENDGRID_IPS Default: None sendgrid_subuser_username \u00a4 Optional. Username of the SendGrid subuser. This role creates a SendGrid subuser for sending emails subusers Environment Variable: SMTP_SUBUSER_USERNAME Default: ibm-mas_$MAS_INSTANCE_ID sendgrid_defaultsendername \u00a4 Optional. Easily readable name displayed in emails sent by the subuser Environment Variable: SENDGRID_DEFAULTSENDERNAME Default: '' sendgrid_defaultshouldemailpasswords \u00a4 Optional. Flag to indicate if the password should be sent by email. Environment Variable: SENDGRID_DEFAULTSHOULDEMAILPASSWORDS Default: false sendgrid_configscope \u00a4 Optional Environment Variable: SENDGRID_CONFIGSCOPE Default: system sendgrid_hostname \u00a4 Optional Environment Variable: SENDGRID_HOSTNAME Default: smtp.sendgrid.net sendgrid_port \u00a4 Optional Environment Variable: SENDGRID_PORT Default: 465 sendgrid_security \u00a4 Optional Environment Variable: SENDGRID_SECURITY Default: SSL sendgrid_authentication \u00a4 Optional Environment Variable: SENDGRID_AUTHENTICATION Default: true sendgrid_api_url \u00a4 Optional. The api URL of the smtp email service. This URL is used for REST calls. Environment Variable: SENDGRID_API_URL Default: api.sendgrid.com custom_labels \u00a4 Optional. List of comma separated key=value pairs for setting custom labels on instance specific resources. Environment Variable: CUSTOM_LABELS Default: None sendgrid_debug \u00a4 Optional. When set to True, the results of the SendGrid REST calls will be displayed in the log. Environment Variable: SENDGRID_DEBUG Default: False Example Playbook \u00a4 Generate a configuration \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me/masconfig smtp_type: sendgrid sendgrid_primary_username: myusername sendgrid_primary_email: myemail@mydomain sendgrid_subuser_username: mysubusername sendgrid_subuser_email: mysubuser@mydomain sendgrid_defaultrecipientemail: myemail@mydomain sendgrid_defaultsendername: 'My Name' sendgrid_primary_apikey: xxxx sendgrid_ips: '[\"XXX.XXX.XXX.XXX\"]' roles: - ibm.mas_devops.smtp License \u00a4 EPL-2.0","title":"smtp"},{"location":"roles/smtp/#smtp","text":"Generate an SMTP configuration that can be directly applied to IBM Maximo Application Suite. The role supports the Twilio SendGrid email provider. Twilio SendGrid Prior to running this role, you must create an account with SendGrid. The SendGrid account needs to support creating subusers. Tip The role will generate a yaml file containing the definition of a Secret and SmtpCfg resource that can be used to configure the smtp email provider for MAS. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/smtp-$MAS_INSTANCE_ID.yml\" or used in conjunction with the suite_config role. This role will create a subuser that must be validated. An email with a validation link will be sent to the primary email address. You need to validate the subuser using this link. If validation fails, you can resend the email using the SendGrid admin UI.","title":"smtp"},{"location":"roles/smtp/#role-variables","text":"","title":"Role Variables"},{"location":"roles/smtp/#smtp_type","text":"Required. Specify the smtp provider. Currently the supported value is sendgrid . Environment Variable: SMTP_TYPE Default: None","title":"smtp_type"},{"location":"roles/smtp/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the SmtpCfg configuration will target. Required. If this or mas_config_dir are not set then the role will not generate a SmtpCfg template Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/smtp/#mas_config_dir","text":"Local directory to save the generated SmtpCfg resource definition. This can be used to manually configure a MAS instance to connect to smtp provider, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a SmtpCfg template. Required. if this or mas_config_dir are not set then the role will not generate a SmtpCfg template Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/smtp/#mas_pod_templates_dir","text":"Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-smtpcfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the SmtpCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None","title":"mas_pod_templates_dir"},{"location":"roles/smtp/#sendgrid_primary_username","text":"Required. Username of the existing SendGrid account. Environment Variable: SMTP_PRIMARY_USERNAME Default: None","title":"sendgrid_primary_username"},{"location":"roles/smtp/#sendgrid_primary_email","text":"Required. Email of the existing SendGrid account. Environment Variable: SMTP_PRIMARY_EMAIL Default: None","title":"sendgrid_primary_email"},{"location":"roles/smtp/#sendgrid_subuser_email","text":"Required. Email of the SendGrid subuser. This role creates a SendGrid subuser for sending emails subusers Environment Variable: SMTP_SUBUSER_EMAIL Default: None","title":"sendgrid_subuser_email"},{"location":"roles/smtp/#sendgrid_defaultrecipientemail","text":"Required. Default destination email address. Environment Variable: SENDGRID_DEFAULTRECIPIENTEMAIL Default: None","title":"sendgrid_defaultrecipientemail"},{"location":"roles/smtp/#sendgrid_primary_apikey","text":"Required. Apikey of the existing SendGrid account. Environment Variable: SENDGRID_PRIMARY_APIKEY Default: None","title":"sendgrid_primary_apikey"},{"location":"roles/smtp/#sendgrid_ips","text":"Required. ips of the existing SendGrid account. The primary SendGrid account has one or more IP Addresses associated with it. Specify the list of SendGrid IP Addresses to associate with the subuser. Environment Variable: SENDGRID_IPS Default: None","title":"sendgrid_ips"},{"location":"roles/smtp/#sendgrid_subuser_username","text":"Optional. Username of the SendGrid subuser. This role creates a SendGrid subuser for sending emails subusers Environment Variable: SMTP_SUBUSER_USERNAME Default: ibm-mas_$MAS_INSTANCE_ID","title":"sendgrid_subuser_username"},{"location":"roles/smtp/#sendgrid_defaultsendername","text":"Optional. Easily readable name displayed in emails sent by the subuser Environment Variable: SENDGRID_DEFAULTSENDERNAME Default: ''","title":"sendgrid_defaultsendername"},{"location":"roles/smtp/#sendgrid_defaultshouldemailpasswords","text":"Optional. Flag to indicate if the password should be sent by email. Environment Variable: SENDGRID_DEFAULTSHOULDEMAILPASSWORDS Default: false","title":"sendgrid_defaultshouldemailpasswords"},{"location":"roles/smtp/#sendgrid_configscope","text":"Optional Environment Variable: SENDGRID_CONFIGSCOPE Default: system","title":"sendgrid_configscope"},{"location":"roles/smtp/#sendgrid_hostname","text":"Optional Environment Variable: SENDGRID_HOSTNAME Default: smtp.sendgrid.net","title":"sendgrid_hostname"},{"location":"roles/smtp/#sendgrid_port","text":"Optional Environment Variable: SENDGRID_PORT Default: 465","title":"sendgrid_port"},{"location":"roles/smtp/#sendgrid_security","text":"Optional Environment Variable: SENDGRID_SECURITY Default: SSL","title":"sendgrid_security"},{"location":"roles/smtp/#sendgrid_authentication","text":"Optional Environment Variable: SENDGRID_AUTHENTICATION Default: true","title":"sendgrid_authentication"},{"location":"roles/smtp/#sendgrid_api_url","text":"Optional. The api URL of the smtp email service. This URL is used for REST calls. Environment Variable: SENDGRID_API_URL Default: api.sendgrid.com","title":"sendgrid_api_url"},{"location":"roles/smtp/#custom_labels","text":"Optional. List of comma separated key=value pairs for setting custom labels on instance specific resources. Environment Variable: CUSTOM_LABELS Default: None","title":"custom_labels"},{"location":"roles/smtp/#sendgrid_debug","text":"Optional. When set to True, the results of the SendGrid REST calls will be displayed in the log. Environment Variable: SENDGRID_DEBUG Default: False","title":"sendgrid_debug"},{"location":"roles/smtp/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/smtp/#generate-a-configuration","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me/masconfig smtp_type: sendgrid sendgrid_primary_username: myusername sendgrid_primary_email: myemail@mydomain sendgrid_subuser_username: mysubusername sendgrid_subuser_email: mysubuser@mydomain sendgrid_defaultrecipientemail: myemail@mydomain sendgrid_defaultsendername: 'My Name' sendgrid_primary_apikey: xxxx sendgrid_ips: '[\"XXX.XXX.XXX.XXX\"]' roles: - ibm.mas_devops.smtp","title":"Generate a configuration"},{"location":"roles/smtp/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_backup_restore/","text":"suite_app_backup_restore \u00a4 Overview \u00a4 This role supports backing up and restoring the data for below MAS applications: manage : Manage namespace resources, persistent volume data (e.g. attachments) iot : IoT namespace resources monitor : Monitor namespace resources health : Health namespace resources, Watson Studio project asset optimizer : Optimizer namespace resources visualinspection : Visual Inspection namespace resources, persistent volume data (e.g. image datasets, models) Supports creating on-demand or scheduled backup jobs for taking full or incremental backups, and optionally creating Kubernetes jobs for running the backup/restore process. Important An application backup can only be restored to an instance with the same MAS instance ID. Role Variables \u00a4 General Variables \u00a4 masbr_action \u00a4 Action to perform on MAS application data. Required Environment Variable: MAS_BR_ACTION Default: None Purpose : Specifies whether to create a backup of MAS application data or restore from a previous backup. When to use : - Set to backup to create a backup of application data - Set to restore to restore application data from a backup - Always required to indicate the operation type Valid values : backup , restore Impact : - backup : Creates backup job (on-demand or scheduled) for application data - restore : Restores application data from specified backup version Related variables : - masbr_restore_from_version : Required when action is restore - masbr_backup_schedule : Optional for scheduled backups - mas_app_id : Application to backup/restore Note : IMPORTANT - This role handles application-specific data (namespace resources, PV data, Watson Studio assets). Database data (Db2, MongoDB) must be backed up/restored separately. An application backup can only be restored to an instance with the same MAS instance ID. mas_app_id \u00a4 MAS application identifier for backup/restore operations. Required Environment Variable: MAS_APP_ID Default: None Purpose : Identifies which MAS application to backup or restore. Different applications support different data types (namespace, PV, Watson Studio). When to use : - Always required for application backup/restore operations - Must match an installed application in the instance - Determines which data types are available for backup Valid values : manage , iot , monitor , health , optimizer , visualinspection Impact : Determines which application's data will be backed up or restored. Each application supports different data types: - manage : namespace, pv (attachments) - iot : namespace - monitor : namespace - health : namespace, wsl (Watson Studio) - optimizer : namespace - visualinspection : namespace, pv (datasets, models) Related variables : - masbr_backup_data / masbr_restore_data : Data types to backup/restore - mas_instance_id : Instance containing this application - mas_workspace_id : Workspace containing this application Note : Database data (Db2, MongoDB) is not included in application backups and must be backed up separately using dedicated roles. mas_instance_id \u00a4 MAS instance identifier for application backup/restore. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the application to backup or restore. Used to locate application resources and ensure restore compatibility. When to use : - Always required for application backup and restore operations - Must match the instance ID from MAS installation - Critical for restore operations (must match original backup instance ID) Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , main ) Impact : Determines which MAS instance's application will be backed up or restored. CRITICAL - An application backup can only be restored to an instance with the same MAS instance ID. Related variables : - mas_app_id : Application within this instance - mas_workspace_id : Workspace within this instance - masbr_restore_from_version : Backup version to restore (for restore action) Note : IMPORTANT - The instance ID must match between backup and restore operations. Attempting to restore a backup to an instance with a different ID will fail. mas_workspace_id \u00a4 Workspace identifier for application backup/restore. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the application to backup or restore. Used to locate application resources. When to use : - Always required for application backup and restore operations - Must match the workspace ID from application installation - Used to construct resource names and locate application data Valid values : Lowercase alphanumeric string (e.g., ws1 , prod , test ) Impact : Determines which workspace's application data will be backed up or restored. Incorrect workspace ID will cause operations to fail. Related variables : - mas_instance_id : Instance containing this workspace - mas_app_id : Application within this workspace Note : The workspace must contain the specified application. Application data is workspace-specific and cannot be restored to a different workspace. masbr_storage_local_folder \u00a4 Local filesystem path for backup storage. Required Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None Purpose : Specifies the local filesystem path where application backup files are stored (for backups) or retrieved from (for restores). This is the persistent storage location for backup data. When to use : - Always required for backup and restore operations - Must be accessible from the system running the role - Should have sufficient space for application data backups - Must be persistent across operations for restore capability Valid values : Absolute filesystem path (e.g., /tmp/masbr , /backup/mas-apps , /mnt/backup ) Impact : Backup files are written to or read from this location. Insufficient space will cause backup failures. Path must exist and be writable. Related variables : - masbr_copy_timeout_sec : Timeout for transferring files to/from this location - masbr_restore_from_version : Backup version stored in this location Note : Ensure the path has sufficient disk space for application backups (especially for Manage attachments and Visual Inspection datasets). For production, use a dedicated backup volume with appropriate retention policies. masbr_confirm_cluster \u00a4 Confirm cluster connection before backup/restore. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false Purpose : Controls whether the role prompts for confirmation of the currently connected cluster before executing backup or restore operations. Safety feature to prevent accidental operations on wrong cluster. When to use : - Set to true for interactive confirmation (recommended for production) - Leave as false (default) for automated/non-interactive operations - Use true when manually running backup/restore to verify correct cluster Valid values : true , false Impact : - true : Role prompts for cluster confirmation before proceeding - false : Role proceeds without confirmation (suitable for automation) Related variables : - masbr_action : Operation requiring cluster confirmation Note : Enabling cluster confirmation is recommended for manual operations, especially in production environments, to prevent accidental backup/restore on the wrong cluster. masbr_copy_timeout_sec \u00a4 File transfer timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) Purpose : Specifies the maximum time allowed for transferring application backup files between cluster and local storage. Prevents operations from hanging indefinitely. When to use : - Use default (12 hours) for most deployments - Increase for very large backups (e.g., Manage attachments, Visual Inspection datasets) - Decrease for smaller backups to fail faster on issues Valid values : Positive integer (seconds), e.g., 3600 (1 hour), 43200 (12 hours), 86400 (24 hours) Impact : Operations exceeding this timeout will fail. Insufficient timeout for large backups will cause failures. Excessive timeout delays error detection. Related variables : - masbr_storage_local_folder : Destination for file transfers Note : The default 12 hours is suitable for most deployments. Adjust based on backup size (especially for Manage attachments and Visual Inspection datasets) and network speed. masbr_job_timezone \u00a4 Time zone for scheduled backup jobs. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: UTC Purpose : Specifies the time zone for scheduled backup CronJobs. Ensures backups run at the intended local time rather than UTC. When to use : - Leave unset to use UTC (default) - Set when you need backups to run at specific local times - Only applies to scheduled backups (when masbr_backup_schedule is set) Valid values : Valid tz database time zone (e.g., America/New_York , Europe/London , Asia/Tokyo ) Impact : Determines when scheduled backups execute. Incorrect time zone may cause backups to run at unexpected times. Related variables : - masbr_backup_schedule : Cron expression interpreted in this time zone Note : Only relevant for scheduled backups. On-demand backups ignore this setting. Use standard tz database names (e.g., America/New_York , not EST ). Backup Variables \u00a4 masbr_backup_type \u00a4 Backup type: full or incremental. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full Purpose : Specifies whether to create a full backup or incremental backup. Incremental backups only capture changes since the last full backup, reducing backup time and storage. When to use : - Use full (default) for complete backups - Use incr for incremental backups of persistent volume data - Incremental backups require a previous full backup Valid values : full , incr Impact : - full : Creates complete backup of all data - incr : Creates incremental backup of PV data only (namespace data is always full) Related variables : - masbr_backup_from_version : Full backup version for incremental backup - masbr_backup_data : Data types to backup Note : IMPORTANT - Incremental backups only apply to persistent volume (PV) data. Namespace and Watson Studio data are always backed up in full regardless of this setting. Incremental backups require a previous full backup as a baseline. masbr_backup_data \u00a4 Data types to include in backup. Optional Environment Variable: MASBR_BACKUP_DATA Default: All supported data types for the application Purpose : Specifies which types of data to backup. Allows selective backup of namespace resources, persistent volumes, or Watson Studio assets. When to use : - Leave unset to backup all supported data types (recommended) - Set to backup specific data types only - Use comma-separated list for multiple types (e.g., namespace,pv ) Valid values : Comma-separated list of: namespace , pv , wsl - namespace : Kubernetes namespace resources - pv : Persistent volume data (attachments, datasets, models) - wsl : Watson Studio project assets (Health only) Impact : Only specified data types are backed up. Unspecified types are excluded from backup. Related variables : - mas_app_id : Determines which data types are supported - masbr_backup_type : Full or incremental (applies to PV data only) Note : Supported data types vary by application: - Manage: namespace , pv - IoT/Monitor/Optimizer: namespace only - Health: namespace , wsl - Visual Inspection: namespace , pv The data types supported by each MAS applications: MAS App Name MAS App ID Data types Manage manage namespace , pv IoT iot namespace Monitor monitor namespace Health health namespace , wsl Optimizer optimizer namespace Visual Inspection visualinspection namespace , pv masbr_backup_from_version \u00a4 Base full backup version for incremental backups. Optional (when masbr_backup_type=incr ) Environment Variable: MASBR_BACKUP_FROM_VERSION Default: Latest full backup (auto-detected) Purpose : Specifies which full backup to use as the baseline for an incremental backup. Incremental backups capture only changes since this version. When to use : - Only applies when masbr_backup_type=incr - Leave unset to automatically use the latest full backup (recommended) - Set explicitly to use a specific full backup as baseline Valid values : Timestamp in YYYYMMDDHHMMSS format (e.g., 20240621021316 for June 21, 2024 at 02:13:16) Impact : Determines which full backup is used as the baseline. Incremental backup captures changes since this version. If not set, automatically uses the latest full backup. Related variables : - masbr_backup_type : Must be incr for this variable to be used - masbr_storage_local_folder : Location where full backup versions are stored Note : Only valid for incremental backups. The specified version must be a full backup (not incremental). Auto-detection finds the latest full backup in storage. masbr_backup_schedule \u00a4 Cron expression for scheduled backups. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None (on-demand backup) Purpose : Defines a schedule for automatic recurring backups using Cron syntax. When set, creates a Kubernetes CronJob for automated backups. When to use : - Leave unset for on-demand backups (manual execution) - Set to create scheduled/recurring backups - Use for automated backup strategies Valid values : Valid Cron expression (e.g., 0 2 * * * for daily at 2 AM, 0 2 * * 0 for weekly on Sunday at 2 AM) Impact : - When set: Creates a Kubernetes CronJob that runs backups automatically on schedule - When unset: Creates an on-demand backup job that runs immediately Related variables : - masbr_job_timezone : Time zone for interpreting the cron schedule - masbr_action : Must be backup for scheduled backups Note : Scheduled backups only apply when masbr_action=backup . The cron expression is interpreted in the time zone specified by masbr_job_timezone (defaults to UTC). Common patterns: 0 2 * * * (daily 2 AM), 0 2 * * 0 (weekly Sunday 2 AM), 0 2 1 * * (monthly 1st at 2 AM). Restore Variables \u00a4 masbr_restore_from_version \u00a4 Backup version timestamp for restore operations. Required (when masbr_action=restore ) Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Purpose : Specifies which backup version to restore from. The version is a timestamp identifying a specific backup. When to use : - Required when masbr_action=restore - Not used for backup operations - Must match an existing backup version in storage Valid values : Timestamp in YYYYMMDDHHMMSS format (e.g., 20240621021316 for June 21, 2024 at 02:13:16) Impact : Determines which backup is restored. Incorrect or non-existent version will cause restore to fail. Related variables : - masbr_action : Must be restore for this variable to be used - masbr_storage_local_folder : Location where backup versions are stored - mas_instance_id : Must match the instance ID from the backup Note : The backup version timestamp is generated automatically during backup creation. List available backups in masbr_storage_local_folder to find valid version timestamps. IMPORTANT - The backup can only be restored to an instance with the same MAS instance ID as the original backup. masbr_restore_data \u00a4 Data types to include in restore. Optional Environment Variable: MASBR_RESTORE_DATA Default: All supported data types for the application Purpose : Specifies which types of data to restore. Allows selective restore of namespace resources, persistent volumes, or Watson Studio assets. When to use : - Leave unset to restore all supported data types (recommended) - Set to restore specific data types only - Use comma-separated list for multiple types (e.g., namespace,pv ) Valid values : Comma-separated list of: namespace , pv , wsl - namespace : Kubernetes namespace resources - pv : Persistent volume data (attachments, datasets, models) - wsl : Watson Studio project assets (Health only) Impact : Only specified data types are restored. Unspecified types remain unchanged. Related variables : - mas_app_id : Determines which data types are supported - masbr_restore_from_version : Backup version containing the data Note : Supported data types vary by application: - Manage: namespace , pv - IoT/Monitor/Optimizer: namespace only - Health: namespace , wsl - Visual Inspection: namespace , pv The data types supported by each MAS applications: MAS App Name MAS App ID Data types Manage manage namespace , pv IoT iot namespace Monitor monitor namespace Health health namespace , wsl Optimizer optimizer namespace Visual Inspection visualinspection namespace , pv Manage Variables \u00a4 masbr_manage_pvc_paths \u00a4 Manage PVC paths for backup/restore (Manage only). Optional Environment Variable: MASBR_MANAGE_PVC_PATHS Default: None Purpose : Specifies which Manage persistent volumes to backup/restore. Defines PVC names, mount paths, and optional subpaths for Manage attachments and custom files. When to use : - Only applies to Manage application ( mas_app_id=manage ) - Required when backing up/restoring Manage PV data - Leave unset to skip Manage PV backup/restore - Set to backup specific Manage PVCs (e.g., attachments, custom files) Valid values : Comma-separated list in format <pvcName>:<mountPath>/<subPath> - Example: manage-doclinks1-pvc:/mnt/doclinks1/attachments - Multiple: manage-doclinks1-pvc:/mnt/doclinks1,manage-doclinks2-pvc:/mnt/doclinks2 Impact : Only specified PVCs are backed up/restored. Unspecified PVCs are excluded. Related variables : - mas_app_id : Must be manage for this variable to apply - masbr_backup_data / masbr_restore_data : Must include pv data type Note : PVC names and mount paths are defined in the ManageWorkspace CR spec.settings.deployment.persistentVolumes . Subpath is optional. If not set, no Manage PV data is backed up/restored. The <pvcName> and <mountPath> are defined in the ManageWorkspace CRD instance spec.settings.deployment.persistentVolumes : persistentVolumes: - accessModes: - ReadWriteMany mountPath: /mnt/doclinks1 pvcName: manage-doclinks1-pvc size: '20' storageClassName: ocs-storagecluster-cephfs volumeName: '' - accessModes: - ReadWriteMany mountPath: /mnt/doclinks2 pvcName: manage-doclinks2-pvc size: '20' storageClassName: ocs-storagecluster-cephfs volumeName: '' If not set a value for this variable, this role will not backup and restore persistent volume data for Manage. Example Playbook \u00a4 Backup \u00a4 Backup Manage attachments, note that this does not include backup of any data in Db2, see the backup action in the db2 role. - hosts: localhost any_errors_fatal: true vars: masbr_action: backup mas_instance_id: main mas_workspace_id: ws1 mas_app_id: manage masbr_backup_data: pv masbr_manage_pvc_paths: \"manage-doclinks1-pvc:/mnt/doclinks1\" masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_app_backup_restore Restore \u00a4 Restore Manage attachments, note that this does not include restore of any data in Db2, see the restore action in the db2 role. - hosts: localhost any_errors_fatal: true vars: masbr_action: restore masbr_restore_from_version: 20240621021316 mas_instance_id: main mas_workspace_id: ws1 mas_app_id: manage masbr_backup_data: pv masbr_manage_pvc_paths: \"manage-doclinks1-pvc:/mnt/doclinks1\" masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_app_backup_restore License \u00a4 EPL-2.0","title":"suite_app_backup_restore"},{"location":"roles/suite_app_backup_restore/#suite_app_backup_restore","text":"","title":"suite_app_backup_restore"},{"location":"roles/suite_app_backup_restore/#overview","text":"This role supports backing up and restoring the data for below MAS applications: manage : Manage namespace resources, persistent volume data (e.g. attachments) iot : IoT namespace resources monitor : Monitor namespace resources health : Health namespace resources, Watson Studio project asset optimizer : Optimizer namespace resources visualinspection : Visual Inspection namespace resources, persistent volume data (e.g. image datasets, models) Supports creating on-demand or scheduled backup jobs for taking full or incremental backups, and optionally creating Kubernetes jobs for running the backup/restore process. Important An application backup can only be restored to an instance with the same MAS instance ID.","title":"Overview"},{"location":"roles/suite_app_backup_restore/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_backup_restore/#general-variables","text":"","title":"General Variables"},{"location":"roles/suite_app_backup_restore/#masbr_action","text":"Action to perform on MAS application data. Required Environment Variable: MAS_BR_ACTION Default: None Purpose : Specifies whether to create a backup of MAS application data or restore from a previous backup. When to use : - Set to backup to create a backup of application data - Set to restore to restore application data from a backup - Always required to indicate the operation type Valid values : backup , restore Impact : - backup : Creates backup job (on-demand or scheduled) for application data - restore : Restores application data from specified backup version Related variables : - masbr_restore_from_version : Required when action is restore - masbr_backup_schedule : Optional for scheduled backups - mas_app_id : Application to backup/restore Note : IMPORTANT - This role handles application-specific data (namespace resources, PV data, Watson Studio assets). Database data (Db2, MongoDB) must be backed up/restored separately. An application backup can only be restored to an instance with the same MAS instance ID.","title":"masbr_action"},{"location":"roles/suite_app_backup_restore/#mas_app_id","text":"MAS application identifier for backup/restore operations. Required Environment Variable: MAS_APP_ID Default: None Purpose : Identifies which MAS application to backup or restore. Different applications support different data types (namespace, PV, Watson Studio). When to use : - Always required for application backup/restore operations - Must match an installed application in the instance - Determines which data types are available for backup Valid values : manage , iot , monitor , health , optimizer , visualinspection Impact : Determines which application's data will be backed up or restored. Each application supports different data types: - manage : namespace, pv (attachments) - iot : namespace - monitor : namespace - health : namespace, wsl (Watson Studio) - optimizer : namespace - visualinspection : namespace, pv (datasets, models) Related variables : - masbr_backup_data / masbr_restore_data : Data types to backup/restore - mas_instance_id : Instance containing this application - mas_workspace_id : Workspace containing this application Note : Database data (Db2, MongoDB) is not included in application backups and must be backed up separately using dedicated roles.","title":"mas_app_id"},{"location":"roles/suite_app_backup_restore/#mas_instance_id","text":"MAS instance identifier for application backup/restore. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the application to backup or restore. Used to locate application resources and ensure restore compatibility. When to use : - Always required for application backup and restore operations - Must match the instance ID from MAS installation - Critical for restore operations (must match original backup instance ID) Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , main ) Impact : Determines which MAS instance's application will be backed up or restored. CRITICAL - An application backup can only be restored to an instance with the same MAS instance ID. Related variables : - mas_app_id : Application within this instance - mas_workspace_id : Workspace within this instance - masbr_restore_from_version : Backup version to restore (for restore action) Note : IMPORTANT - The instance ID must match between backup and restore operations. Attempting to restore a backup to an instance with a different ID will fail.","title":"mas_instance_id"},{"location":"roles/suite_app_backup_restore/#mas_workspace_id","text":"Workspace identifier for application backup/restore. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the application to backup or restore. Used to locate application resources. When to use : - Always required for application backup and restore operations - Must match the workspace ID from application installation - Used to construct resource names and locate application data Valid values : Lowercase alphanumeric string (e.g., ws1 , prod , test ) Impact : Determines which workspace's application data will be backed up or restored. Incorrect workspace ID will cause operations to fail. Related variables : - mas_instance_id : Instance containing this workspace - mas_app_id : Application within this workspace Note : The workspace must contain the specified application. Application data is workspace-specific and cannot be restored to a different workspace.","title":"mas_workspace_id"},{"location":"roles/suite_app_backup_restore/#masbr_storage_local_folder","text":"Local filesystem path for backup storage. Required Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None Purpose : Specifies the local filesystem path where application backup files are stored (for backups) or retrieved from (for restores). This is the persistent storage location for backup data. When to use : - Always required for backup and restore operations - Must be accessible from the system running the role - Should have sufficient space for application data backups - Must be persistent across operations for restore capability Valid values : Absolute filesystem path (e.g., /tmp/masbr , /backup/mas-apps , /mnt/backup ) Impact : Backup files are written to or read from this location. Insufficient space will cause backup failures. Path must exist and be writable. Related variables : - masbr_copy_timeout_sec : Timeout for transferring files to/from this location - masbr_restore_from_version : Backup version stored in this location Note : Ensure the path has sufficient disk space for application backups (especially for Manage attachments and Visual Inspection datasets). For production, use a dedicated backup volume with appropriate retention policies.","title":"masbr_storage_local_folder"},{"location":"roles/suite_app_backup_restore/#masbr_confirm_cluster","text":"Confirm cluster connection before backup/restore. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false Purpose : Controls whether the role prompts for confirmation of the currently connected cluster before executing backup or restore operations. Safety feature to prevent accidental operations on wrong cluster. When to use : - Set to true for interactive confirmation (recommended for production) - Leave as false (default) for automated/non-interactive operations - Use true when manually running backup/restore to verify correct cluster Valid values : true , false Impact : - true : Role prompts for cluster confirmation before proceeding - false : Role proceeds without confirmation (suitable for automation) Related variables : - masbr_action : Operation requiring cluster confirmation Note : Enabling cluster confirmation is recommended for manual operations, especially in production environments, to prevent accidental backup/restore on the wrong cluster.","title":"masbr_confirm_cluster"},{"location":"roles/suite_app_backup_restore/#masbr_copy_timeout_sec","text":"File transfer timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) Purpose : Specifies the maximum time allowed for transferring application backup files between cluster and local storage. Prevents operations from hanging indefinitely. When to use : - Use default (12 hours) for most deployments - Increase for very large backups (e.g., Manage attachments, Visual Inspection datasets) - Decrease for smaller backups to fail faster on issues Valid values : Positive integer (seconds), e.g., 3600 (1 hour), 43200 (12 hours), 86400 (24 hours) Impact : Operations exceeding this timeout will fail. Insufficient timeout for large backups will cause failures. Excessive timeout delays error detection. Related variables : - masbr_storage_local_folder : Destination for file transfers Note : The default 12 hours is suitable for most deployments. Adjust based on backup size (especially for Manage attachments and Visual Inspection datasets) and network speed.","title":"masbr_copy_timeout_sec"},{"location":"roles/suite_app_backup_restore/#masbr_job_timezone","text":"Time zone for scheduled backup jobs. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: UTC Purpose : Specifies the time zone for scheduled backup CronJobs. Ensures backups run at the intended local time rather than UTC. When to use : - Leave unset to use UTC (default) - Set when you need backups to run at specific local times - Only applies to scheduled backups (when masbr_backup_schedule is set) Valid values : Valid tz database time zone (e.g., America/New_York , Europe/London , Asia/Tokyo ) Impact : Determines when scheduled backups execute. Incorrect time zone may cause backups to run at unexpected times. Related variables : - masbr_backup_schedule : Cron expression interpreted in this time zone Note : Only relevant for scheduled backups. On-demand backups ignore this setting. Use standard tz database names (e.g., America/New_York , not EST ).","title":"masbr_job_timezone"},{"location":"roles/suite_app_backup_restore/#backup-variables","text":"","title":"Backup Variables"},{"location":"roles/suite_app_backup_restore/#masbr_backup_type","text":"Backup type: full or incremental. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full Purpose : Specifies whether to create a full backup or incremental backup. Incremental backups only capture changes since the last full backup, reducing backup time and storage. When to use : - Use full (default) for complete backups - Use incr for incremental backups of persistent volume data - Incremental backups require a previous full backup Valid values : full , incr Impact : - full : Creates complete backup of all data - incr : Creates incremental backup of PV data only (namespace data is always full) Related variables : - masbr_backup_from_version : Full backup version for incremental backup - masbr_backup_data : Data types to backup Note : IMPORTANT - Incremental backups only apply to persistent volume (PV) data. Namespace and Watson Studio data are always backed up in full regardless of this setting. Incremental backups require a previous full backup as a baseline.","title":"masbr_backup_type"},{"location":"roles/suite_app_backup_restore/#masbr_backup_data","text":"Data types to include in backup. Optional Environment Variable: MASBR_BACKUP_DATA Default: All supported data types for the application Purpose : Specifies which types of data to backup. Allows selective backup of namespace resources, persistent volumes, or Watson Studio assets. When to use : - Leave unset to backup all supported data types (recommended) - Set to backup specific data types only - Use comma-separated list for multiple types (e.g., namespace,pv ) Valid values : Comma-separated list of: namespace , pv , wsl - namespace : Kubernetes namespace resources - pv : Persistent volume data (attachments, datasets, models) - wsl : Watson Studio project assets (Health only) Impact : Only specified data types are backed up. Unspecified types are excluded from backup. Related variables : - mas_app_id : Determines which data types are supported - masbr_backup_type : Full or incremental (applies to PV data only) Note : Supported data types vary by application: - Manage: namespace , pv - IoT/Monitor/Optimizer: namespace only - Health: namespace , wsl - Visual Inspection: namespace , pv The data types supported by each MAS applications: MAS App Name MAS App ID Data types Manage manage namespace , pv IoT iot namespace Monitor monitor namespace Health health namespace , wsl Optimizer optimizer namespace Visual Inspection visualinspection namespace , pv","title":"masbr_backup_data"},{"location":"roles/suite_app_backup_restore/#masbr_backup_from_version","text":"Base full backup version for incremental backups. Optional (when masbr_backup_type=incr ) Environment Variable: MASBR_BACKUP_FROM_VERSION Default: Latest full backup (auto-detected) Purpose : Specifies which full backup to use as the baseline for an incremental backup. Incremental backups capture only changes since this version. When to use : - Only applies when masbr_backup_type=incr - Leave unset to automatically use the latest full backup (recommended) - Set explicitly to use a specific full backup as baseline Valid values : Timestamp in YYYYMMDDHHMMSS format (e.g., 20240621021316 for June 21, 2024 at 02:13:16) Impact : Determines which full backup is used as the baseline. Incremental backup captures changes since this version. If not set, automatically uses the latest full backup. Related variables : - masbr_backup_type : Must be incr for this variable to be used - masbr_storage_local_folder : Location where full backup versions are stored Note : Only valid for incremental backups. The specified version must be a full backup (not incremental). Auto-detection finds the latest full backup in storage.","title":"masbr_backup_from_version"},{"location":"roles/suite_app_backup_restore/#masbr_backup_schedule","text":"Cron expression for scheduled backups. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None (on-demand backup) Purpose : Defines a schedule for automatic recurring backups using Cron syntax. When set, creates a Kubernetes CronJob for automated backups. When to use : - Leave unset for on-demand backups (manual execution) - Set to create scheduled/recurring backups - Use for automated backup strategies Valid values : Valid Cron expression (e.g., 0 2 * * * for daily at 2 AM, 0 2 * * 0 for weekly on Sunday at 2 AM) Impact : - When set: Creates a Kubernetes CronJob that runs backups automatically on schedule - When unset: Creates an on-demand backup job that runs immediately Related variables : - masbr_job_timezone : Time zone for interpreting the cron schedule - masbr_action : Must be backup for scheduled backups Note : Scheduled backups only apply when masbr_action=backup . The cron expression is interpreted in the time zone specified by masbr_job_timezone (defaults to UTC). Common patterns: 0 2 * * * (daily 2 AM), 0 2 * * 0 (weekly Sunday 2 AM), 0 2 1 * * (monthly 1st at 2 AM).","title":"masbr_backup_schedule"},{"location":"roles/suite_app_backup_restore/#restore-variables","text":"","title":"Restore Variables"},{"location":"roles/suite_app_backup_restore/#masbr_restore_from_version","text":"Backup version timestamp for restore operations. Required (when masbr_action=restore ) Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Purpose : Specifies which backup version to restore from. The version is a timestamp identifying a specific backup. When to use : - Required when masbr_action=restore - Not used for backup operations - Must match an existing backup version in storage Valid values : Timestamp in YYYYMMDDHHMMSS format (e.g., 20240621021316 for June 21, 2024 at 02:13:16) Impact : Determines which backup is restored. Incorrect or non-existent version will cause restore to fail. Related variables : - masbr_action : Must be restore for this variable to be used - masbr_storage_local_folder : Location where backup versions are stored - mas_instance_id : Must match the instance ID from the backup Note : The backup version timestamp is generated automatically during backup creation. List available backups in masbr_storage_local_folder to find valid version timestamps. IMPORTANT - The backup can only be restored to an instance with the same MAS instance ID as the original backup.","title":"masbr_restore_from_version"},{"location":"roles/suite_app_backup_restore/#masbr_restore_data","text":"Data types to include in restore. Optional Environment Variable: MASBR_RESTORE_DATA Default: All supported data types for the application Purpose : Specifies which types of data to restore. Allows selective restore of namespace resources, persistent volumes, or Watson Studio assets. When to use : - Leave unset to restore all supported data types (recommended) - Set to restore specific data types only - Use comma-separated list for multiple types (e.g., namespace,pv ) Valid values : Comma-separated list of: namespace , pv , wsl - namespace : Kubernetes namespace resources - pv : Persistent volume data (attachments, datasets, models) - wsl : Watson Studio project assets (Health only) Impact : Only specified data types are restored. Unspecified types remain unchanged. Related variables : - mas_app_id : Determines which data types are supported - masbr_restore_from_version : Backup version containing the data Note : Supported data types vary by application: - Manage: namespace , pv - IoT/Monitor/Optimizer: namespace only - Health: namespace , wsl - Visual Inspection: namespace , pv The data types supported by each MAS applications: MAS App Name MAS App ID Data types Manage manage namespace , pv IoT iot namespace Monitor monitor namespace Health health namespace , wsl Optimizer optimizer namespace Visual Inspection visualinspection namespace , pv","title":"masbr_restore_data"},{"location":"roles/suite_app_backup_restore/#manage-variables","text":"","title":"Manage Variables"},{"location":"roles/suite_app_backup_restore/#masbr_manage_pvc_paths","text":"Manage PVC paths for backup/restore (Manage only). Optional Environment Variable: MASBR_MANAGE_PVC_PATHS Default: None Purpose : Specifies which Manage persistent volumes to backup/restore. Defines PVC names, mount paths, and optional subpaths for Manage attachments and custom files. When to use : - Only applies to Manage application ( mas_app_id=manage ) - Required when backing up/restoring Manage PV data - Leave unset to skip Manage PV backup/restore - Set to backup specific Manage PVCs (e.g., attachments, custom files) Valid values : Comma-separated list in format <pvcName>:<mountPath>/<subPath> - Example: manage-doclinks1-pvc:/mnt/doclinks1/attachments - Multiple: manage-doclinks1-pvc:/mnt/doclinks1,manage-doclinks2-pvc:/mnt/doclinks2 Impact : Only specified PVCs are backed up/restored. Unspecified PVCs are excluded. Related variables : - mas_app_id : Must be manage for this variable to apply - masbr_backup_data / masbr_restore_data : Must include pv data type Note : PVC names and mount paths are defined in the ManageWorkspace CR spec.settings.deployment.persistentVolumes . Subpath is optional. If not set, no Manage PV data is backed up/restored. The <pvcName> and <mountPath> are defined in the ManageWorkspace CRD instance spec.settings.deployment.persistentVolumes : persistentVolumes: - accessModes: - ReadWriteMany mountPath: /mnt/doclinks1 pvcName: manage-doclinks1-pvc size: '20' storageClassName: ocs-storagecluster-cephfs volumeName: '' - accessModes: - ReadWriteMany mountPath: /mnt/doclinks2 pvcName: manage-doclinks2-pvc size: '20' storageClassName: ocs-storagecluster-cephfs volumeName: '' If not set a value for this variable, this role will not backup and restore persistent volume data for Manage.","title":"masbr_manage_pvc_paths"},{"location":"roles/suite_app_backup_restore/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_app_backup_restore/#backup","text":"Backup Manage attachments, note that this does not include backup of any data in Db2, see the backup action in the db2 role. - hosts: localhost any_errors_fatal: true vars: masbr_action: backup mas_instance_id: main mas_workspace_id: ws1 mas_app_id: manage masbr_backup_data: pv masbr_manage_pvc_paths: \"manage-doclinks1-pvc:/mnt/doclinks1\" masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_app_backup_restore","title":"Backup"},{"location":"roles/suite_app_backup_restore/#restore","text":"Restore Manage attachments, note that this does not include restore of any data in Db2, see the restore action in the db2 role. - hosts: localhost any_errors_fatal: true vars: masbr_action: restore masbr_restore_from_version: 20240621021316 mas_instance_id: main mas_workspace_id: ws1 mas_app_id: manage masbr_backup_data: pv masbr_manage_pvc_paths: \"manage-doclinks1-pvc:/mnt/doclinks1\" masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_app_backup_restore","title":"Restore"},{"location":"roles/suite_app_backup_restore/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_config/","text":"suite_app_config \u00a4 This role is used to configure specific components of the application workspace after the application has been installed in the Maximo Application Suite. Role Variables \u00a4 General Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier for the target installation. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to configure. This must match the instance ID used during MAS Core installation. When to use : - Always required when configuring application workspaces - Must match the instance ID from suite_install role - Used to locate the correct Suite and Workspace resources Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance's application workspace will be configured. Incorrect instance ID will cause configuration to fail. Related variables : - mas_app_id : Specifies which application to configure - mas_workspace_id : Specifies which workspace to configure Note : This must be an existing MAS instance. The role does not create new instances. mas_app_id \u00a4 MAS application to configure. Required Environment Variable: MAS_APP_ID Default: None Purpose : Specifies which MAS application workspace to configure. Different applications have different configuration options and requirements. When to use : - Always required when configuring application workspaces - Must match an installed application in the MAS instance - Determines which application-specific configuration is applied Valid values : assist , iot , facilities , manage , monitor , optimizer , predict , visualinspection Impact : Determines which application workspace is configured and which configuration options are available. Each application has unique configuration requirements. Related variables : - mas_instance_id : The MAS instance containing this application - mas_workspace_id : The workspace to configure - mas_appws_components : Application-specific components to configure Note : The application must already be installed via suite_app_install role before configuration. Different applications support different configuration variables. mas_workspace_id \u00a4 Workspace identifier for the application workspace to configure. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the application to configure. Workspaces allow multiple isolated environments within a single application. When to use : - Always required when configuring application workspaces - Must match an existing workspace created during application installation - Typically matches the workspace ID used in suite_app_install Valid values : Lowercase alphanumeric string (e.g., masdev , prod , test ) Impact : Determines which workspace is configured. Each workspace has its own configuration, data, and users. Related variables : - mas_instance_id : The MAS instance containing this workspace - mas_app_id : The application containing this workspace - mas_appws_components : Components to configure in this workspace Note : The workspace must already exist (created during suite_app_install). This role configures existing workspaces, it does not create new ones. aiservice_instance_id \u00a4 AI Service instance ID for Manage integration (Manage only). Optional Environment Variable: AISERVICE_INSTANCE_ID Default: None Purpose : Enables automatic AI Service integration with Manage application. When set, the role retrieves credentials, configures connection, and imports certificates. When to use : - Only when configuring Manage application ( mas_app_id=manage ) - When you want to integrate AI Service capabilities into Manage - Requires AI Service to be installed and running in the cluster - Must be used together with aiservice_tenant_id Valid values : Valid AI Service instance ID (e.g., aiservice1 , ai-prod ) Impact : When configured, automatically: - Retrieves API key from tenant-specific secret - Extracts AI Service URL from aibroker route - Imports AI Service TLS certificate into Manage - Configures AI Service connection properties - Verifies AI Service health before proceeding Related variables : - aiservice_tenant_id : Required tenant ID for the integration - mas_app_id : Must be manage for this to apply Note : AI Service must be installed and healthy before configuration. The role performs automatic health checks and credential retrieval from cluster resources. aiservice_tenant_id \u00a4 AI Service tenant ID for the integration (Manage only). Optional Environment Variable: AISERVICE_TENANT_ID Default: None Purpose : Specifies which AI Service tenant to use for Manage integration. Combined with instance ID to form the fully qualified tenant identifier. When to use : - Required when aiservice_instance_id is set - Only applies to Manage application - Must match an existing tenant in the AI Service instance Valid values : Valid AI Service tenant ID string (e.g., tenant1 , prod-tenant ) Impact : Combined with aiservice_instance_id to locate tenant-specific resources (API key secret, configuration). Incorrect tenant ID will cause integration to fail. Related variables : - aiservice_instance_id : Required instance ID for the integration - Forms secret name: aiservice-{instance_id}-{tenant_id}----apikey-secret Note : The tenant must already exist in the AI Service instance. The role retrieves credentials from the tenant-specific secret in the cluster. custom_labels \u00a4 Comma-separated list of key=value labels to apply to workspace resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to application workspace resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,app=manage ) Impact : Labels are applied to workspace-specific resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect workspace functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management. Note : Labels help with resource organization and are especially useful in multi-tenant or multi-workspace environments. Workspace Configuration Variables \u00a4 mas_appws_spec \u00a4 Custom workspace deployment specification (overrides component-based configuration). Optional Environment Variable: MAS_APPWS_SPEC Default: Application-specific defaults in vars/defaultspecs/{mas_app_id}.yml Purpose : Provides complete control over workspace deployment specification. Allows advanced customization beyond what component-based configuration offers. When to use : - Use for advanced workspace customization - Use when you need full control over the workspace spec - Use when component-based configuration is insufficient - Leave unset for standard deployments using mas_appws_components Valid values : Valid workspace specification YAML/JSON matching the application's workspace CR schema Impact : WARNING - Overrides all settings from mas_appws_components . Provides complete control but requires deep knowledge of workspace CR structure. Incorrect specs can cause deployment failures. Related variables : - mas_appws_components : Simpler component-based configuration (overridden by this) - Application-specific default specs in vars/defaultspecs/ Note : Use mas_appws_components for standard deployments. Only use this variable when you need advanced customization or have specific requirements not covered by component-based configuration. mas_appws_bindings_jdbc \u00a4 JDBC binding scope for the workspace. Optional Environment Variable: MAS_APPWS_BINDINGS_JDBC Default: system Purpose : Controls the scope of JDBC database binding for the workspace. Different scopes provide different levels of isolation and sharing. When to use : - Use system (default) for shared system-level database configuration - Use application for application-level database configuration - Use workspace for workspace-specific database configuration - Use workspace-application for Maximo Real Estate and Facilities (recommended) Valid values : system , application , workspace , workspace-application Impact : - system : Uses system-level JDBC configuration (shared across all workspaces) - application : Uses application-level JDBC configuration - workspace : Uses workspace-specific JDBC configuration - workspace-application : Combines workspace and application scopes Related variables : JDBC configuration must exist at the specified scope level. Note : IMPORTANT - For Maximo Real Estate and Facilities applications, use workspace-application scope. The default system scope is suitable for most other applications. mas_appws_components \u00a4 Application components and versions to configure in the workspace. Optional Environment Variable: MAS_APPWS_COMPONENTS Default: Application-specific defaults Purpose : Specifies which application components to configure and their versions. Different applications have different available components. When to use : - Use to enable specific application components - Use to control component versions - Leave unset to use application-specific defaults - Overridden by mas_appws_spec if that is set Valid values : Comma-separated component=version pairs (e.g., base=latest,health=latest , base=latest,civil=latest ) Impact : Determines which components are configured in the workspace. Available components vary by application (e.g., Manage has base, health, civil, etc.). Related variables : - mas_appws_spec : Overrides this if set - mas_app_id : Determines available components Note : Component availability depends on the application. For Manage: base , health , civil , etc. Refer to application documentation for available components. Use latest for most recent version or specify exact version. mas_pod_templates_dir \u00a4 Directory containing pod template configuration files (Manage only). Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Specifies a directory containing pod template YAML files for customizing Manage workspace and component workloads. Enables resource requests/limits, node selectors, tolerations, and other pod-level customizations. When to use : - Only for Manage application ( mas_app_id=manage ) - Use to customize pod resources (CPU, memory) - Use to apply node selectors or tolerations - Use to configure pod-level settings beyond defaults Valid values : Absolute path to directory containing pod template files Impact : Pod templates from this directory are merged into the ManageWorkspace CR. Files are applied to specific components or the workspace itself based on filename. Related variables : - mas_app_id : Must be manage for this to apply - Files must follow specific naming convention Note : Expected filenames: - ibm-mas-manage-manageworkspace.yml \u2192 workspace-level pod templates - ibm-mas-manage-imagestitching.yml \u2192 civil component pod templates - ibm-mas-manage-slackproxy.yml \u2192 component pod templates - ibm-mas-manage-healthextworkspace.yml \u2192 health component pod templates Refer to MAS CLI BestEfforts templates and Customizing Pod Templates documentation. Predict Configuration Variables \u00a4 mas_appws_settings_deployment_size \u00a4 Workload size for Predict containers (Predict only). Optional (Predict only) Environment Variable: MAS_APPWS_SETTINGS_DEPLOYMENT_SIZE Default: small Purpose : Controls the deployment size and replica count for Predict application containers. Different sizes provide different levels of availability and performance. When to use : - Only for Predict application ( mas_app_id=predict ) - Use developer for single-node development environments - Use small (default) for standard production deployments - Use medium for higher availability requirements - Use large for maximum availability (if supported) Valid values : developer , small , medium , large Impact : - developer : 1 replica (no high availability) - small : 2 replicas (basic high availability) - medium : 3 replicas (enhanced high availability) - large : Higher replica count (if supported) Related variables : - mas_app_id : Must be predict for this to apply Note : The developer size is suitable only for development/testing. Production environments should use small or larger for high availability. Watson Studio Local Variables \u00a4 These variables are only used when using this role to configure Predict , or Health & Predict Utilities . cpd_wsl_project_id \u00a4 Watson Studio analytics project ID (Predict/HP Utilities only). Required (unless cpd_wsl_project_name and mas_config_dir are set) Environment Variable: CPD_WSL_PROJECT_ID Default: None Purpose : Specifies the ID of the Watson Studio analytics project to use for Predict or Health & Predict Utilities configuration. When to use : - Required for Predict or HP Utilities configuration - Use when you know the project ID directly - Alternative: use cpd_wsl_project_name + mas_config_dir to retrieve ID from saved file Valid values : Valid Watson Studio project ID (UUID format) Impact : Links the MAS application to the specified Watson Studio project for analytics capabilities. Incorrect project ID will cause configuration to fail. Related variables : - cpd_wsl_project_name : Alternative method using project name - mas_config_dir : Required with cpd_wsl_project_name Note : The project must already exist in Watson Studio (created by cp4d_service role). Either provide this ID directly or use the name-based alternative. cpd_wsl_project_name \u00a4 Filename containing Watson Studio project ID (Predict/HP Utilities only). Optional Environment Variable: CPD_WSL_PROJECT_NAME Default: wsl-mas-${mas_instance_id}-hputilities Purpose : Specifies the filename in mas_config_dir where the Watson Studio project ID is saved. Alternative to providing cpd_wsl_project_id directly. When to use : - Use with mas_config_dir as alternative to cpd_wsl_project_id - Use when project ID was saved by cp4d_service role - Allows retrieving project ID from saved configuration Valid values : Filename (without path) where project ID is stored Impact : Role reads the project ID from this file in mas_config_dir . File must exist and contain valid project ID. Related variables : - mas_config_dir : Required directory containing this file - cpd_wsl_project_id : Alternative direct ID specification Note : The default filename matches the pattern used by cp4d_service role. The file should contain the Watson Studio project ID created during CP4D service setup. mas_config_dir \u00a4 Local directory for configuration files (Predict/HP Utilities only). Optional Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies directory containing configuration files, particularly Watson Studio project ID files. Used with cpd_wsl_project_name to retrieve project IDs. When to use : - Use with cpd_wsl_project_name to retrieve Watson Studio project ID - Use when project ID was saved by cp4d_service role - Should match the directory used in cp4d_service role Valid values : Absolute path to existing directory (e.g., /home/user/masconfig , ~/masconfig ) Impact : Role reads Watson Studio project ID from files in this directory. Directory must exist and contain the specified project file. Related variables : - cpd_wsl_project_name : Filename to read from this directory - cpd_wsl_project_id : Alternative direct ID specification Note : Use the same directory across all MAS setup roles for consistency. The cp4d_service role saves project IDs here, and this role retrieves them. Watson Machine Learning Variables \u00a4 These variables are only used when using this role to configure Predict . cpd_product_version \u00a4 Cloud Pak for Data version (Predict only). Required (Predict only) Environment Variable: CPD_PRODUCT_VERSION Default: None Purpose : Specifies the CP4D version to infer the correct Watson Machine Learning version for Predict workspace configuration. When to use : - Required when configuring Predict application - Must match the installed CP4D version in the cluster - Used to determine compatible WML version Valid values : Valid CP4D version string (e.g., 4.8.0 , 4.8.5 , 5.0.0 ) Impact : Determines which Watson Machine Learning version is configured in Predict. Incorrect version may cause compatibility issues. Related variables : - cpd_wml_instance_id : WML instance to configure - cpd_wml_url : WML service URL - mas_app_id : Must be predict for this to apply Note : This must match the actual CP4D version installed in your cluster. The role uses this to select the appropriate WML version for Predict configuration. cpd_wml_instance_id \u00a4 Watson Machine Learning instance identifier (Predict only). Optional (Predict only) Environment Variable: CPD_WML_INSTANCE_ID Default: openshift Purpose : Specifies the Watson Machine Learning instance identifier to configure in Predict workspace. When to use : - Only for Predict application configuration - Use default ( openshift ) for standard deployments - Set custom value if using non-default WML instance Valid values : Valid WML instance identifier string Impact : Identifies which WML instance Predict will use for machine learning operations. Must match an existing WML instance in CP4D. Related variables : - cpd_product_version : CP4D version for WML compatibility - cpd_wml_url : WML service URL - mas_app_id : Must be predict for this to apply Note : The default openshift value is suitable for most deployments. Only change if you have a specific WML instance identifier. cpd_wml_url \u00a4 Watson Machine Learning service URL (Predict only). Optional (Predict only) Environment Variable: CPD_WML_URL Default: https://internal-nginx-svc.ibm-cpd.svc:12443 Purpose : Specifies the URL to access Watson Machine Learning service. Typically the same as the Cloud Pak for Data URL. When to use : - Only for Predict application configuration - Use default if CP4D is in ibm-cpd namespace - Set custom URL if CP4D is in different namespace or uses custom service name Valid values : Valid HTTPS URL to WML service (e.g., https://internal-nginx-svc.{namespace}.svc:12443 ) Impact : Determines how Predict connects to Watson Machine Learning. Incorrect URL will prevent Predict from accessing WML services. Related variables : - cpd_product_version : CP4D version - cpd_wml_instance_id : WML instance identifier - mas_app_id : Must be predict for this to apply Note : The default assumes CP4D is installed in the ibm-cpd namespace. If CP4D is in a different namespace, update the URL accordingly (e.g., https://internal-nginx-svc.my-cpd-namespace.svc:12443 ). Manage Workspace Variables \u00a4 AI Service Integration \u00a4 aiservice_instance_id \u00a4 AI Service instance ID to integrate with Manage application. Optional Environment Variable: AISERVICE_INSTANCE_ID Default: None When configured, the role will: - Retrieve AI Service API key from the tenant-specific secret - Extract AI Service URL from the aibroker route - Import AI Service TLS certificate into Manage - Configure AI Service connection properties in Manage encryption secret - Verify AI Service health status before proceeding aiservice_tenant_id \u00a4 AI Service tenant ID to use for the integration. This is combined with the instance ID to form the fully qualified tenant name. Required when aiservice_instance_id is set Environment Variable: AISERVICE_TENANT_ID Default: None Note: The AI Service integration automatically retrieves the following from the cluster: - API key from secret: aiservice-{instance_id}-{tenant_id}----apikey-secret - Service URL from route: aibroker in namespace aiservice-{instance_id} - TLS certificate from secret: {instance_id}-public-aibroker-tls The integration also performs a health check to verify AI Service is running before completing the configuration. Health Integration \u00a4 mas_appws_bindings_health_wsl_flag \u00a4 Enable Watson Studio binding for Health (Manage only). Optional (Manage Health only) Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL_FLAG Default: false Purpose : Controls whether Watson Studio should be bound to the Manage Health component. Requires a system-level WatsonStudioCfg to be applied in the cluster. When to use : - Only for Manage application with Health component - Set to true to enable Watson Studio integration with Health - Leave as false (default) if Watson Studio integration is not needed Valid values : true , false Impact : When true , binds Watson Studio to Health component, enabling advanced analytics capabilities. Requires Watson Studio to be configured at system level. Related variables : - mas_appws_bindings_health_wsl : Binding scope (typically system ) - mas_app_id : Must be manage with Health component Note : A system-level WatsonStudioCfg must exist in the cluster before enabling this. Watson Studio must be installed and configured via CP4D. mas_appws_bindings_health_wsl \u00a4 Watson Studio binding scope for Health (Manage only). Optional (Manage Health only) Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL Default: None Purpose : Specifies the binding scope for Watson Studio integration with Manage Health component. When to use : - Only for Manage application with Health component - Set to system when Watson Studio is configured at system level - Used together with mas_appws_bindings_health_wsl_flag=true Valid values : system Impact : Binds Watson Studio at the specified scope to Health component, enabling advanced analytics and AI capabilities. Related variables : - mas_appws_bindings_health_wsl_flag : Must be true to enable binding - mas_app_id : Must be manage with Health component Note : Watson Studio must be installed and configured via CP4D with a system-level WatsonStudioCfg before using this binding. - Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL - Default: None mas_app_settings_aio_flag \u00a4 Flag indicating if Asset Investment Optimization (AIO) resource must be loaded or not. It can be loaded only when Optimizer application is installed. Optional , only supported when Optimizer application is installed Environment Variable: MAS_APP_SETTINGS_AIO_FLAG Default: true DB2 Settings \u00a4 mas_app_settings_db_schema \u00a4 Name of the schema where Manage database lives in. Code also supports deprecated mas_app_settings_db2_schema variable name. Optional Environment Variable: MAS_APP_SETTINGS_DB_SCHEMA Default: maximo mas_app_settings_demodata \u00a4 Flag indicating if manage demodata should be loaded or not. Optional Environment Variable: MAS_APP_SETTINGS_DEMODATA Default: false (do not load demodata) mas_app_settings_db2vargraphic \u00a4 Flag indicating if VARGRAPHIC (if true) or VARCHAR (if false) is used. Details: https://www.ibm.com/docs/en/mas-cd/continuous-delivery?topic=deploy-language-support Optional Environment Variable: MAS_APP_SETTINGS_DB2VARGRAPHIC Default: true mas_app_settings_tablespace \u00a4 Name of the Manage database tablespace. Optional Environment Variable: MAS_APP_SETTINGS_TABLESPACE Default: MAXDATA mas_app_settings_indexspace \u00a4 Name of the Manage database indexspace. Optional Environment Variable: MAS_APP_SETTINGS_INDEXSPACE Default: MAXINDEX Persistent Volumes \u00a4 mas_app_settings_persistent_volumes_flag \u00a4 Flag indicating if persistent volumes should be configured by default during Manage Workspace activation. There are two defaulted File Storage Persistent Volumes Claim resources that will be created out of the box for Manage if this flag is set to true : /DOCLINKS : Persistent volume used to store doclinks/attachments /bim : Persistent volume used to store Building Information Models related artifacts (models, docs and import) Optional Environment Variable: MAS_APP_SETTINGS_PERSISTENT_VOLUMES_FLAG Default: false JMS Queues \u00a4 The following properties can be defined to customize the persistent volumes for the JMS queues setup for Manage. mas_app_settings_jms_queue_pvc_storage_class \u00a4 Provide the persistent volume storage class to be used for JMS queue configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) access modes are supported. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes mas_app_settings_jms_queue_pvc_name \u00a4 Provide the persistent volume claim name to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_NAME Default: manage-jms mas_app_settings_jms_queue_pvc_size \u00a4 Provide the persistent volume claim size to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_SIZE Default: 20Gi mas_app_settings_jms_queue_mount_path \u00a4 Provide the persistent volume storage mount path to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_MOUNT_PATH Default: /jms mas_app_settings_jms_queue_pvc_accessmode \u00a4 Provide the persistent volume storage access-mode to be used for JMS queue configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_ACCESSMODE Default: ReadWriteMany mas_app_settings_default_jms \u00a4 Set this to true if you want to have JMS continuous queues configured. Optional Environment Variable: MAS_APP_SETTINGS_DEFAULT_JMS Default: false Doclinks/Attachments \u00a4 The following properties can be defined to customize the persistent volumes for the Doclinks/Attachments setup for Manage. mas_app_settings_doclinks_pvc_storage_class \u00a4 Provide the persistent volume storage class to be used for doclinks/attachments configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes mas_app_settings_doclinks_pvc_name \u00a4 Provide the persistent volume claim name to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_NAME Default: manage-doclinks mas_app_settings_doclinks_pvc_size \u00a4 Provide the persistent volume claim size to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_SIZE Default: 20Gi mas_app_settings_doclinks_mount_path \u00a4 Provide the persistent volume storage mount path to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_MOUNT_PATH Default: /DOCLINKS mas_app_settings_doclinks_pvc_accessmode \u00a4 Provide the persistent volume storage access-mode to be used for doclinks/attachments configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_ACCESSMODE Default: ReadWriteMany BIM (Building Information Models) \u00a4 The following properties can be defined to customize the persistent volumes for the Building Information Models setup for Manage. mas_app_settings_bim_pvc_storage_class \u00a4 Provide the persistent volume storage class to be used for Building Information Models configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes mas_app_settings_bim_pvc_name \u00a4 Provide the persistent volume claim name to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_NAME Default: manage-bim mas_app_settings_bim_pvc_size \u00a4 Provide the persistent volume claim size to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_SIZE Default: 20Gi mas_app_settings_bim_mount_path \u00a4 Provide the persistent volume storage mount path to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_MOUNT_PATH Default: /bim mas_app_settings_bim_pvc_accessmode \u00a4 Provide the persistent volume storage access-mode to be used for Building Information Models configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_ACCESSMODE Default: ReadWriteMany Supported Languages \u00a4 mas_app_settings_base_lang \u00a4 Provide the base language for Manage application. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. Optional Environment Variable: MAS_APP_SETTINGS_BASE_LANG Default: EN (English) mas_app_settings_secondary_langs \u00a4 Provide a list of additional secondary languages for Manage application. Optional Environment Variable: MAS_APP_SETTINGS_SECONDARY_LANGS Default: None Note: The more languages you add, the longer Manage will take to install and activate. Export the MAS_APP_SETTINGS_SECONDARY_LANGS variable with the language codes as comma-separated values. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. For example, use the following to enable Manage application with Arabic, Deutsch and Japanese as secondary languages: export MAS_APP_SETTINGS_SECONDARY_LANGS='AR,DE,JA' Server Bundle Configuration \u00a4 mas_app_settings_server_bundles_size \u00a4 Provides different flavors of server bundle configuration to handle workload for Manage application. Optional Environment Variable: MAS_APP_SETTINGS_SERVER_BUNDLES_SIZE Default: dev For more details about Manage application server bundle configuration, refer to Setting the server bundles for Manage application . Currently supported server bundle sizes are: - dev - Deploys Manage with the default server bundle configuration (i.e just 1 bundle pod handling all Manage application workload) - small - Deploys Manage with the most common deployment configuration (i.e 4 bundle pods, each one handling workload for each main capabilities: mea , cron , report and ui ) - jms - Can be used for Manage 8.4 and above. Same server bundle configuration as small and includes jms bundle pod. Enabling JMS pod workload will also configure Manage to use default JMS messaging queues to be stored in /{{ mas_app_settings_jms_queue_mount_path }}/jmsstore persistent volume mount path - snojms - Can be used for Manage 8.4 and above. Includes all and jms bundle pods. Enabling JMS pod workload will also configure Manage to use default JMS messaging queues to be stored in /{{ mas_app_settings_jms_queue_mount_path }}/jmsstore persistent volume mount path Customization Archive Settings \u00a4 mas_app_settings_customization_archive_url \u00a4 Provide a custom archive/file path to be included as part of Manage deployment. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOMIZATION_ARCHIVE_URL Default: None mas_app_settings_customization_archive_name \u00a4 Provide a custom archive file name to be associated with the archive/file path provided. Only used when mas_app_settings_customization_archive_url is defined. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOMIZATION_ARCHIVE_NAME Default: manage-custom-archive Database Encryption and AI Service Secrets \u00a4 Note: The encryption secret stores both database encryption keys and AI Service integration properties when aiservice_instance_id is configured. The secret will contain: - Database encryption keys: MXE_SECURITY_CRYPTO_KEY , MXE_SECURITY_CRYPTOX_KEY , MXE_SECURITY_OLD_CRYPTO_KEY , MXE_SECURITY_OLD_CRYPTOX_KEY - AI Service connection details: mxe.int.aibrokerapikey , mxe.int.aibrokerapiurl , mxe.int.aibrokertenantid mas_manage_encryptionsecret_crypto_key \u00a4 This defines the MXE_SECURITY_CRYPTO_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Optional Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_CRYPTO_KEY Default: Auto-generated mas_manage_encryptionsecret_cryptox_key \u00a4 This defines the MXE_SECURITY_CRYPTOX_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Required if mas_manage_encryptionsecret_crypto_key is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_CRYPTOX_KEY Default: Auto-generated mas_manage_encryptionsecret_old_crypto_key \u00a4 This defines the MXE_SECURITY_OLD_CRYPTO_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Optional Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_OLD_CRYPTO_KEY Default: None mas_manage_encryptionsecret_old_cryptox_key \u00a4 This defines the MXE_SECURITY_OLD_CRYPTOX_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Required if mas_manage_encryptionsecret_old_crypto_key is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_OLD_CRYPTOX_KEY Default: None mas_manage_encryptionsecret_aiservice_apikey \u00a4 The AI Service API key to configure in the encryption secret. When set along with the URL and FQN, the role will add these properties to the encryption secret. Optional Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_AISERVICE_APIKEY Default: Auto-retrieved from AI Service tenant secret when aiservice_instance_id is configured mas_manage_encryptionsecret_aiservice_url \u00a4 The AI Service broker URL to configure in the encryption secret. Required if mas_manage_encryptionsecret_aiservice_apikey is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_AISERVICE_URL Default: Auto-retrieved from AI Service route when aiservice_instance_id is configured mas_manage_encryptionsecret_aiservice_fqn \u00a4 The fully qualified AI Service tenant name (format: {instance_id}.{tenant_id} ) to configure in the encryption secret. Required if mas_manage_encryptionsecret_aiservice_apikey is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_AISERVICE_FQN Default: Auto-generated from aiservice_instance_id and aiservice_tenant_id when configured Server Timezone Setting \u00a4 mas_app_settings_server_timezone \u00a4 Sets the Manage server timezone. If you also want to have the Manage's DB2 database aligned with the same timezone, you must set DB2_TIMEZONE while provisioning the corresponding DB2 instance using db2 role. Optional Environment Variable: MAS_APP_SETTINGS_SERVER_TIMEZONE Default: GMT Facilities Workspace Variables \u00a4 mas_ws_facilities_size \u00a4 Sets the size of deployment. Optional Environment Variable: MAS_FACILITIES_SIZE Default: small Available options are small , medium and large mas_ws_facilities_pull_policy \u00a4 Sets the imagePullPolicy strategy for all deployments. The default is set to IfNotPresent to reduce the pulling operations in the cluster. Optional Environment Variable: MAS_FACILITIES_PULL_POLICY Default: IfNotPresent mas_ws_facilities_liberty_extension_xml_secret_name \u00a4 Provide the secret name of the secret which contains additional XML tags that needs to be added into the existing Liberty Server XML to configure the application accordingly. NOTE: The Secret name MUST be <workspaceId>-facilities-lexml--sn Optional Environment Variable: MAS_FACILITIES_LIBERTY_EXTENSION_XML_SECRET_NAME Default: None Sample Secret Template: cat <<EOF | oc create -f - kind: Secret apiVersion: v1 metadata: name: <MAS_FACILITIES_LIBERTY_EXTENSION_XML_SECRET_NAME> namespace: mas-<instanceId>-facilities data: extensions.xml: <!-- Custom XML tags --> type: Opaque EOF mas_ws_facilities_vault_secret_name \u00a4 Provide the name of the secret which contains a password to the vault with AES Encryption key. By default, this secret will be generated automatically. NOTE: The Secret name MUST be <workspaceId>-facilities-vs--sn Optional Environment Variable: MAS_FACILITIES_VAULT_SECRET_NAME Default: None Sample Secret Template: cat <<EOF | oc create -f - kind: Secret apiVersion: v1 metadata: name: <MAS_FACILITIES_VAULT_SECRET_NAME> namespace: mas-<instanceId>-facilities data: pwd: <your password> type: Opaque EOF mas_ws_facilities_dwfagents \u00a4 Allows the user to add dedicated workflow agents (DWFA) to MREF. To specify a DWFA it's required to specify a JSON with a unique name and members . Each member has a unique name and class that can be classified as user or group . Below an example of the structure of the JSON: export MAS_FACILITIES_DWFAGENTS='[{\"name\":\"dwfa1\",\"members\":[{\"name\": \"u1\", \"class\": \"user\"}]}, {\"name\":\"dwfa2\",\"members\":[{\"name\": \"u2\", \"class\": \"user\"},{\"name\":\"g1\", \"class\":\"group\"}]}]' Optional Environment Variable: MAS_FACILITIES_DWFAGENTS Default: [] Facilities Database Settings \u00a4 mas_ws_facilities_db_maxconnpoolsize \u00a4 Sets the maximum connection pool size for database. Optional Environment Variable: MAS_FACILITIES_DB_MAX_POOLSIZE Default: 200 Facilities Routes Settings \u00a4 mas_ws_facilities_db_timout \u00a4 Sets the timeout of the application. It is a string with the structure <timeout_value><time_unit> , where timeout_value is any non zero unsigned integer number and the supported time_unit are microseconds (us), milliseconds (ms), seconds (s), minutes (m), hours (h), or days (d). Optional Environment Variable: MAS_FACILITIES_ROUTES_TIMEOUT Default: 600s Facilities Storage Settings \u00a4 mas_ws_facilities_storage_log_class \u00a4 Sets the storage class name for the Log Persistent Volume Claim used for MREF agents. Optional Environment Variable: MAS_FACILITIES_STORAGE_LOG_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes mas_ws_facilities_storage_log_mode \u00a4 Sets the attach mode of the Log PVC. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_FACILITIES_STORAGE_LOG_MODE Default: ReadWriteOnce mas_ws_facilities_storage_log_size \u00a4 Sets the size of the Log PVC. Defaults to 30 Gigabytes. Optional Environment Variable: MAS_FACILITIES_STORAGE_LOG_SIZE Default: 30 mas_ws_facilities_storage_userfiles_class \u00a4 Sets the storage class name for the Userfiles PVC used for MREF agents. Optional Environment Variable: MAS_FACILITIES_STORAGE_USERFILES_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes mas_ws_facilities_storage_userfiles_mode \u00a4 Sets the attach mode of the Userfiles PVC. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_FACILITIES_STORAGE_USERFILES_MODE Default: ReadWriteOnce mas_ws_facilities_storage_userfiles_size \u00a4 Sets the size of the Log PVC. Defaults to 50 Gigabytes. Optional Environment Variable: MAS_FACILITIES_STORAGE_USERFILES_SIZE Default: 50 Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS workspace configuration mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" mas_appws_spec: bindings: jdbc: \"{{ mas_appws_jdbc_binding | default( 'system' , true) }}\" roles: - ibm.mas_devops.suite_app_config License \u00a4 EPL-2.0","title":"suite_app_config"},{"location":"roles/suite_app_config/#suite_app_config","text":"This role is used to configure specific components of the application workspace after the application has been installed in the Maximo Application Suite.","title":"suite_app_config"},{"location":"roles/suite_app_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_config/#general-variables","text":"","title":"General Variables"},{"location":"roles/suite_app_config/#mas_instance_id","text":"MAS instance identifier for the target installation. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to configure. This must match the instance ID used during MAS Core installation. When to use : - Always required when configuring application workspaces - Must match the instance ID from suite_install role - Used to locate the correct Suite and Workspace resources Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance's application workspace will be configured. Incorrect instance ID will cause configuration to fail. Related variables : - mas_app_id : Specifies which application to configure - mas_workspace_id : Specifies which workspace to configure Note : This must be an existing MAS instance. The role does not create new instances.","title":"mas_instance_id"},{"location":"roles/suite_app_config/#mas_app_id","text":"MAS application to configure. Required Environment Variable: MAS_APP_ID Default: None Purpose : Specifies which MAS application workspace to configure. Different applications have different configuration options and requirements. When to use : - Always required when configuring application workspaces - Must match an installed application in the MAS instance - Determines which application-specific configuration is applied Valid values : assist , iot , facilities , manage , monitor , optimizer , predict , visualinspection Impact : Determines which application workspace is configured and which configuration options are available. Each application has unique configuration requirements. Related variables : - mas_instance_id : The MAS instance containing this application - mas_workspace_id : The workspace to configure - mas_appws_components : Application-specific components to configure Note : The application must already be installed via suite_app_install role before configuration. Different applications support different configuration variables.","title":"mas_app_id"},{"location":"roles/suite_app_config/#mas_workspace_id","text":"Workspace identifier for the application workspace to configure. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the application to configure. Workspaces allow multiple isolated environments within a single application. When to use : - Always required when configuring application workspaces - Must match an existing workspace created during application installation - Typically matches the workspace ID used in suite_app_install Valid values : Lowercase alphanumeric string (e.g., masdev , prod , test ) Impact : Determines which workspace is configured. Each workspace has its own configuration, data, and users. Related variables : - mas_instance_id : The MAS instance containing this workspace - mas_app_id : The application containing this workspace - mas_appws_components : Components to configure in this workspace Note : The workspace must already exist (created during suite_app_install). This role configures existing workspaces, it does not create new ones.","title":"mas_workspace_id"},{"location":"roles/suite_app_config/#aiservice_instance_id","text":"AI Service instance ID for Manage integration (Manage only). Optional Environment Variable: AISERVICE_INSTANCE_ID Default: None Purpose : Enables automatic AI Service integration with Manage application. When set, the role retrieves credentials, configures connection, and imports certificates. When to use : - Only when configuring Manage application ( mas_app_id=manage ) - When you want to integrate AI Service capabilities into Manage - Requires AI Service to be installed and running in the cluster - Must be used together with aiservice_tenant_id Valid values : Valid AI Service instance ID (e.g., aiservice1 , ai-prod ) Impact : When configured, automatically: - Retrieves API key from tenant-specific secret - Extracts AI Service URL from aibroker route - Imports AI Service TLS certificate into Manage - Configures AI Service connection properties - Verifies AI Service health before proceeding Related variables : - aiservice_tenant_id : Required tenant ID for the integration - mas_app_id : Must be manage for this to apply Note : AI Service must be installed and healthy before configuration. The role performs automatic health checks and credential retrieval from cluster resources.","title":"aiservice_instance_id"},{"location":"roles/suite_app_config/#aiservice_tenant_id","text":"AI Service tenant ID for the integration (Manage only). Optional Environment Variable: AISERVICE_TENANT_ID Default: None Purpose : Specifies which AI Service tenant to use for Manage integration. Combined with instance ID to form the fully qualified tenant identifier. When to use : - Required when aiservice_instance_id is set - Only applies to Manage application - Must match an existing tenant in the AI Service instance Valid values : Valid AI Service tenant ID string (e.g., tenant1 , prod-tenant ) Impact : Combined with aiservice_instance_id to locate tenant-specific resources (API key secret, configuration). Incorrect tenant ID will cause integration to fail. Related variables : - aiservice_instance_id : Required instance ID for the integration - Forms secret name: aiservice-{instance_id}-{tenant_id}----apikey-secret Note : The tenant must already exist in the AI Service instance. The role retrieves credentials from the tenant-specific secret in the cluster.","title":"aiservice_tenant_id"},{"location":"roles/suite_app_config/#custom_labels","text":"Comma-separated list of key=value labels to apply to workspace resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to application workspace resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,app=manage ) Impact : Labels are applied to workspace-specific resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect workspace functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management. Note : Labels help with resource organization and are especially useful in multi-tenant or multi-workspace environments.","title":"custom_labels"},{"location":"roles/suite_app_config/#workspace-configuration-variables","text":"","title":"Workspace Configuration Variables"},{"location":"roles/suite_app_config/#mas_appws_spec","text":"Custom workspace deployment specification (overrides component-based configuration). Optional Environment Variable: MAS_APPWS_SPEC Default: Application-specific defaults in vars/defaultspecs/{mas_app_id}.yml Purpose : Provides complete control over workspace deployment specification. Allows advanced customization beyond what component-based configuration offers. When to use : - Use for advanced workspace customization - Use when you need full control over the workspace spec - Use when component-based configuration is insufficient - Leave unset for standard deployments using mas_appws_components Valid values : Valid workspace specification YAML/JSON matching the application's workspace CR schema Impact : WARNING - Overrides all settings from mas_appws_components . Provides complete control but requires deep knowledge of workspace CR structure. Incorrect specs can cause deployment failures. Related variables : - mas_appws_components : Simpler component-based configuration (overridden by this) - Application-specific default specs in vars/defaultspecs/ Note : Use mas_appws_components for standard deployments. Only use this variable when you need advanced customization or have specific requirements not covered by component-based configuration.","title":"mas_appws_spec"},{"location":"roles/suite_app_config/#mas_appws_bindings_jdbc","text":"JDBC binding scope for the workspace. Optional Environment Variable: MAS_APPWS_BINDINGS_JDBC Default: system Purpose : Controls the scope of JDBC database binding for the workspace. Different scopes provide different levels of isolation and sharing. When to use : - Use system (default) for shared system-level database configuration - Use application for application-level database configuration - Use workspace for workspace-specific database configuration - Use workspace-application for Maximo Real Estate and Facilities (recommended) Valid values : system , application , workspace , workspace-application Impact : - system : Uses system-level JDBC configuration (shared across all workspaces) - application : Uses application-level JDBC configuration - workspace : Uses workspace-specific JDBC configuration - workspace-application : Combines workspace and application scopes Related variables : JDBC configuration must exist at the specified scope level. Note : IMPORTANT - For Maximo Real Estate and Facilities applications, use workspace-application scope. The default system scope is suitable for most other applications.","title":"mas_appws_bindings_jdbc"},{"location":"roles/suite_app_config/#mas_appws_components","text":"Application components and versions to configure in the workspace. Optional Environment Variable: MAS_APPWS_COMPONENTS Default: Application-specific defaults Purpose : Specifies which application components to configure and their versions. Different applications have different available components. When to use : - Use to enable specific application components - Use to control component versions - Leave unset to use application-specific defaults - Overridden by mas_appws_spec if that is set Valid values : Comma-separated component=version pairs (e.g., base=latest,health=latest , base=latest,civil=latest ) Impact : Determines which components are configured in the workspace. Available components vary by application (e.g., Manage has base, health, civil, etc.). Related variables : - mas_appws_spec : Overrides this if set - mas_app_id : Determines available components Note : Component availability depends on the application. For Manage: base , health , civil , etc. Refer to application documentation for available components. Use latest for most recent version or specify exact version.","title":"mas_appws_components"},{"location":"roles/suite_app_config/#mas_pod_templates_dir","text":"Directory containing pod template configuration files (Manage only). Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Specifies a directory containing pod template YAML files for customizing Manage workspace and component workloads. Enables resource requests/limits, node selectors, tolerations, and other pod-level customizations. When to use : - Only for Manage application ( mas_app_id=manage ) - Use to customize pod resources (CPU, memory) - Use to apply node selectors or tolerations - Use to configure pod-level settings beyond defaults Valid values : Absolute path to directory containing pod template files Impact : Pod templates from this directory are merged into the ManageWorkspace CR. Files are applied to specific components or the workspace itself based on filename. Related variables : - mas_app_id : Must be manage for this to apply - Files must follow specific naming convention Note : Expected filenames: - ibm-mas-manage-manageworkspace.yml \u2192 workspace-level pod templates - ibm-mas-manage-imagestitching.yml \u2192 civil component pod templates - ibm-mas-manage-slackproxy.yml \u2192 component pod templates - ibm-mas-manage-healthextworkspace.yml \u2192 health component pod templates Refer to MAS CLI BestEfforts templates and Customizing Pod Templates documentation.","title":"mas_pod_templates_dir"},{"location":"roles/suite_app_config/#predict-configuration-variables","text":"","title":"Predict Configuration Variables"},{"location":"roles/suite_app_config/#mas_appws_settings_deployment_size","text":"Workload size for Predict containers (Predict only). Optional (Predict only) Environment Variable: MAS_APPWS_SETTINGS_DEPLOYMENT_SIZE Default: small Purpose : Controls the deployment size and replica count for Predict application containers. Different sizes provide different levels of availability and performance. When to use : - Only for Predict application ( mas_app_id=predict ) - Use developer for single-node development environments - Use small (default) for standard production deployments - Use medium for higher availability requirements - Use large for maximum availability (if supported) Valid values : developer , small , medium , large Impact : - developer : 1 replica (no high availability) - small : 2 replicas (basic high availability) - medium : 3 replicas (enhanced high availability) - large : Higher replica count (if supported) Related variables : - mas_app_id : Must be predict for this to apply Note : The developer size is suitable only for development/testing. Production environments should use small or larger for high availability.","title":"mas_appws_settings_deployment_size"},{"location":"roles/suite_app_config/#watson-studio-local-variables","text":"These variables are only used when using this role to configure Predict , or Health & Predict Utilities .","title":"Watson Studio Local Variables"},{"location":"roles/suite_app_config/#cpd_wsl_project_id","text":"Watson Studio analytics project ID (Predict/HP Utilities only). Required (unless cpd_wsl_project_name and mas_config_dir are set) Environment Variable: CPD_WSL_PROJECT_ID Default: None Purpose : Specifies the ID of the Watson Studio analytics project to use for Predict or Health & Predict Utilities configuration. When to use : - Required for Predict or HP Utilities configuration - Use when you know the project ID directly - Alternative: use cpd_wsl_project_name + mas_config_dir to retrieve ID from saved file Valid values : Valid Watson Studio project ID (UUID format) Impact : Links the MAS application to the specified Watson Studio project for analytics capabilities. Incorrect project ID will cause configuration to fail. Related variables : - cpd_wsl_project_name : Alternative method using project name - mas_config_dir : Required with cpd_wsl_project_name Note : The project must already exist in Watson Studio (created by cp4d_service role). Either provide this ID directly or use the name-based alternative.","title":"cpd_wsl_project_id"},{"location":"roles/suite_app_config/#cpd_wsl_project_name","text":"Filename containing Watson Studio project ID (Predict/HP Utilities only). Optional Environment Variable: CPD_WSL_PROJECT_NAME Default: wsl-mas-${mas_instance_id}-hputilities Purpose : Specifies the filename in mas_config_dir where the Watson Studio project ID is saved. Alternative to providing cpd_wsl_project_id directly. When to use : - Use with mas_config_dir as alternative to cpd_wsl_project_id - Use when project ID was saved by cp4d_service role - Allows retrieving project ID from saved configuration Valid values : Filename (without path) where project ID is stored Impact : Role reads the project ID from this file in mas_config_dir . File must exist and contain valid project ID. Related variables : - mas_config_dir : Required directory containing this file - cpd_wsl_project_id : Alternative direct ID specification Note : The default filename matches the pattern used by cp4d_service role. The file should contain the Watson Studio project ID created during CP4D service setup.","title":"cpd_wsl_project_name"},{"location":"roles/suite_app_config/#mas_config_dir","text":"Local directory for configuration files (Predict/HP Utilities only). Optional Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies directory containing configuration files, particularly Watson Studio project ID files. Used with cpd_wsl_project_name to retrieve project IDs. When to use : - Use with cpd_wsl_project_name to retrieve Watson Studio project ID - Use when project ID was saved by cp4d_service role - Should match the directory used in cp4d_service role Valid values : Absolute path to existing directory (e.g., /home/user/masconfig , ~/masconfig ) Impact : Role reads Watson Studio project ID from files in this directory. Directory must exist and contain the specified project file. Related variables : - cpd_wsl_project_name : Filename to read from this directory - cpd_wsl_project_id : Alternative direct ID specification Note : Use the same directory across all MAS setup roles for consistency. The cp4d_service role saves project IDs here, and this role retrieves them.","title":"mas_config_dir"},{"location":"roles/suite_app_config/#watson-machine-learning-variables","text":"These variables are only used when using this role to configure Predict .","title":"Watson Machine Learning Variables"},{"location":"roles/suite_app_config/#cpd_product_version","text":"Cloud Pak for Data version (Predict only). Required (Predict only) Environment Variable: CPD_PRODUCT_VERSION Default: None Purpose : Specifies the CP4D version to infer the correct Watson Machine Learning version for Predict workspace configuration. When to use : - Required when configuring Predict application - Must match the installed CP4D version in the cluster - Used to determine compatible WML version Valid values : Valid CP4D version string (e.g., 4.8.0 , 4.8.5 , 5.0.0 ) Impact : Determines which Watson Machine Learning version is configured in Predict. Incorrect version may cause compatibility issues. Related variables : - cpd_wml_instance_id : WML instance to configure - cpd_wml_url : WML service URL - mas_app_id : Must be predict for this to apply Note : This must match the actual CP4D version installed in your cluster. The role uses this to select the appropriate WML version for Predict configuration.","title":"cpd_product_version"},{"location":"roles/suite_app_config/#cpd_wml_instance_id","text":"Watson Machine Learning instance identifier (Predict only). Optional (Predict only) Environment Variable: CPD_WML_INSTANCE_ID Default: openshift Purpose : Specifies the Watson Machine Learning instance identifier to configure in Predict workspace. When to use : - Only for Predict application configuration - Use default ( openshift ) for standard deployments - Set custom value if using non-default WML instance Valid values : Valid WML instance identifier string Impact : Identifies which WML instance Predict will use for machine learning operations. Must match an existing WML instance in CP4D. Related variables : - cpd_product_version : CP4D version for WML compatibility - cpd_wml_url : WML service URL - mas_app_id : Must be predict for this to apply Note : The default openshift value is suitable for most deployments. Only change if you have a specific WML instance identifier.","title":"cpd_wml_instance_id"},{"location":"roles/suite_app_config/#cpd_wml_url","text":"Watson Machine Learning service URL (Predict only). Optional (Predict only) Environment Variable: CPD_WML_URL Default: https://internal-nginx-svc.ibm-cpd.svc:12443 Purpose : Specifies the URL to access Watson Machine Learning service. Typically the same as the Cloud Pak for Data URL. When to use : - Only for Predict application configuration - Use default if CP4D is in ibm-cpd namespace - Set custom URL if CP4D is in different namespace or uses custom service name Valid values : Valid HTTPS URL to WML service (e.g., https://internal-nginx-svc.{namespace}.svc:12443 ) Impact : Determines how Predict connects to Watson Machine Learning. Incorrect URL will prevent Predict from accessing WML services. Related variables : - cpd_product_version : CP4D version - cpd_wml_instance_id : WML instance identifier - mas_app_id : Must be predict for this to apply Note : The default assumes CP4D is installed in the ibm-cpd namespace. If CP4D is in a different namespace, update the URL accordingly (e.g., https://internal-nginx-svc.my-cpd-namespace.svc:12443 ).","title":"cpd_wml_url"},{"location":"roles/suite_app_config/#manage-workspace-variables","text":"","title":"Manage Workspace Variables"},{"location":"roles/suite_app_config/#ai-service-integration","text":"","title":"AI Service Integration"},{"location":"roles/suite_app_config/#aiservice_instance_id_1","text":"AI Service instance ID to integrate with Manage application. Optional Environment Variable: AISERVICE_INSTANCE_ID Default: None When configured, the role will: - Retrieve AI Service API key from the tenant-specific secret - Extract AI Service URL from the aibroker route - Import AI Service TLS certificate into Manage - Configure AI Service connection properties in Manage encryption secret - Verify AI Service health status before proceeding","title":"aiservice_instance_id"},{"location":"roles/suite_app_config/#aiservice_tenant_id_1","text":"AI Service tenant ID to use for the integration. This is combined with the instance ID to form the fully qualified tenant name. Required when aiservice_instance_id is set Environment Variable: AISERVICE_TENANT_ID Default: None Note: The AI Service integration automatically retrieves the following from the cluster: - API key from secret: aiservice-{instance_id}-{tenant_id}----apikey-secret - Service URL from route: aibroker in namespace aiservice-{instance_id} - TLS certificate from secret: {instance_id}-public-aibroker-tls The integration also performs a health check to verify AI Service is running before completing the configuration.","title":"aiservice_tenant_id"},{"location":"roles/suite_app_config/#health-integration","text":"","title":"Health Integration"},{"location":"roles/suite_app_config/#mas_appws_bindings_health_wsl_flag","text":"Enable Watson Studio binding for Health (Manage only). Optional (Manage Health only) Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL_FLAG Default: false Purpose : Controls whether Watson Studio should be bound to the Manage Health component. Requires a system-level WatsonStudioCfg to be applied in the cluster. When to use : - Only for Manage application with Health component - Set to true to enable Watson Studio integration with Health - Leave as false (default) if Watson Studio integration is not needed Valid values : true , false Impact : When true , binds Watson Studio to Health component, enabling advanced analytics capabilities. Requires Watson Studio to be configured at system level. Related variables : - mas_appws_bindings_health_wsl : Binding scope (typically system ) - mas_app_id : Must be manage with Health component Note : A system-level WatsonStudioCfg must exist in the cluster before enabling this. Watson Studio must be installed and configured via CP4D.","title":"mas_appws_bindings_health_wsl_flag"},{"location":"roles/suite_app_config/#mas_appws_bindings_health_wsl","text":"Watson Studio binding scope for Health (Manage only). Optional (Manage Health only) Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL Default: None Purpose : Specifies the binding scope for Watson Studio integration with Manage Health component. When to use : - Only for Manage application with Health component - Set to system when Watson Studio is configured at system level - Used together with mas_appws_bindings_health_wsl_flag=true Valid values : system Impact : Binds Watson Studio at the specified scope to Health component, enabling advanced analytics and AI capabilities. Related variables : - mas_appws_bindings_health_wsl_flag : Must be true to enable binding - mas_app_id : Must be manage with Health component Note : Watson Studio must be installed and configured via CP4D with a system-level WatsonStudioCfg before using this binding. - Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL - Default: None","title":"mas_appws_bindings_health_wsl"},{"location":"roles/suite_app_config/#mas_app_settings_aio_flag","text":"Flag indicating if Asset Investment Optimization (AIO) resource must be loaded or not. It can be loaded only when Optimizer application is installed. Optional , only supported when Optimizer application is installed Environment Variable: MAS_APP_SETTINGS_AIO_FLAG Default: true","title":"mas_app_settings_aio_flag"},{"location":"roles/suite_app_config/#db2-settings","text":"","title":"DB2 Settings"},{"location":"roles/suite_app_config/#mas_app_settings_db_schema","text":"Name of the schema where Manage database lives in. Code also supports deprecated mas_app_settings_db2_schema variable name. Optional Environment Variable: MAS_APP_SETTINGS_DB_SCHEMA Default: maximo","title":"mas_app_settings_db_schema"},{"location":"roles/suite_app_config/#mas_app_settings_demodata","text":"Flag indicating if manage demodata should be loaded or not. Optional Environment Variable: MAS_APP_SETTINGS_DEMODATA Default: false (do not load demodata)","title":"mas_app_settings_demodata"},{"location":"roles/suite_app_config/#mas_app_settings_db2vargraphic","text":"Flag indicating if VARGRAPHIC (if true) or VARCHAR (if false) is used. Details: https://www.ibm.com/docs/en/mas-cd/continuous-delivery?topic=deploy-language-support Optional Environment Variable: MAS_APP_SETTINGS_DB2VARGRAPHIC Default: true","title":"mas_app_settings_db2vargraphic"},{"location":"roles/suite_app_config/#mas_app_settings_tablespace","text":"Name of the Manage database tablespace. Optional Environment Variable: MAS_APP_SETTINGS_TABLESPACE Default: MAXDATA","title":"mas_app_settings_tablespace"},{"location":"roles/suite_app_config/#mas_app_settings_indexspace","text":"Name of the Manage database indexspace. Optional Environment Variable: MAS_APP_SETTINGS_INDEXSPACE Default: MAXINDEX","title":"mas_app_settings_indexspace"},{"location":"roles/suite_app_config/#persistent-volumes","text":"","title":"Persistent Volumes"},{"location":"roles/suite_app_config/#mas_app_settings_persistent_volumes_flag","text":"Flag indicating if persistent volumes should be configured by default during Manage Workspace activation. There are two defaulted File Storage Persistent Volumes Claim resources that will be created out of the box for Manage if this flag is set to true : /DOCLINKS : Persistent volume used to store doclinks/attachments /bim : Persistent volume used to store Building Information Models related artifacts (models, docs and import) Optional Environment Variable: MAS_APP_SETTINGS_PERSISTENT_VOLUMES_FLAG Default: false","title":"mas_app_settings_persistent_volumes_flag"},{"location":"roles/suite_app_config/#jms-queues","text":"The following properties can be defined to customize the persistent volumes for the JMS queues setup for Manage.","title":"JMS Queues"},{"location":"roles/suite_app_config/#mas_app_settings_jms_queue_pvc_storage_class","text":"Provide the persistent volume storage class to be used for JMS queue configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) access modes are supported. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes","title":"mas_app_settings_jms_queue_pvc_storage_class"},{"location":"roles/suite_app_config/#mas_app_settings_jms_queue_pvc_name","text":"Provide the persistent volume claim name to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_NAME Default: manage-jms","title":"mas_app_settings_jms_queue_pvc_name"},{"location":"roles/suite_app_config/#mas_app_settings_jms_queue_pvc_size","text":"Provide the persistent volume claim size to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_SIZE Default: 20Gi","title":"mas_app_settings_jms_queue_pvc_size"},{"location":"roles/suite_app_config/#mas_app_settings_jms_queue_mount_path","text":"Provide the persistent volume storage mount path to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_MOUNT_PATH Default: /jms","title":"mas_app_settings_jms_queue_mount_path"},{"location":"roles/suite_app_config/#mas_app_settings_jms_queue_pvc_accessmode","text":"Provide the persistent volume storage access-mode to be used for JMS queue configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_ACCESSMODE Default: ReadWriteMany","title":"mas_app_settings_jms_queue_pvc_accessmode"},{"location":"roles/suite_app_config/#mas_app_settings_default_jms","text":"Set this to true if you want to have JMS continuous queues configured. Optional Environment Variable: MAS_APP_SETTINGS_DEFAULT_JMS Default: false","title":"mas_app_settings_default_jms"},{"location":"roles/suite_app_config/#doclinksattachments","text":"The following properties can be defined to customize the persistent volumes for the Doclinks/Attachments setup for Manage.","title":"Doclinks/Attachments"},{"location":"roles/suite_app_config/#mas_app_settings_doclinks_pvc_storage_class","text":"Provide the persistent volume storage class to be used for doclinks/attachments configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes","title":"mas_app_settings_doclinks_pvc_storage_class"},{"location":"roles/suite_app_config/#mas_app_settings_doclinks_pvc_name","text":"Provide the persistent volume claim name to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_NAME Default: manage-doclinks","title":"mas_app_settings_doclinks_pvc_name"},{"location":"roles/suite_app_config/#mas_app_settings_doclinks_pvc_size","text":"Provide the persistent volume claim size to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_SIZE Default: 20Gi","title":"mas_app_settings_doclinks_pvc_size"},{"location":"roles/suite_app_config/#mas_app_settings_doclinks_mount_path","text":"Provide the persistent volume storage mount path to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_MOUNT_PATH Default: /DOCLINKS","title":"mas_app_settings_doclinks_mount_path"},{"location":"roles/suite_app_config/#mas_app_settings_doclinks_pvc_accessmode","text":"Provide the persistent volume storage access-mode to be used for doclinks/attachments configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_ACCESSMODE Default: ReadWriteMany","title":"mas_app_settings_doclinks_pvc_accessmode"},{"location":"roles/suite_app_config/#bim-building-information-models","text":"The following properties can be defined to customize the persistent volumes for the Building Information Models setup for Manage.","title":"BIM (Building Information Models)"},{"location":"roles/suite_app_config/#mas_app_settings_bim_pvc_storage_class","text":"Provide the persistent volume storage class to be used for Building Information Models configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes","title":"mas_app_settings_bim_pvc_storage_class"},{"location":"roles/suite_app_config/#mas_app_settings_bim_pvc_name","text":"Provide the persistent volume claim name to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_NAME Default: manage-bim","title":"mas_app_settings_bim_pvc_name"},{"location":"roles/suite_app_config/#mas_app_settings_bim_pvc_size","text":"Provide the persistent volume claim size to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_SIZE Default: 20Gi","title":"mas_app_settings_bim_pvc_size"},{"location":"roles/suite_app_config/#mas_app_settings_bim_mount_path","text":"Provide the persistent volume storage mount path to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_MOUNT_PATH Default: /bim","title":"mas_app_settings_bim_mount_path"},{"location":"roles/suite_app_config/#mas_app_settings_bim_pvc_accessmode","text":"Provide the persistent volume storage access-mode to be used for Building Information Models configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_ACCESSMODE Default: ReadWriteMany","title":"mas_app_settings_bim_pvc_accessmode"},{"location":"roles/suite_app_config/#supported-languages","text":"","title":"Supported Languages"},{"location":"roles/suite_app_config/#mas_app_settings_base_lang","text":"Provide the base language for Manage application. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. Optional Environment Variable: MAS_APP_SETTINGS_BASE_LANG Default: EN (English)","title":"mas_app_settings_base_lang"},{"location":"roles/suite_app_config/#mas_app_settings_secondary_langs","text":"Provide a list of additional secondary languages for Manage application. Optional Environment Variable: MAS_APP_SETTINGS_SECONDARY_LANGS Default: None Note: The more languages you add, the longer Manage will take to install and activate. Export the MAS_APP_SETTINGS_SECONDARY_LANGS variable with the language codes as comma-separated values. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. For example, use the following to enable Manage application with Arabic, Deutsch and Japanese as secondary languages: export MAS_APP_SETTINGS_SECONDARY_LANGS='AR,DE,JA'","title":"mas_app_settings_secondary_langs"},{"location":"roles/suite_app_config/#server-bundle-configuration","text":"","title":"Server Bundle Configuration"},{"location":"roles/suite_app_config/#mas_app_settings_server_bundles_size","text":"Provides different flavors of server bundle configuration to handle workload for Manage application. Optional Environment Variable: MAS_APP_SETTINGS_SERVER_BUNDLES_SIZE Default: dev For more details about Manage application server bundle configuration, refer to Setting the server bundles for Manage application . Currently supported server bundle sizes are: - dev - Deploys Manage with the default server bundle configuration (i.e just 1 bundle pod handling all Manage application workload) - small - Deploys Manage with the most common deployment configuration (i.e 4 bundle pods, each one handling workload for each main capabilities: mea , cron , report and ui ) - jms - Can be used for Manage 8.4 and above. Same server bundle configuration as small and includes jms bundle pod. Enabling JMS pod workload will also configure Manage to use default JMS messaging queues to be stored in /{{ mas_app_settings_jms_queue_mount_path }}/jmsstore persistent volume mount path - snojms - Can be used for Manage 8.4 and above. Includes all and jms bundle pods. Enabling JMS pod workload will also configure Manage to use default JMS messaging queues to be stored in /{{ mas_app_settings_jms_queue_mount_path }}/jmsstore persistent volume mount path","title":"mas_app_settings_server_bundles_size"},{"location":"roles/suite_app_config/#customization-archive-settings","text":"","title":"Customization Archive Settings"},{"location":"roles/suite_app_config/#mas_app_settings_customization_archive_url","text":"Provide a custom archive/file path to be included as part of Manage deployment. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOMIZATION_ARCHIVE_URL Default: None","title":"mas_app_settings_customization_archive_url"},{"location":"roles/suite_app_config/#mas_app_settings_customization_archive_name","text":"Provide a custom archive file name to be associated with the archive/file path provided. Only used when mas_app_settings_customization_archive_url is defined. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOMIZATION_ARCHIVE_NAME Default: manage-custom-archive","title":"mas_app_settings_customization_archive_name"},{"location":"roles/suite_app_config/#database-encryption-and-ai-service-secrets","text":"Note: The encryption secret stores both database encryption keys and AI Service integration properties when aiservice_instance_id is configured. The secret will contain: - Database encryption keys: MXE_SECURITY_CRYPTO_KEY , MXE_SECURITY_CRYPTOX_KEY , MXE_SECURITY_OLD_CRYPTO_KEY , MXE_SECURITY_OLD_CRYPTOX_KEY - AI Service connection details: mxe.int.aibrokerapikey , mxe.int.aibrokerapiurl , mxe.int.aibrokertenantid","title":"Database Encryption and AI Service Secrets"},{"location":"roles/suite_app_config/#mas_manage_encryptionsecret_crypto_key","text":"This defines the MXE_SECURITY_CRYPTO_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Optional Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_CRYPTO_KEY Default: Auto-generated","title":"mas_manage_encryptionsecret_crypto_key"},{"location":"roles/suite_app_config/#mas_manage_encryptionsecret_cryptox_key","text":"This defines the MXE_SECURITY_CRYPTOX_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Required if mas_manage_encryptionsecret_crypto_key is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_CRYPTOX_KEY Default: Auto-generated","title":"mas_manage_encryptionsecret_cryptox_key"},{"location":"roles/suite_app_config/#mas_manage_encryptionsecret_old_crypto_key","text":"This defines the MXE_SECURITY_OLD_CRYPTO_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Optional Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_OLD_CRYPTO_KEY Default: None","title":"mas_manage_encryptionsecret_old_crypto_key"},{"location":"roles/suite_app_config/#mas_manage_encryptionsecret_old_cryptox_key","text":"This defines the MXE_SECURITY_OLD_CRYPTOX_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Required if mas_manage_encryptionsecret_old_crypto_key is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_OLD_CRYPTOX_KEY Default: None","title":"mas_manage_encryptionsecret_old_cryptox_key"},{"location":"roles/suite_app_config/#mas_manage_encryptionsecret_aiservice_apikey","text":"The AI Service API key to configure in the encryption secret. When set along with the URL and FQN, the role will add these properties to the encryption secret. Optional Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_AISERVICE_APIKEY Default: Auto-retrieved from AI Service tenant secret when aiservice_instance_id is configured","title":"mas_manage_encryptionsecret_aiservice_apikey"},{"location":"roles/suite_app_config/#mas_manage_encryptionsecret_aiservice_url","text":"The AI Service broker URL to configure in the encryption secret. Required if mas_manage_encryptionsecret_aiservice_apikey is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_AISERVICE_URL Default: Auto-retrieved from AI Service route when aiservice_instance_id is configured","title":"mas_manage_encryptionsecret_aiservice_url"},{"location":"roles/suite_app_config/#mas_manage_encryptionsecret_aiservice_fqn","text":"The fully qualified AI Service tenant name (format: {instance_id}.{tenant_id} ) to configure in the encryption secret. Required if mas_manage_encryptionsecret_aiservice_apikey is set Environment Variable: MAS_MANAGE_ENCRYPTIONSECRET_AISERVICE_FQN Default: Auto-generated from aiservice_instance_id and aiservice_tenant_id when configured","title":"mas_manage_encryptionsecret_aiservice_fqn"},{"location":"roles/suite_app_config/#server-timezone-setting","text":"","title":"Server Timezone Setting"},{"location":"roles/suite_app_config/#mas_app_settings_server_timezone","text":"Sets the Manage server timezone. If you also want to have the Manage's DB2 database aligned with the same timezone, you must set DB2_TIMEZONE while provisioning the corresponding DB2 instance using db2 role. Optional Environment Variable: MAS_APP_SETTINGS_SERVER_TIMEZONE Default: GMT","title":"mas_app_settings_server_timezone"},{"location":"roles/suite_app_config/#facilities-workspace-variables","text":"","title":"Facilities Workspace Variables"},{"location":"roles/suite_app_config/#mas_ws_facilities_size","text":"Sets the size of deployment. Optional Environment Variable: MAS_FACILITIES_SIZE Default: small Available options are small , medium and large","title":"mas_ws_facilities_size"},{"location":"roles/suite_app_config/#mas_ws_facilities_pull_policy","text":"Sets the imagePullPolicy strategy for all deployments. The default is set to IfNotPresent to reduce the pulling operations in the cluster. Optional Environment Variable: MAS_FACILITIES_PULL_POLICY Default: IfNotPresent","title":"mas_ws_facilities_pull_policy"},{"location":"roles/suite_app_config/#mas_ws_facilities_liberty_extension_xml_secret_name","text":"Provide the secret name of the secret which contains additional XML tags that needs to be added into the existing Liberty Server XML to configure the application accordingly. NOTE: The Secret name MUST be <workspaceId>-facilities-lexml--sn Optional Environment Variable: MAS_FACILITIES_LIBERTY_EXTENSION_XML_SECRET_NAME Default: None Sample Secret Template: cat <<EOF | oc create -f - kind: Secret apiVersion: v1 metadata: name: <MAS_FACILITIES_LIBERTY_EXTENSION_XML_SECRET_NAME> namespace: mas-<instanceId>-facilities data: extensions.xml: <!-- Custom XML tags --> type: Opaque EOF","title":"mas_ws_facilities_liberty_extension_xml_secret_name"},{"location":"roles/suite_app_config/#mas_ws_facilities_vault_secret_name","text":"Provide the name of the secret which contains a password to the vault with AES Encryption key. By default, this secret will be generated automatically. NOTE: The Secret name MUST be <workspaceId>-facilities-vs--sn Optional Environment Variable: MAS_FACILITIES_VAULT_SECRET_NAME Default: None Sample Secret Template: cat <<EOF | oc create -f - kind: Secret apiVersion: v1 metadata: name: <MAS_FACILITIES_VAULT_SECRET_NAME> namespace: mas-<instanceId>-facilities data: pwd: <your password> type: Opaque EOF","title":"mas_ws_facilities_vault_secret_name"},{"location":"roles/suite_app_config/#mas_ws_facilities_dwfagents","text":"Allows the user to add dedicated workflow agents (DWFA) to MREF. To specify a DWFA it's required to specify a JSON with a unique name and members . Each member has a unique name and class that can be classified as user or group . Below an example of the structure of the JSON: export MAS_FACILITIES_DWFAGENTS='[{\"name\":\"dwfa1\",\"members\":[{\"name\": \"u1\", \"class\": \"user\"}]}, {\"name\":\"dwfa2\",\"members\":[{\"name\": \"u2\", \"class\": \"user\"},{\"name\":\"g1\", \"class\":\"group\"}]}]' Optional Environment Variable: MAS_FACILITIES_DWFAGENTS Default: []","title":"mas_ws_facilities_dwfagents"},{"location":"roles/suite_app_config/#facilities-database-settings","text":"","title":"Facilities Database Settings"},{"location":"roles/suite_app_config/#mas_ws_facilities_db_maxconnpoolsize","text":"Sets the maximum connection pool size for database. Optional Environment Variable: MAS_FACILITIES_DB_MAX_POOLSIZE Default: 200","title":"mas_ws_facilities_db_maxconnpoolsize"},{"location":"roles/suite_app_config/#facilities-routes-settings","text":"","title":"Facilities Routes Settings"},{"location":"roles/suite_app_config/#mas_ws_facilities_db_timout","text":"Sets the timeout of the application. It is a string with the structure <timeout_value><time_unit> , where timeout_value is any non zero unsigned integer number and the supported time_unit are microseconds (us), milliseconds (ms), seconds (s), minutes (m), hours (h), or days (d). Optional Environment Variable: MAS_FACILITIES_ROUTES_TIMEOUT Default: 600s","title":"mas_ws_facilities_db_timout"},{"location":"roles/suite_app_config/#facilities-storage-settings","text":"","title":"Facilities Storage Settings"},{"location":"roles/suite_app_config/#mas_ws_facilities_storage_log_class","text":"Sets the storage class name for the Log Persistent Volume Claim used for MREF agents. Optional Environment Variable: MAS_FACILITIES_STORAGE_LOG_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes","title":"mas_ws_facilities_storage_log_class"},{"location":"roles/suite_app_config/#mas_ws_facilities_storage_log_mode","text":"Sets the attach mode of the Log PVC. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_FACILITIES_STORAGE_LOG_MODE Default: ReadWriteOnce","title":"mas_ws_facilities_storage_log_mode"},{"location":"roles/suite_app_config/#mas_ws_facilities_storage_log_size","text":"Sets the size of the Log PVC. Defaults to 30 Gigabytes. Optional Environment Variable: MAS_FACILITIES_STORAGE_LOG_SIZE Default: 30","title":"mas_ws_facilities_storage_log_size"},{"location":"roles/suite_app_config/#mas_ws_facilities_storage_userfiles_class","text":"Sets the storage class name for the Userfiles PVC used for MREF agents. Optional Environment Variable: MAS_FACILITIES_STORAGE_USERFILES_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes","title":"mas_ws_facilities_storage_userfiles_class"},{"location":"roles/suite_app_config/#mas_ws_facilities_storage_userfiles_mode","text":"Sets the attach mode of the Userfiles PVC. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_FACILITIES_STORAGE_USERFILES_MODE Default: ReadWriteOnce","title":"mas_ws_facilities_storage_userfiles_mode"},{"location":"roles/suite_app_config/#mas_ws_facilities_storage_userfiles_size","text":"Sets the size of the Log PVC. Defaults to 50 Gigabytes. Optional Environment Variable: MAS_FACILITIES_STORAGE_USERFILES_SIZE Default: 50","title":"mas_ws_facilities_storage_userfiles_size"},{"location":"roles/suite_app_config/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS workspace configuration mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" mas_appws_spec: bindings: jdbc: \"{{ mas_appws_jdbc_binding | default( 'system' , true) }}\" roles: - ibm.mas_devops.suite_app_config","title":"Example Playbook"},{"location":"roles/suite_app_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_install/","text":"suite_app_install \u00a4 This role is used to install a specified application in Maximo Application Suite. Role Variables \u00a4 General \u00a4 mas_instance_id \u00a4 Unique identifier for the MAS instance where the application will be installed. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to install the application into. This must match the instance ID used during the MAS core installation to ensure the application is deployed to the correct MAS environment. When to use : - Always required for any MAS application installation - Must match the instance ID from your MAS core installation - Use the same value across all application installations for a given MAS instance Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : The application will be installed into the namespace mas-{mas_instance_id}-{mas_app_id} . An incorrect instance ID will cause the installation to fail or create resources in the wrong namespace. Related variables : Works with mas_app_id to determine the target namespace for the application. Note : This must match the instance ID used when installing MAS core. Cannot be changed after application installation. mas_app_id \u00a4 Specifies which MAS application to install. Required Environment Variable: MAS_APP_ID Default: None Purpose : Identifies the specific MAS application to be installed, determining which operator subscription is created and which application resources are deployed. When to use : - Always required for any MAS application installation - Set to the specific application you want to install - Each application requires a separate role execution Valid values : assist , iot , facilities , manage , monitor , predict , visualinspection , optimizer , arcgis Impact : Determines which application operator is installed and which namespace is created ( mas-{mas_instance_id}-{mas_app_id} ). Different applications have different configuration requirements and dependencies. Related variables : - mas_app_channel : Must be set to a valid channel for the selected application - mas_app_catalog_source : Must contain the operator for the selected application - Application-specific settings variables (e.g., mas_app_settings_iot_* for IoT) Note : Each application has its own set of configuration variables. Refer to the application-specific sections below for additional required variables. mas_app_catalog_source \u00a4 Specifies the OpenShift operator catalog source containing the MAS application operator subscription. Optional Environment Variable: MAS_APP_CATALOG_SOURCE Default: ibm-operator-catalog Purpose : Controls which operator catalog is used to locate and install the MAS application operator. This determines where OpenShift looks for the application operator images and metadata during installation. When to use : - Use default ibm-operator-catalog for production installations with official IBM releases - Use ibm-mas-{mas_app_id}-operators for development builds from Artifactory (e.g., ibm-mas-manage-operators ) - Only change if directed to use a custom catalog for specific testing or airgap scenarios Valid values : Any valid CatalogSource name present in the openshift-marketplace namespace Impact : Changing this value affects which operator versions are available for installation. An invalid catalog source will cause the subscription to fail. Development catalogs require additional authentication via artifactory_username and artifactory_token . Related variables : - mas_app_channel : Works together to determine the specific operator version installed - artifactory_username and artifactory_token : Required when using development catalogs Note : For development catalogs, the naming pattern is ibm-mas-{mas_app_id}-operators where {mas_app_id} is the application name (e.g., manage , monitor ). mas_app_channel \u00a4 Specifies the MAS application operator subscription channel, which determines the version stream you'll receive updates from. Required Environment Variable: MAS_APP_CHANNEL Default: None Purpose : Controls which version of the MAS application will be installed and which updates will be automatically applied. The channel corresponds to major.minor version releases and determines the feature set and compatibility level of your application installation. When to use : - Set to the latest stable channel for new production deployments - Use specific older channels when compatibility with MAS core or other applications requires it - Consult the MAS compatibility matrix before selecting a channel - Change channels only during planned upgrade windows as this triggers version updates Valid values : Application-specific channels (e.g., 8.6.x , 8.7.x , 8.8.x for Manage; check the IBM Operator Catalog for currently available channels for your application) Impact : The channel determines which application version is installed and which automatic updates are applied. Changing channels after installation will trigger an upgrade to the latest version in that channel, which may require application reconfiguration and testing. Related variables : - mas_app_catalog_source : Works together to determine available channels - mas_instance_id : Application must be compatible with the MAS core version Note : Each MAS application has its own set of available channels. Ensure the selected channel is compatible with your MAS core version. Review the application upgrade documentation before changing this value. custom_labels \u00a4 Comma-separated list of key=value labels to apply to MAS application resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to application resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,app=manage ) Impact : Labels are applied to application resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect application functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management. Pre-Release Support \u00a4 artifactory_username \u00a4 Username for authenticating to IBM Artifactory to access development builds of MAS applications. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None Purpose : Provides authentication credentials to pull development/pre-release MAS application operator images from IBM's Artifactory registry. Required only when installing development builds for testing or early access. When to use : - Required when mas_app_catalog_source is set to a development catalog (e.g., ibm-mas-manage-operators ) - Not needed for production installations using ibm-operator-catalog - Use your IBM w3Id username for development builds Valid values : Valid IBM Artifactory username (typically your w3Id) Impact : Without valid credentials, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_token : Must be set together with this username - mas_app_catalog_source : Determines if Artifactory credentials are needed - mas_entitlement_username : Used for production installations instead Note : Only required for development/pre-release builds. Production installations use mas_entitlement_key instead. artifactory_token \u00a4 API token for authenticating to IBM Artifactory to access development builds of MAS applications. Optional Environment Variable: ARTIFACTORY_TOKEN Default: None Purpose : Provides the API token/password credential to authenticate with IBM's Artifactory registry when pulling development/pre-release MAS application operator images. When to use : - Required when mas_app_catalog_source is set to a development catalog (e.g., ibm-mas-manage-operators ) - Not needed for production installations using ibm-operator-catalog - Use your IBM Artifactory API key for development builds Valid values : Valid IBM Artifactory API token Impact : Without a valid token, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_username : Must be set together with this token - mas_app_catalog_source : Determines if Artifactory credentials are needed - mas_entitlement_key : Used for production installations instead Note : Only required for development/pre-release builds. Production installations use mas_entitlement_key instead. Keep this token secure and do not commit to source control. mas_entitlement_username \u00a4 Username for authenticating to IBM Container Registry to pull MAS application images. Optional Environment Variable: MAS_ENTITLEMENT_USERNAME Default: cp for production installations Purpose : Provides the username credential for authenticating with IBM's entitled container registry when pulling MAS application operator and component images. When to use : - Set to cp for production installations using IBM entitlement keys - Set to your w3Id for development builds from Artifactory - Usually can be left at default for production installations Valid values : - cp - For production installations with IBM entitlement key - Your IBM w3Id - For development builds Impact : Used together with mas_entitlement_key to create image pull secrets for the application namespace. Incorrect username will cause image pull authentication failures. Related variables : - mas_entitlement_key : Must be set together with this username - artifactory_username : Alternative for development builds Note : For production installations, the default value cp is typically correct when used with an IBM entitlement key. mas_entitlement_key \u00a4 IBM entitlement key for authenticating access to IBM Container Registry to pull MAS application images. Optional Environment Variable: MAS_ENTITLEMENT_KEY Default: None Purpose : Provides authentication credentials to pull MAS application container images from IBM's entitled registry. This key is tied to your IBM Cloud account and product entitlements, proving you have the right to use MAS application software. When to use : - Required for production installations using official IBM releases - Obtain from IBM Container Library at https://myibm.ibm.com/products-services/containerlibrary - For development builds, use your Artifactory API key instead - Key must be valid and have active MAS application entitlements Valid values : - IBM entitlement key string (typically starts with \"eyJ...\") - Must be a valid, non-expired key with MAS product entitlements - For development: Artifactory API key Impact : Invalid or expired keys will cause image pull failures during application installation. The key is stored in a Kubernetes secret and used to create image pull secrets for all application pods. Related variables : - mas_entitlement_username : Username paired with this key (default: cp ) - artifactory_token : Alternative for development builds Note : Keep this key secure. Do not commit it to source control. Use environment variables or secure secret management. Application Configuration \u00a4 mas_app_spec \u00a4 Custom YAML specification to override default application configuration settings. Optional Environment Variable: None Default: Application-specific defaults in vars/defaultspecs/{{mas_app_id}}.yml Purpose : Allows advanced users to provide a complete custom specification for the application installation, bypassing individual configuration variables. This enables fine-grained control over application settings not exposed through standard variables. When to use : - Use for advanced customization scenarios not covered by standard variables - Use when you need to set application-specific settings not exposed as role variables - Use with caution as it overrides all other configuration variables - Leave unset for standard installations using individual configuration variables Valid values : Valid YAML dictionary matching the application's Custom Resource specification Impact : When set, this completely overrides all other application configuration variables (e.g., mas_app_settings_* , mas_app_bindings_* ). The role will use this specification directly in the application CR. Related variables : Overrides all mas_app_settings_* and mas_app_bindings_* variables when set. Note : Requires deep knowledge of the application's Custom Resource specification. Incorrect specifications can cause installation failures. Use individual configuration variables unless you have specific advanced requirements. mas_app_bindings_jdbc \u00a4 Specifies the scope for JDBC database configuration binding. Optional Environment Variable: MAS_APP_BINDINGS_JDBC Default: system Purpose : Controls whether the application uses system-level JDBC configuration (shared across all applications) or application-specific JDBC configuration. This determines which database configuration the application will use. When to use : - Use system (default) when all applications share the same database configuration - Use application when this application needs its own dedicated database configuration - Most deployments use system for simplified management Valid values : system , application Impact : - system : Application uses the JDBC configuration defined at the MAS instance level (JdbcCfg with scope=system) - application : Application requires its own application-scoped JDBC configuration (JdbcCfg with scope=application) Related variables : Requires corresponding JdbcCfg resource to be configured at the appropriate scope. Note : Changing this after installation may require reconfiguration of database connections. Ensure the appropriate JdbcCfg resource exists before changing this value. mas_app_plan \u00a4 Specifies the licensing plan/tier for the application installation. Optional Environment Variable: MAS_APP_PLAN Default: Application-specific (varies by application) Purpose : Determines which feature set and licensing tier is activated for the application. Different plans may enable or restrict certain features based on your license entitlements. When to use : - Set according to your license entitlements - Consult your IBM license agreement for available plans - Leave as default if you have standard licensing Valid values : Application-specific - Optimizer : full , limited (v8.2+, defaults to full ) - Other applications may have different plan options Impact : The plan determines which features are available in the application. Using a plan not covered by your license may result in compliance issues. Some features may be disabled or unavailable depending on the selected plan. Related variables : Must align with your IBM license entitlements for the application. Note : Ensure the selected plan matches your license agreement. Contact IBM if you're unsure which plan to use. mas_pod_templates_dir \u00a4 Local directory path containing pod template customization files for the application. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Allows customization of pod specifications for application workloads, enabling control over resource limits, node affinity, tolerations, and other Kubernetes pod settings. This is essential for production deployments with specific infrastructure requirements. When to use : - Use to set custom resource limits (CPU, memory) for application pods - Use to configure node affinity or anti-affinity rules - Use to add tolerations for tainted nodes - Use to apply custom security contexts or service accounts - Leave unset for default pod configurations Valid values : Any valid local filesystem path containing application-specific pod template YAML files Impact : Pod template files in this directory will be applied to the application's Custom Resource, affecting how application pods are scheduled and configured. Invalid templates can cause pod scheduling failures. Related variables : - mas_app_id : Determines which pod template files are expected - See application-specific sections below for required file names Note : Each application expects specific file names. Refer to the application-specific mas_pod_templates_dir documentation below for details. For full documentation, see Customizing Pod Templates in the product documentation. Visual Inspection Configuration \u00a4 mas_app_settings_visualinspection_storage_class \u00a4 Storage class for Visual Inspection user data persistent volumes. Optional Environment Variable: MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_CLASS Default: Auto-selected from available ReadWriteMany (RWX) storage classes in the cluster Purpose : Specifies which storage class provides persistent volumes for Visual Inspection user data, including uploaded images, trained models, and inspection results. This storage must support concurrent access from multiple pods. When to use : - Set explicitly when you have multiple RWX storage classes and want to control which is used - Set when the auto-selection doesn't choose your preferred storage class - Leave unset to allow automatic selection of an appropriate RWX storage class Valid values : Any valid storage class name in your cluster that supports ReadWriteMany (RWX) access mode Impact : Affects performance and reliability of Visual Inspection data storage. The storage class must support RWX access mode for Visual Inspection to function correctly. Related variables : mas_app_settings_visualinspection_storage_size - Determines the size of the PVC using this storage class. Note : ReadWriteMany (RWX) support is required . Verify your storage class supports RWX with oc get storageclass before deployment. Common RWX storage classes include NFS, CephFS, and cloud provider file storage. mas_app_settings_visualinspection_storage_size \u00a4 Size of the persistent volume for Visual Inspection user data. Optional Environment Variable: MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for Visual Inspection user data, including uploaded images, trained AI models, inspection results, and datasets. Proper sizing prevents storage exhaustion. When to use : - Increase for production environments with large image datasets - Increase for environments with many trained models or long data retention - Use default (100Gi) for development, testing, or small deployments - Consider your data retention policies and expected growth Valid values : Any valid Kubernetes storage size (e.g., 100Gi , 500Gi , 1Ti , 2Ti ) Impact : Larger values consume more cluster storage resources. Insufficient storage will prevent uploading new images or training models. PVC expansion capability depends on the storage class. Related variables : mas_app_settings_visualinspection_storage_class - Must support volume expansion if you plan to increase size later. Note : Plan for growth when setting initial size. Monitor storage usage to avoid running out of space. Check if your storage class supports PVC expansion before deployment. IoT Configuration \u00a4 mas_app_settings_iot_deployment_size \u00a4 Specifies the deployment size profile for MAS IoT, which determines resource allocations and scaling characteristics. Optional Environment Variable: MAS_APP_SETTINGS_IOT_DEPLOYMENT_SIZE Default: small Purpose : Controls the resource allocation profile for IoT components, affecting CPU, memory, and replica counts. Different sizes are optimized for different workload scales and environments. When to use : - Use dev for development and testing environments with minimal resource requirements - Use small for production environments with moderate device counts and data volumes - Use large for production environments with high device counts, data volumes, or throughput requirements Valid values : dev , small , large Impact : - dev : Minimal resources, suitable for development only - small : Moderate resources, suitable for small to medium production deployments - large : Maximum resources, suitable for large-scale production deployments Related variables : Affects overall cluster resource consumption for IoT components. Note : Application Support: IoT 8.6+ . Ensure your cluster has sufficient resources for the selected size. Cannot be easily changed after deployment without reinstallation. mas_app_settings_iot_fpl_pvc_storage_class \u00a4 Storage class for IoT Function Pipeline (FPL) component persistent volumes. Optional Environment Variable: MAS_APP_SETTINGS_IOT_FPL_PVC_STORAGE_CLASS Default: Auto-selected from available ReadWriteOnce (RWO) storage classes in the cluster Purpose : Specifies which storage class provides persistent volumes for IoT FPL component transient state storage. FPL processes IoT data pipelines and requires persistent storage for state management. When to use : - Set explicitly when you have multiple RWO storage classes and want to control which is used - Set when the auto-selection doesn't choose your preferred storage class - Leave unset to allow automatic selection of an appropriate RWO storage class Valid values : Any valid storage class name in your cluster that supports ReadWriteOnce (RWO) access mode Impact : Affects performance and reliability of IoT data pipeline processing. The storage class determines I/O performance for pipeline state operations. Related variables : - mas_app_settings_iot_fpl_router_pvc_size - Size for FPL router PVC - mas_app_settings_iot_fpl_executor_pvc_size - Size for FPL executor PVC Note : Application Support: IoT 8.6+ . ReadWriteOnce (RWO) access mode is required. Verify with oc get storageclass before deployment. mas_app_settings_iot_fpl_router_pvc_size \u00a4 Size of the persistent volume for IoT FPL pipeline router component. Optional Environment Variable: MAS_APP_SETTINGS_IOT_FPL_ROUTER_PVC_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for the IoT Function Pipeline router component's transient state storage. The router manages pipeline execution and requires persistent storage for state management. When to use : - Increase for production environments with high pipeline throughput - Increase for environments with many concurrent pipelines or complex pipeline logic - Use default (100Gi) for development, testing, or moderate workloads - Consider your pipeline complexity and execution frequency Valid values : Any valid Kubernetes storage size (e.g., 50Gi , 100Gi , 200Gi , 500Gi ) Impact : Larger values consume more cluster storage resources. Insufficient storage may cause pipeline execution failures or state management issues. Related variables : - mas_app_settings_iot_fpl_pvc_storage_class - Storage class for this PVC - mas_app_settings_iot_fpl_executor_pvc_size - Size for FPL executor component Note : Application Support: IoT 8.6+ . Monitor storage usage and adjust as needed. Check if your storage class supports PVC expansion. mas_app_settings_iot_fpl_executor_pvc_size \u00a4 Size of the persistent volume for IoT FPL pipeline executor component. Optional Environment Variable: MAS_APP_SETTINGS_IOT_FPL_EXECUTOR_PVC_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for the IoT Function Pipeline executor component's transient state storage. The executor runs pipeline logic and requires persistent storage for intermediate data and state. When to use : - Increase for production environments with high data processing volumes - Increase for pipelines that process large datasets or generate significant intermediate data - Use default (100Gi) for development, testing, or moderate workloads - Consider your pipeline data processing requirements Valid values : Any valid Kubernetes storage size (e.g., 50Gi , 100Gi , 200Gi , 500Gi ) Impact : Larger values consume more cluster storage resources. Insufficient storage may cause pipeline execution failures or data processing issues. Related variables : - mas_app_settings_iot_fpl_pvc_storage_class - Storage class for this PVC - mas_app_settings_iot_fpl_router_pvc_size - Size for FPL router component Note : Application Support: IoT 8.6+ . Monitor storage usage and adjust as needed. Check if your storage class supports PVC expansion. mas_app_settings_iot_mqttbroker_pvc_storage_class \u00a4 Storage class for IoT MQTT broker (MessageSight) persistent volumes. Optional Environment Variable: MAS_APP_SETTINGS_IOT_MQTTBROKER_PVC_STORAGE_CLASS Default: Auto-selected from available ReadWriteOnce (RWO) storage classes in the cluster Purpose : Specifies which storage class provides persistent volumes for the IoT MQTT broker component. The MQTT broker handles device connectivity and message routing, requiring persistent storage for message queues and broker state. When to use : - Set explicitly when you have multiple RWO storage classes and want to control which is used - Set when the auto-selection doesn't choose your preferred storage class - Set when you need specific performance characteristics for MQTT message handling - Leave unset to allow automatic selection of an appropriate RWO storage class Valid values : Any valid storage class name in your cluster that supports ReadWriteOnce (RWO) access mode Impact : Affects performance and reliability of IoT device connectivity and message routing. The storage class determines I/O performance for message queue operations. Related variables : mas_app_settings_iot_mqttbroker_pvc_size - Determines the size of the PVC using this storage class. Note : Application Support: IoT 8.3+ . ReadWriteOnce (RWO) access mode is required. Verify with oc get storageclass before deployment. mas_app_settings_iot_mqttbroker_pvc_size \u00a4 Size of the persistent volume for IoT MQTT broker (MessageSight) component. Optional Environment Variable: MAS_APP_SETTINGS_IOT_MQTTBROKER_PVC_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for the IoT MQTT broker's persistent storage, including message queues, broker state, and retained messages. Proper sizing ensures reliable device connectivity and message handling. When to use : - Increase for production environments with many connected devices - Increase for environments with high message volumes or long message retention - Increase if devices frequently disconnect and require message queuing - Use default (100Gi) for development, testing, or moderate device counts Valid values : Any valid Kubernetes storage size (e.g., 50Gi , 100Gi , 200Gi , 500Gi ) Impact : Larger values consume more cluster storage resources. Insufficient storage may cause message loss, device connection failures, or broker instability. Related variables : mas_app_settings_iot_mqttbroker_pvc_storage_class - Storage class for this PVC. Note : Application Support: IoT 8.3+ . Monitor storage usage to prevent message queue overflow. Check if your storage class supports PVC expansion. mas_pod_templates_dir (IoT) \u00a4 This role will look for configuration files named: ibm-mas-iot-iot.yml , ibm-mas-iot-actions.yml , ibm-mas-iot-auth.yml , ibm-mas-iot-datapower.yml , ibm-mas-iot-devops.yml , ibm-mas-iot-dm.yml , ibm-mas-iot-dsc.yml , ibm-mas-iot-edgeconfig.yml , ibm-mas-iot-fpl.yml , ibm-mas-iot-guardian.yml , ibm-mas-iot-mbgx.yml , ibm-mas-iot-mfgx.yml , ibm-mas-iot-monitor.yml , ibm-mas-iot-orgmgmt.yml , ibm-mas-iot-provision.yml , ibm-mas-iot-registry.yml , ibm-mas-iot-state.yml , ibm-mas-iot-webui.yml . The content of the configuration file should be the yaml block that you wish to be inserted into the IoT CR. ibm-mas-iot-iot.yml will be inserted into the main IoT CR spec -> podTemplates whereas the component ones e.g, ibm-mas-iot-actions.yml will be under spec -> components -> {componentName} -> podTemplates . The ibm-mas-iot operator will then pass this on to the corresponding component CR when available. This is an example of one of the components (actions) - refer to the BestEfforts reference configuration in the MAS CLI . For full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Manage Configuration \u00a4 mas_pod_templates_dir (Manage) \u00a4 This role will look for a configuration file named ibm-mas-manage-manageapp.yml . The content of the configuration file should be the yaml block that you wish to be inserted into the ManageApp CR. ibm-mas-manage-manageapp.yml will be inserted into the ManageApp CR spec -> podTemplates . The ibm-mas-manage operator will then pass this on to the corresponding deployments when available. For full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None mas_appws_upgrade_type \u00a4 Specifies the upgrade strategy for Manage workspace database schema updates. Optional Environment Variable: MAS_APPWS_UPGRADE_TYPE Default: regularUpgrade Purpose : Controls how Manage performs database schema upgrades during application updates, balancing between system downtime and upgrade complexity. Different strategies offer different trade-offs between availability and upgrade duration. When to use : - Use regularUpgrade for standard upgrades with planned downtime windows - Use onlineUpgrade to minimize downtime during upgrades (requires more resources and time) - Consider your maintenance window constraints and availability requirements Valid values : - regularUpgrade - Standard upgrade with full system downtime - onlineUpgrade - Minimized downtime upgrade (requires additional resources) Impact : - regularUpgrade : Shorter upgrade time but requires full system downtime - onlineUpgrade : Longer upgrade time but minimizes system downtime; requires additional database resources during upgrade Related variables : Applies to Manage application upgrades only. Note : For full documentation of upgrade types and requirements, refer to the Manage Upgrade information in the product documentation. Online upgrades require careful planning and additional resources. Monitor Configuration \u00a4 mas_app_settings_monitor_deployment_size \u00a4 Specifies the deployment size profile for MAS Monitor, which determines resource allocations and scaling characteristics. Optional Environment Variable: MAS_APP_SETTINGS_MONITOR_DEPLOYMENT_SIZE Default: dev Purpose : Controls the resource allocation profile for Monitor components, affecting CPU, memory, and replica counts. Different sizes are optimized for different workload scales and environments. When to use : - Use dev for development and testing environments with minimal resource requirements - Use small for production environments with moderate monitoring requirements - Use large for production environments with extensive monitoring, dashboards, or high data volumes Valid values : dev , small , large Impact : - dev : Minimal resources, suitable for development only - small : Moderate resources, suitable for small to medium production deployments - large : Maximum resources, suitable for large-scale production deployments with extensive monitoring Related variables : Affects overall cluster resource consumption for Monitor components. Note : Application Support: Monitor 8.6+ . Ensure your cluster has sufficient resources for the selected size. Cannot be easily changed after deployment without reinstallation. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # Choose which catalog source to use for the MAS install, default to the IBM operator catalog mas_app_catalog_source: \"{{ lookup('env', 'MAS_APP_CATALOG_SOURCE') | default('ibm-operator-catalog', true) }}\" # Which MAS channel to subscribe to mas_app_channel: \"{{ lookup('env', 'MAS_APP_CHANNEL') | default('8.x', true) }}\" # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS configuration - Entitlement mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') | default('cp', true) }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" # Application Configuration - Spec mas_app_spec: bindings: jdbc: system mongo: system kafka: system settings: messagesight: storage: class: block1000p size: 100Gi deployment: size: medium # Application Configuration - Install Plan mas_app_plan: \"{{ lookup('env', 'MAS_APP_PLAN') | default('full', true) }}\" roles: - ibm.mas_devops.suite_app_install License \u00a4 EPL-2.0","title":"suite_app_install"},{"location":"roles/suite_app_install/#suite_app_install","text":"This role is used to install a specified application in Maximo Application Suite.","title":"suite_app_install"},{"location":"roles/suite_app_install/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_install/#general","text":"","title":"General"},{"location":"roles/suite_app_install/#mas_instance_id","text":"Unique identifier for the MAS instance where the application will be installed. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to install the application into. This must match the instance ID used during the MAS core installation to ensure the application is deployed to the correct MAS environment. When to use : - Always required for any MAS application installation - Must match the instance ID from your MAS core installation - Use the same value across all application installations for a given MAS instance Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : The application will be installed into the namespace mas-{mas_instance_id}-{mas_app_id} . An incorrect instance ID will cause the installation to fail or create resources in the wrong namespace. Related variables : Works with mas_app_id to determine the target namespace for the application. Note : This must match the instance ID used when installing MAS core. Cannot be changed after application installation.","title":"mas_instance_id"},{"location":"roles/suite_app_install/#mas_app_id","text":"Specifies which MAS application to install. Required Environment Variable: MAS_APP_ID Default: None Purpose : Identifies the specific MAS application to be installed, determining which operator subscription is created and which application resources are deployed. When to use : - Always required for any MAS application installation - Set to the specific application you want to install - Each application requires a separate role execution Valid values : assist , iot , facilities , manage , monitor , predict , visualinspection , optimizer , arcgis Impact : Determines which application operator is installed and which namespace is created ( mas-{mas_instance_id}-{mas_app_id} ). Different applications have different configuration requirements and dependencies. Related variables : - mas_app_channel : Must be set to a valid channel for the selected application - mas_app_catalog_source : Must contain the operator for the selected application - Application-specific settings variables (e.g., mas_app_settings_iot_* for IoT) Note : Each application has its own set of configuration variables. Refer to the application-specific sections below for additional required variables.","title":"mas_app_id"},{"location":"roles/suite_app_install/#mas_app_catalog_source","text":"Specifies the OpenShift operator catalog source containing the MAS application operator subscription. Optional Environment Variable: MAS_APP_CATALOG_SOURCE Default: ibm-operator-catalog Purpose : Controls which operator catalog is used to locate and install the MAS application operator. This determines where OpenShift looks for the application operator images and metadata during installation. When to use : - Use default ibm-operator-catalog for production installations with official IBM releases - Use ibm-mas-{mas_app_id}-operators for development builds from Artifactory (e.g., ibm-mas-manage-operators ) - Only change if directed to use a custom catalog for specific testing or airgap scenarios Valid values : Any valid CatalogSource name present in the openshift-marketplace namespace Impact : Changing this value affects which operator versions are available for installation. An invalid catalog source will cause the subscription to fail. Development catalogs require additional authentication via artifactory_username and artifactory_token . Related variables : - mas_app_channel : Works together to determine the specific operator version installed - artifactory_username and artifactory_token : Required when using development catalogs Note : For development catalogs, the naming pattern is ibm-mas-{mas_app_id}-operators where {mas_app_id} is the application name (e.g., manage , monitor ).","title":"mas_app_catalog_source"},{"location":"roles/suite_app_install/#mas_app_channel","text":"Specifies the MAS application operator subscription channel, which determines the version stream you'll receive updates from. Required Environment Variable: MAS_APP_CHANNEL Default: None Purpose : Controls which version of the MAS application will be installed and which updates will be automatically applied. The channel corresponds to major.minor version releases and determines the feature set and compatibility level of your application installation. When to use : - Set to the latest stable channel for new production deployments - Use specific older channels when compatibility with MAS core or other applications requires it - Consult the MAS compatibility matrix before selecting a channel - Change channels only during planned upgrade windows as this triggers version updates Valid values : Application-specific channels (e.g., 8.6.x , 8.7.x , 8.8.x for Manage; check the IBM Operator Catalog for currently available channels for your application) Impact : The channel determines which application version is installed and which automatic updates are applied. Changing channels after installation will trigger an upgrade to the latest version in that channel, which may require application reconfiguration and testing. Related variables : - mas_app_catalog_source : Works together to determine available channels - mas_instance_id : Application must be compatible with the MAS core version Note : Each MAS application has its own set of available channels. Ensure the selected channel is compatible with your MAS core version. Review the application upgrade documentation before changing this value.","title":"mas_app_channel"},{"location":"roles/suite_app_install/#custom_labels","text":"Comma-separated list of key=value labels to apply to MAS application resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to application resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,app=manage ) Impact : Labels are applied to application resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect application functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management.","title":"custom_labels"},{"location":"roles/suite_app_install/#pre-release-support","text":"","title":"Pre-Release Support"},{"location":"roles/suite_app_install/#artifactory_username","text":"Username for authenticating to IBM Artifactory to access development builds of MAS applications. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None Purpose : Provides authentication credentials to pull development/pre-release MAS application operator images from IBM's Artifactory registry. Required only when installing development builds for testing or early access. When to use : - Required when mas_app_catalog_source is set to a development catalog (e.g., ibm-mas-manage-operators ) - Not needed for production installations using ibm-operator-catalog - Use your IBM w3Id username for development builds Valid values : Valid IBM Artifactory username (typically your w3Id) Impact : Without valid credentials, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_token : Must be set together with this username - mas_app_catalog_source : Determines if Artifactory credentials are needed - mas_entitlement_username : Used for production installations instead Note : Only required for development/pre-release builds. Production installations use mas_entitlement_key instead.","title":"artifactory_username"},{"location":"roles/suite_app_install/#artifactory_token","text":"API token for authenticating to IBM Artifactory to access development builds of MAS applications. Optional Environment Variable: ARTIFACTORY_TOKEN Default: None Purpose : Provides the API token/password credential to authenticate with IBM's Artifactory registry when pulling development/pre-release MAS application operator images. When to use : - Required when mas_app_catalog_source is set to a development catalog (e.g., ibm-mas-manage-operators ) - Not needed for production installations using ibm-operator-catalog - Use your IBM Artifactory API key for development builds Valid values : Valid IBM Artifactory API token Impact : Without a valid token, development catalog subscriptions will fail to pull operator images. This variable is ignored when using production catalogs. Related variables : - artifactory_username : Must be set together with this token - mas_app_catalog_source : Determines if Artifactory credentials are needed - mas_entitlement_key : Used for production installations instead Note : Only required for development/pre-release builds. Production installations use mas_entitlement_key instead. Keep this token secure and do not commit to source control.","title":"artifactory_token"},{"location":"roles/suite_app_install/#mas_entitlement_username","text":"Username for authenticating to IBM Container Registry to pull MAS application images. Optional Environment Variable: MAS_ENTITLEMENT_USERNAME Default: cp for production installations Purpose : Provides the username credential for authenticating with IBM's entitled container registry when pulling MAS application operator and component images. When to use : - Set to cp for production installations using IBM entitlement keys - Set to your w3Id for development builds from Artifactory - Usually can be left at default for production installations Valid values : - cp - For production installations with IBM entitlement key - Your IBM w3Id - For development builds Impact : Used together with mas_entitlement_key to create image pull secrets for the application namespace. Incorrect username will cause image pull authentication failures. Related variables : - mas_entitlement_key : Must be set together with this username - artifactory_username : Alternative for development builds Note : For production installations, the default value cp is typically correct when used with an IBM entitlement key.","title":"mas_entitlement_username"},{"location":"roles/suite_app_install/#mas_entitlement_key","text":"IBM entitlement key for authenticating access to IBM Container Registry to pull MAS application images. Optional Environment Variable: MAS_ENTITLEMENT_KEY Default: None Purpose : Provides authentication credentials to pull MAS application container images from IBM's entitled registry. This key is tied to your IBM Cloud account and product entitlements, proving you have the right to use MAS application software. When to use : - Required for production installations using official IBM releases - Obtain from IBM Container Library at https://myibm.ibm.com/products-services/containerlibrary - For development builds, use your Artifactory API key instead - Key must be valid and have active MAS application entitlements Valid values : - IBM entitlement key string (typically starts with \"eyJ...\") - Must be a valid, non-expired key with MAS product entitlements - For development: Artifactory API key Impact : Invalid or expired keys will cause image pull failures during application installation. The key is stored in a Kubernetes secret and used to create image pull secrets for all application pods. Related variables : - mas_entitlement_username : Username paired with this key (default: cp ) - artifactory_token : Alternative for development builds Note : Keep this key secure. Do not commit it to source control. Use environment variables or secure secret management.","title":"mas_entitlement_key"},{"location":"roles/suite_app_install/#application-configuration","text":"","title":"Application Configuration"},{"location":"roles/suite_app_install/#mas_app_spec","text":"Custom YAML specification to override default application configuration settings. Optional Environment Variable: None Default: Application-specific defaults in vars/defaultspecs/{{mas_app_id}}.yml Purpose : Allows advanced users to provide a complete custom specification for the application installation, bypassing individual configuration variables. This enables fine-grained control over application settings not exposed through standard variables. When to use : - Use for advanced customization scenarios not covered by standard variables - Use when you need to set application-specific settings not exposed as role variables - Use with caution as it overrides all other configuration variables - Leave unset for standard installations using individual configuration variables Valid values : Valid YAML dictionary matching the application's Custom Resource specification Impact : When set, this completely overrides all other application configuration variables (e.g., mas_app_settings_* , mas_app_bindings_* ). The role will use this specification directly in the application CR. Related variables : Overrides all mas_app_settings_* and mas_app_bindings_* variables when set. Note : Requires deep knowledge of the application's Custom Resource specification. Incorrect specifications can cause installation failures. Use individual configuration variables unless you have specific advanced requirements.","title":"mas_app_spec"},{"location":"roles/suite_app_install/#mas_app_bindings_jdbc","text":"Specifies the scope for JDBC database configuration binding. Optional Environment Variable: MAS_APP_BINDINGS_JDBC Default: system Purpose : Controls whether the application uses system-level JDBC configuration (shared across all applications) or application-specific JDBC configuration. This determines which database configuration the application will use. When to use : - Use system (default) when all applications share the same database configuration - Use application when this application needs its own dedicated database configuration - Most deployments use system for simplified management Valid values : system , application Impact : - system : Application uses the JDBC configuration defined at the MAS instance level (JdbcCfg with scope=system) - application : Application requires its own application-scoped JDBC configuration (JdbcCfg with scope=application) Related variables : Requires corresponding JdbcCfg resource to be configured at the appropriate scope. Note : Changing this after installation may require reconfiguration of database connections. Ensure the appropriate JdbcCfg resource exists before changing this value.","title":"mas_app_bindings_jdbc"},{"location":"roles/suite_app_install/#mas_app_plan","text":"Specifies the licensing plan/tier for the application installation. Optional Environment Variable: MAS_APP_PLAN Default: Application-specific (varies by application) Purpose : Determines which feature set and licensing tier is activated for the application. Different plans may enable or restrict certain features based on your license entitlements. When to use : - Set according to your license entitlements - Consult your IBM license agreement for available plans - Leave as default if you have standard licensing Valid values : Application-specific - Optimizer : full , limited (v8.2+, defaults to full ) - Other applications may have different plan options Impact : The plan determines which features are available in the application. Using a plan not covered by your license may result in compliance issues. Some features may be disabled or unavailable depending on the selected plan. Related variables : Must align with your IBM license entitlements for the application. Note : Ensure the selected plan matches your license agreement. Contact IBM if you're unsure which plan to use.","title":"mas_app_plan"},{"location":"roles/suite_app_install/#mas_pod_templates_dir","text":"Local directory path containing pod template customization files for the application. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Allows customization of pod specifications for application workloads, enabling control over resource limits, node affinity, tolerations, and other Kubernetes pod settings. This is essential for production deployments with specific infrastructure requirements. When to use : - Use to set custom resource limits (CPU, memory) for application pods - Use to configure node affinity or anti-affinity rules - Use to add tolerations for tainted nodes - Use to apply custom security contexts or service accounts - Leave unset for default pod configurations Valid values : Any valid local filesystem path containing application-specific pod template YAML files Impact : Pod template files in this directory will be applied to the application's Custom Resource, affecting how application pods are scheduled and configured. Invalid templates can cause pod scheduling failures. Related variables : - mas_app_id : Determines which pod template files are expected - See application-specific sections below for required file names Note : Each application expects specific file names. Refer to the application-specific mas_pod_templates_dir documentation below for details. For full documentation, see Customizing Pod Templates in the product documentation.","title":"mas_pod_templates_dir"},{"location":"roles/suite_app_install/#visual-inspection-configuration","text":"","title":"Visual Inspection Configuration"},{"location":"roles/suite_app_install/#mas_app_settings_visualinspection_storage_class","text":"Storage class for Visual Inspection user data persistent volumes. Optional Environment Variable: MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_CLASS Default: Auto-selected from available ReadWriteMany (RWX) storage classes in the cluster Purpose : Specifies which storage class provides persistent volumes for Visual Inspection user data, including uploaded images, trained models, and inspection results. This storage must support concurrent access from multiple pods. When to use : - Set explicitly when you have multiple RWX storage classes and want to control which is used - Set when the auto-selection doesn't choose your preferred storage class - Leave unset to allow automatic selection of an appropriate RWX storage class Valid values : Any valid storage class name in your cluster that supports ReadWriteMany (RWX) access mode Impact : Affects performance and reliability of Visual Inspection data storage. The storage class must support RWX access mode for Visual Inspection to function correctly. Related variables : mas_app_settings_visualinspection_storage_size - Determines the size of the PVC using this storage class. Note : ReadWriteMany (RWX) support is required . Verify your storage class supports RWX with oc get storageclass before deployment. Common RWX storage classes include NFS, CephFS, and cloud provider file storage.","title":"mas_app_settings_visualinspection_storage_class"},{"location":"roles/suite_app_install/#mas_app_settings_visualinspection_storage_size","text":"Size of the persistent volume for Visual Inspection user data. Optional Environment Variable: MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for Visual Inspection user data, including uploaded images, trained AI models, inspection results, and datasets. Proper sizing prevents storage exhaustion. When to use : - Increase for production environments with large image datasets - Increase for environments with many trained models or long data retention - Use default (100Gi) for development, testing, or small deployments - Consider your data retention policies and expected growth Valid values : Any valid Kubernetes storage size (e.g., 100Gi , 500Gi , 1Ti , 2Ti ) Impact : Larger values consume more cluster storage resources. Insufficient storage will prevent uploading new images or training models. PVC expansion capability depends on the storage class. Related variables : mas_app_settings_visualinspection_storage_class - Must support volume expansion if you plan to increase size later. Note : Plan for growth when setting initial size. Monitor storage usage to avoid running out of space. Check if your storage class supports PVC expansion before deployment.","title":"mas_app_settings_visualinspection_storage_size"},{"location":"roles/suite_app_install/#iot-configuration","text":"","title":"IoT Configuration"},{"location":"roles/suite_app_install/#mas_app_settings_iot_deployment_size","text":"Specifies the deployment size profile for MAS IoT, which determines resource allocations and scaling characteristics. Optional Environment Variable: MAS_APP_SETTINGS_IOT_DEPLOYMENT_SIZE Default: small Purpose : Controls the resource allocation profile for IoT components, affecting CPU, memory, and replica counts. Different sizes are optimized for different workload scales and environments. When to use : - Use dev for development and testing environments with minimal resource requirements - Use small for production environments with moderate device counts and data volumes - Use large for production environments with high device counts, data volumes, or throughput requirements Valid values : dev , small , large Impact : - dev : Minimal resources, suitable for development only - small : Moderate resources, suitable for small to medium production deployments - large : Maximum resources, suitable for large-scale production deployments Related variables : Affects overall cluster resource consumption for IoT components. Note : Application Support: IoT 8.6+ . Ensure your cluster has sufficient resources for the selected size. Cannot be easily changed after deployment without reinstallation.","title":"mas_app_settings_iot_deployment_size"},{"location":"roles/suite_app_install/#mas_app_settings_iot_fpl_pvc_storage_class","text":"Storage class for IoT Function Pipeline (FPL) component persistent volumes. Optional Environment Variable: MAS_APP_SETTINGS_IOT_FPL_PVC_STORAGE_CLASS Default: Auto-selected from available ReadWriteOnce (RWO) storage classes in the cluster Purpose : Specifies which storage class provides persistent volumes for IoT FPL component transient state storage. FPL processes IoT data pipelines and requires persistent storage for state management. When to use : - Set explicitly when you have multiple RWO storage classes and want to control which is used - Set when the auto-selection doesn't choose your preferred storage class - Leave unset to allow automatic selection of an appropriate RWO storage class Valid values : Any valid storage class name in your cluster that supports ReadWriteOnce (RWO) access mode Impact : Affects performance and reliability of IoT data pipeline processing. The storage class determines I/O performance for pipeline state operations. Related variables : - mas_app_settings_iot_fpl_router_pvc_size - Size for FPL router PVC - mas_app_settings_iot_fpl_executor_pvc_size - Size for FPL executor PVC Note : Application Support: IoT 8.6+ . ReadWriteOnce (RWO) access mode is required. Verify with oc get storageclass before deployment.","title":"mas_app_settings_iot_fpl_pvc_storage_class"},{"location":"roles/suite_app_install/#mas_app_settings_iot_fpl_router_pvc_size","text":"Size of the persistent volume for IoT FPL pipeline router component. Optional Environment Variable: MAS_APP_SETTINGS_IOT_FPL_ROUTER_PVC_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for the IoT Function Pipeline router component's transient state storage. The router manages pipeline execution and requires persistent storage for state management. When to use : - Increase for production environments with high pipeline throughput - Increase for environments with many concurrent pipelines or complex pipeline logic - Use default (100Gi) for development, testing, or moderate workloads - Consider your pipeline complexity and execution frequency Valid values : Any valid Kubernetes storage size (e.g., 50Gi , 100Gi , 200Gi , 500Gi ) Impact : Larger values consume more cluster storage resources. Insufficient storage may cause pipeline execution failures or state management issues. Related variables : - mas_app_settings_iot_fpl_pvc_storage_class - Storage class for this PVC - mas_app_settings_iot_fpl_executor_pvc_size - Size for FPL executor component Note : Application Support: IoT 8.6+ . Monitor storage usage and adjust as needed. Check if your storage class supports PVC expansion.","title":"mas_app_settings_iot_fpl_router_pvc_size"},{"location":"roles/suite_app_install/#mas_app_settings_iot_fpl_executor_pvc_size","text":"Size of the persistent volume for IoT FPL pipeline executor component. Optional Environment Variable: MAS_APP_SETTINGS_IOT_FPL_EXECUTOR_PVC_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for the IoT Function Pipeline executor component's transient state storage. The executor runs pipeline logic and requires persistent storage for intermediate data and state. When to use : - Increase for production environments with high data processing volumes - Increase for pipelines that process large datasets or generate significant intermediate data - Use default (100Gi) for development, testing, or moderate workloads - Consider your pipeline data processing requirements Valid values : Any valid Kubernetes storage size (e.g., 50Gi , 100Gi , 200Gi , 500Gi ) Impact : Larger values consume more cluster storage resources. Insufficient storage may cause pipeline execution failures or data processing issues. Related variables : - mas_app_settings_iot_fpl_pvc_storage_class - Storage class for this PVC - mas_app_settings_iot_fpl_router_pvc_size - Size for FPL router component Note : Application Support: IoT 8.6+ . Monitor storage usage and adjust as needed. Check if your storage class supports PVC expansion.","title":"mas_app_settings_iot_fpl_executor_pvc_size"},{"location":"roles/suite_app_install/#mas_app_settings_iot_mqttbroker_pvc_storage_class","text":"Storage class for IoT MQTT broker (MessageSight) persistent volumes. Optional Environment Variable: MAS_APP_SETTINGS_IOT_MQTTBROKER_PVC_STORAGE_CLASS Default: Auto-selected from available ReadWriteOnce (RWO) storage classes in the cluster Purpose : Specifies which storage class provides persistent volumes for the IoT MQTT broker component. The MQTT broker handles device connectivity and message routing, requiring persistent storage for message queues and broker state. When to use : - Set explicitly when you have multiple RWO storage classes and want to control which is used - Set when the auto-selection doesn't choose your preferred storage class - Set when you need specific performance characteristics for MQTT message handling - Leave unset to allow automatic selection of an appropriate RWO storage class Valid values : Any valid storage class name in your cluster that supports ReadWriteOnce (RWO) access mode Impact : Affects performance and reliability of IoT device connectivity and message routing. The storage class determines I/O performance for message queue operations. Related variables : mas_app_settings_iot_mqttbroker_pvc_size - Determines the size of the PVC using this storage class. Note : Application Support: IoT 8.3+ . ReadWriteOnce (RWO) access mode is required. Verify with oc get storageclass before deployment.","title":"mas_app_settings_iot_mqttbroker_pvc_storage_class"},{"location":"roles/suite_app_install/#mas_app_settings_iot_mqttbroker_pvc_size","text":"Size of the persistent volume for IoT MQTT broker (MessageSight) component. Optional Environment Variable: MAS_APP_SETTINGS_IOT_MQTTBROKER_PVC_SIZE Default: 100Gi Purpose : Determines the amount of disk space allocated for the IoT MQTT broker's persistent storage, including message queues, broker state, and retained messages. Proper sizing ensures reliable device connectivity and message handling. When to use : - Increase for production environments with many connected devices - Increase for environments with high message volumes or long message retention - Increase if devices frequently disconnect and require message queuing - Use default (100Gi) for development, testing, or moderate device counts Valid values : Any valid Kubernetes storage size (e.g., 50Gi , 100Gi , 200Gi , 500Gi ) Impact : Larger values consume more cluster storage resources. Insufficient storage may cause message loss, device connection failures, or broker instability. Related variables : mas_app_settings_iot_mqttbroker_pvc_storage_class - Storage class for this PVC. Note : Application Support: IoT 8.3+ . Monitor storage usage to prevent message queue overflow. Check if your storage class supports PVC expansion.","title":"mas_app_settings_iot_mqttbroker_pvc_size"},{"location":"roles/suite_app_install/#mas_pod_templates_dir-iot","text":"This role will look for configuration files named: ibm-mas-iot-iot.yml , ibm-mas-iot-actions.yml , ibm-mas-iot-auth.yml , ibm-mas-iot-datapower.yml , ibm-mas-iot-devops.yml , ibm-mas-iot-dm.yml , ibm-mas-iot-dsc.yml , ibm-mas-iot-edgeconfig.yml , ibm-mas-iot-fpl.yml , ibm-mas-iot-guardian.yml , ibm-mas-iot-mbgx.yml , ibm-mas-iot-mfgx.yml , ibm-mas-iot-monitor.yml , ibm-mas-iot-orgmgmt.yml , ibm-mas-iot-provision.yml , ibm-mas-iot-registry.yml , ibm-mas-iot-state.yml , ibm-mas-iot-webui.yml . The content of the configuration file should be the yaml block that you wish to be inserted into the IoT CR. ibm-mas-iot-iot.yml will be inserted into the main IoT CR spec -> podTemplates whereas the component ones e.g, ibm-mas-iot-actions.yml will be under spec -> components -> {componentName} -> podTemplates . The ibm-mas-iot operator will then pass this on to the corresponding component CR when available. This is an example of one of the components (actions) - refer to the BestEfforts reference configuration in the MAS CLI . For full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None","title":"mas_pod_templates_dir (IoT)"},{"location":"roles/suite_app_install/#manage-configuration","text":"","title":"Manage Configuration"},{"location":"roles/suite_app_install/#mas_pod_templates_dir-manage","text":"This role will look for a configuration file named ibm-mas-manage-manageapp.yml . The content of the configuration file should be the yaml block that you wish to be inserted into the ManageApp CR. ibm-mas-manage-manageapp.yml will be inserted into the ManageApp CR spec -> podTemplates . The ibm-mas-manage operator will then pass this on to the corresponding deployments when available. For full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None","title":"mas_pod_templates_dir (Manage)"},{"location":"roles/suite_app_install/#mas_appws_upgrade_type","text":"Specifies the upgrade strategy for Manage workspace database schema updates. Optional Environment Variable: MAS_APPWS_UPGRADE_TYPE Default: regularUpgrade Purpose : Controls how Manage performs database schema upgrades during application updates, balancing between system downtime and upgrade complexity. Different strategies offer different trade-offs between availability and upgrade duration. When to use : - Use regularUpgrade for standard upgrades with planned downtime windows - Use onlineUpgrade to minimize downtime during upgrades (requires more resources and time) - Consider your maintenance window constraints and availability requirements Valid values : - regularUpgrade - Standard upgrade with full system downtime - onlineUpgrade - Minimized downtime upgrade (requires additional resources) Impact : - regularUpgrade : Shorter upgrade time but requires full system downtime - onlineUpgrade : Longer upgrade time but minimizes system downtime; requires additional database resources during upgrade Related variables : Applies to Manage application upgrades only. Note : For full documentation of upgrade types and requirements, refer to the Manage Upgrade information in the product documentation. Online upgrades require careful planning and additional resources.","title":"mas_appws_upgrade_type"},{"location":"roles/suite_app_install/#monitor-configuration","text":"","title":"Monitor Configuration"},{"location":"roles/suite_app_install/#mas_app_settings_monitor_deployment_size","text":"Specifies the deployment size profile for MAS Monitor, which determines resource allocations and scaling characteristics. Optional Environment Variable: MAS_APP_SETTINGS_MONITOR_DEPLOYMENT_SIZE Default: dev Purpose : Controls the resource allocation profile for Monitor components, affecting CPU, memory, and replica counts. Different sizes are optimized for different workload scales and environments. When to use : - Use dev for development and testing environments with minimal resource requirements - Use small for production environments with moderate monitoring requirements - Use large for production environments with extensive monitoring, dashboards, or high data volumes Valid values : dev , small , large Impact : - dev : Minimal resources, suitable for development only - small : Moderate resources, suitable for small to medium production deployments - large : Maximum resources, suitable for large-scale production deployments with extensive monitoring Related variables : Affects overall cluster resource consumption for Monitor components. Note : Application Support: Monitor 8.6+ . Ensure your cluster has sufficient resources for the selected size. Cannot be easily changed after deployment without reinstallation.","title":"mas_app_settings_monitor_deployment_size"},{"location":"roles/suite_app_install/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Choose which catalog source to use for the MAS install, default to the IBM operator catalog mas_app_catalog_source: \"{{ lookup('env', 'MAS_APP_CATALOG_SOURCE') | default('ibm-operator-catalog', true) }}\" # Which MAS channel to subscribe to mas_app_channel: \"{{ lookup('env', 'MAS_APP_CHANNEL') | default('8.x', true) }}\" # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS configuration - Entitlement mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') | default('cp', true) }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" # Application Configuration - Spec mas_app_spec: bindings: jdbc: system mongo: system kafka: system settings: messagesight: storage: class: block1000p size: 100Gi deployment: size: medium # Application Configuration - Install Plan mas_app_plan: \"{{ lookup('env', 'MAS_APP_PLAN') | default('full', true) }}\" roles: - ibm.mas_devops.suite_app_install","title":"Example Playbook"},{"location":"roles/suite_app_install/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_rollback/","text":"suite_app_rollback \u00a4 This role is to roll back Maximo Application Suite Applications to an earlier version. Currently, this is designed for Manage Application only. Rollback is possible only in 8.7 and later. From 8.7 onwards, every version comes with a set of supported versions to which the Application can be rolled back. For example, you can roll back Manage Application from 8.7.x to 8.7.0. This role will rollback the version for an installed MAS application after validating: - That the specified version of application is compatible to rollback from the current version - That the specified version of application is compatible with the running MAS core platform It will rollback the Manage Application to the desired version and validate that the Manage Application has been successfully reconciled at the rolled back version. Role Variables \u00a4 mas_instance_id \u00a4 Set the instance ID for the MAS installation where you wish to rollback the application. Required Environment Variable: MAS_INSTANCE_ID Default: None mas_app_id \u00a4 The name of the Maximo Application Suite Application. This will be used to lookup for application namespace and resources. Please be aware that at present, manage is the only supported value for this variable. Required Environment Variable: MAS_APP_ID Default: None mas_app_version \u00a4 The version you wish to rollback to. Built-in validation will ensure that the rollback will only proceed if a supportable rollback path is chosen. It is required when any of the rollback_mas_app and verify_app_version variables is set to true . Required Environment Variable: MAS_APP_VERSION Default: None rollback_mas_app \u00a4 When set to true will ensure that the role performs rollback operation. Optional Environment Variable: ROLLBACK_MAS_APP Default: True verify_app_version \u00a4 When set to true will ensure that the role checks the current Manage Application version matches with specified version. Optional Environment Variable: VERIFY_APP_VERSION Default: False Example Playbook \u00a4 Automatic Target Selection \u00a4 Running this playbook will rollback Manage Application to the 8.7.1 version. If you run this playbook when you are already on the same version it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: manage mas_app_version: 8.7.1 roles: - ibm.mas_devops.suite_app_rollback Verify Manage App Version \u00a4 Running this playbook will attempt to verify the current version of Manage Application matches with the specified version. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: manage mas_app_version: 8.7.1 rollback_mas_app: False verify_app_version: True roles: - ibm.mas_devops.suite_app_rollback License \u00a4 EPL-2.0","title":"suite_app_rollback"},{"location":"roles/suite_app_rollback/#suite_app_rollback","text":"This role is to roll back Maximo Application Suite Applications to an earlier version. Currently, this is designed for Manage Application only. Rollback is possible only in 8.7 and later. From 8.7 onwards, every version comes with a set of supported versions to which the Application can be rolled back. For example, you can roll back Manage Application from 8.7.x to 8.7.0. This role will rollback the version for an installed MAS application after validating: - That the specified version of application is compatible to rollback from the current version - That the specified version of application is compatible with the running MAS core platform It will rollback the Manage Application to the desired version and validate that the Manage Application has been successfully reconciled at the rolled back version.","title":"suite_app_rollback"},{"location":"roles/suite_app_rollback/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_rollback/#mas_instance_id","text":"Set the instance ID for the MAS installation where you wish to rollback the application. Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_app_rollback/#mas_app_id","text":"The name of the Maximo Application Suite Application. This will be used to lookup for application namespace and resources. Please be aware that at present, manage is the only supported value for this variable. Required Environment Variable: MAS_APP_ID Default: None","title":"mas_app_id"},{"location":"roles/suite_app_rollback/#mas_app_version","text":"The version you wish to rollback to. Built-in validation will ensure that the rollback will only proceed if a supportable rollback path is chosen. It is required when any of the rollback_mas_app and verify_app_version variables is set to true . Required Environment Variable: MAS_APP_VERSION Default: None","title":"mas_app_version"},{"location":"roles/suite_app_rollback/#rollback_mas_app","text":"When set to true will ensure that the role performs rollback operation. Optional Environment Variable: ROLLBACK_MAS_APP Default: True","title":"rollback_mas_app"},{"location":"roles/suite_app_rollback/#verify_app_version","text":"When set to true will ensure that the role checks the current Manage Application version matches with specified version. Optional Environment Variable: VERIFY_APP_VERSION Default: False","title":"verify_app_version"},{"location":"roles/suite_app_rollback/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_app_rollback/#automatic-target-selection","text":"Running this playbook will rollback Manage Application to the 8.7.1 version. If you run this playbook when you are already on the same version it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: manage mas_app_version: 8.7.1 roles: - ibm.mas_devops.suite_app_rollback","title":"Automatic Target Selection"},{"location":"roles/suite_app_rollback/#verify-manage-app-version","text":"Running this playbook will attempt to verify the current version of Manage Application matches with the specified version. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: manage mas_app_version: 8.7.1 rollback_mas_app: False verify_app_version: True roles: - ibm.mas_devops.suite_app_rollback","title":"Verify Manage App Version"},{"location":"roles/suite_app_rollback/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_uninstall/","text":"suite_app_uninstall \u00a4 This role is used to uninstall a specified application in Maximo Application Suite. Role Variables \u00a4 mas_instance_id \u00a4 Defines the MAS instance id from which an application will be uninstalled. Required Environment Variable: MAS_INSTANCE_ID Default: None mas_app_id \u00a4 Defines the kind of application that will be uninstalled such as assist , iot , manage , monitor , optimizer , predict , visualinspection or facilities . Required Environment Variable: MAS_APP_ID Default: None Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" roles: - ibm.mas_devops.suite_app_uninstall License \u00a4 EPL-2.0","title":"suite_app_uninstall"},{"location":"roles/suite_app_uninstall/#suite_app_uninstall","text":"This role is used to uninstall a specified application in Maximo Application Suite.","title":"suite_app_uninstall"},{"location":"roles/suite_app_uninstall/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_uninstall/#mas_instance_id","text":"Defines the MAS instance id from which an application will be uninstalled. Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_app_uninstall/#mas_app_id","text":"Defines the kind of application that will be uninstalled such as assist , iot , manage , monitor , optimizer , predict , visualinspection or facilities . Required Environment Variable: MAS_APP_ID Default: None","title":"mas_app_id"},{"location":"roles/suite_app_uninstall/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" roles: - ibm.mas_devops.suite_app_uninstall","title":"Example Playbook"},{"location":"roles/suite_app_uninstall/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_upgrade/","text":"suite_app_upgrade \u00a4 This role will upgrade the subscription channel for an installed MAS application after validating: That the application is installed and in a healthy state That the new version of the application can be upgraded to from the existing version That the new version of the application is compatible with the running MAS core platform Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier for application upgrade. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the application to upgrade. Used to locate and validate the application installation. When to use : - Always required for application upgrades - Must match the instance ID from MAS installation - Used to validate application health before upgrade Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance's application will be upgraded. Incorrect instance ID will cause upgrade to fail. Related variables : - mas_app_id : Application to upgrade in this instance - mas_app_channel : Target upgrade channel Note : The role validates that the application is installed and healthy in this instance before proceeding with the upgrade. mas_app_id \u00a4 MAS application identifier to upgrade. Required Environment Variable: MAS_APP_ID Default: None Purpose : Specifies which MAS application to upgrade. The role validates the application is installed and healthy before upgrading. When to use : - Always required for application upgrades - Must match an installed application in the instance - Application must be in healthy state for upgrade Valid values : Valid MAS application ID (e.g., iot , manage , monitor , predict , health , assist , visualinspection , optimizer ) Impact : Determines which application's subscription channel will be upgraded. The role validates compatibility and upgrade path before proceeding. Related variables : - mas_instance_id : Instance containing this application - mas_app_channel : Target upgrade channel for this application Note : The role performs comprehensive validation including application health, upgrade path compatibility, and MAS core platform compatibility before upgrading. mas_app_channel \u00a4 Target subscription channel for application upgrade. Required Environment Variable: MAS_APP_CHANNEL Default: None Purpose : Specifies the target subscription channel to upgrade the application to. The role validates that a supported upgrade path exists from the current version. When to use : - Always required for application upgrades - Must be a valid channel for the application - Should represent a newer version than currently installed Valid values : Valid subscription channel for the application (e.g., 8.5.x , 8.6.x , 8.7.x for IoT; 8.4.x , 8.5.x , 8.6.x for Manage) Impact : Determines the target version for the application upgrade. The role validates: - Upgrade path compatibility (can upgrade from current to target) - MAS core platform compatibility (target version works with current MAS) - Application health before proceeding Related variables : - mas_app_id : Application being upgraded - skip_compatibility_check : Whether to skip validation (not recommended) Note : Built-in validation ensures safe upgrades. The role will fail if the upgrade path is not supported or if the target version is incompatible with the current MAS core platform. mas_upgrade_dryrun \u00a4 Dry-run mode for upgrade validation only. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: false Purpose : Enables dry-run mode where the role performs all validation checks without making any changes to the installation. Useful for testing upgrade paths. When to use : - Set to true to validate upgrade without executing it - Use for testing and planning upgrade paths - Recommended before production upgrades - Leave as false (default) to perform actual upgrade Valid values : true , false Impact : - true : Performs validation only (health check, compatibility check, upgrade path validation) without modifying the installation - false : Performs validation and executes the upgrade if validation passes Related variables : - skip_compatibility_check : Controls whether compatibility validation is performed Note : Dry-run mode is highly recommended before production upgrades to identify potential issues. All validation checks are performed, but no changes are made to the subscription channel or application. skip_compatibility_check \u00a4 Skip compatibility validation before upgrade. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: false Purpose : Controls whether compatibility validation is performed before upgrade. Validation checks if the target channel is compatible with current MAS and application versions. When to use : - Leave as false (default) for safe upgrades with validation - Set to true only in exceptional cases (not recommended) - Use only when you have verified compatibility manually Valid values : true , false Impact : - false (default): Performs compatibility validation before upgrade (recommended) - true : Skips compatibility validation, allowing potentially incompatible upgrades Related variables : - mas_upgrade_dryrun : Controls whether upgrade is executed or only validated - mas_app_channel : Target channel being validated Note : WARNING - Skipping compatibility checks can lead to failed upgrades or unstable installations. Only skip validation if you have manually verified the upgrade path is supported. The default validation protects against incompatible upgrades. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: iot mas_app_channel: 8.5.x roles: - ibm.mas_devops.suite_app_upgrade License \u00a4 EPL-2.0","title":"suite_app_upgrade"},{"location":"roles/suite_app_upgrade/#suite_app_upgrade","text":"This role will upgrade the subscription channel for an installed MAS application after validating: That the application is installed and in a healthy state That the new version of the application can be upgraded to from the existing version That the new version of the application is compatible with the running MAS core platform","title":"suite_app_upgrade"},{"location":"roles/suite_app_upgrade/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_upgrade/#mas_instance_id","text":"MAS instance identifier for application upgrade. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the application to upgrade. Used to locate and validate the application installation. When to use : - Always required for application upgrades - Must match the instance ID from MAS installation - Used to validate application health before upgrade Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance's application will be upgraded. Incorrect instance ID will cause upgrade to fail. Related variables : - mas_app_id : Application to upgrade in this instance - mas_app_channel : Target upgrade channel Note : The role validates that the application is installed and healthy in this instance before proceeding with the upgrade.","title":"mas_instance_id"},{"location":"roles/suite_app_upgrade/#mas_app_id","text":"MAS application identifier to upgrade. Required Environment Variable: MAS_APP_ID Default: None Purpose : Specifies which MAS application to upgrade. The role validates the application is installed and healthy before upgrading. When to use : - Always required for application upgrades - Must match an installed application in the instance - Application must be in healthy state for upgrade Valid values : Valid MAS application ID (e.g., iot , manage , monitor , predict , health , assist , visualinspection , optimizer ) Impact : Determines which application's subscription channel will be upgraded. The role validates compatibility and upgrade path before proceeding. Related variables : - mas_instance_id : Instance containing this application - mas_app_channel : Target upgrade channel for this application Note : The role performs comprehensive validation including application health, upgrade path compatibility, and MAS core platform compatibility before upgrading.","title":"mas_app_id"},{"location":"roles/suite_app_upgrade/#mas_app_channel","text":"Target subscription channel for application upgrade. Required Environment Variable: MAS_APP_CHANNEL Default: None Purpose : Specifies the target subscription channel to upgrade the application to. The role validates that a supported upgrade path exists from the current version. When to use : - Always required for application upgrades - Must be a valid channel for the application - Should represent a newer version than currently installed Valid values : Valid subscription channel for the application (e.g., 8.5.x , 8.6.x , 8.7.x for IoT; 8.4.x , 8.5.x , 8.6.x for Manage) Impact : Determines the target version for the application upgrade. The role validates: - Upgrade path compatibility (can upgrade from current to target) - MAS core platform compatibility (target version works with current MAS) - Application health before proceeding Related variables : - mas_app_id : Application being upgraded - skip_compatibility_check : Whether to skip validation (not recommended) Note : Built-in validation ensures safe upgrades. The role will fail if the upgrade path is not supported or if the target version is incompatible with the current MAS core platform.","title":"mas_app_channel"},{"location":"roles/suite_app_upgrade/#mas_upgrade_dryrun","text":"Dry-run mode for upgrade validation only. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: false Purpose : Enables dry-run mode where the role performs all validation checks without making any changes to the installation. Useful for testing upgrade paths. When to use : - Set to true to validate upgrade without executing it - Use for testing and planning upgrade paths - Recommended before production upgrades - Leave as false (default) to perform actual upgrade Valid values : true , false Impact : - true : Performs validation only (health check, compatibility check, upgrade path validation) without modifying the installation - false : Performs validation and executes the upgrade if validation passes Related variables : - skip_compatibility_check : Controls whether compatibility validation is performed Note : Dry-run mode is highly recommended before production upgrades to identify potential issues. All validation checks are performed, but no changes are made to the subscription channel or application.","title":"mas_upgrade_dryrun"},{"location":"roles/suite_app_upgrade/#skip_compatibility_check","text":"Skip compatibility validation before upgrade. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: false Purpose : Controls whether compatibility validation is performed before upgrade. Validation checks if the target channel is compatible with current MAS and application versions. When to use : - Leave as false (default) for safe upgrades with validation - Set to true only in exceptional cases (not recommended) - Use only when you have verified compatibility manually Valid values : true , false Impact : - false (default): Performs compatibility validation before upgrade (recommended) - true : Skips compatibility validation, allowing potentially incompatible upgrades Related variables : - mas_upgrade_dryrun : Controls whether upgrade is executed or only validated - mas_app_channel : Target channel being validated Note : WARNING - Skipping compatibility checks can lead to failed upgrades or unstable installations. Only skip validation if you have manually verified the upgrade path is supported. The default validation protects against incompatible upgrades.","title":"skip_compatibility_check"},{"location":"roles/suite_app_upgrade/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: iot mas_app_channel: 8.5.x roles: - ibm.mas_devops.suite_app_upgrade","title":"Example Playbook"},{"location":"roles/suite_app_upgrade/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_backup_restore/","text":"suite_backup_restore \u00a4 This role supports backing up and restoring MAS Core namespace resources; supports creating on-demand or scheduled backup jobs for taking full or incremental backups, and optionally creating Kubernetes jobs for running the backup/restore process. Important A backup can only be restored to an instance with the same MAS instance ID. Role Variables \u00a4 General \u00a4 masbr_action \u00a4 Action to perform on MAS Core namespace. Required Environment Variable: MASBR_ACTION Default: None Purpose : Specifies whether to create a backup of MAS Core namespace resources or restore from a previous backup. When to use : - Set to backup to create a backup of MAS Core namespace resources - Set to restore to restore MAS Core namespace from a backup - Always required to indicate the operation type Valid values : backup , restore Impact : - backup : Creates backup job (on-demand or scheduled) for MAS Core namespace resources - restore : Restores MAS Core namespace from specified backup version Related variables : - masbr_restore_from_version : Required when action is restore - masbr_backup_schedule : Optional for scheduled backups - mas_instance_id : Instance to backup/restore Note : IMPORTANT - This role handles MAS Core namespace resources only. MongoDB data must be backed up/restored separately using the mongodb role. A backup can only be restored to an instance with the same MAS instance ID. mas_instance_id \u00a4 MAS instance identifier for backup/restore operations. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to backup or restore. Used to locate MAS Core namespace resources and ensure restore compatibility. When to use : - Always required for backup and restore operations - Must match the instance ID from MAS installation - Critical for restore operations (must match original backup instance ID) Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , main ) Impact : Determines which MAS instance's Core namespace will be backed up or restored. CRITICAL - A backup can only be restored to an instance with the same MAS instance ID. Related variables : - masbr_action : Whether backing up or restoring this instance - masbr_restore_from_version : Backup version to restore (for restore action) Note : IMPORTANT - The instance ID must match between backup and restore operations. Attempting to restore a backup to an instance with a different ID will fail. masbr_confirm_cluster \u00a4 Confirm cluster connection before backup/restore. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false Purpose : Controls whether the role prompts for confirmation of the currently connected cluster before executing backup or restore operations. Safety feature to prevent accidental operations on wrong cluster. When to use : - Set to true for interactive confirmation (recommended for production) - Leave as false (default) for automated/non-interactive operations - Use true when manually running backup/restore to verify correct cluster Valid values : true , false Impact : - true : Role prompts for cluster confirmation before proceeding - false : Role proceeds without confirmation (suitable for automation) Related variables : - masbr_action : Operation requiring cluster confirmation Note : Enabling cluster confirmation is recommended for manual operations, especially in production environments, to prevent accidental backup/restore on the wrong cluster. masbr_copy_timeout_sec \u00a4 File transfer timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) Purpose : Specifies the maximum time allowed for transferring backup files between cluster and local storage. Prevents operations from hanging indefinitely. When to use : - Use default (12 hours) for most deployments - Increase for very large backups or slow network connections - Decrease for smaller backups to fail faster on issues Valid values : Positive integer (seconds), e.g., 3600 (1 hour), 43200 (12 hours), 86400 (24 hours) Impact : Operations exceeding this timeout will fail. Insufficient timeout for large backups will cause failures. Excessive timeout delays error detection. Related variables : - masbr_storage_local_folder : Destination for file transfers Note : The default 12 hours is suitable for most deployments. Adjust based on backup size and network speed. Monitor actual transfer times to optimize this setting. masbr_job_timezone \u00a4 Time zone for scheduled backup jobs. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: UTC Purpose : Specifies the time zone for scheduled backup CronJobs. Ensures backups run at the intended local time rather than UTC. When to use : - Leave unset to use UTC (default) - Set when you need backups to run at specific local times - Only applies to scheduled backups (when masbr_backup_schedule is set) Valid values : Valid tz database time zone (e.g., America/New_York , Europe/London , Asia/Tokyo ) Impact : Determines when scheduled backups execute. Incorrect time zone may cause backups to run at unexpected times. Related variables : - masbr_backup_schedule : Cron expression interpreted in this time zone Note : Only relevant for scheduled backups. On-demand backups ignore this setting. Use standard tz database names (e.g., America/New_York , not EST ). masbr_storage_local_folder \u00a4 Local filesystem path for backup storage. Required Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None Purpose : Specifies the local filesystem path where backup files are stored (for backups) or retrieved from (for restores). This is the persistent storage location for backup data. When to use : - Always required for backup and restore operations - Must be accessible from the system running the role - Should have sufficient space for backup files - Must be persistent across operations for restore capability Valid values : Absolute filesystem path (e.g., /tmp/masbr , /backup/mas , /mnt/backup ) Impact : Backup files are written to or read from this location. Insufficient space will cause backup failures. Path must exist and be writable. Related variables : - masbr_copy_timeout_sec : Timeout for transferring files to/from this location - masbr_restore_from_version : Backup version stored in this location Note : Ensure the path has sufficient disk space for backups. For production, use a dedicated backup volume with appropriate retention policies. The path must be accessible during both backup and restore operations. Backup \u00a4 masbr_backup_schedule \u00a4 Cron expression for scheduled backups. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None (on-demand backup) Purpose : Defines a schedule for automatic recurring backups using Cron syntax. When set, creates a Kubernetes CronJob for automated backups. When to use : - Leave unset for on-demand backups (manual execution) - Set to create scheduled/recurring backups - Use for automated backup strategies Valid values : Valid Cron expression (e.g., 0 2 * * * for daily at 2 AM, 0 2 * * 0 for weekly on Sunday at 2 AM) Impact : - When set: Creates a Kubernetes CronJob that runs backups automatically on schedule - When unset: Creates an on-demand backup job that runs immediately Related variables : - masbr_job_timezone : Time zone for interpreting the cron schedule - masbr_action : Must be backup for scheduled backups Note : Scheduled backups only apply when masbr_action=backup . The cron expression is interpreted in the time zone specified by masbr_job_timezone (defaults to UTC). Common patterns: 0 2 * * * (daily 2 AM), 0 2 * * 0 (weekly Sunday 2 AM), 0 2 1 * * (monthly 1st at 2 AM). Restore \u00a4 masbr_restore_from_version \u00a4 Backup version timestamp for restore operations. Required (when masbr_action=restore ) Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Purpose : Specifies which backup version to restore from. The version is a timestamp identifying a specific backup. When to use : - Required when masbr_action=restore - Not used for backup operations - Must match an existing backup version in storage Valid values : Timestamp in YYYYMMDDHHMMSS format (e.g., 20240621021316 for June 21, 2024 at 02:13:16) Impact : Determines which backup is restored. Incorrect or non-existent version will cause restore to fail. Related variables : - masbr_action : Must be restore for this variable to be used - masbr_storage_local_folder : Location where backup versions are stored - mas_instance_id : Must match the instance ID from the backup Note : The backup version timestamp is generated automatically during backup creation. List available backups in masbr_storage_local_folder to find valid version timestamps. IMPORTANT - The backup can only be restored to an instance with the same MAS instance ID as the original backup. Example Playbook \u00a4 Backup \u00a4 Backup MAS Core namespace resources, note that this does not include backup of any data in MongoDb, see the backup action in the mongodb role. - hosts: localhost any_errors_fatal: true vars: masbr_action: backup mas_instance_id: main masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_backup_restore Restore \u00a4 Restore MAS Core namespace resources, note that this does not include backup of any data in MongoDb, see the restore action in the mongodb role. - hosts: localhost any_errors_fatal: true vars: masbr_action: restore masbr_restore_from_version: 20240621021316 mas_instance_id: main masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_backup_restore License \u00a4 EPL-2.0","title":"suite_backup_restore"},{"location":"roles/suite_backup_restore/#suite_backup_restore","text":"This role supports backing up and restoring MAS Core namespace resources; supports creating on-demand or scheduled backup jobs for taking full or incremental backups, and optionally creating Kubernetes jobs for running the backup/restore process. Important A backup can only be restored to an instance with the same MAS instance ID.","title":"suite_backup_restore"},{"location":"roles/suite_backup_restore/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_backup_restore/#general","text":"","title":"General"},{"location":"roles/suite_backup_restore/#masbr_action","text":"Action to perform on MAS Core namespace. Required Environment Variable: MASBR_ACTION Default: None Purpose : Specifies whether to create a backup of MAS Core namespace resources or restore from a previous backup. When to use : - Set to backup to create a backup of MAS Core namespace resources - Set to restore to restore MAS Core namespace from a backup - Always required to indicate the operation type Valid values : backup , restore Impact : - backup : Creates backup job (on-demand or scheduled) for MAS Core namespace resources - restore : Restores MAS Core namespace from specified backup version Related variables : - masbr_restore_from_version : Required when action is restore - masbr_backup_schedule : Optional for scheduled backups - mas_instance_id : Instance to backup/restore Note : IMPORTANT - This role handles MAS Core namespace resources only. MongoDB data must be backed up/restored separately using the mongodb role. A backup can only be restored to an instance with the same MAS instance ID.","title":"masbr_action"},{"location":"roles/suite_backup_restore/#mas_instance_id","text":"MAS instance identifier for backup/restore operations. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to backup or restore. Used to locate MAS Core namespace resources and ensure restore compatibility. When to use : - Always required for backup and restore operations - Must match the instance ID from MAS installation - Critical for restore operations (must match original backup instance ID) Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , main ) Impact : Determines which MAS instance's Core namespace will be backed up or restored. CRITICAL - A backup can only be restored to an instance with the same MAS instance ID. Related variables : - masbr_action : Whether backing up or restoring this instance - masbr_restore_from_version : Backup version to restore (for restore action) Note : IMPORTANT - The instance ID must match between backup and restore operations. Attempting to restore a backup to an instance with a different ID will fail.","title":"mas_instance_id"},{"location":"roles/suite_backup_restore/#masbr_confirm_cluster","text":"Confirm cluster connection before backup/restore. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false Purpose : Controls whether the role prompts for confirmation of the currently connected cluster before executing backup or restore operations. Safety feature to prevent accidental operations on wrong cluster. When to use : - Set to true for interactive confirmation (recommended for production) - Leave as false (default) for automated/non-interactive operations - Use true when manually running backup/restore to verify correct cluster Valid values : true , false Impact : - true : Role prompts for cluster confirmation before proceeding - false : Role proceeds without confirmation (suitable for automation) Related variables : - masbr_action : Operation requiring cluster confirmation Note : Enabling cluster confirmation is recommended for manual operations, especially in production environments, to prevent accidental backup/restore on the wrong cluster.","title":"masbr_confirm_cluster"},{"location":"roles/suite_backup_restore/#masbr_copy_timeout_sec","text":"File transfer timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) Purpose : Specifies the maximum time allowed for transferring backup files between cluster and local storage. Prevents operations from hanging indefinitely. When to use : - Use default (12 hours) for most deployments - Increase for very large backups or slow network connections - Decrease for smaller backups to fail faster on issues Valid values : Positive integer (seconds), e.g., 3600 (1 hour), 43200 (12 hours), 86400 (24 hours) Impact : Operations exceeding this timeout will fail. Insufficient timeout for large backups will cause failures. Excessive timeout delays error detection. Related variables : - masbr_storage_local_folder : Destination for file transfers Note : The default 12 hours is suitable for most deployments. Adjust based on backup size and network speed. Monitor actual transfer times to optimize this setting.","title":"masbr_copy_timeout_sec"},{"location":"roles/suite_backup_restore/#masbr_job_timezone","text":"Time zone for scheduled backup jobs. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: UTC Purpose : Specifies the time zone for scheduled backup CronJobs. Ensures backups run at the intended local time rather than UTC. When to use : - Leave unset to use UTC (default) - Set when you need backups to run at specific local times - Only applies to scheduled backups (when masbr_backup_schedule is set) Valid values : Valid tz database time zone (e.g., America/New_York , Europe/London , Asia/Tokyo ) Impact : Determines when scheduled backups execute. Incorrect time zone may cause backups to run at unexpected times. Related variables : - masbr_backup_schedule : Cron expression interpreted in this time zone Note : Only relevant for scheduled backups. On-demand backups ignore this setting. Use standard tz database names (e.g., America/New_York , not EST ).","title":"masbr_job_timezone"},{"location":"roles/suite_backup_restore/#masbr_storage_local_folder","text":"Local filesystem path for backup storage. Required Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None Purpose : Specifies the local filesystem path where backup files are stored (for backups) or retrieved from (for restores). This is the persistent storage location for backup data. When to use : - Always required for backup and restore operations - Must be accessible from the system running the role - Should have sufficient space for backup files - Must be persistent across operations for restore capability Valid values : Absolute filesystem path (e.g., /tmp/masbr , /backup/mas , /mnt/backup ) Impact : Backup files are written to or read from this location. Insufficient space will cause backup failures. Path must exist and be writable. Related variables : - masbr_copy_timeout_sec : Timeout for transferring files to/from this location - masbr_restore_from_version : Backup version stored in this location Note : Ensure the path has sufficient disk space for backups. For production, use a dedicated backup volume with appropriate retention policies. The path must be accessible during both backup and restore operations.","title":"masbr_storage_local_folder"},{"location":"roles/suite_backup_restore/#backup","text":"","title":"Backup"},{"location":"roles/suite_backup_restore/#masbr_backup_schedule","text":"Cron expression for scheduled backups. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None (on-demand backup) Purpose : Defines a schedule for automatic recurring backups using Cron syntax. When set, creates a Kubernetes CronJob for automated backups. When to use : - Leave unset for on-demand backups (manual execution) - Set to create scheduled/recurring backups - Use for automated backup strategies Valid values : Valid Cron expression (e.g., 0 2 * * * for daily at 2 AM, 0 2 * * 0 for weekly on Sunday at 2 AM) Impact : - When set: Creates a Kubernetes CronJob that runs backups automatically on schedule - When unset: Creates an on-demand backup job that runs immediately Related variables : - masbr_job_timezone : Time zone for interpreting the cron schedule - masbr_action : Must be backup for scheduled backups Note : Scheduled backups only apply when masbr_action=backup . The cron expression is interpreted in the time zone specified by masbr_job_timezone (defaults to UTC). Common patterns: 0 2 * * * (daily 2 AM), 0 2 * * 0 (weekly Sunday 2 AM), 0 2 1 * * (monthly 1st at 2 AM).","title":"masbr_backup_schedule"},{"location":"roles/suite_backup_restore/#restore","text":"","title":"Restore"},{"location":"roles/suite_backup_restore/#masbr_restore_from_version","text":"Backup version timestamp for restore operations. Required (when masbr_action=restore ) Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Purpose : Specifies which backup version to restore from. The version is a timestamp identifying a specific backup. When to use : - Required when masbr_action=restore - Not used for backup operations - Must match an existing backup version in storage Valid values : Timestamp in YYYYMMDDHHMMSS format (e.g., 20240621021316 for June 21, 2024 at 02:13:16) Impact : Determines which backup is restored. Incorrect or non-existent version will cause restore to fail. Related variables : - masbr_action : Must be restore for this variable to be used - masbr_storage_local_folder : Location where backup versions are stored - mas_instance_id : Must match the instance ID from the backup Note : The backup version timestamp is generated automatically during backup creation. List available backups in masbr_storage_local_folder to find valid version timestamps. IMPORTANT - The backup can only be restored to an instance with the same MAS instance ID as the original backup.","title":"masbr_restore_from_version"},{"location":"roles/suite_backup_restore/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_backup_restore/#backup_1","text":"Backup MAS Core namespace resources, note that this does not include backup of any data in MongoDb, see the backup action in the mongodb role. - hosts: localhost any_errors_fatal: true vars: masbr_action: backup mas_instance_id: main masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_backup_restore","title":"Backup"},{"location":"roles/suite_backup_restore/#restore_1","text":"Restore MAS Core namespace resources, note that this does not include backup of any data in MongoDb, see the restore action in the mongodb role. - hosts: localhost any_errors_fatal: true vars: masbr_action: restore masbr_restore_from_version: 20240621021316 mas_instance_id: main masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_backup_restore","title":"Restore"},{"location":"roles/suite_backup_restore/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_certs/","text":"suite_certs \u00a4 This role iterates through the subdirectories in $MAS_CONFIG_DIR/certs which are named as core or name of the apps like monitor , manage , iot and so on. It looks for tls.crt, tls.key and ca.crt in these subdirectories. The names of the subdirectories in $MAS_CONFIG_DIR/certs are used to construct namespace to create/identify it and also creates the TLS secret with the tls/ca certs in those namespaces. So these subdirectories should be named correctly as the app names used in namespace suffixes. Directory structure example, \u00a4 $MAS_CONFIG_DIR/certs/core/tls.crt $MAS_CONFIG_DIR/certs/core/tls.key $MAS_CONFIG_DIR/certs/core/ca.crt $MAS_CONFIG_DIR/certs/<apps>/tls.crt $MAS_CONFIG_DIR/certs/<apps>/tls.key $MAS_CONFIG_DIR/certs/<apps>/ca.crt TLS Secret \u00a4 tls.crt , tls.key and ca.crt are mandatory files in these subdirectories. They are used to create TLS secret in each applications' namespace. The role will fail if an empty app subdirectory is present or an app subdirectory missing a mandatory file Note: \u00a4 Currently the secret names for core and each app are maintained in suite_certs/defaults/main.yml . Any changes to the existing secret name or adding new apps needs to be done here. Role Variables \u00a4 mas_instance_id \u00a4 The instance ID of the Maximo Application Suite installation to verify. Required Environment Variable: MAS_INSTANCE_ID Default Value: None mas_manual_cert_mgmt \u00a4 Set this to True if you want to enable manual certificate management mode. Environment Variable: MAS_MANUAL_CERT_MGMT Default Value: False mas_config_dir \u00a4 Path to the mas config directory. Required Environment Variable: MAS_CONFIG_DIR gitops \u00a4 Boolean flag to indicate whether to run role in gitops mode. True means that no openshift resources are created on the cluster. Optional Environment Variable: GITOPS Default Value: False Role Variables - CIS as DNS Provider (Optional) \u00a4 Optional variables for users using IBM Cloud Internet Services to manage DNS. This role will guarantee that your CNAMES related to MAS routes are created or updated in the informed CIS instance. dns_provider \u00a4 Set this to cis if you manage DNS using IBM Cloud Internet. If this variable is informed with a value different than cis it results in error (except blank, as it is optional). Optional Environment Variable: DNS_PROVIDER mas_workspace_id \u00a4 Workspace Id will be used as part of CNAMES definition when using cis as dns_provider. Required if dns_provider is defined and is cis Environment Variable: MAS_WORKSPACE_ID cis_crn \u00a4 CRN Key identifying the CIS in IBM Cloud. You can find that information in the page of your CIS instance. Required if dns_provider is defined and is cis Environment Variable: CIS_CRN cis_apikey \u00a4 API Key used to access the CIS in IBM CLoud. Required if dns_provider is defined and is cis Environment Variable: CIS_APIKEY cis_subdomain \u00a4 Subdomain will be used as part of CNAMES definition when using cis as dns_provider. Required if dns_provider is defined and is cis Environment Variable: CIS_SUBDOMAIN cis_proxy \u00a4 Set this to True if you want enable proxy in your CIS CNames leveraging security rules defined for this software. Optional Environment Variable: CIS_PROXY Default Value: False The directory structure for the certificates must be like below $MAS_CONFIG_DIR/certs/core/tls.crt $MAS_CONFIG_DIR/certs/core/tls.key $MAS_CONFIG_DIR/certs/core/ca.crt $MAS_CONFIG_DIR/certs/manage/tls.crt $MAS_CONFIG_DIR/certs/manage/tls.key $MAS_CONFIG_DIR/certs/manage/ca.crt $MAS_CONFIG_DIR/certs/<app>/tls.crt $MAS_CONFIG_DIR/certs/<app>/tls.key $MAS_CONFIG_DIR/certs/<app>/ca.crt the subdirectory name in the $MAS_CONFIG_DIR/certs directory is used to construct the namespace where the TLS secret will be applied to. So name the directory approriately. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_manual_cert_mgmt: True mas_config_dir: /Users/johnbarnes/Document/masconfig roles: - ibm.mas_devops.suite_certs More Detailed View of Directory Structure \u00a4 MAS_CONFIG_DIR | |---certs | | | | | |---core | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---iot | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---monitor | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---manage | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---add | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---assist | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---optimizer | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---visualinspection | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---facilities | | | | | |---tls.crt | | |---tls.key | | |---ca.crt License \u00a4 EPL-2.0","title":"suite_certs"},{"location":"roles/suite_certs/#suite_certs","text":"This role iterates through the subdirectories in $MAS_CONFIG_DIR/certs which are named as core or name of the apps like monitor , manage , iot and so on. It looks for tls.crt, tls.key and ca.crt in these subdirectories. The names of the subdirectories in $MAS_CONFIG_DIR/certs are used to construct namespace to create/identify it and also creates the TLS secret with the tls/ca certs in those namespaces. So these subdirectories should be named correctly as the app names used in namespace suffixes.","title":"suite_certs"},{"location":"roles/suite_certs/#directory-structure-example","text":"$MAS_CONFIG_DIR/certs/core/tls.crt $MAS_CONFIG_DIR/certs/core/tls.key $MAS_CONFIG_DIR/certs/core/ca.crt $MAS_CONFIG_DIR/certs/<apps>/tls.crt $MAS_CONFIG_DIR/certs/<apps>/tls.key $MAS_CONFIG_DIR/certs/<apps>/ca.crt","title":"Directory structure example,"},{"location":"roles/suite_certs/#tls-secret","text":"tls.crt , tls.key and ca.crt are mandatory files in these subdirectories. They are used to create TLS secret in each applications' namespace. The role will fail if an empty app subdirectory is present or an app subdirectory missing a mandatory file","title":"TLS Secret"},{"location":"roles/suite_certs/#note","text":"Currently the secret names for core and each app are maintained in suite_certs/defaults/main.yml . Any changes to the existing secret name or adding new apps needs to be done here.","title":"Note:"},{"location":"roles/suite_certs/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_certs/#mas_instance_id","text":"The instance ID of the Maximo Application Suite installation to verify. Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_certs/#mas_manual_cert_mgmt","text":"Set this to True if you want to enable manual certificate management mode. Environment Variable: MAS_MANUAL_CERT_MGMT Default Value: False","title":"mas_manual_cert_mgmt"},{"location":"roles/suite_certs/#mas_config_dir","text":"Path to the mas config directory. Required Environment Variable: MAS_CONFIG_DIR","title":"mas_config_dir"},{"location":"roles/suite_certs/#gitops","text":"Boolean flag to indicate whether to run role in gitops mode. True means that no openshift resources are created on the cluster. Optional Environment Variable: GITOPS Default Value: False","title":"gitops"},{"location":"roles/suite_certs/#role-variables-cis-as-dns-provider-optional","text":"Optional variables for users using IBM Cloud Internet Services to manage DNS. This role will guarantee that your CNAMES related to MAS routes are created or updated in the informed CIS instance.","title":"Role Variables - CIS as DNS Provider (Optional)"},{"location":"roles/suite_certs/#dns_provider","text":"Set this to cis if you manage DNS using IBM Cloud Internet. If this variable is informed with a value different than cis it results in error (except blank, as it is optional). Optional Environment Variable: DNS_PROVIDER","title":"dns_provider"},{"location":"roles/suite_certs/#mas_workspace_id","text":"Workspace Id will be used as part of CNAMES definition when using cis as dns_provider. Required if dns_provider is defined and is cis Environment Variable: MAS_WORKSPACE_ID","title":"mas_workspace_id"},{"location":"roles/suite_certs/#cis_crn","text":"CRN Key identifying the CIS in IBM Cloud. You can find that information in the page of your CIS instance. Required if dns_provider is defined and is cis Environment Variable: CIS_CRN","title":"cis_crn"},{"location":"roles/suite_certs/#cis_apikey","text":"API Key used to access the CIS in IBM CLoud. Required if dns_provider is defined and is cis Environment Variable: CIS_APIKEY","title":"cis_apikey"},{"location":"roles/suite_certs/#cis_subdomain","text":"Subdomain will be used as part of CNAMES definition when using cis as dns_provider. Required if dns_provider is defined and is cis Environment Variable: CIS_SUBDOMAIN","title":"cis_subdomain"},{"location":"roles/suite_certs/#cis_proxy","text":"Set this to True if you want enable proxy in your CIS CNames leveraging security rules defined for this software. Optional Environment Variable: CIS_PROXY Default Value: False The directory structure for the certificates must be like below $MAS_CONFIG_DIR/certs/core/tls.crt $MAS_CONFIG_DIR/certs/core/tls.key $MAS_CONFIG_DIR/certs/core/ca.crt $MAS_CONFIG_DIR/certs/manage/tls.crt $MAS_CONFIG_DIR/certs/manage/tls.key $MAS_CONFIG_DIR/certs/manage/ca.crt $MAS_CONFIG_DIR/certs/<app>/tls.crt $MAS_CONFIG_DIR/certs/<app>/tls.key $MAS_CONFIG_DIR/certs/<app>/ca.crt the subdirectory name in the $MAS_CONFIG_DIR/certs directory is used to construct the namespace where the TLS secret will be applied to. So name the directory approriately.","title":"cis_proxy"},{"location":"roles/suite_certs/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_manual_cert_mgmt: True mas_config_dir: /Users/johnbarnes/Document/masconfig roles: - ibm.mas_devops.suite_certs","title":"Example Playbook"},{"location":"roles/suite_certs/#more-detailed-view-of-directory-structure","text":"MAS_CONFIG_DIR | |---certs | | | | | |---core | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---iot | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---monitor | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---manage | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---add | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---assist | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---optimizer | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---visualinspection | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---facilities | | | | | |---tls.crt | | |---tls.key | | |---ca.crt","title":"More Detailed View of Directory Structure"},{"location":"roles/suite_certs/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_config/","text":"suite_config \u00a4 This role applies configuration files to a Maximo Application Suite installation. It searches for YAML configuration files in a specified directory and applies them to the cluster using the Kubernetes API. This is typically used after installing MAS to configure various aspects of the suite such as workspace configurations, JDBC configurations, SMTP settings, and other custom resources. Role Variables \u00a4 mas_instance_id \u00a4 Unique identifier for the MAS instance to apply configurations to. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to target when applying configuration files. This ensures configurations are applied to the correct MAS installation when multiple instances exist in the cluster. When to use : - Always required for any MAS configuration operation - Must match the instance ID used during MAS core installation - Use the same value across all configuration operations for a given MAS instance Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Configurations will be applied to the namespace mas-{mas_instance_id}-core and related application namespaces. An incorrect instance ID will cause configurations to be applied to the wrong MAS instance or fail if the instance doesn't exist. Related variables : Works with mas_config_dir to determine which configuration files to apply. Note : This must match the instance ID used when installing MAS core. Verify the instance ID before applying configurations to avoid misconfiguration. mas_config_dir \u00a4 Local directory path containing MAS configuration YAML files to apply to the cluster. Required Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the directory containing MAS configuration files (MongoCfg, JdbcCfg, BASCfg, SLSCfg, etc.) that will be automatically applied to configure the MAS instance. This enables automated configuration of MAS dependencies and settings. When to use : - Always required when using this role to apply configurations - Use the same directory where dependency roles (mongodb, db2, sls) generate their configuration files - Typically set to a consistent location across all MAS setup roles (e.g., /home/user/masconfig ) Valid values : Any valid local filesystem path containing YAML configuration files (e.g., /home/user/masconfig , ~/masconfig , ./config ) Impact : The role recursively searches this directory for all *.yaml and *.yml files and applies them to the cluster using oc apply . Invalid YAML files or incorrect configurations will cause the role to fail. Files matching jdbc-aiservice*.yml or jdbc-aiservice*.yaml patterns are automatically excluded from processing. Related variables : - mas_instance_id : Determines which MAS instance receives the configurations - Used by dependency roles (mongodb, db2, sls) as output directory for generated configs Note : Ensure all YAML files in this directory are valid Kubernetes resources intended for this MAS instance. The role applies all matching files, so remove or move any files not intended for application. AI Service JDBC configurations are excluded as they require special handling. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_config_dir: \"/home/user/masconfig\" roles: - ibm.mas_devops.suite_config License \u00a4 EPL-2.0","title":"suite_config"},{"location":"roles/suite_config/#suite_config","text":"This role applies configuration files to a Maximo Application Suite installation. It searches for YAML configuration files in a specified directory and applies them to the cluster using the Kubernetes API. This is typically used after installing MAS to configure various aspects of the suite such as workspace configurations, JDBC configurations, SMTP settings, and other custom resources.","title":"suite_config"},{"location":"roles/suite_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_config/#mas_instance_id","text":"Unique identifier for the MAS instance to apply configurations to. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to target when applying configuration files. This ensures configurations are applied to the correct MAS installation when multiple instances exist in the cluster. When to use : - Always required for any MAS configuration operation - Must match the instance ID used during MAS core installation - Use the same value across all configuration operations for a given MAS instance Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Configurations will be applied to the namespace mas-{mas_instance_id}-core and related application namespaces. An incorrect instance ID will cause configurations to be applied to the wrong MAS instance or fail if the instance doesn't exist. Related variables : Works with mas_config_dir to determine which configuration files to apply. Note : This must match the instance ID used when installing MAS core. Verify the instance ID before applying configurations to avoid misconfiguration.","title":"mas_instance_id"},{"location":"roles/suite_config/#mas_config_dir","text":"Local directory path containing MAS configuration YAML files to apply to the cluster. Required Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Specifies the directory containing MAS configuration files (MongoCfg, JdbcCfg, BASCfg, SLSCfg, etc.) that will be automatically applied to configure the MAS instance. This enables automated configuration of MAS dependencies and settings. When to use : - Always required when using this role to apply configurations - Use the same directory where dependency roles (mongodb, db2, sls) generate their configuration files - Typically set to a consistent location across all MAS setup roles (e.g., /home/user/masconfig ) Valid values : Any valid local filesystem path containing YAML configuration files (e.g., /home/user/masconfig , ~/masconfig , ./config ) Impact : The role recursively searches this directory for all *.yaml and *.yml files and applies them to the cluster using oc apply . Invalid YAML files or incorrect configurations will cause the role to fail. Files matching jdbc-aiservice*.yml or jdbc-aiservice*.yaml patterns are automatically excluded from processing. Related variables : - mas_instance_id : Determines which MAS instance receives the configurations - Used by dependency roles (mongodb, db2, sls) as output directory for generated configs Note : Ensure all YAML files in this directory are valid Kubernetes resources intended for this MAS instance. The role applies all matching files, so remove or move any files not intended for application. AI Service JDBC configurations are excluded as they require special handling.","title":"mas_config_dir"},{"location":"roles/suite_config/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_config_dir: \"/home/user/masconfig\" roles: - ibm.mas_devops.suite_config","title":"Example Playbook"},{"location":"roles/suite_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_db2_setup_for_facilities/","text":"suite_db2_setup_for_facilities \u00a4 This role shouldn't need to exist, it should be part of the Facilities operator, but is not so we have to do it as a separate step in the install flow for now. The role will install DB2 with the required properties and perform some initial setup on the Db2 instance that is needed to prepare it for use with the Real estate and Facilities application. The role will copy scripts into the Db2 pod and execute it inside the container, this script will perform a number of configuration changes to the database as well as configuring the tablespaces for Maximo Real estate and facilities because the operator is not yet able to do this itself. Role Variables \u00a4 db2_instance_name \u00a4 The name of the db2 instance to execute the setup in. Required Environment Variable: DB2_INSTANCE_NAME Default Value: None db2_namespace \u00a4 The namespace where the Db2 instance is running. Optional Environment Variable: DB2_NAMESPACE Default Value: db2u db2_username \u00a4 The username that will be used to connect to the database specified by db2_dbname . Optional Environment Variable: None Default Value: tridata db2_dbname \u00a4 The name of the database in the instance to connect to when executing the setup script. Optional Environment Variable: None Default Value: BLUDB db2_schema \u00a4 The name of the Manage schema where the hack should be targeted in. Optional Environment Variable: None Default Value: TRIDATA db2_tablespace_data_size \u00a4 The size of the tablespace data in the database. Optional Environment Variable: DB2_TABLESPACE_DATA_SIZE Default Value: 5000 M db2_tablespace_index_size \u00a4 The size of the tablespace indexes in the database. Optional Environment Variable: DB2_TABLESPACE_INDEX_SIZE Default Value: 5000 M db2_config_version \u00a4 Version of the enhanced DB2 parameters, currently support 1.0.0 Required Environment Variable: DB2_CONFIG_VERSION Default: 1.0.0 enforce_db2_config \u00a4 Flag to indicate restart the DB2 instance or not, the enhanced DB2 parameters required restart DB2 instance, this will cause downtime, should execute during customer maintenance window or newly created DB2 instance if set to True Required Environment Variable: ENFORCE_DB2_CONFIG Default: True Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # Configuration for the Db2 cluster db2_namespace: db2u db2_instance_name: db2u-db01 db2_dbname: BLUDB roles: - ibm.mas_devops.suite_db2_setup_for_facilities License \u00a4 EPL-2.0","title":"suite_db2_setup_for_facilities"},{"location":"roles/suite_db2_setup_for_facilities/#suite_db2_setup_for_facilities","text":"This role shouldn't need to exist, it should be part of the Facilities operator, but is not so we have to do it as a separate step in the install flow for now. The role will install DB2 with the required properties and perform some initial setup on the Db2 instance that is needed to prepare it for use with the Real estate and Facilities application. The role will copy scripts into the Db2 pod and execute it inside the container, this script will perform a number of configuration changes to the database as well as configuring the tablespaces for Maximo Real estate and facilities because the operator is not yet able to do this itself.","title":"suite_db2_setup_for_facilities"},{"location":"roles/suite_db2_setup_for_facilities/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_db2_setup_for_facilities/#db2_instance_name","text":"The name of the db2 instance to execute the setup in. Required Environment Variable: DB2_INSTANCE_NAME Default Value: None","title":"db2_instance_name"},{"location":"roles/suite_db2_setup_for_facilities/#db2_namespace","text":"The namespace where the Db2 instance is running. Optional Environment Variable: DB2_NAMESPACE Default Value: db2u","title":"db2_namespace"},{"location":"roles/suite_db2_setup_for_facilities/#db2_username","text":"The username that will be used to connect to the database specified by db2_dbname . Optional Environment Variable: None Default Value: tridata","title":"db2_username"},{"location":"roles/suite_db2_setup_for_facilities/#db2_dbname","text":"The name of the database in the instance to connect to when executing the setup script. Optional Environment Variable: None Default Value: BLUDB","title":"db2_dbname"},{"location":"roles/suite_db2_setup_for_facilities/#db2_schema","text":"The name of the Manage schema where the hack should be targeted in. Optional Environment Variable: None Default Value: TRIDATA","title":"db2_schema"},{"location":"roles/suite_db2_setup_for_facilities/#db2_tablespace_data_size","text":"The size of the tablespace data in the database. Optional Environment Variable: DB2_TABLESPACE_DATA_SIZE Default Value: 5000 M","title":"db2_tablespace_data_size"},{"location":"roles/suite_db2_setup_for_facilities/#db2_tablespace_index_size","text":"The size of the tablespace indexes in the database. Optional Environment Variable: DB2_TABLESPACE_INDEX_SIZE Default Value: 5000 M","title":"db2_tablespace_index_size"},{"location":"roles/suite_db2_setup_for_facilities/#db2_config_version","text":"Version of the enhanced DB2 parameters, currently support 1.0.0 Required Environment Variable: DB2_CONFIG_VERSION Default: 1.0.0","title":"db2_config_version"},{"location":"roles/suite_db2_setup_for_facilities/#enforce_db2_config","text":"Flag to indicate restart the DB2 instance or not, the enhanced DB2 parameters required restart DB2 instance, this will cause downtime, should execute during customer maintenance window or newly created DB2 instance if set to True Required Environment Variable: ENFORCE_DB2_CONFIG Default: True","title":"enforce_db2_config"},{"location":"roles/suite_db2_setup_for_facilities/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Configuration for the Db2 cluster db2_namespace: db2u db2_instance_name: db2u-db01 db2_dbname: BLUDB roles: - ibm.mas_devops.suite_db2_setup_for_facilities","title":"Example Playbook"},{"location":"roles/suite_db2_setup_for_facilities/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_db2_setup_for_manage/","text":"suite_db2_setup_for_manage \u00a4 This role shouldn't need to exist, it should be part of the Manage operator, but is not so we have to do it as a seperate step in the install flow for now. The role will perform some initial setup on the Db2 instance that is needed to prepare it for use with the Manage application and supports both CP4D version 3.5 and 4.0. The role will copy a bash script (setupdb.sh) into the Db2 pod and execute it inside the container, this script will perform a number of configuration changes to the database as well as configuring the tablespaces for Maximo Manage because the operator is not yet able to do this itself. Role Variables \u00a4 db2_instance_name \u00a4 The name of the db2 instance to execute the setup in. Required Environment Variable: DB2_INSTANCE_NAME Default Value: None db2_namespace \u00a4 The namespace where the Db2 instance is running. Optional Environment Variable: DB2_NAMESPACE Default Value: db2u db2_username \u00a4 The username that will be used to connect to the database specified by db2_dbname . Optional Environment Variable: None Default Value: db2inst1 db2_dbname \u00a4 The name of the database in the instance to connect to when executing the setup script. Optional Environment Variable: None Default Value: BLUDB db2_schema \u00a4 The name of the Manage schema where the hack should be targeted in. Optional Environment Variable: None Default Value: maximo db2_tablespace_data_size \u00a4 The size of the tablespace data in the database. Optional Environment Variable: DB2_TABLESPACE_DATA_SIZE Default Value: 5000 M db2_tablespace_index_size \u00a4 The size of the tablespace indexes in the database. Optional Environment Variable: DB2_TABLESPACE_INDEX_SIZE Default Value: 5000 M db2_config_version \u00a4 Version of the enhanced DB2 parameters, currently support 1.0.0 Required Environment Variable: DB2_CONFIG_VERSION Default: 1.0.0 enforce_db2_config \u00a4 Flag to indicate restart the DB2 instance or not, the enhanced DB2 parameters required restart DB2 instance, this will cause downtime, should execute during customer maintenance window or newly created DB2 instance if set to True Required Environment Variable: ENFORCE_DB2_CONFIG Default: True Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: db2_instancename: mydb2 db2_namespace: db2u db2_config_version: \"1.0.0\" # It will cause downtime if set to true, please be careful. enforce_db2_config: true roles: - ibm.mas_devops.suite_db2_setup_for_manage License \u00a4 EPL-2.0","title":"suite_db2_setup_for_manage"},{"location":"roles/suite_db2_setup_for_manage/#suite_db2_setup_for_manage","text":"This role shouldn't need to exist, it should be part of the Manage operator, but is not so we have to do it as a seperate step in the install flow for now. The role will perform some initial setup on the Db2 instance that is needed to prepare it for use with the Manage application and supports both CP4D version 3.5 and 4.0. The role will copy a bash script (setupdb.sh) into the Db2 pod and execute it inside the container, this script will perform a number of configuration changes to the database as well as configuring the tablespaces for Maximo Manage because the operator is not yet able to do this itself.","title":"suite_db2_setup_for_manage"},{"location":"roles/suite_db2_setup_for_manage/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_db2_setup_for_manage/#db2_instance_name","text":"The name of the db2 instance to execute the setup in. Required Environment Variable: DB2_INSTANCE_NAME Default Value: None","title":"db2_instance_name"},{"location":"roles/suite_db2_setup_for_manage/#db2_namespace","text":"The namespace where the Db2 instance is running. Optional Environment Variable: DB2_NAMESPACE Default Value: db2u","title":"db2_namespace"},{"location":"roles/suite_db2_setup_for_manage/#db2_username","text":"The username that will be used to connect to the database specified by db2_dbname . Optional Environment Variable: None Default Value: db2inst1","title":"db2_username"},{"location":"roles/suite_db2_setup_for_manage/#db2_dbname","text":"The name of the database in the instance to connect to when executing the setup script. Optional Environment Variable: None Default Value: BLUDB","title":"db2_dbname"},{"location":"roles/suite_db2_setup_for_manage/#db2_schema","text":"The name of the Manage schema where the hack should be targeted in. Optional Environment Variable: None Default Value: maximo","title":"db2_schema"},{"location":"roles/suite_db2_setup_for_manage/#db2_tablespace_data_size","text":"The size of the tablespace data in the database. Optional Environment Variable: DB2_TABLESPACE_DATA_SIZE Default Value: 5000 M","title":"db2_tablespace_data_size"},{"location":"roles/suite_db2_setup_for_manage/#db2_tablespace_index_size","text":"The size of the tablespace indexes in the database. Optional Environment Variable: DB2_TABLESPACE_INDEX_SIZE Default Value: 5000 M","title":"db2_tablespace_index_size"},{"location":"roles/suite_db2_setup_for_manage/#db2_config_version","text":"Version of the enhanced DB2 parameters, currently support 1.0.0 Required Environment Variable: DB2_CONFIG_VERSION Default: 1.0.0","title":"db2_config_version"},{"location":"roles/suite_db2_setup_for_manage/#enforce_db2_config","text":"Flag to indicate restart the DB2 instance or not, the enhanced DB2 parameters required restart DB2 instance, this will cause downtime, should execute during customer maintenance window or newly created DB2 instance if set to True Required Environment Variable: ENFORCE_DB2_CONFIG Default: True","title":"enforce_db2_config"},{"location":"roles/suite_db2_setup_for_manage/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: db2_instancename: mydb2 db2_namespace: db2u db2_config_version: \"1.0.0\" # It will cause downtime if set to true, please be careful. enforce_db2_config: true roles: - ibm.mas_devops.suite_db2_setup_for_manage","title":"Example Playbook"},{"location":"roles/suite_db2_setup_for_manage/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_dns/","text":"suite_dns \u00a4 This role will manage MAS and DNS provider integration. IBM Cloud Internet Services, Cloudflare, and AWS Route 53 are the supported DNS providers. It will also create a secure route (https://cp4d. ) to the CP4D web client using the custom domain used in this role. Note : this role will take no action when mas_manual_cert_mgmt is set to True DNS Management \u00a4 There are two different ways this role controls DNS entries in the provider: Top Level DNS Entries \u00a4 This mode will create the entries directly under your DNS zone. Use this when the DNS zone matches the MAS domain exactly. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mymas.mycompany.com then you will be creating top-level DNS entries for MAS, e.g. admin , home , & api . Subdomain DNS Entries \u00a4 This mode will create DNS entries in the zone under a subdomain. Use this when your DNS zone will be used for more than just one MAS instance. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mycompany.com then you will be creating subdomain DNS entries for MAS, e.g. admin.mymas , home.mymas , & api.mymas . CIS and Cloudflare integrations support both modes of DNS management. A single optional variable is required to enable subdomain DNS management, in the examples above you would set these to mymas : cis_subdomain cloudflare_subdomain Let's Encrypt Integration \u00a4 Both the CIS and Cloudflare options also enable integration with Let's Encrypt for automatic certificate management via IBM Certificate Manager. Each will create a new ClusterIssuer which can be used when installing Maximo Application Suite: Cloudflare Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cloudflare-le-prod IBM Cloud Internet Services Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cis-le-prod If you want to use Let's Encrypt certificates in your MAS installation you will need to configure the mas_cluster_issuer variable in the suite_install role, setting it to the name of the ClusterIssuer as documented above. Note There are issues with how cert-manager works with LetsEncrypt staging servers. It creates a secret for the certificate that doesn't contain the LetsEncrypt CA, but the staging service does not use a well known cert so we end up with MAS unable to trust the certificates generated by LetsEncrypt staging. At present there is no workaround for this, so do not use the LetsEncrypt staging certificate issuer. Role Variables \u00a4 General \u00a4 dns_provider \u00a4 Specifies which DNS provider to use for managing MAS domain DNS entries. Required Environment Variable: DNS_PROVIDER Default: None Purpose : Determines which DNS service will be used to create and manage DNS records for MAS routes and endpoints. Different providers offer different features and integration capabilities. When to use : - Set to cloudflare when using Cloudflare DNS service - Set to cis when using IBM Cloud Internet Services - Set to route53 when using AWS Route 53 - Choose based on your organization's DNS infrastructure Valid values : cis , cloudflare , route53 Impact : The selected provider determines which additional variables are required and which features are available (e.g., Let's Encrypt integration is available for CIS and Cloudflare but not Route53). Related variables : - When cloudflare : Requires cloudflare_email , cloudflare_apitoken , cloudflare_zone - When cis : Requires cis_email , cis_apikey , cis_crn - When route53 : Requires AWS Route53 specific variables Note : This role takes no action when mas_manual_cert_mgmt is set to True . CIS and Cloudflare support Let's Encrypt integration for automatic certificate management. mas_instance_id \u00a4 Unique identifier for the MAS instance requiring DNS configuration. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to configure DNS entries for. This ID is used to generate DNS record names and ClusterIssuer resources specific to this MAS installation. When to use : - Always required for DNS configuration - Must match the instance ID used during MAS core installation - Used to create unique ClusterIssuer names for Let's Encrypt integration Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : This ID is embedded in DNS record names and ClusterIssuer names (e.g., {mas_instance_id}-cloudflare-le-prod ). Incorrect ID will create DNS entries for the wrong instance. Related variables : Works with mas_domain to construct full DNS names for MAS routes. Note : Must match the instance ID from MAS core installation. Used in ClusterIssuer names for Let's Encrypt integration. mas_workspace_id \u00a4 Workspace identifier for the MAS installation requiring DNS configuration. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which MAS workspace to configure DNS entries for. Workspaces are logical divisions within a MAS instance that can have separate configurations and applications. When to use : - Always required for DNS configuration - Must match a workspace ID configured in your MAS instance - Typically set to the primary workspace ID (e.g., masdev , prod ) Valid values : Lowercase alphanumeric string (e.g., masdev , prod , ws1 ) Impact : This ID is used to generate workspace-specific DNS entries and routes. Incorrect workspace ID may result in DNS entries that don't match your MAS workspace configuration. Related variables : Works with mas_instance_id and mas_domain to construct workspace-specific DNS names. Note : Must match an existing workspace in your MAS instance. Each workspace can have its own DNS configuration. mas_domain \u00a4 Custom domain name for accessing MAS web interfaces and APIs. Required Environment Variable: MAS_DOMAIN Default: None Purpose : Defines the base domain used to construct all MAS URLs and DNS entries (e.g., admin.{mas_domain}, home.{mas_domain}, api.{mas_domain}). This domain must be managed by your chosen DNS provider. When to use : - Always required for DNS configuration - Must match the domain configured in your DNS provider's zone - Should align with your organization's domain naming conventions - Must be the same domain used in MAS core installation Valid values : Any valid DNS domain name (e.g., mas.mycompany.com , prod-mas.example.org ) Impact : DNS entries will be created for this domain in your DNS provider. The domain must be properly configured in your DNS provider's zone. All MAS routes will use this domain. Related variables : - Must align with DNS provider zone configuration ( cloudflare_zone , cis_crn , etc.) - Works with mas_instance_id to create unique DNS entries - May use subdomain mode with cloudflare_subdomain or cis_subdomain Note : Ensure this domain is properly configured in your DNS provider before running this role. DNS propagation may take time after entries are created. ocp_ingress \u00a4 Override for the OpenShift cluster ingress domain used in DNS entries. Optional Environment Variable: OCP_INGRESS Default: None (automatically detected from cluster) Purpose : Allows manual specification of the OpenShift cluster's ingress domain when automatic detection fails or when you need to use a specific ingress controller. The ingress domain is used as the target for DNS CNAME records. When to use : - Leave unset to allow automatic detection (recommended) - Set only when automatic detection fails - Set when using a non-default ingress controller - Set in airgap or restricted network environments where detection may not work Valid values : Valid OpenShift ingress domain (e.g., apps.cluster-name.domain.com ) Impact : When set, this value is used as the target for DNS CNAME records instead of the automatically detected ingress. Incorrect value will cause DNS entries to point to the wrong location. Related variables : Works with DNS provider variables to create CNAME records pointing to this ingress. Note : Automatic detection is usually sufficient. Only override if you encounter issues or have specific ingress requirements. cert_manager_namespace \u00a4 OpenShift namespace where Certificate Manager is installed. Optional Environment Variable: CERT_MANAGER_NAMESPACE Default: None (automatically detected) Purpose : Specifies the namespace where Certificate Manager is deployed. This is needed to create ClusterIssuer resources for Let's Encrypt integration in the correct location. When to use : - Leave unset to allow automatic detection (recommended) - Set only if Certificate Manager is installed in a non-standard namespace - Set if automatic detection fails Valid values : Any valid Kubernetes namespace name where Certificate Manager is installed Impact : ClusterIssuer resources for Let's Encrypt will be created referencing this namespace. Incorrect namespace will cause ClusterIssuer creation to fail. Related variables : Used when creating ClusterIssuer for Cloudflare or CIS Let's Encrypt integration. Note : Automatic detection typically finds Certificate Manager in standard namespaces. Only override if using a custom installation. custom_labels \u00a4 Comma-separated list of key=value labels to apply to DNS-related resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to DNS-related resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,component=dns ) Impact : Labels are applied to DNS-related resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect DNS functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management. mas_manual_cert_mgmt \u00a4 Controls whether to disable automatic DNS and certificate management. Optional Environment Variable: MAS_MANUAL_CERT_MGMT Default: false Purpose : Allows you to opt out of automatic DNS entry creation and certificate management when you prefer to manage these manually or through alternative methods. When to use : - Set to true when managing DNS entries manually - Set to true when using custom certificate management solutions - Set to true when DNS provider integration is not available - Leave as false (default) for automated DNS and certificate management Valid values : true , false Impact : - When true : This role takes no action; DNS entries and certificates must be managed manually - When false : Role automatically creates DNS entries and optionally configures Let's Encrypt integration Related variables : When true , all other DNS provider variables are ignored. Note : Setting to true means you are responsible for creating all required DNS entries and managing certificates manually. Ensure proper DNS configuration before MAS installation. output_dir \u00a4 Local directory path where the edge routes output file will be saved. Optional Environment Variable: OUTPUT_DIR Default: . (current directory) Purpose : Specifies where to save the edge-routes-{mas_instance_id}.txt file containing information about the DNS entries and routes created by this role. This file is useful for verification and troubleshooting. When to use : - Set to a specific directory for organized output file management - Use default (current directory) for simple deployments - Set to a shared location for team access to route information Valid values : Any valid local filesystem path (e.g., /home/user/mas-output , ~/masconfig , ./output ) Impact : The edge routes file will be created in this directory. The file contains DNS entry details and can be used to verify DNS configuration. Related variables : Output filename includes mas_instance_id for identification. Note : Ensure the directory exists and is writable. The file provides useful information for verifying DNS setup and troubleshooting connectivity issues. Cloudflare DNS Integration \u00a4 cloudflare_email \u00a4 Email address associated with your Cloudflare account. Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_EMAIL Default: None Purpose : Provides the email address for authenticating with Cloudflare API. This email must be associated with a Cloudflare account that has access to manage the DNS zone. When to use : - Required when dns_provider=cloudflare - Must be the email address used to log into Cloudflare - Account must have permissions to manage DNS records in the target zone Valid values : Valid email address associated with a Cloudflare account Impact : Used together with cloudflare_apitoken to authenticate API requests. Incorrect email will cause authentication failures. Related variables : - cloudflare_apitoken : Must be set together with this email - cloudflare_zone : Zone that this account has access to manage Note : Ensure the Cloudflare account has appropriate permissions to create and manage DNS records in the target zone. cloudflare_apitoken \u00a4 API token for authenticating with Cloudflare API. Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_APITOKEN Default: None Purpose : Provides the API token credential for authenticating with Cloudflare to create and manage DNS records. This token must have appropriate permissions for DNS management. When to use : - Required when dns_provider=cloudflare - Generate from Cloudflare dashboard with DNS edit permissions - Token must have access to the target DNS zone Valid values : Valid Cloudflare API token string Impact : Used together with cloudflare_email to authenticate API requests. Invalid or insufficient permissions will cause DNS operations to fail. Related variables : - cloudflare_email : Must be set together with this token - cloudflare_zone : Zone that this token has permissions to manage Note : Generate the API token following the Cloudflare documentation . Ensure the token has DNS edit permissions for the target zone. Keep this token secure and do not commit to source control. cloudflare_zone \u00a4 DNS zone name managed by Cloudflare where MAS DNS entries will be created. Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_ZONE Default: None Purpose : Specifies the Cloudflare DNS zone (domain) where MAS DNS records will be created. This must be a domain that is already configured and active in your Cloudflare account. When to use : - Required when dns_provider=cloudflare - Must match a zone configured in your Cloudflare account - Should be the parent domain of mas_domain (or equal to it for top-level entries) Valid values : Valid domain name managed in Cloudflare (e.g., mydomain.com , example.org ) Impact : DNS entries will be created in this zone. The zone must exist in Cloudflare and be accessible with the provided credentials. Incorrect zone will cause DNS operations to fail. Related variables : - mas_domain : Should be within this zone (e.g., if zone is mydomain.com , mas_domain could be mas.mydomain.com ) - cloudflare_subdomain : Optional subdomain within this zone for MAS entries - cloudflare_email and cloudflare_apitoken : Must have access to this zone Note : Ensure the zone is active in Cloudflare and DNS is properly delegated before running this role. cloudflare_subdomain \u00a4 Subdomain within the Cloudflare zone for MAS DNS entries (subdomain mode). Optional Environment Variable: CLOUDFLARE_SUBDOMAIN Default: None (uses top-level DNS entries) Purpose : Enables subdomain mode for DNS management, where MAS DNS entries are created under a subdomain rather than at the top level of the zone. This allows multiple MAS instances or other services to share the same DNS zone. When to use : - Use when your DNS zone will host multiple MAS instances or other services - Use when mas_domain is a subdomain of cloudflare_zone - Leave unset when mas_domain equals cloudflare_zone (top-level mode) Valid values : Subdomain name (e.g., if cloudflare_zone=mycompany.com and mas_domain=mas.mycompany.com , set to mas ) Impact : - When set: DNS entries created as admin.{cloudflare_subdomain} , home.{cloudflare_subdomain} , etc. - When unset: DNS entries created as admin , home , etc. directly in the zone Related variables : - cloudflare_zone : Parent zone for this subdomain - mas_domain : Should equal {cloudflare_subdomain}.{cloudflare_zone} Note : The relationship must be: cloudflare_subdomain.cloudflare_zone = mas_domain . For example, if zone is mycompany.com and mas_domain is mas.mycompany.com , set subdomain to mas . IBM Cloud Internet Services DNS Integration \u00a4 Note When using CIS integration, some resources will be installed in the cluster such as RBACs, API Services and CIS Webhook deployments. In OCP 4.12+, to avoid CIS webhook deployment failure at start up, this role will grant anyuid permission to cert-manager-webhook-ibm-cis service account so it can fully access the cert-manager-webhook-ibm-cis deployment pod as a workaround: oc adm policy add-scc-to-user anyuid -z cert-manager-webhook-ibm-cis -n ibm-common-services cis_email \u00a4 Email address for Let's Encrypt ClusterIssuer registration when using IBM CIS. Required if dns_provider is set to cis Environment Variable: CIS_EMAIL Default: None Purpose : Provides the email address that will be registered with Let's Encrypt when creating the ClusterIssuer for automatic certificate management. Let's Encrypt uses this email for certificate expiration notifications and account recovery. When to use : - Required when dns_provider=cis - Use a monitored email address to receive Let's Encrypt notifications - Typically use a team or service email rather than personal email Valid values : Valid email address Impact : This email is embedded in the ClusterIssuer resource and registered with Let's Encrypt. You'll receive notifications about certificate renewals and any issues at this address. Related variables : - cis_apikey and cis_crn : Required together for CIS integration - Used in ClusterIssuer: {mas_instance_id}-cis-le-prod Note : Use a monitored email address as Let's Encrypt sends important notifications about certificate expiration and renewal issues. cis_apikey \u00a4 IBM Cloud API key for authenticating with IBM Cloud Internet Services. Required if dns_provider is set to cis Environment Variable: CIS_APIKEY Default: None Purpose : Provides the IBM Cloud API key credential for authenticating with IBM Cloud Internet Services to create and manage DNS records and configure Let's Encrypt integration. When to use : - Required when dns_provider=cis - Generate from IBM Cloud IAM with appropriate CIS permissions - Key must have access to the CIS instance specified by cis_crn Valid values : Valid IBM Cloud API key string Impact : Used to authenticate all CIS API operations including DNS record management and ClusterIssuer configuration. Invalid key or insufficient permissions will cause operations to fail. Related variables : - cis_crn : CIS instance that this API key has access to - cis_email : Email for Let's Encrypt registration Note : Generate the API key from IBM Cloud IAM. Ensure it has appropriate permissions for the CIS instance. Keep this key secure and do not commit to source control. cis_crn \u00a4 Cloud Resource Name (CRN) identifying the IBM Cloud Internet Services instance. Required if dns_provider is set to cis Environment Variable: CIS_CRN Default: None Purpose : Uniquely identifies the specific IBM Cloud Internet Services instance where DNS records will be managed. The CRN ensures operations target the correct CIS instance when multiple instances exist. When to use : - Required when dns_provider=cis - Obtain from IBM Cloud CIS instance details - Must correspond to the CIS instance containing your DNS zone Valid values : Valid IBM Cloud CRN string (format: crn:v1:bluemix:public:internet-svcs:... ) Impact : All DNS operations will target this CIS instance. Incorrect CRN will cause operations to fail or target the wrong instance. Related variables : - cis_apikey : Must have permissions for this CIS instance - DNS zone must be configured in this CIS instance Note : Find the CRN in the IBM Cloud console under your CIS instance details. Ensure the CRN matches the instance containing your target DNS zone. cis_subdomain \u00a4 Subdomain within the CIS zone for MAS DNS entries (subdomain mode). Optional Environment Variable: CIS_SUBDOMAIN Default: None (uses top-level DNS entries) Purpose : Enables subdomain mode for DNS management, where MAS DNS entries are created under a subdomain rather than at the top level of the CIS zone. This allows multiple MAS instances or other services to share the same DNS zone. When to use : - Use when your CIS zone will host multiple MAS instances or other services - Use when mas_domain is a subdomain of your CIS zone domain - Leave unset when mas_domain equals the CIS zone domain (top-level mode) Valid values : Subdomain name (e.g., if CIS zone is mycompany.com and mas_domain=mas.mycompany.com , set to mas ) Impact : - When set: DNS entries created as admin.{cis_subdomain} , home.{cis_subdomain} , etc. - When unset: DNS entries created as admin , home , etc. directly in the zone Related variables : - CIS zone domain: Parent zone for this subdomain - mas_domain : Should equal {cis_subdomain}.{cis_zone_domain} Note : The relationship must be: cis_subdomain.{cis_zone_domain} = mas_domain . For example, if CIS zone is mycompany.com and mas_domain is mas.mycompany.com , set subdomain to mas . cis_enhanced_security \u00a4 Enables enhanced security features for IBM CIS DNS integration. Optional Environment Variable: CIS_ENHANCED_SECURITY Default: false Purpose : Activates a comprehensive set of security enhancements for CIS including WAF configuration, proxy mode, expanded DNS entries, wildcard prevention, and edge certificates. This provides optimal security for MAS deployments using CIS. When to use : - Set to true for production environments requiring enhanced security - Set to true when security compliance requires WAF and proxy protection - Leave as false for development/test environments or when enhanced features aren't needed Valid values : true , false Impact : When enabled, this configures: - WAF firewall with rules optimized for MAS - Proxy mode for DNS entries (traffic routed through CIS) - Expanded list of DNS entries for comprehensive coverage - Prevention of wildcard DNS entries - Edge certificates in CIS instance Related variables : - cis_waf : Controls WAF specifically (when enhanced_security is true) - cis_proxy : Controls proxy mode (when enhanced_security is true) Note : See IBM Cloud CIS security documentation for details. Enhanced security may impact performance due to proxy routing. Enhanced IBM CIS DNS Integration Security \u00a4 See the cis_enhanced_security variable above for details. cis_waf \u00a4 Controls Web Application Firewall (WAF) configuration for CIS. Optional Environment Variable: CIS_WAF Default: true Purpose : Enables or disables WAF configuration with rules optimized for MAS application functionality. WAF provides protection against common web attacks while ensuring MAS features work correctly. When to use : - Leave as true (default) for production environments requiring WAF protection - Set to false only if WAF causes issues or conflicts with other security tools - Typically used in conjunction with cis_enhanced_security Valid values : true , false Impact : - When true : Configures WAF with rules that protect MAS while allowing required functionality - When false : WAF is not configured (less security protection) Related variables : - cis_enhanced_security : When true, this setting is part of enhanced security features - cis_proxy : Often used together for comprehensive security Note : WAF rules are specifically tuned to avoid blocking legitimate MAS application traffic. Default is true for security best practices. cis_proxy \u00a4 Controls whether DNS entries use CIS proxy mode (traffic routed through CIS). Optional Environment Variable: CIS_PROXY Default: false Purpose : Enables proxy mode for DNS entries, routing traffic through IBM CIS infrastructure. This provides additional security, DDoS protection, and performance optimization but adds latency. When to use : - Set to true for production environments requiring DDoS protection - Set to true when using CIS security features (WAF, rate limiting) - Set to false for direct routing (lower latency, less protection) - Typically enabled with cis_enhanced_security Valid values : true , false Impact : - When true : Traffic routes through CIS (orange cloud icon in CIS console), enabling security features but adding latency - When false : DNS entries point directly to OpenShift ingress (grey cloud icon), lower latency but no CIS protection Related variables : - cis_enhanced_security : When true, proxy is typically enabled - cis_waf : Requires proxy mode to function Note : Proxy mode is required for WAF and other CIS security features to work. Consider the latency vs. security tradeoff for your use case. cis_service_name \u00a4 Custom name for the CIS service resources created in the cluster. Optional Environment Variable: CIS_SERVICE_NAME Default: {ClusterName}-cis-{mas_instance_id} (auto-generated) Purpose : Allows customization of the CIS service name used for resources created in the OpenShift cluster. The default name is automatically generated from the cluster name and MAS instance ID. When to use : - Leave unset to use the auto-generated name (recommended) - Set only when you need a specific naming convention - Set when the auto-generated name conflicts with existing resources Valid values : Valid Kubernetes resource name (lowercase alphanumeric with hyphens) Impact : This name is used for CIS-related resources in the cluster. Changing it after initial deployment may cause issues with resource management. Related variables : Works with mas_instance_id in the default naming pattern. Note : The default auto-generated name is usually sufficient. Only override if you have specific naming requirements or conflicts. - Default: None update_dns \u00a4 Set this to false if you want to not update DNS entries if they already exist. Optional Environment Variable: UPDATE_DNS_ENTRIES Default: true delete_wildcards \u00a4 Set this to true to force deletion of wildcard dns entries in cis. Optional Environment Variable: DELETE_WILDCARDS Default: false override_edge_certs \u00a4 Set this to false to not override and delete any existing edge certificates in cis instance when creating new edge certificates. Optional Environment Variable: OVERRIDE_EDGE_CERTS Default: true cis_entries_to_add \u00a4 Comma separated list of entries to add for edge certificates. These are broken down into functional areas of MAS. The options are: all (include all entries - default), core (MAS Core), health (MAS Health App), iot (MAS IoT app), manage (MAS Manage app), monitor (MAS Monitor app), predict (MAS Predict app), visualinspection (MAS VisualInspection app), optimizer (MAS Optimizer app), assist (MAS Assist app), arcgis (MAS Arcgis), reportdb (MAS ReportDB), facilities (MAS Facilities app). Optional Environment Variable: CIS_ENTRIES_TO_ADD Default: all AWS Route 53 \u00a4 Prerequisites: To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Note In some cases, the Route 53 zone may not resolve the certificate challenges generated by IBM Certificate Manager, which could cause a problem while issuing the public certificates via Let's Encrypt. In this case, a manual workaround might be needed in cert-manager-controller pod to enable recursive nameservers. For more details on how to apply this workaround, refer to this documentation . This workaround is not automated, as most of the times this is not needed, but when it is, it requires the cert-manager-controller pod to be stopped, thus we want to avoid making this behavior as standard approach. route53_hosted_zone_name \u00a4 AWS Route53 Hosted Zone name. Required if dns_provider is set to route53 Environment Variable: ROUTE53_HOSTED_ZONE_NAME Default: None route53_hosted_zone_region \u00a4 AWS Route53 Hosted Zone region. Optional Environment Variable: ROUTE53_HOSTED_ZONE_REGION Default: Same value as defined in AWS_REGION , or if none defined, then us-east-2 is the defaulted region route53_subdomain \u00a4 If a subdomain is defined, this will be used to create the corresponding CNAME entries in the targeted Route53 hosted zone instance. Therefore, the Route53 subdomain + the Route53 hosted zone name defined, when combined, needs to match with the chosen MAS Domain, otherwise the DNS records won't be able to get resolved. Example: MAS Top Level Domain my-mas-instance.mycompany.com , AWS Route53 hosted zone name mycompany.com , AWS Route53 subdomain my-mas-instance . Optional Environment Variable: ROUTE53_SUBDOMAIN Default: None route53_email \u00a4 AWS Route53 contact e-mail. Will be set in the cluster issuer created in order to receive alerts. Optional Environment Variable: ROUTE53_EMAIL Default: None aws_access_key_id \u00a4 AWS access key ID for authentication. Required if dns_provider is set to route53 Environment Variable: AWS_ACCESS_KEY_ID Default: None aws_secret_access_key \u00a4 AWS secret access key for authentication. Required if dns_provider is set to route53 Environment Variable: AWS_SECRET_ACCESS_KEY Default: None CloudPak for Data \u00a4 cpd_instance_namespace \u00a4 Namespace where the Cloud Pak for Data is installed and the cpd route exists. If set, then this role will also attempt to configure public certificates to the CPD route using the DNS provider defined. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default: None cpd_prod_issuer_name \u00a4 Define the certificate issuer responsible for generating the public certificate for your CPD route. If not set, then it will use same issuer set for MAS instance. Optional Environment Variable: CPD_PROD_ISSUER_NAME Default: Same certificate issuer used for MAS instance cpd_custom_domain \u00a4 Define the custom domain for your CPD route. If not set, then it will use same domain set for MAS instance. Optional Environment Variable: CPD_CUSTOM_DOMAIN Default: cp4d.{{ mas_domain }} Example Playbook \u00a4 CIS or Cloudflare \u00a4 - hosts: localhost any_errors_fatal: true vars: dns_provider: cis # or cloudflare mas_instance_id: inst1 mas_domain: mydomain.com cis_crn: xxx cis_apikey: xxx cis_email: xxx roles: - ibm.mas_devops.suite_dns AWS Route 53 \u00a4 - hosts: localhost any_errors_fatal: true vars: dns_provider: route53 mas_instance_id: inst1 mas_domain: inst1.mydomain.com aws_access_key_id: xxx aws_secret_access_key: xxx route53_hosted_zone_name: mydomain.com route53_hosted_zone_region: us-east-2 route53_subdomain: inst1 route53_email: anyemail@test.com roles: - ibm.mas_devops.suite_dns License \u00a4 EPL-2.0","title":"suite_dns"},{"location":"roles/suite_dns/#suite_dns","text":"This role will manage MAS and DNS provider integration. IBM Cloud Internet Services, Cloudflare, and AWS Route 53 are the supported DNS providers. It will also create a secure route (https://cp4d. ) to the CP4D web client using the custom domain used in this role. Note : this role will take no action when mas_manual_cert_mgmt is set to True","title":"suite_dns"},{"location":"roles/suite_dns/#dns-management","text":"There are two different ways this role controls DNS entries in the provider:","title":"DNS Management"},{"location":"roles/suite_dns/#top-level-dns-entries","text":"This mode will create the entries directly under your DNS zone. Use this when the DNS zone matches the MAS domain exactly. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mymas.mycompany.com then you will be creating top-level DNS entries for MAS, e.g. admin , home , & api .","title":"Top Level DNS Entries"},{"location":"roles/suite_dns/#subdomain-dns-entries","text":"This mode will create DNS entries in the zone under a subdomain. Use this when your DNS zone will be used for more than just one MAS instance. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mycompany.com then you will be creating subdomain DNS entries for MAS, e.g. admin.mymas , home.mymas , & api.mymas . CIS and Cloudflare integrations support both modes of DNS management. A single optional variable is required to enable subdomain DNS management, in the examples above you would set these to mymas : cis_subdomain cloudflare_subdomain","title":"Subdomain DNS Entries"},{"location":"roles/suite_dns/#lets-encrypt-integration","text":"Both the CIS and Cloudflare options also enable integration with Let's Encrypt for automatic certificate management via IBM Certificate Manager. Each will create a new ClusterIssuer which can be used when installing Maximo Application Suite: Cloudflare Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cloudflare-le-prod IBM Cloud Internet Services Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cis-le-prod If you want to use Let's Encrypt certificates in your MAS installation you will need to configure the mas_cluster_issuer variable in the suite_install role, setting it to the name of the ClusterIssuer as documented above. Note There are issues with how cert-manager works with LetsEncrypt staging servers. It creates a secret for the certificate that doesn't contain the LetsEncrypt CA, but the staging service does not use a well known cert so we end up with MAS unable to trust the certificates generated by LetsEncrypt staging. At present there is no workaround for this, so do not use the LetsEncrypt staging certificate issuer.","title":"Let's Encrypt Integration"},{"location":"roles/suite_dns/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_dns/#general","text":"","title":"General"},{"location":"roles/suite_dns/#dns_provider","text":"Specifies which DNS provider to use for managing MAS domain DNS entries. Required Environment Variable: DNS_PROVIDER Default: None Purpose : Determines which DNS service will be used to create and manage DNS records for MAS routes and endpoints. Different providers offer different features and integration capabilities. When to use : - Set to cloudflare when using Cloudflare DNS service - Set to cis when using IBM Cloud Internet Services - Set to route53 when using AWS Route 53 - Choose based on your organization's DNS infrastructure Valid values : cis , cloudflare , route53 Impact : The selected provider determines which additional variables are required and which features are available (e.g., Let's Encrypt integration is available for CIS and Cloudflare but not Route53). Related variables : - When cloudflare : Requires cloudflare_email , cloudflare_apitoken , cloudflare_zone - When cis : Requires cis_email , cis_apikey , cis_crn - When route53 : Requires AWS Route53 specific variables Note : This role takes no action when mas_manual_cert_mgmt is set to True . CIS and Cloudflare support Let's Encrypt integration for automatic certificate management.","title":"dns_provider"},{"location":"roles/suite_dns/#mas_instance_id","text":"Unique identifier for the MAS instance requiring DNS configuration. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to configure DNS entries for. This ID is used to generate DNS record names and ClusterIssuer resources specific to this MAS installation. When to use : - Always required for DNS configuration - Must match the instance ID used during MAS core installation - Used to create unique ClusterIssuer names for Let's Encrypt integration Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : This ID is embedded in DNS record names and ClusterIssuer names (e.g., {mas_instance_id}-cloudflare-le-prod ). Incorrect ID will create DNS entries for the wrong instance. Related variables : Works with mas_domain to construct full DNS names for MAS routes. Note : Must match the instance ID from MAS core installation. Used in ClusterIssuer names for Let's Encrypt integration.","title":"mas_instance_id"},{"location":"roles/suite_dns/#mas_workspace_id","text":"Workspace identifier for the MAS installation requiring DNS configuration. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which MAS workspace to configure DNS entries for. Workspaces are logical divisions within a MAS instance that can have separate configurations and applications. When to use : - Always required for DNS configuration - Must match a workspace ID configured in your MAS instance - Typically set to the primary workspace ID (e.g., masdev , prod ) Valid values : Lowercase alphanumeric string (e.g., masdev , prod , ws1 ) Impact : This ID is used to generate workspace-specific DNS entries and routes. Incorrect workspace ID may result in DNS entries that don't match your MAS workspace configuration. Related variables : Works with mas_instance_id and mas_domain to construct workspace-specific DNS names. Note : Must match an existing workspace in your MAS instance. Each workspace can have its own DNS configuration.","title":"mas_workspace_id"},{"location":"roles/suite_dns/#mas_domain","text":"Custom domain name for accessing MAS web interfaces and APIs. Required Environment Variable: MAS_DOMAIN Default: None Purpose : Defines the base domain used to construct all MAS URLs and DNS entries (e.g., admin.{mas_domain}, home.{mas_domain}, api.{mas_domain}). This domain must be managed by your chosen DNS provider. When to use : - Always required for DNS configuration - Must match the domain configured in your DNS provider's zone - Should align with your organization's domain naming conventions - Must be the same domain used in MAS core installation Valid values : Any valid DNS domain name (e.g., mas.mycompany.com , prod-mas.example.org ) Impact : DNS entries will be created for this domain in your DNS provider. The domain must be properly configured in your DNS provider's zone. All MAS routes will use this domain. Related variables : - Must align with DNS provider zone configuration ( cloudflare_zone , cis_crn , etc.) - Works with mas_instance_id to create unique DNS entries - May use subdomain mode with cloudflare_subdomain or cis_subdomain Note : Ensure this domain is properly configured in your DNS provider before running this role. DNS propagation may take time after entries are created.","title":"mas_domain"},{"location":"roles/suite_dns/#ocp_ingress","text":"Override for the OpenShift cluster ingress domain used in DNS entries. Optional Environment Variable: OCP_INGRESS Default: None (automatically detected from cluster) Purpose : Allows manual specification of the OpenShift cluster's ingress domain when automatic detection fails or when you need to use a specific ingress controller. The ingress domain is used as the target for DNS CNAME records. When to use : - Leave unset to allow automatic detection (recommended) - Set only when automatic detection fails - Set when using a non-default ingress controller - Set in airgap or restricted network environments where detection may not work Valid values : Valid OpenShift ingress domain (e.g., apps.cluster-name.domain.com ) Impact : When set, this value is used as the target for DNS CNAME records instead of the automatically detected ingress. Incorrect value will cause DNS entries to point to the wrong location. Related variables : Works with DNS provider variables to create CNAME records pointing to this ingress. Note : Automatic detection is usually sufficient. Only override if you encounter issues or have specific ingress requirements.","title":"ocp_ingress"},{"location":"roles/suite_dns/#cert_manager_namespace","text":"OpenShift namespace where Certificate Manager is installed. Optional Environment Variable: CERT_MANAGER_NAMESPACE Default: None (automatically detected) Purpose : Specifies the namespace where Certificate Manager is deployed. This is needed to create ClusterIssuer resources for Let's Encrypt integration in the correct location. When to use : - Leave unset to allow automatic detection (recommended) - Set only if Certificate Manager is installed in a non-standard namespace - Set if automatic detection fails Valid values : Any valid Kubernetes namespace name where Certificate Manager is installed Impact : ClusterIssuer resources for Let's Encrypt will be created referencing this namespace. Incorrect namespace will cause ClusterIssuer creation to fail. Related variables : Used when creating ClusterIssuer for Cloudflare or CIS Let's Encrypt integration. Note : Automatic detection typically finds Certificate Manager in standard namespaces. Only override if using a custom installation.","title":"cert_manager_namespace"},{"location":"roles/suite_dns/#custom_labels","text":"Comma-separated list of key=value labels to apply to DNS-related resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to DNS-related resources for organization, selection, and filtering. Labels enable resource tracking, cost allocation, and custom automation. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,component=dns ) Impact : Labels are applied to DNS-related resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect DNS functionality. Related variables : Works alongside Kubernetes resource labels for comprehensive resource management.","title":"custom_labels"},{"location":"roles/suite_dns/#mas_manual_cert_mgmt","text":"Controls whether to disable automatic DNS and certificate management. Optional Environment Variable: MAS_MANUAL_CERT_MGMT Default: false Purpose : Allows you to opt out of automatic DNS entry creation and certificate management when you prefer to manage these manually or through alternative methods. When to use : - Set to true when managing DNS entries manually - Set to true when using custom certificate management solutions - Set to true when DNS provider integration is not available - Leave as false (default) for automated DNS and certificate management Valid values : true , false Impact : - When true : This role takes no action; DNS entries and certificates must be managed manually - When false : Role automatically creates DNS entries and optionally configures Let's Encrypt integration Related variables : When true , all other DNS provider variables are ignored. Note : Setting to true means you are responsible for creating all required DNS entries and managing certificates manually. Ensure proper DNS configuration before MAS installation.","title":"mas_manual_cert_mgmt"},{"location":"roles/suite_dns/#output_dir","text":"Local directory path where the edge routes output file will be saved. Optional Environment Variable: OUTPUT_DIR Default: . (current directory) Purpose : Specifies where to save the edge-routes-{mas_instance_id}.txt file containing information about the DNS entries and routes created by this role. This file is useful for verification and troubleshooting. When to use : - Set to a specific directory for organized output file management - Use default (current directory) for simple deployments - Set to a shared location for team access to route information Valid values : Any valid local filesystem path (e.g., /home/user/mas-output , ~/masconfig , ./output ) Impact : The edge routes file will be created in this directory. The file contains DNS entry details and can be used to verify DNS configuration. Related variables : Output filename includes mas_instance_id for identification. Note : Ensure the directory exists and is writable. The file provides useful information for verifying DNS setup and troubleshooting connectivity issues.","title":"output_dir"},{"location":"roles/suite_dns/#cloudflare-dns-integration","text":"","title":"Cloudflare DNS Integration"},{"location":"roles/suite_dns/#cloudflare_email","text":"Email address associated with your Cloudflare account. Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_EMAIL Default: None Purpose : Provides the email address for authenticating with Cloudflare API. This email must be associated with a Cloudflare account that has access to manage the DNS zone. When to use : - Required when dns_provider=cloudflare - Must be the email address used to log into Cloudflare - Account must have permissions to manage DNS records in the target zone Valid values : Valid email address associated with a Cloudflare account Impact : Used together with cloudflare_apitoken to authenticate API requests. Incorrect email will cause authentication failures. Related variables : - cloudflare_apitoken : Must be set together with this email - cloudflare_zone : Zone that this account has access to manage Note : Ensure the Cloudflare account has appropriate permissions to create and manage DNS records in the target zone.","title":"cloudflare_email"},{"location":"roles/suite_dns/#cloudflare_apitoken","text":"API token for authenticating with Cloudflare API. Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_APITOKEN Default: None Purpose : Provides the API token credential for authenticating with Cloudflare to create and manage DNS records. This token must have appropriate permissions for DNS management. When to use : - Required when dns_provider=cloudflare - Generate from Cloudflare dashboard with DNS edit permissions - Token must have access to the target DNS zone Valid values : Valid Cloudflare API token string Impact : Used together with cloudflare_email to authenticate API requests. Invalid or insufficient permissions will cause DNS operations to fail. Related variables : - cloudflare_email : Must be set together with this token - cloudflare_zone : Zone that this token has permissions to manage Note : Generate the API token following the Cloudflare documentation . Ensure the token has DNS edit permissions for the target zone. Keep this token secure and do not commit to source control.","title":"cloudflare_apitoken"},{"location":"roles/suite_dns/#cloudflare_zone","text":"DNS zone name managed by Cloudflare where MAS DNS entries will be created. Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_ZONE Default: None Purpose : Specifies the Cloudflare DNS zone (domain) where MAS DNS records will be created. This must be a domain that is already configured and active in your Cloudflare account. When to use : - Required when dns_provider=cloudflare - Must match a zone configured in your Cloudflare account - Should be the parent domain of mas_domain (or equal to it for top-level entries) Valid values : Valid domain name managed in Cloudflare (e.g., mydomain.com , example.org ) Impact : DNS entries will be created in this zone. The zone must exist in Cloudflare and be accessible with the provided credentials. Incorrect zone will cause DNS operations to fail. Related variables : - mas_domain : Should be within this zone (e.g., if zone is mydomain.com , mas_domain could be mas.mydomain.com ) - cloudflare_subdomain : Optional subdomain within this zone for MAS entries - cloudflare_email and cloudflare_apitoken : Must have access to this zone Note : Ensure the zone is active in Cloudflare and DNS is properly delegated before running this role.","title":"cloudflare_zone"},{"location":"roles/suite_dns/#cloudflare_subdomain","text":"Subdomain within the Cloudflare zone for MAS DNS entries (subdomain mode). Optional Environment Variable: CLOUDFLARE_SUBDOMAIN Default: None (uses top-level DNS entries) Purpose : Enables subdomain mode for DNS management, where MAS DNS entries are created under a subdomain rather than at the top level of the zone. This allows multiple MAS instances or other services to share the same DNS zone. When to use : - Use when your DNS zone will host multiple MAS instances or other services - Use when mas_domain is a subdomain of cloudflare_zone - Leave unset when mas_domain equals cloudflare_zone (top-level mode) Valid values : Subdomain name (e.g., if cloudflare_zone=mycompany.com and mas_domain=mas.mycompany.com , set to mas ) Impact : - When set: DNS entries created as admin.{cloudflare_subdomain} , home.{cloudflare_subdomain} , etc. - When unset: DNS entries created as admin , home , etc. directly in the zone Related variables : - cloudflare_zone : Parent zone for this subdomain - mas_domain : Should equal {cloudflare_subdomain}.{cloudflare_zone} Note : The relationship must be: cloudflare_subdomain.cloudflare_zone = mas_domain . For example, if zone is mycompany.com and mas_domain is mas.mycompany.com , set subdomain to mas .","title":"cloudflare_subdomain"},{"location":"roles/suite_dns/#ibm-cloud-internet-services-dns-integration","text":"Note When using CIS integration, some resources will be installed in the cluster such as RBACs, API Services and CIS Webhook deployments. In OCP 4.12+, to avoid CIS webhook deployment failure at start up, this role will grant anyuid permission to cert-manager-webhook-ibm-cis service account so it can fully access the cert-manager-webhook-ibm-cis deployment pod as a workaround: oc adm policy add-scc-to-user anyuid -z cert-manager-webhook-ibm-cis -n ibm-common-services","title":"IBM Cloud Internet Services DNS Integration"},{"location":"roles/suite_dns/#cis_email","text":"Email address for Let's Encrypt ClusterIssuer registration when using IBM CIS. Required if dns_provider is set to cis Environment Variable: CIS_EMAIL Default: None Purpose : Provides the email address that will be registered with Let's Encrypt when creating the ClusterIssuer for automatic certificate management. Let's Encrypt uses this email for certificate expiration notifications and account recovery. When to use : - Required when dns_provider=cis - Use a monitored email address to receive Let's Encrypt notifications - Typically use a team or service email rather than personal email Valid values : Valid email address Impact : This email is embedded in the ClusterIssuer resource and registered with Let's Encrypt. You'll receive notifications about certificate renewals and any issues at this address. Related variables : - cis_apikey and cis_crn : Required together for CIS integration - Used in ClusterIssuer: {mas_instance_id}-cis-le-prod Note : Use a monitored email address as Let's Encrypt sends important notifications about certificate expiration and renewal issues.","title":"cis_email"},{"location":"roles/suite_dns/#cis_apikey","text":"IBM Cloud API key for authenticating with IBM Cloud Internet Services. Required if dns_provider is set to cis Environment Variable: CIS_APIKEY Default: None Purpose : Provides the IBM Cloud API key credential for authenticating with IBM Cloud Internet Services to create and manage DNS records and configure Let's Encrypt integration. When to use : - Required when dns_provider=cis - Generate from IBM Cloud IAM with appropriate CIS permissions - Key must have access to the CIS instance specified by cis_crn Valid values : Valid IBM Cloud API key string Impact : Used to authenticate all CIS API operations including DNS record management and ClusterIssuer configuration. Invalid key or insufficient permissions will cause operations to fail. Related variables : - cis_crn : CIS instance that this API key has access to - cis_email : Email for Let's Encrypt registration Note : Generate the API key from IBM Cloud IAM. Ensure it has appropriate permissions for the CIS instance. Keep this key secure and do not commit to source control.","title":"cis_apikey"},{"location":"roles/suite_dns/#cis_crn","text":"Cloud Resource Name (CRN) identifying the IBM Cloud Internet Services instance. Required if dns_provider is set to cis Environment Variable: CIS_CRN Default: None Purpose : Uniquely identifies the specific IBM Cloud Internet Services instance where DNS records will be managed. The CRN ensures operations target the correct CIS instance when multiple instances exist. When to use : - Required when dns_provider=cis - Obtain from IBM Cloud CIS instance details - Must correspond to the CIS instance containing your DNS zone Valid values : Valid IBM Cloud CRN string (format: crn:v1:bluemix:public:internet-svcs:... ) Impact : All DNS operations will target this CIS instance. Incorrect CRN will cause operations to fail or target the wrong instance. Related variables : - cis_apikey : Must have permissions for this CIS instance - DNS zone must be configured in this CIS instance Note : Find the CRN in the IBM Cloud console under your CIS instance details. Ensure the CRN matches the instance containing your target DNS zone.","title":"cis_crn"},{"location":"roles/suite_dns/#cis_subdomain","text":"Subdomain within the CIS zone for MAS DNS entries (subdomain mode). Optional Environment Variable: CIS_SUBDOMAIN Default: None (uses top-level DNS entries) Purpose : Enables subdomain mode for DNS management, where MAS DNS entries are created under a subdomain rather than at the top level of the CIS zone. This allows multiple MAS instances or other services to share the same DNS zone. When to use : - Use when your CIS zone will host multiple MAS instances or other services - Use when mas_domain is a subdomain of your CIS zone domain - Leave unset when mas_domain equals the CIS zone domain (top-level mode) Valid values : Subdomain name (e.g., if CIS zone is mycompany.com and mas_domain=mas.mycompany.com , set to mas ) Impact : - When set: DNS entries created as admin.{cis_subdomain} , home.{cis_subdomain} , etc. - When unset: DNS entries created as admin , home , etc. directly in the zone Related variables : - CIS zone domain: Parent zone for this subdomain - mas_domain : Should equal {cis_subdomain}.{cis_zone_domain} Note : The relationship must be: cis_subdomain.{cis_zone_domain} = mas_domain . For example, if CIS zone is mycompany.com and mas_domain is mas.mycompany.com , set subdomain to mas .","title":"cis_subdomain"},{"location":"roles/suite_dns/#cis_enhanced_security","text":"Enables enhanced security features for IBM CIS DNS integration. Optional Environment Variable: CIS_ENHANCED_SECURITY Default: false Purpose : Activates a comprehensive set of security enhancements for CIS including WAF configuration, proxy mode, expanded DNS entries, wildcard prevention, and edge certificates. This provides optimal security for MAS deployments using CIS. When to use : - Set to true for production environments requiring enhanced security - Set to true when security compliance requires WAF and proxy protection - Leave as false for development/test environments or when enhanced features aren't needed Valid values : true , false Impact : When enabled, this configures: - WAF firewall with rules optimized for MAS - Proxy mode for DNS entries (traffic routed through CIS) - Expanded list of DNS entries for comprehensive coverage - Prevention of wildcard DNS entries - Edge certificates in CIS instance Related variables : - cis_waf : Controls WAF specifically (when enhanced_security is true) - cis_proxy : Controls proxy mode (when enhanced_security is true) Note : See IBM Cloud CIS security documentation for details. Enhanced security may impact performance due to proxy routing.","title":"cis_enhanced_security"},{"location":"roles/suite_dns/#enhanced-ibm-cis-dns-integration-security","text":"See the cis_enhanced_security variable above for details.","title":"Enhanced IBM CIS DNS Integration Security"},{"location":"roles/suite_dns/#cis_waf","text":"Controls Web Application Firewall (WAF) configuration for CIS. Optional Environment Variable: CIS_WAF Default: true Purpose : Enables or disables WAF configuration with rules optimized for MAS application functionality. WAF provides protection against common web attacks while ensuring MAS features work correctly. When to use : - Leave as true (default) for production environments requiring WAF protection - Set to false only if WAF causes issues or conflicts with other security tools - Typically used in conjunction with cis_enhanced_security Valid values : true , false Impact : - When true : Configures WAF with rules that protect MAS while allowing required functionality - When false : WAF is not configured (less security protection) Related variables : - cis_enhanced_security : When true, this setting is part of enhanced security features - cis_proxy : Often used together for comprehensive security Note : WAF rules are specifically tuned to avoid blocking legitimate MAS application traffic. Default is true for security best practices.","title":"cis_waf"},{"location":"roles/suite_dns/#cis_proxy","text":"Controls whether DNS entries use CIS proxy mode (traffic routed through CIS). Optional Environment Variable: CIS_PROXY Default: false Purpose : Enables proxy mode for DNS entries, routing traffic through IBM CIS infrastructure. This provides additional security, DDoS protection, and performance optimization but adds latency. When to use : - Set to true for production environments requiring DDoS protection - Set to true when using CIS security features (WAF, rate limiting) - Set to false for direct routing (lower latency, less protection) - Typically enabled with cis_enhanced_security Valid values : true , false Impact : - When true : Traffic routes through CIS (orange cloud icon in CIS console), enabling security features but adding latency - When false : DNS entries point directly to OpenShift ingress (grey cloud icon), lower latency but no CIS protection Related variables : - cis_enhanced_security : When true, proxy is typically enabled - cis_waf : Requires proxy mode to function Note : Proxy mode is required for WAF and other CIS security features to work. Consider the latency vs. security tradeoff for your use case.","title":"cis_proxy"},{"location":"roles/suite_dns/#cis_service_name","text":"Custom name for the CIS service resources created in the cluster. Optional Environment Variable: CIS_SERVICE_NAME Default: {ClusterName}-cis-{mas_instance_id} (auto-generated) Purpose : Allows customization of the CIS service name used for resources created in the OpenShift cluster. The default name is automatically generated from the cluster name and MAS instance ID. When to use : - Leave unset to use the auto-generated name (recommended) - Set only when you need a specific naming convention - Set when the auto-generated name conflicts with existing resources Valid values : Valid Kubernetes resource name (lowercase alphanumeric with hyphens) Impact : This name is used for CIS-related resources in the cluster. Changing it after initial deployment may cause issues with resource management. Related variables : Works with mas_instance_id in the default naming pattern. Note : The default auto-generated name is usually sufficient. Only override if you have specific naming requirements or conflicts. - Default: None","title":"cis_service_name"},{"location":"roles/suite_dns/#update_dns","text":"Set this to false if you want to not update DNS entries if they already exist. Optional Environment Variable: UPDATE_DNS_ENTRIES Default: true","title":"update_dns"},{"location":"roles/suite_dns/#delete_wildcards","text":"Set this to true to force deletion of wildcard dns entries in cis. Optional Environment Variable: DELETE_WILDCARDS Default: false","title":"delete_wildcards"},{"location":"roles/suite_dns/#override_edge_certs","text":"Set this to false to not override and delete any existing edge certificates in cis instance when creating new edge certificates. Optional Environment Variable: OVERRIDE_EDGE_CERTS Default: true","title":"override_edge_certs"},{"location":"roles/suite_dns/#cis_entries_to_add","text":"Comma separated list of entries to add for edge certificates. These are broken down into functional areas of MAS. The options are: all (include all entries - default), core (MAS Core), health (MAS Health App), iot (MAS IoT app), manage (MAS Manage app), monitor (MAS Monitor app), predict (MAS Predict app), visualinspection (MAS VisualInspection app), optimizer (MAS Optimizer app), assist (MAS Assist app), arcgis (MAS Arcgis), reportdb (MAS ReportDB), facilities (MAS Facilities app). Optional Environment Variable: CIS_ENTRIES_TO_ADD Default: all","title":"cis_entries_to_add"},{"location":"roles/suite_dns/#aws-route-53","text":"Prerequisites: To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Note In some cases, the Route 53 zone may not resolve the certificate challenges generated by IBM Certificate Manager, which could cause a problem while issuing the public certificates via Let's Encrypt. In this case, a manual workaround might be needed in cert-manager-controller pod to enable recursive nameservers. For more details on how to apply this workaround, refer to this documentation . This workaround is not automated, as most of the times this is not needed, but when it is, it requires the cert-manager-controller pod to be stopped, thus we want to avoid making this behavior as standard approach.","title":"AWS Route 53"},{"location":"roles/suite_dns/#route53_hosted_zone_name","text":"AWS Route53 Hosted Zone name. Required if dns_provider is set to route53 Environment Variable: ROUTE53_HOSTED_ZONE_NAME Default: None","title":"route53_hosted_zone_name"},{"location":"roles/suite_dns/#route53_hosted_zone_region","text":"AWS Route53 Hosted Zone region. Optional Environment Variable: ROUTE53_HOSTED_ZONE_REGION Default: Same value as defined in AWS_REGION , or if none defined, then us-east-2 is the defaulted region","title":"route53_hosted_zone_region"},{"location":"roles/suite_dns/#route53_subdomain","text":"If a subdomain is defined, this will be used to create the corresponding CNAME entries in the targeted Route53 hosted zone instance. Therefore, the Route53 subdomain + the Route53 hosted zone name defined, when combined, needs to match with the chosen MAS Domain, otherwise the DNS records won't be able to get resolved. Example: MAS Top Level Domain my-mas-instance.mycompany.com , AWS Route53 hosted zone name mycompany.com , AWS Route53 subdomain my-mas-instance . Optional Environment Variable: ROUTE53_SUBDOMAIN Default: None","title":"route53_subdomain"},{"location":"roles/suite_dns/#route53_email","text":"AWS Route53 contact e-mail. Will be set in the cluster issuer created in order to receive alerts. Optional Environment Variable: ROUTE53_EMAIL Default: None","title":"route53_email"},{"location":"roles/suite_dns/#aws_access_key_id","text":"AWS access key ID for authentication. Required if dns_provider is set to route53 Environment Variable: AWS_ACCESS_KEY_ID Default: None","title":"aws_access_key_id"},{"location":"roles/suite_dns/#aws_secret_access_key","text":"AWS secret access key for authentication. Required if dns_provider is set to route53 Environment Variable: AWS_SECRET_ACCESS_KEY Default: None","title":"aws_secret_access_key"},{"location":"roles/suite_dns/#cloudpak-for-data","text":"","title":"CloudPak for Data"},{"location":"roles/suite_dns/#cpd_instance_namespace","text":"Namespace where the Cloud Pak for Data is installed and the cpd route exists. If set, then this role will also attempt to configure public certificates to the CPD route using the DNS provider defined. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default: None","title":"cpd_instance_namespace"},{"location":"roles/suite_dns/#cpd_prod_issuer_name","text":"Define the certificate issuer responsible for generating the public certificate for your CPD route. If not set, then it will use same issuer set for MAS instance. Optional Environment Variable: CPD_PROD_ISSUER_NAME Default: Same certificate issuer used for MAS instance","title":"cpd_prod_issuer_name"},{"location":"roles/suite_dns/#cpd_custom_domain","text":"Define the custom domain for your CPD route. If not set, then it will use same domain set for MAS instance. Optional Environment Variable: CPD_CUSTOM_DOMAIN Default: cp4d.{{ mas_domain }}","title":"cpd_custom_domain"},{"location":"roles/suite_dns/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_dns/#cis-or-cloudflare","text":"- hosts: localhost any_errors_fatal: true vars: dns_provider: cis # or cloudflare mas_instance_id: inst1 mas_domain: mydomain.com cis_crn: xxx cis_apikey: xxx cis_email: xxx roles: - ibm.mas_devops.suite_dns","title":"CIS or Cloudflare"},{"location":"roles/suite_dns/#aws-route-53_1","text":"- hosts: localhost any_errors_fatal: true vars: dns_provider: route53 mas_instance_id: inst1 mas_domain: inst1.mydomain.com aws_access_key_id: xxx aws_secret_access_key: xxx route53_hosted_zone_name: mydomain.com route53_hosted_zone_region: us-east-2 route53_subdomain: inst1 route53_email: anyemail@test.com roles: - ibm.mas_devops.suite_dns","title":"AWS Route 53"},{"location":"roles/suite_dns/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_install/","text":"suite_install \u00a4 This role installs Maximo Application Suite. It internally resolves the namespace based on the mas_instance_id as mas-{mas_instance_id}-core . Role Variables \u00a4 Basic Install \u00a4 mas_catalog_source \u00a4 Specifies the OpenShift operator catalog source containing the MAS operator subscription. Optional Environment Variable: MAS_CATALOG_SOURCE Default: ibm-operator-catalog Purpose : Controls which operator catalog is used to locate and install the MAS operator. This determines where OpenShift looks for the operator images and metadata during installation. When to use : - Use default ibm-operator-catalog for production installations with official IBM releases - Use ibm-operator-catalog for development installations as well (supports both use cases) - Only change if directed to use a custom catalog for specific testing or airgap scenarios Valid values : Any valid CatalogSource name present in the openshift-marketplace namespace (typically ibm-operator-catalog ) Impact : Changing this value affects which operator versions are available for installation. An invalid catalog source will cause the subscription to fail. Related variables : Works with mas_channel to determine the specific operator version installed. mas_channel \u00a4 Specifies the MAS operator subscription channel, which determines the version stream you'll receive updates from. Required Environment Variable: MAS_CHANNEL Default: None Purpose : Controls which version of MAS will be installed and which updates will be automatically applied. The channel corresponds to major.minor version releases (e.g., 8.11.x , 9.0.x ) and determines the feature set and compatibility level of your MAS installation. When to use : - Set to the latest stable channel for new production deployments to receive the newest features - Use specific older channels when compatibility with existing applications or dependencies requires it - Consult the MAS compatibility matrix before selecting a channel to ensure compatibility with your applications - Change channels only during planned upgrade windows as this triggers version updates Valid values : 8.9.x , 8.10.x , 8.11.x , 9.0.x (check the IBM Operator Catalog for currently available channels) Impact : The channel determines which MAS version is installed and which automatic updates are applied. Changing channels after installation will trigger an upgrade to the latest version in that channel, which may require application reconfiguration and testing. Related variables : Works with mas_catalog_source to determine available channels. Note : Once installed, changing channels requires careful planning. Review the MAS upgrade documentation before changing this value. Basic Configuration \u00a4 mas_domain \u00a4 Specifies the custom domain name for accessing MAS web interfaces and APIs. Optional Environment Variable: MAS_DOMAIN Default: None (uses cluster default subdomain) Purpose : Defines the base domain used to construct all MAS URLs (e.g., admin.{mas_domain}, home.{mas_domain}). This allows you to use a custom domain instead of the default OpenShift cluster subdomain, which is important for production environments with specific DNS requirements. When to use : - Set for production environments where you need branded or corporate domain names - Set when integrating with external DNS providers (Cloudflare, Route53, IBM CIS) - Set when using custom SSL certificates tied to specific domains - Leave unset for development/testing to use the default cluster subdomain automatically Valid values : Any valid DNS domain name (e.g., mas.mycompany.com , prod-mas.example.org ) Impact : When set, MAS will use this domain for all routes and certificates. You must ensure DNS is properly configured to resolve this domain to your cluster. When not set, MAS automatically uses the cluster's default ingress subdomain. Related variables : - Used by suite_dns role to configure DNS entries - Affects certificate generation when using mas_cluster_issuer - Must align with mas_routing_mode configuration mas_instance_id \u00a4 Unique identifier for this MAS installation within the cluster. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Provides a unique identifier that distinguishes this MAS installation from others that may exist in the same cluster. This ID is used to generate namespace names, resource names, and configuration identifiers throughout the installation. When to use : - Always required for any MAS installation - Use short, descriptive names (e.g., prod , dev , test , inst1 ) - Must be unique within the cluster if running multiple MAS instances - Cannot be changed after installation without reinstalling Valid values : Lowercase alphanumeric string, 3-12 characters, starting with a letter (e.g., prod , dev01 , mastest ) Impact : This ID becomes part of the core namespace name ( mas-{mas_instance_id}-core ) and is embedded in many resource names. It cannot be changed after installation. All MAS configurations and applications will reference this instance ID. Related variables : - Used by all MAS configuration roles to target the correct instance - Referenced in application installation roles - Used in backup/restore operations to identify the instance Note : Choose carefully as this cannot be changed after installation. Use consistent naming across environments (e.g., dev , test , prod ). mas_entitlement_key \u00a4 IBM entitlement key for authenticating access to IBM Container Registry. Required Environment Variable: MAS_ENTITLEMENT_KEY Default: Value of IBM_ENTITLEMENT_KEY if set Purpose : Provides authentication credentials to pull MAS container images from IBM's entitled registry. This key is tied to your IBM Cloud account and product entitlements, proving you have the right to use MAS software. When to use : - Required for all production installations using official IBM releases - Obtain from IBM Container Library at https://myibm.ibm.com/products-services/containerlibrary - For development builds on Artifactory, use your Artifactory API key instead - Key must be valid and have active MAS entitlements Valid values : - IBM entitlement key string (typically starts with \"eyJ...\") - Must be a valid, non-expired key with MAS product entitlements - For development: Artifactory API key Impact : Invalid or expired keys will cause image pull failures during installation. The key is stored in a Kubernetes secret and used to create image pull secrets for all MAS pods. Without a valid key, the installation cannot proceed. Related variables : - Falls back to IBM_ENTITLEMENT_KEY environment variable if not set - Used with mas_entitlement_username for registry authentication - Related to mas_icr_cp and mas_icr_cpopen registry settings Note : Keep this key secure. Do not commit it to source control. Use environment variables or secure secret management. mas_config_dir \u00a4 Local directory path containing additional Kubernetes configuration files to apply during MAS installation. Optional Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Allows you to provide custom Kubernetes resources (YAML files) that will be automatically applied to the cluster during MAS installation. This enables advanced configuration scenarios and customization beyond the standard role variables. When to use : - Use to apply MAS configuration resources (MongoCfg, BASCfg, JdbcCfg, etc.) generated by other roles - Use to apply custom ConfigMaps, Secrets, or other Kubernetes resources - Use to pre-configure MAS settings before the suite becomes fully operational - Leave unset if you plan to apply configurations manually after installation Valid values : Any valid local filesystem path (e.g., /home/user/masconfig , ~/mas-configs , ./config ) Impact : All *.yaml and *.yml files in this directory will be applied to the cluster using oc apply . Files are processed in alphabetical order. Invalid YAML or resources will cause the role to fail. Related variables : - Used by mongodb , db2 , sls and other dependency roles to output configuration files - Used by suite_config role to apply configurations - Must be set if using generated configuration files from dependency roles Note : Ensure all files in this directory are valid Kubernetes resources. The role does not validate file contents before applying. Advanced Configuration \u00a4 mas_annotations \u00a4 Comma-separated list of key=value annotations to apply to all MAS resources. Optional Environment Variable: MAS_ANNOTATIONS Default: None Purpose : Adds Kubernetes annotations to all resources created by the MAS operator. Annotations are used to attach metadata that can control operator behavior, enable specific features, or provide information to other tools and operators. When to use : - Set to mas.ibm.com/operationalMode=nonproduction for non-production environments (reduces resource requirements) - Use to add custom metadata for organizational tracking or automation - Use to enable specific MAS features controlled by annotations - Leave unset for default production configuration Valid values : Comma-separated list of key=value pairs (e.g., key1=value1,key2=value2 ) Impact : Annotations affect how the MAS operator configures resources. The operationalMode=nonproduction annotation significantly reduces CPU and memory requirements but is not suitable for production workloads. Related variables : Works alongside custom_labels for resource metadata. Note : The operationalMode=nonproduction annotation should only be used in development/test environments. It reduces resource allocations below production requirements. mas_img_pull_policy \u00a4 Controls the image pull policy for all MAS container images. Optional Environment Variable: MAS_IMG_PULL_POLICY Default: None (uses operator default) Purpose : Determines when Kubernetes will pull container images from the registry. This affects deployment speed, network usage, and whether you get the latest image updates. When to use : - Use Always in development to ensure latest images are pulled on every pod restart - Use IfNotPresent in production to reduce network traffic and improve pod startup time - Use Never in airgap environments where images are pre-loaded - Leave unset to use the operator's default policy (typically IfNotPresent ) Valid values : Always , IfNotPresent , Never Impact : - Always : Slower pod starts, higher network usage, always gets latest image - IfNotPresent : Faster pod starts, uses cached images, may miss updates - Never : Fastest starts, requires images pre-loaded, fails if image not present Related variables : Affects all images pulled by MAS operator and workloads. custom_labels \u00a4 Comma-separated list of key=value labels to apply to all MAS resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to all resources created by the MAS operator. Labels are used for resource organization, selection, and filtering. They enable you to query, group, and manage MAS resources using standard Kubernetes tools. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,cost-center=12345 ) Impact : Labels are applied to all MAS resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect MAS functionality but are essential for resource management. Related variables : Works alongside mas_annotations for resource metadata. mas_manual_cert_mgmt \u00a4 Enables manual certificate management mode, disabling automatic certificate generation. Optional Environment Variable: MAS_MANUAL_CERT_MGMT Default: false Purpose : Controls whether MAS uses automatic certificate management via cert-manager or requires manually provided certificates. This is critical for environments with specific certificate requirements or where cert-manager cannot be used. When to use : - Set to true when you must use certificates from a specific Certificate Authority - Set to true in environments where cert-manager is not available or not permitted - Set to true when organizational policy requires manual certificate management - Leave as false (default) to use automatic certificate management with cert-manager Valid values : true , false Impact : When true , you must manually create and manage all certificates required by MAS. The suite_dns role will not configure automatic certificate issuers. You are responsible for certificate renewal before expiration. When false , cert-manager automatically generates and renews certificates. Related variables : - When false : Requires mas_cluster_issuer to be set for automatic certificate generation - Affects behavior of suite_dns role - Related to mas_certificate_duration and mas_certificate_renew_before Note : Manual certificate management requires significant operational overhead. Use automatic management unless you have specific requirements. mas_routing_mode \u00a4 Defines the URL routing strategy for MAS applications and services. Optional Environment Variable: MAS_ROUTING_MODE Default: subdomain Purpose : Controls how MAS constructs URLs for different applications and services. This affects the URL structure users see and how DNS must be configured. When to use : - Use subdomain (default) for cleaner URLs and better DNS organization (e.g., admin.mas.example.com , home.mas.example.com ) - Use path when subdomain routing is not possible due to DNS or certificate limitations (e.g., mas.example.com/admin , mas.example.com/home ) - Consider DNS provider capabilities and certificate management when choosing Valid values : subdomain , path Impact : - subdomain : Creates separate DNS entries for each service (requires wildcard DNS or multiple DNS entries) - path : Uses single DNS entry with path-based routing (simpler DNS, more complex URL structure) - Cannot be changed after installation without reinstalling Related variables : - Affects DNS configuration in suite_dns role - Impacts certificate requirements (wildcard vs single certificate) - Must align with mas_domain configuration Note : Choose carefully as this cannot be changed after installation. Subdomain routing is recommended for production environments. mas_trust_default_cas \u00a4 Controls whether default system Certificate Authorities are included in MAS trust stores. Optional Environment Variable: MAS_TRUST_DEFAULT_CAS Default: true Purpose : Determines if MAS will trust certificates signed by standard public Certificate Authorities (like Let's Encrypt, DigiCert, etc.) in addition to any custom CAs you configure. This affects MAS's ability to connect to external services using standard SSL certificates. When to use : - Leave as true (default) for most installations to enable connections to public services - Set to false only in highly restricted environments where you want to explicitly control all trusted CAs - Set to false in airgap environments where external connections are not permitted Valid values : true , false Impact : When true , MAS can connect to any service using certificates from well-known public CAs. When false , MAS will only trust explicitly configured custom CAs, which may break connections to external services. Related variables : Works with custom CA configuration in MAS. Note : Only available in MAS 8.11 and above. Has no effect in earlier versions. mas_pod_templates_dir \u00a4 Directory containing pod template customization files for MAS workloads. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Allows you to customize resource requests, limits, node selectors, tolerations, and other pod-level configurations for MAS workloads. This enables you to optimize MAS for your specific cluster configuration and resource availability. When to use : - Use to apply resource constraints in resource-limited environments - Use to configure node affinity for specific hardware (e.g., GPU nodes) - Use to apply tolerations for tainted nodes - Use to implement best-effort or guaranteed QoS classes - Leave unset to use default MAS pod configurations Valid values : Path to directory containing pod template YAML files: - ibm-mas-suite.yml - Core suite workloads - ibm-mas-coreidp.yml - Identity provider workloads - ibm-data-dictionary-assetdatadictionary.yml - Data dictionary workloads Impact : Pod templates directly affect resource allocation, scheduling, and performance of MAS workloads. Incorrect configurations can cause pods to fail scheduling or perform poorly. Related variables : Similar pod template configuration available in other roles (SLS, applications). Note : See MAS CLI pod templates for examples and product documentation for full details. Certificate Management \u00a4 mas_cluster_issuer \u00a4 Name of the cert-manager ClusterIssuer to use for automatic certificate generation. Optional Environment Variable: MAS_CLUSTER_ISSUER Default: None Purpose : Specifies which cert-manager ClusterIssuer will generate and manage SSL/TLS certificates for MAS. The ClusterIssuer defines the Certificate Authority and authentication method used for certificate issuance. When to use : - Required when mas_manual_cert_mgmt is false (automatic certificate management) - Set to the ClusterIssuer created by suite_dns role (e.g., {mas_instance_id}-cloudflare-le-prod ) - Set to a custom ClusterIssuer if you have specific certificate requirements - Leave unset only when using manual certificate management Valid values : Name of any valid ClusterIssuer resource in the cluster (e.g., prod-le-issuer , {mas_instance_id}-cloudflare-le-prod ) Impact : The specified ClusterIssuer will be used to generate all MAS certificates. If the ClusterIssuer is not properly configured or lacks necessary credentials, certificate generation will fail and MAS will not be accessible. Related variables : - Only used when mas_manual_cert_mgmt is false - Created by suite_dns role for Let's Encrypt integration - Works with mas_certificate_duration and mas_certificate_renew_before Note : Ensure the ClusterIssuer is created and functional before installing MAS. Test certificate generation with a test Certificate resource first. mas_certificate_duration \u00a4 Specifies the validity period for MAS certificates. Optional Environment Variable: MAS_CERTIFICATE_DURATION Default: 8760h0m0s (1 year) Purpose : Defines how long certificates will be valid before they expire. This affects how often certificates need to be renewed and the security posture of your installation. When to use : - Use default (8760h = 1 year) for most installations - Reduce for higher security environments requiring frequent rotation - Increase only if certificate renewal is problematic in your environment - Must be longer than mas_certificate_renew_before Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 8760h0m0s , 2160h0m0s , 17520h0m0s ) Impact : Shorter durations increase security but require more frequent renewals. Longer durations reduce renewal frequency but increase risk if certificates are compromised. Cert-manager will automatically renew certificates before expiration. Related variables : - Must be greater than mas_certificate_renew_before - Only applies when using automatic certificate management - Affects all MAS certificates mas_certificate_renew_before \u00a4 Specifies when to renew certificates before they expire. Optional Environment Variable: MAS_CERTIFICATE_RENEW_BEFORE Default: 720h0m0s (30 days) Purpose : Defines the renewal window - how far in advance cert-manager will renew certificates before they expire. This ensures certificates are renewed with sufficient time to handle any renewal issues. When to use : - Use default (720h = 30 days) for most installations - Increase in environments where certificate renewal may be slow or problematic - Decrease only if you need to minimize the number of certificate changes - Must be shorter than mas_certificate_duration Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 720h0m0s , 1440h0m0s , 168h0m0s ) Impact : Longer renewal windows provide more time to resolve renewal issues but result in more frequent certificate changes. Shorter windows reduce certificate churn but increase risk of expiration if renewal fails. Related variables : - Must be less than mas_certificate_duration - Only applies when using automatic certificate management - Affects all MAS certificates Note : Ensure this value provides adequate time to detect and resolve certificate renewal issues before expiration. Superuser Account \u00a4 The MAS Superuser account username and password can be customized during the install by setting both of these variables. mas_superuser_username \u00a4 Custom username for the MAS superuser administrator account. Optional Environment Variable: MAS_SUPERUSER_USERNAME Default: None (uses MAS default superuser) Purpose : Allows you to set a custom username for the MAS superuser account instead of using the default. The superuser has full administrative access to all MAS functions and applications. When to use : - Set both username and password to customize the superuser account - Use in environments with specific account naming requirements - Leave unset to use the default superuser account - Must be set together with mas_superuser_password Valid values : Any valid username string (alphanumeric, may include underscores and hyphens) Impact : When set (along with password), creates a superuser account with the specified username. If only one of username/password is set, both are ignored and default account is used. Related variables : Must be set together with mas_superuser_password to take effect. Note : Both username and password must be set for customization to take effect. Setting only one has no effect. mas_superuser_password \u00a4 Custom password for the MAS superuser administrator account. Optional Environment Variable: MAS_SUPERUSER_PASSWORD Default: None (uses auto-generated password) Purpose : Allows you to set a custom password for the MAS superuser account instead of using the auto-generated default. This enables you to control the initial superuser credentials. When to use : - Set both username and password to customize the superuser account - Use when you need to know the superuser password in advance - Use to comply with organizational password policies - Must be set together with mas_superuser_username Valid values : Any string meeting MAS password requirements (minimum length, complexity requirements) Impact : When set (along with username), creates a superuser account with the specified password. If only one of username/password is set, both are ignored and default account is used with auto-generated password. Related variables : Must be set together with mas_superuser_username to take effect. Note : Both username and password must be set for customization to take effect. Keep the password secure and do not commit to source control. Developer Mode \u00a4 artifactory_username \u00a4 Username for authenticating to Artifactory for development builds. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None Purpose : Provides authentication credentials for accessing development builds of MAS stored in Artifactory. This is only used for internal development and testing, not for production installations. When to use : - Required only when installing development builds from Artifactory - Not needed for production installations using IBM Container Registry - Use your IBM w3Id username for Artifactory access - Must be set together with artifactory_token Valid values : Your IBM w3Id username Impact : Used to create image pull secrets for accessing Artifactory registries. Without valid credentials, development image pulls will fail. Related variables : - Must be set with artifactory_token - Used with mas_icr_cp and mas_icr_cpopen when pointing to Artifactory - Related to mas_entitlement_username for registry authentication Note : Only for development use. Production installations should use IBM Container Registry with entitlement keys. artifactory_token \u00a4 API token for authenticating to Artifactory for development builds. Optional Environment Variable: ARTIFACTORY_TOKEN Default: None Purpose : Provides authentication token for accessing development builds of MAS stored in Artifactory. This is only used for internal development and testing, not for production installations. When to use : - Required only when installing development builds from Artifactory - Not needed for production installations using IBM Container Registry - Use your Artifactory API key - Must be set together with artifactory_username Valid values : Your Artifactory API key/token Impact : Used to create image pull secrets for accessing Artifactory registries. Without valid credentials, development image pulls will fail. Related variables : - Must be set with artifactory_username - Used with mas_icr_cp and mas_icr_cpopen when pointing to Artifactory - Can be used as mas_entitlement_key for development builds Note : Only for development use. Keep tokens secure. Production installations should use IBM Container Registry with entitlement keys. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_config_dir: \"/home/david/masconfig\" mas_entitlement_key: \"{{ lookup('env', 'IBM_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify License \u00a4 EPL-2.0","title":"suite_install"},{"location":"roles/suite_install/#suite_install","text":"This role installs Maximo Application Suite. It internally resolves the namespace based on the mas_instance_id as mas-{mas_instance_id}-core .","title":"suite_install"},{"location":"roles/suite_install/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_install/#basic-install","text":"","title":"Basic Install"},{"location":"roles/suite_install/#mas_catalog_source","text":"Specifies the OpenShift operator catalog source containing the MAS operator subscription. Optional Environment Variable: MAS_CATALOG_SOURCE Default: ibm-operator-catalog Purpose : Controls which operator catalog is used to locate and install the MAS operator. This determines where OpenShift looks for the operator images and metadata during installation. When to use : - Use default ibm-operator-catalog for production installations with official IBM releases - Use ibm-operator-catalog for development installations as well (supports both use cases) - Only change if directed to use a custom catalog for specific testing or airgap scenarios Valid values : Any valid CatalogSource name present in the openshift-marketplace namespace (typically ibm-operator-catalog ) Impact : Changing this value affects which operator versions are available for installation. An invalid catalog source will cause the subscription to fail. Related variables : Works with mas_channel to determine the specific operator version installed.","title":"mas_catalog_source"},{"location":"roles/suite_install/#mas_channel","text":"Specifies the MAS operator subscription channel, which determines the version stream you'll receive updates from. Required Environment Variable: MAS_CHANNEL Default: None Purpose : Controls which version of MAS will be installed and which updates will be automatically applied. The channel corresponds to major.minor version releases (e.g., 8.11.x , 9.0.x ) and determines the feature set and compatibility level of your MAS installation. When to use : - Set to the latest stable channel for new production deployments to receive the newest features - Use specific older channels when compatibility with existing applications or dependencies requires it - Consult the MAS compatibility matrix before selecting a channel to ensure compatibility with your applications - Change channels only during planned upgrade windows as this triggers version updates Valid values : 8.9.x , 8.10.x , 8.11.x , 9.0.x (check the IBM Operator Catalog for currently available channels) Impact : The channel determines which MAS version is installed and which automatic updates are applied. Changing channels after installation will trigger an upgrade to the latest version in that channel, which may require application reconfiguration and testing. Related variables : Works with mas_catalog_source to determine available channels. Note : Once installed, changing channels requires careful planning. Review the MAS upgrade documentation before changing this value.","title":"mas_channel"},{"location":"roles/suite_install/#basic-configuration","text":"","title":"Basic Configuration"},{"location":"roles/suite_install/#mas_domain","text":"Specifies the custom domain name for accessing MAS web interfaces and APIs. Optional Environment Variable: MAS_DOMAIN Default: None (uses cluster default subdomain) Purpose : Defines the base domain used to construct all MAS URLs (e.g., admin.{mas_domain}, home.{mas_domain}). This allows you to use a custom domain instead of the default OpenShift cluster subdomain, which is important for production environments with specific DNS requirements. When to use : - Set for production environments where you need branded or corporate domain names - Set when integrating with external DNS providers (Cloudflare, Route53, IBM CIS) - Set when using custom SSL certificates tied to specific domains - Leave unset for development/testing to use the default cluster subdomain automatically Valid values : Any valid DNS domain name (e.g., mas.mycompany.com , prod-mas.example.org ) Impact : When set, MAS will use this domain for all routes and certificates. You must ensure DNS is properly configured to resolve this domain to your cluster. When not set, MAS automatically uses the cluster's default ingress subdomain. Related variables : - Used by suite_dns role to configure DNS entries - Affects certificate generation when using mas_cluster_issuer - Must align with mas_routing_mode configuration","title":"mas_domain"},{"location":"roles/suite_install/#mas_instance_id","text":"Unique identifier for this MAS installation within the cluster. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Provides a unique identifier that distinguishes this MAS installation from others that may exist in the same cluster. This ID is used to generate namespace names, resource names, and configuration identifiers throughout the installation. When to use : - Always required for any MAS installation - Use short, descriptive names (e.g., prod , dev , test , inst1 ) - Must be unique within the cluster if running multiple MAS instances - Cannot be changed after installation without reinstalling Valid values : Lowercase alphanumeric string, 3-12 characters, starting with a letter (e.g., prod , dev01 , mastest ) Impact : This ID becomes part of the core namespace name ( mas-{mas_instance_id}-core ) and is embedded in many resource names. It cannot be changed after installation. All MAS configurations and applications will reference this instance ID. Related variables : - Used by all MAS configuration roles to target the correct instance - Referenced in application installation roles - Used in backup/restore operations to identify the instance Note : Choose carefully as this cannot be changed after installation. Use consistent naming across environments (e.g., dev , test , prod ).","title":"mas_instance_id"},{"location":"roles/suite_install/#mas_entitlement_key","text":"IBM entitlement key for authenticating access to IBM Container Registry. Required Environment Variable: MAS_ENTITLEMENT_KEY Default: Value of IBM_ENTITLEMENT_KEY if set Purpose : Provides authentication credentials to pull MAS container images from IBM's entitled registry. This key is tied to your IBM Cloud account and product entitlements, proving you have the right to use MAS software. When to use : - Required for all production installations using official IBM releases - Obtain from IBM Container Library at https://myibm.ibm.com/products-services/containerlibrary - For development builds on Artifactory, use your Artifactory API key instead - Key must be valid and have active MAS entitlements Valid values : - IBM entitlement key string (typically starts with \"eyJ...\") - Must be a valid, non-expired key with MAS product entitlements - For development: Artifactory API key Impact : Invalid or expired keys will cause image pull failures during installation. The key is stored in a Kubernetes secret and used to create image pull secrets for all MAS pods. Without a valid key, the installation cannot proceed. Related variables : - Falls back to IBM_ENTITLEMENT_KEY environment variable if not set - Used with mas_entitlement_username for registry authentication - Related to mas_icr_cp and mas_icr_cpopen registry settings Note : Keep this key secure. Do not commit it to source control. Use environment variables or secure secret management.","title":"mas_entitlement_key"},{"location":"roles/suite_install/#mas_config_dir","text":"Local directory path containing additional Kubernetes configuration files to apply during MAS installation. Optional Environment Variable: MAS_CONFIG_DIR Default: None Purpose : Allows you to provide custom Kubernetes resources (YAML files) that will be automatically applied to the cluster during MAS installation. This enables advanced configuration scenarios and customization beyond the standard role variables. When to use : - Use to apply MAS configuration resources (MongoCfg, BASCfg, JdbcCfg, etc.) generated by other roles - Use to apply custom ConfigMaps, Secrets, or other Kubernetes resources - Use to pre-configure MAS settings before the suite becomes fully operational - Leave unset if you plan to apply configurations manually after installation Valid values : Any valid local filesystem path (e.g., /home/user/masconfig , ~/mas-configs , ./config ) Impact : All *.yaml and *.yml files in this directory will be applied to the cluster using oc apply . Files are processed in alphabetical order. Invalid YAML or resources will cause the role to fail. Related variables : - Used by mongodb , db2 , sls and other dependency roles to output configuration files - Used by suite_config role to apply configurations - Must be set if using generated configuration files from dependency roles Note : Ensure all files in this directory are valid Kubernetes resources. The role does not validate file contents before applying.","title":"mas_config_dir"},{"location":"roles/suite_install/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"roles/suite_install/#mas_annotations","text":"Comma-separated list of key=value annotations to apply to all MAS resources. Optional Environment Variable: MAS_ANNOTATIONS Default: None Purpose : Adds Kubernetes annotations to all resources created by the MAS operator. Annotations are used to attach metadata that can control operator behavior, enable specific features, or provide information to other tools and operators. When to use : - Set to mas.ibm.com/operationalMode=nonproduction for non-production environments (reduces resource requirements) - Use to add custom metadata for organizational tracking or automation - Use to enable specific MAS features controlled by annotations - Leave unset for default production configuration Valid values : Comma-separated list of key=value pairs (e.g., key1=value1,key2=value2 ) Impact : Annotations affect how the MAS operator configures resources. The operationalMode=nonproduction annotation significantly reduces CPU and memory requirements but is not suitable for production workloads. Related variables : Works alongside custom_labels for resource metadata. Note : The operationalMode=nonproduction annotation should only be used in development/test environments. It reduces resource allocations below production requirements.","title":"mas_annotations"},{"location":"roles/suite_install/#mas_img_pull_policy","text":"Controls the image pull policy for all MAS container images. Optional Environment Variable: MAS_IMG_PULL_POLICY Default: None (uses operator default) Purpose : Determines when Kubernetes will pull container images from the registry. This affects deployment speed, network usage, and whether you get the latest image updates. When to use : - Use Always in development to ensure latest images are pulled on every pod restart - Use IfNotPresent in production to reduce network traffic and improve pod startup time - Use Never in airgap environments where images are pre-loaded - Leave unset to use the operator's default policy (typically IfNotPresent ) Valid values : Always , IfNotPresent , Never Impact : - Always : Slower pod starts, higher network usage, always gets latest image - IfNotPresent : Faster pod starts, uses cached images, may miss updates - Never : Fastest starts, requires images pre-loaded, fails if image not present Related variables : Affects all images pulled by MAS operator and workloads.","title":"mas_img_pull_policy"},{"location":"roles/suite_install/#custom_labels","text":"Comma-separated list of key=value labels to apply to all MAS resources. Optional Environment Variable: CUSTOM_LABELS Default: None Purpose : Adds Kubernetes labels to all resources created by the MAS operator. Labels are used for resource organization, selection, and filtering. They enable you to query, group, and manage MAS resources using standard Kubernetes tools. When to use : - Use to add organizational metadata (e.g., cost-center=engineering , environment=production ) - Use to enable resource tracking and cost allocation - Use to support custom automation or monitoring tools - Use to comply with organizational labeling standards Valid values : Comma-separated list of key=value pairs (e.g., env=prod,team=platform,cost-center=12345 ) Impact : Labels are applied to all MAS resources and can be used for filtering with oc get commands, monitoring queries, and automation scripts. Labels do not affect MAS functionality but are essential for resource management. Related variables : Works alongside mas_annotations for resource metadata.","title":"custom_labels"},{"location":"roles/suite_install/#mas_manual_cert_mgmt","text":"Enables manual certificate management mode, disabling automatic certificate generation. Optional Environment Variable: MAS_MANUAL_CERT_MGMT Default: false Purpose : Controls whether MAS uses automatic certificate management via cert-manager or requires manually provided certificates. This is critical for environments with specific certificate requirements or where cert-manager cannot be used. When to use : - Set to true when you must use certificates from a specific Certificate Authority - Set to true in environments where cert-manager is not available or not permitted - Set to true when organizational policy requires manual certificate management - Leave as false (default) to use automatic certificate management with cert-manager Valid values : true , false Impact : When true , you must manually create and manage all certificates required by MAS. The suite_dns role will not configure automatic certificate issuers. You are responsible for certificate renewal before expiration. When false , cert-manager automatically generates and renews certificates. Related variables : - When false : Requires mas_cluster_issuer to be set for automatic certificate generation - Affects behavior of suite_dns role - Related to mas_certificate_duration and mas_certificate_renew_before Note : Manual certificate management requires significant operational overhead. Use automatic management unless you have specific requirements.","title":"mas_manual_cert_mgmt"},{"location":"roles/suite_install/#mas_routing_mode","text":"Defines the URL routing strategy for MAS applications and services. Optional Environment Variable: MAS_ROUTING_MODE Default: subdomain Purpose : Controls how MAS constructs URLs for different applications and services. This affects the URL structure users see and how DNS must be configured. When to use : - Use subdomain (default) for cleaner URLs and better DNS organization (e.g., admin.mas.example.com , home.mas.example.com ) - Use path when subdomain routing is not possible due to DNS or certificate limitations (e.g., mas.example.com/admin , mas.example.com/home ) - Consider DNS provider capabilities and certificate management when choosing Valid values : subdomain , path Impact : - subdomain : Creates separate DNS entries for each service (requires wildcard DNS or multiple DNS entries) - path : Uses single DNS entry with path-based routing (simpler DNS, more complex URL structure) - Cannot be changed after installation without reinstalling Related variables : - Affects DNS configuration in suite_dns role - Impacts certificate requirements (wildcard vs single certificate) - Must align with mas_domain configuration Note : Choose carefully as this cannot be changed after installation. Subdomain routing is recommended for production environments.","title":"mas_routing_mode"},{"location":"roles/suite_install/#mas_trust_default_cas","text":"Controls whether default system Certificate Authorities are included in MAS trust stores. Optional Environment Variable: MAS_TRUST_DEFAULT_CAS Default: true Purpose : Determines if MAS will trust certificates signed by standard public Certificate Authorities (like Let's Encrypt, DigiCert, etc.) in addition to any custom CAs you configure. This affects MAS's ability to connect to external services using standard SSL certificates. When to use : - Leave as true (default) for most installations to enable connections to public services - Set to false only in highly restricted environments where you want to explicitly control all trusted CAs - Set to false in airgap environments where external connections are not permitted Valid values : true , false Impact : When true , MAS can connect to any service using certificates from well-known public CAs. When false , MAS will only trust explicitly configured custom CAs, which may break connections to external services. Related variables : Works with custom CA configuration in MAS. Note : Only available in MAS 8.11 and above. Has no effect in earlier versions.","title":"mas_trust_default_cas"},{"location":"roles/suite_install/#mas_pod_templates_dir","text":"Directory containing pod template customization files for MAS workloads. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Purpose : Allows you to customize resource requests, limits, node selectors, tolerations, and other pod-level configurations for MAS workloads. This enables you to optimize MAS for your specific cluster configuration and resource availability. When to use : - Use to apply resource constraints in resource-limited environments - Use to configure node affinity for specific hardware (e.g., GPU nodes) - Use to apply tolerations for tainted nodes - Use to implement best-effort or guaranteed QoS classes - Leave unset to use default MAS pod configurations Valid values : Path to directory containing pod template YAML files: - ibm-mas-suite.yml - Core suite workloads - ibm-mas-coreidp.yml - Identity provider workloads - ibm-data-dictionary-assetdatadictionary.yml - Data dictionary workloads Impact : Pod templates directly affect resource allocation, scheduling, and performance of MAS workloads. Incorrect configurations can cause pods to fail scheduling or perform poorly. Related variables : Similar pod template configuration available in other roles (SLS, applications). Note : See MAS CLI pod templates for examples and product documentation for full details.","title":"mas_pod_templates_dir"},{"location":"roles/suite_install/#certificate-management","text":"","title":"Certificate Management"},{"location":"roles/suite_install/#mas_cluster_issuer","text":"Name of the cert-manager ClusterIssuer to use for automatic certificate generation. Optional Environment Variable: MAS_CLUSTER_ISSUER Default: None Purpose : Specifies which cert-manager ClusterIssuer will generate and manage SSL/TLS certificates for MAS. The ClusterIssuer defines the Certificate Authority and authentication method used for certificate issuance. When to use : - Required when mas_manual_cert_mgmt is false (automatic certificate management) - Set to the ClusterIssuer created by suite_dns role (e.g., {mas_instance_id}-cloudflare-le-prod ) - Set to a custom ClusterIssuer if you have specific certificate requirements - Leave unset only when using manual certificate management Valid values : Name of any valid ClusterIssuer resource in the cluster (e.g., prod-le-issuer , {mas_instance_id}-cloudflare-le-prod ) Impact : The specified ClusterIssuer will be used to generate all MAS certificates. If the ClusterIssuer is not properly configured or lacks necessary credentials, certificate generation will fail and MAS will not be accessible. Related variables : - Only used when mas_manual_cert_mgmt is false - Created by suite_dns role for Let's Encrypt integration - Works with mas_certificate_duration and mas_certificate_renew_before Note : Ensure the ClusterIssuer is created and functional before installing MAS. Test certificate generation with a test Certificate resource first.","title":"mas_cluster_issuer"},{"location":"roles/suite_install/#mas_certificate_duration","text":"Specifies the validity period for MAS certificates. Optional Environment Variable: MAS_CERTIFICATE_DURATION Default: 8760h0m0s (1 year) Purpose : Defines how long certificates will be valid before they expire. This affects how often certificates need to be renewed and the security posture of your installation. When to use : - Use default (8760h = 1 year) for most installations - Reduce for higher security environments requiring frequent rotation - Increase only if certificate renewal is problematic in your environment - Must be longer than mas_certificate_renew_before Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 8760h0m0s , 2160h0m0s , 17520h0m0s ) Impact : Shorter durations increase security but require more frequent renewals. Longer durations reduce renewal frequency but increase risk if certificates are compromised. Cert-manager will automatically renew certificates before expiration. Related variables : - Must be greater than mas_certificate_renew_before - Only applies when using automatic certificate management - Affects all MAS certificates","title":"mas_certificate_duration"},{"location":"roles/suite_install/#mas_certificate_renew_before","text":"Specifies when to renew certificates before they expire. Optional Environment Variable: MAS_CERTIFICATE_RENEW_BEFORE Default: 720h0m0s (30 days) Purpose : Defines the renewal window - how far in advance cert-manager will renew certificates before they expire. This ensures certificates are renewed with sufficient time to handle any renewal issues. When to use : - Use default (720h = 30 days) for most installations - Increase in environments where certificate renewal may be slow or problematic - Decrease only if you need to minimize the number of certificate changes - Must be shorter than mas_certificate_duration Valid values : Duration string in format {hours}h{minutes}m{seconds}s (e.g., 720h0m0s , 1440h0m0s , 168h0m0s ) Impact : Longer renewal windows provide more time to resolve renewal issues but result in more frequent certificate changes. Shorter windows reduce certificate churn but increase risk of expiration if renewal fails. Related variables : - Must be less than mas_certificate_duration - Only applies when using automatic certificate management - Affects all MAS certificates Note : Ensure this value provides adequate time to detect and resolve certificate renewal issues before expiration.","title":"mas_certificate_renew_before"},{"location":"roles/suite_install/#superuser-account","text":"The MAS Superuser account username and password can be customized during the install by setting both of these variables.","title":"Superuser Account"},{"location":"roles/suite_install/#mas_superuser_username","text":"Custom username for the MAS superuser administrator account. Optional Environment Variable: MAS_SUPERUSER_USERNAME Default: None (uses MAS default superuser) Purpose : Allows you to set a custom username for the MAS superuser account instead of using the default. The superuser has full administrative access to all MAS functions and applications. When to use : - Set both username and password to customize the superuser account - Use in environments with specific account naming requirements - Leave unset to use the default superuser account - Must be set together with mas_superuser_password Valid values : Any valid username string (alphanumeric, may include underscores and hyphens) Impact : When set (along with password), creates a superuser account with the specified username. If only one of username/password is set, both are ignored and default account is used. Related variables : Must be set together with mas_superuser_password to take effect. Note : Both username and password must be set for customization to take effect. Setting only one has no effect.","title":"mas_superuser_username"},{"location":"roles/suite_install/#mas_superuser_password","text":"Custom password for the MAS superuser administrator account. Optional Environment Variable: MAS_SUPERUSER_PASSWORD Default: None (uses auto-generated password) Purpose : Allows you to set a custom password for the MAS superuser account instead of using the auto-generated default. This enables you to control the initial superuser credentials. When to use : - Set both username and password to customize the superuser account - Use when you need to know the superuser password in advance - Use to comply with organizational password policies - Must be set together with mas_superuser_username Valid values : Any string meeting MAS password requirements (minimum length, complexity requirements) Impact : When set (along with username), creates a superuser account with the specified password. If only one of username/password is set, both are ignored and default account is used with auto-generated password. Related variables : Must be set together with mas_superuser_username to take effect. Note : Both username and password must be set for customization to take effect. Keep the password secure and do not commit to source control.","title":"mas_superuser_password"},{"location":"roles/suite_install/#developer-mode","text":"","title":"Developer Mode"},{"location":"roles/suite_install/#artifactory_username","text":"Username for authenticating to Artifactory for development builds. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None Purpose : Provides authentication credentials for accessing development builds of MAS stored in Artifactory. This is only used for internal development and testing, not for production installations. When to use : - Required only when installing development builds from Artifactory - Not needed for production installations using IBM Container Registry - Use your IBM w3Id username for Artifactory access - Must be set together with artifactory_token Valid values : Your IBM w3Id username Impact : Used to create image pull secrets for accessing Artifactory registries. Without valid credentials, development image pulls will fail. Related variables : - Must be set with artifactory_token - Used with mas_icr_cp and mas_icr_cpopen when pointing to Artifactory - Related to mas_entitlement_username for registry authentication Note : Only for development use. Production installations should use IBM Container Registry with entitlement keys.","title":"artifactory_username"},{"location":"roles/suite_install/#artifactory_token","text":"API token for authenticating to Artifactory for development builds. Optional Environment Variable: ARTIFACTORY_TOKEN Default: None Purpose : Provides authentication token for accessing development builds of MAS stored in Artifactory. This is only used for internal development and testing, not for production installations. When to use : - Required only when installing development builds from Artifactory - Not needed for production installations using IBM Container Registry - Use your Artifactory API key - Must be set together with artifactory_username Valid values : Your Artifactory API key/token Impact : Used to create image pull secrets for accessing Artifactory registries. Without valid credentials, development image pulls will fail. Related variables : - Must be set with artifactory_username - Used with mas_icr_cp and mas_icr_cpopen when pointing to Artifactory - Can be used as mas_entitlement_key for development builds Note : Only for development use. Keep tokens secure. Production installations should use IBM Container Registry with entitlement keys.","title":"artifactory_token"},{"location":"roles/suite_install/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_config_dir: \"/home/david/masconfig\" mas_entitlement_key: \"{{ lookup('env', 'IBM_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify","title":"Example Playbook"},{"location":"roles/suite_install/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_attachments_config/","text":"suite_manage_attachments_config \u00a4 This role extends support for configuring IBM Cloud Object Storage or Persistent Volume/File Storages for Manage application attachments. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior configuring attachments features. By default, Manage attachments configuration uses filestorage provider; it corresponds to your cluster's default file storage system to persist the files. Alternatively, you can provide an existing IBM Cloud Object Storage by using ibm provider, or even provision a new instance by using cos role. Finally, an existing AWS S3 service can also be provided via aws provider by this same role (see details in the Role Variables below). Role Variables \u00a4 Attachment Configuration \u00a4 mas_manage_attachments_provider \u00a4 Defines the storage provider type to be used to store Manage application's attachments. Available options are filestorage (default), ibm , or aws . Required Environment Variable: MAS_MANAGE_ATTACHMENTS_PROVIDER Default Value: filestorage Provider Options: - filestorage (default option): Configures cluster's file storage system for Manage attachments. - ibm : Configures IBM Cloud Object Storage as storage system for Manage attachments. - aws : Configures Amazon S3 buckets as storage system for Manage attachments. Note: If using ibm or aws as attachments provider, the cos_bucket role will be executed to setup a new or existing targeted COS bucket to be used to store Manage attachments, therefore make sure you set the expected variables to customize your COS bucket for Manage attachments, i.e. COS_APIKEY and COS_INSTANCE_NAME . Note about S3: To run this role successfully for AWS s3 buckets, you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. mas_manage_attachment_configuration_mode \u00a4 Defines how attachment properties will be configured in Manage. Possible values are cr and db . When cr is selected, attachment properties will be entered in ManageWorkspace CR for each bundle, under bundleProperties key. For this mode, manage_workspace_cr_name must be informed. When db is selected, attachment properties will be updated directly in the database via SQL updates. For this mode, db2_instance_name , db2_namespace and db2_dbname must be informed. Required Environment Variable: MAS_MANAGE_ATTACHMENT_CONFIGURATION_MODE Default Value: db mas_instance_id \u00a4 The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \u00a4 The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_WORKSPACE_ID Default Value: None manage_workspace_cr_name \u00a4 Name of the ManageWorkspace Custom Resource that will be targeted to configure the new PVC definitions. Required when mas_manage_attachment_configuration_mode is set as cr . Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID db2_instance_name \u00a4 The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. Required when mas_manage_attachment_configuration_mode is set as db . Note : In order to obtain the value for this variable, go to the namespace where db2 is installed and look for the pod where label=engine . Select/describe the pod representing your database, and look for the value of label app . That is your db2 instance name. Optional Environment Variable: DB2_INSTANCE_NAME Default Value: None db2_namespace \u00a4 The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Required when mas_manage_attachment_configuration_mode is set as db . Optional Environment Variable: DB2_NAMESPACE Default Value: db2u db2_dbname \u00a4 Name of the database within the instance. Required when mas_manage_attachment_configuration_mode is set as db . Optional Environment Variable: DB2_DBNAME Default Value: BLUDB Example Playbook \u00a4 Configure COS via ManageWorkspace CR \u00a4 The following sample can be used to configure COS for an existing Manage application instance's attachments via ManageWorkspace CR update (note cr as configuration mode + mas_instance_id and mas_workspace_id , which will be used to infer the CR name): - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: cr mas_instance_id: masinst1 mas_workspace_id: masdev cos_instance_name: cos-masinst1 cos_bucket_name: manage-attachments-bucket ibmcloud_apikey: xxxx mas_manage_attachments_provider: ibm roles: - ibm.mas_devops.suite_manage_attachments_config Provision COS and Configure via Database \u00a4 The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance's attachments via SQL updates in database (note db as configuration mode + db2_instance_name ): - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: db mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_instance_name: cos-masinst1 cos_bucket_name: manage-attachments-bucket ibmcloud_apikey: xxxx mas_manage_attachments_provider: ibm roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_attachments_config Configure File Storage for Attachments \u00a4 The following sample playbook can be used to deploy Manage with default persistent storage for Manage attachments (PVC mount path /DOCLINKS ), and configure Manage system properties with the corresponding attachments settings via SQL updates in database: - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: db mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_manage_attachments_provider: filestorage roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_attachments_config Configure AWS S3 Buckets \u00a4 The following sample can be used to configure AWS S3 buckets for an existing Manage application instance's attachments: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage aws_bucket_name: manage-attachments-bucket mas_manage_attachments_provider: aws roles: - ibm.mas_devops.suite_manage_attachments_config License \u00a4 EPL-2.0","title":"suite_manage_attachments_config"},{"location":"roles/suite_manage_attachments_config/#suite_manage_attachments_config","text":"This role extends support for configuring IBM Cloud Object Storage or Persistent Volume/File Storages for Manage application attachments. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior configuring attachments features. By default, Manage attachments configuration uses filestorage provider; it corresponds to your cluster's default file storage system to persist the files. Alternatively, you can provide an existing IBM Cloud Object Storage by using ibm provider, or even provision a new instance by using cos role. Finally, an existing AWS S3 service can also be provided via aws provider by this same role (see details in the Role Variables below).","title":"suite_manage_attachments_config"},{"location":"roles/suite_manage_attachments_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_attachments_config/#attachment-configuration","text":"","title":"Attachment Configuration"},{"location":"roles/suite_manage_attachments_config/#mas_manage_attachments_provider","text":"Defines the storage provider type to be used to store Manage application's attachments. Available options are filestorage (default), ibm , or aws . Required Environment Variable: MAS_MANAGE_ATTACHMENTS_PROVIDER Default Value: filestorage Provider Options: - filestorage (default option): Configures cluster's file storage system for Manage attachments. - ibm : Configures IBM Cloud Object Storage as storage system for Manage attachments. - aws : Configures Amazon S3 buckets as storage system for Manage attachments. Note: If using ibm or aws as attachments provider, the cos_bucket role will be executed to setup a new or existing targeted COS bucket to be used to store Manage attachments, therefore make sure you set the expected variables to customize your COS bucket for Manage attachments, i.e. COS_APIKEY and COS_INSTANCE_NAME . Note about S3: To run this role successfully for AWS s3 buckets, you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"mas_manage_attachments_provider"},{"location":"roles/suite_manage_attachments_config/#mas_manage_attachment_configuration_mode","text":"Defines how attachment properties will be configured in Manage. Possible values are cr and db . When cr is selected, attachment properties will be entered in ManageWorkspace CR for each bundle, under bundleProperties key. For this mode, manage_workspace_cr_name must be informed. When db is selected, attachment properties will be updated directly in the database via SQL updates. For this mode, db2_instance_name , db2_namespace and db2_dbname must be informed. Required Environment Variable: MAS_MANAGE_ATTACHMENT_CONFIGURATION_MODE Default Value: db","title":"mas_manage_attachment_configuration_mode"},{"location":"roles/suite_manage_attachments_config/#mas_instance_id","text":"The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_attachments_config/#mas_workspace_id","text":"The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/suite_manage_attachments_config/#manage_workspace_cr_name","text":"Name of the ManageWorkspace Custom Resource that will be targeted to configure the new PVC definitions. Required when mas_manage_attachment_configuration_mode is set as cr . Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID","title":"manage_workspace_cr_name"},{"location":"roles/suite_manage_attachments_config/#db2_instance_name","text":"The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. Required when mas_manage_attachment_configuration_mode is set as db . Note : In order to obtain the value for this variable, go to the namespace where db2 is installed and look for the pod where label=engine . Select/describe the pod representing your database, and look for the value of label app . That is your db2 instance name. Optional Environment Variable: DB2_INSTANCE_NAME Default Value: None","title":"db2_instance_name"},{"location":"roles/suite_manage_attachments_config/#db2_namespace","text":"The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Required when mas_manage_attachment_configuration_mode is set as db . Optional Environment Variable: DB2_NAMESPACE Default Value: db2u","title":"db2_namespace"},{"location":"roles/suite_manage_attachments_config/#db2_dbname","text":"Name of the database within the instance. Required when mas_manage_attachment_configuration_mode is set as db . Optional Environment Variable: DB2_DBNAME Default Value: BLUDB","title":"db2_dbname"},{"location":"roles/suite_manage_attachments_config/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_manage_attachments_config/#configure-cos-via-manageworkspace-cr","text":"The following sample can be used to configure COS for an existing Manage application instance's attachments via ManageWorkspace CR update (note cr as configuration mode + mas_instance_id and mas_workspace_id , which will be used to infer the CR name): - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: cr mas_instance_id: masinst1 mas_workspace_id: masdev cos_instance_name: cos-masinst1 cos_bucket_name: manage-attachments-bucket ibmcloud_apikey: xxxx mas_manage_attachments_provider: ibm roles: - ibm.mas_devops.suite_manage_attachments_config","title":"Configure COS via ManageWorkspace CR"},{"location":"roles/suite_manage_attachments_config/#provision-cos-and-configure-via-database","text":"The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance's attachments via SQL updates in database (note db as configuration mode + db2_instance_name ): - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: db mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_instance_name: cos-masinst1 cos_bucket_name: manage-attachments-bucket ibmcloud_apikey: xxxx mas_manage_attachments_provider: ibm roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_attachments_config","title":"Provision COS and Configure via Database"},{"location":"roles/suite_manage_attachments_config/#configure-file-storage-for-attachments","text":"The following sample playbook can be used to deploy Manage with default persistent storage for Manage attachments (PVC mount path /DOCLINKS ), and configure Manage system properties with the corresponding attachments settings via SQL updates in database: - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: db mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_manage_attachments_provider: filestorage roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_attachments_config","title":"Configure File Storage for Attachments"},{"location":"roles/suite_manage_attachments_config/#configure-aws-s3-buckets","text":"The following sample can be used to configure AWS S3 buckets for an existing Manage application instance's attachments: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage aws_bucket_name: manage-attachments-bucket mas_manage_attachments_provider: aws roles: - ibm.mas_devops.suite_manage_attachments_config","title":"Configure AWS S3 Buckets"},{"location":"roles/suite_manage_attachments_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_bim_config/","text":"suite_manage_bim_config \u00a4 This role extends support for configuring existing PVC mounted path for BIM (Building Information Models) in Manage application. In order for this task to run successfully your Manage application must have been configured with a proper persistent volume and mounted path. You can run suite_app_config with mas_app_settings_persistent_volumes_flag: true while installing mas_app_id: manage to have a default persistent storage configured as part of Manage deployment that can be used in this role to setup BIM. For more details on how to configure persistent storage for Manage refer to Configuring persistent volume claims . Role Variables \u00a4 BIM Configuration \u00a4 mas_app_settings_bim_mount_path \u00a4 Defines the persistent volume mount path to be used while configuring Manage BIM folders. If you used suite_app_config role to configure the persistent volumes while deploying Manage application, the default BIM persistent volume mount path will be the same. Required Environment Variable: MAS_APP_SETTINGS_BIM_MOUNT_PATH Default Value: /bim mas_instance_id \u00a4 The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_INSTANCE_ID Default Value: None Database Configuration \u00a4 db2_instance_name \u00a4 The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the BIM system properties. Required Environment Variable: DB2_INSTANCE_NAME Default Value: None db2_namespace \u00a4 The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the BIM system properties. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Optional Environment Variable: DB2_NAMESPACE Default Value: db2u db2_dbname \u00a4 Name of the database within the instance. Optional Environment Variable: DB2_DBNAME Default Value: BLUDB Example Playbook \u00a4 Configure BIM for Existing Manage Instance \u00a4 The following sample can be used to configure BIM for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 db2_instance_name: db2w-manage mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.suite_manage_bim_config Deploy Manage with BIM Configuration \u00a4 The following sample playbook can be used to deploy Manage with default persistent storage for BIM (PVC mount path /bim ), and configure Manage system properties with the corresponding BIM settings: - hosts: localhost any_errors_fatal: true vars: mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_bim_config License \u00a4 EPL-2.0","title":"suite_manage_bim_config"},{"location":"roles/suite_manage_bim_config/#suite_manage_bim_config","text":"This role extends support for configuring existing PVC mounted path for BIM (Building Information Models) in Manage application. In order for this task to run successfully your Manage application must have been configured with a proper persistent volume and mounted path. You can run suite_app_config with mas_app_settings_persistent_volumes_flag: true while installing mas_app_id: manage to have a default persistent storage configured as part of Manage deployment that can be used in this role to setup BIM. For more details on how to configure persistent storage for Manage refer to Configuring persistent volume claims .","title":"suite_manage_bim_config"},{"location":"roles/suite_manage_bim_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_bim_config/#bim-configuration","text":"","title":"BIM Configuration"},{"location":"roles/suite_manage_bim_config/#mas_app_settings_bim_mount_path","text":"Defines the persistent volume mount path to be used while configuring Manage BIM folders. If you used suite_app_config role to configure the persistent volumes while deploying Manage application, the default BIM persistent volume mount path will be the same. Required Environment Variable: MAS_APP_SETTINGS_BIM_MOUNT_PATH Default Value: /bim","title":"mas_app_settings_bim_mount_path"},{"location":"roles/suite_manage_bim_config/#mas_instance_id","text":"The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_bim_config/#database-configuration","text":"","title":"Database Configuration"},{"location":"roles/suite_manage_bim_config/#db2_instance_name","text":"The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the BIM system properties. Required Environment Variable: DB2_INSTANCE_NAME Default Value: None","title":"db2_instance_name"},{"location":"roles/suite_manage_bim_config/#db2_namespace","text":"The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the BIM system properties. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Optional Environment Variable: DB2_NAMESPACE Default Value: db2u","title":"db2_namespace"},{"location":"roles/suite_manage_bim_config/#db2_dbname","text":"Name of the database within the instance. Optional Environment Variable: DB2_DBNAME Default Value: BLUDB","title":"db2_dbname"},{"location":"roles/suite_manage_bim_config/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_manage_bim_config/#configure-bim-for-existing-manage-instance","text":"The following sample can be used to configure BIM for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 db2_instance_name: db2w-manage mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.suite_manage_bim_config","title":"Configure BIM for Existing Manage Instance"},{"location":"roles/suite_manage_bim_config/#deploy-manage-with-bim-configuration","text":"The following sample playbook can be used to deploy Manage with default persistent storage for BIM (PVC mount path /bim ), and configure Manage system properties with the corresponding BIM settings: - hosts: localhost any_errors_fatal: true vars: mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_bim_config","title":"Deploy Manage with BIM Configuration"},{"location":"roles/suite_manage_bim_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_birt_report_config/","text":"suite_manage_birt_report_config \u00a4 This role extends support for configuring Birt Report in Manage application as a separate and dedicated report bundle server workload. The following Manage properties will be added to every and each Manage server bundle: mxe.report.birt.viewerurl = https://{{ mas_workspace_id }}-{{ manage_report_bundle_server_name }}.manage.{{ mas_domain }} mxe.report.birt.disablequeuemanager = 0 (if bundle type = report ) or 1 (if bundle type != report ) The goal for this role is to setup the specific Manage Report route to be the endpoint for the generated reports in Manage (which will forward the report workload to the dedicated report type bundle pod). Role Variables \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_INSTANCE_ID Default: None mas_workspace_id \u00a4 The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_WORKSPACE_ID Default: None manage_workspace_cr_name \u00a4 Name of the ManageWorkspace Custom Resource that will be targeted to configure the new PVC definitions. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID manage_report_bundle_server_name \u00a4 Name of the Manage report bundle server. It will be used to configure the Manage's report bundle server and its corresponding route. Not needed if the report bundle server is already configured. Optional Environment Variable: MANAGE_REPORT_BUNDLE_SERVER_NAME Default: rpt Example Playbook \u00a4 The following sample can be used to configure BIRT report for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: main roles: - ibm.mas_devops.suite_manage_birt_report_config Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export MAS_INSTANCE_ID=masinst1 export MAS_WORKSPACE_ID=main export MANAGE_REPORT_BUNDLE_SERVER_NAME=report ROLE_NAME='suite_manage_birt_report_config' ansible-playbook playbooks/run_role.yml License \u00a4 EPL-2.0","title":"suite_manage_birt_report_config"},{"location":"roles/suite_manage_birt_report_config/#suite_manage_birt_report_config","text":"This role extends support for configuring Birt Report in Manage application as a separate and dedicated report bundle server workload. The following Manage properties will be added to every and each Manage server bundle: mxe.report.birt.viewerurl = https://{{ mas_workspace_id }}-{{ manage_report_bundle_server_name }}.manage.{{ mas_domain }} mxe.report.birt.disablequeuemanager = 0 (if bundle type = report ) or 1 (if bundle type != report ) The goal for this role is to setup the specific Manage Report route to be the endpoint for the generated reports in Manage (which will forward the report workload to the dedicated report type bundle pod).","title":"suite_manage_birt_report_config"},{"location":"roles/suite_manage_birt_report_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_birt_report_config/#mas_instance_id","text":"The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_manage_birt_report_config/#mas_workspace_id","text":"The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/suite_manage_birt_report_config/#manage_workspace_cr_name","text":"Name of the ManageWorkspace Custom Resource that will be targeted to configure the new PVC definitions. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID","title":"manage_workspace_cr_name"},{"location":"roles/suite_manage_birt_report_config/#manage_report_bundle_server_name","text":"Name of the Manage report bundle server. It will be used to configure the Manage's report bundle server and its corresponding route. Not needed if the report bundle server is already configured. Optional Environment Variable: MANAGE_REPORT_BUNDLE_SERVER_NAME Default: rpt","title":"manage_report_bundle_server_name"},{"location":"roles/suite_manage_birt_report_config/#example-playbook","text":"The following sample can be used to configure BIRT report for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: main roles: - ibm.mas_devops.suite_manage_birt_report_config","title":"Example Playbook"},{"location":"roles/suite_manage_birt_report_config/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export MAS_INSTANCE_ID=masinst1 export MAS_WORKSPACE_ID=main export MANAGE_REPORT_BUNDLE_SERVER_NAME=report ROLE_NAME='suite_manage_birt_report_config' ansible-playbook playbooks/run_role.yml","title":"Run Role Playbook"},{"location":"roles/suite_manage_birt_report_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_customer_files_config/","text":"suite_manage_customer_files_config \u00a4 This role extends support for configuring S3 / Cloud Object Storage to store Manage application customer files. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior configuring customer files features. You can run cos role to provision an IBM Cloud Object Storage or you can provide existing IBM Cloud Object Storage or AWS S3 information to use it as storage for Manage application customer files. As part of this role, three defaulted buckets will be created: - {{ mas_instance_id }}-{{ mas_workspace_id }}-custfiles - {{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesbackup - {{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesrecovery These buckets will be used to store Manage's customer documents and files, and also will be used on the backup and recovery process. Role Variables \u00a4 Storage Configuration \u00a4 cos_type \u00a4 Defines the storage provider type to be used to store Manage application's customer files. Currently available options are ibm or aws . Required Environment Variable: COS_TYPE Default Value: None Provider Options: - ibm : Configures IBM Cloud Object Storage as storage system for Manage customer files. - aws : Configures Amazon S3 buckets as storage system for Manage customer files. Note: If using ibm or aws as customer files provider, the cos_bucket role will be executed to setup a new or existing targeted COS bucket to be used to store Manage customer files, therefore make sure you set the expected variables to customize your COS bucket for Manage customer files. Note about S3: To run this role successfully for AWS S3 buckets, you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Bucket Configuration \u00a4 custfiles_bucketname \u00a4 The main customer files bucket name. Optional Environment Variable: MANAGE_CUSTFILES_BUCKET_NAME Default Value: {{ mas_instance_id }}-{{ mas_workspace_id }}-custfiles custfiles_bucketname_backup \u00a4 The customer files bucket name used for backup. Optional Environment Variable: MANAGE_CUSTFILES_BACKUP_BUCKET_NAME Default Value: {{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesbackup custfiles_bucketname_recovery \u00a4 The customer files bucket name used for recovery. Optional Environment Variable: MANAGE_CUSTFILES_RECOVERY_BUCKET_NAME Default Value: {{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesrecovery MAS Configuration \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \u00a4 The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_WORKSPACE_ID Default Value: None manage_workspace_cr_name \u00a4 Name of the ManageWorkspace Custom Resource that will be targeted to configure the new customer files definitions. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID Example Playbook \u00a4 Configure COS for Existing Manage Instance \u00a4 The following sample can be used to configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx roles: - ibm.mas_devops.suite_manage_customer_files_config Provision and Configure IBM Cloud COS \u00a4 The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_customer_files_config Configure AWS S3 Buckets \u00a4 The following sample can be used to configure AWS S3 buckets for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: aws roles: - ibm.mas_devops.suite_manage_customer_files_config License \u00a4 EPL-2.0","title":"suite_manage_customer_files_config"},{"location":"roles/suite_manage_customer_files_config/#suite_manage_customer_files_config","text":"This role extends support for configuring S3 / Cloud Object Storage to store Manage application customer files. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior configuring customer files features. You can run cos role to provision an IBM Cloud Object Storage or you can provide existing IBM Cloud Object Storage or AWS S3 information to use it as storage for Manage application customer files. As part of this role, three defaulted buckets will be created: - {{ mas_instance_id }}-{{ mas_workspace_id }}-custfiles - {{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesbackup - {{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesrecovery These buckets will be used to store Manage's customer documents and files, and also will be used on the backup and recovery process.","title":"suite_manage_customer_files_config"},{"location":"roles/suite_manage_customer_files_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_customer_files_config/#storage-configuration","text":"","title":"Storage Configuration"},{"location":"roles/suite_manage_customer_files_config/#cos_type","text":"Defines the storage provider type to be used to store Manage application's customer files. Currently available options are ibm or aws . Required Environment Variable: COS_TYPE Default Value: None Provider Options: - ibm : Configures IBM Cloud Object Storage as storage system for Manage customer files. - aws : Configures Amazon S3 buckets as storage system for Manage customer files. Note: If using ibm or aws as customer files provider, the cos_bucket role will be executed to setup a new or existing targeted COS bucket to be used to store Manage customer files, therefore make sure you set the expected variables to customize your COS bucket for Manage customer files. Note about S3: To run this role successfully for AWS S3 buckets, you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"cos_type"},{"location":"roles/suite_manage_customer_files_config/#bucket-configuration","text":"","title":"Bucket Configuration"},{"location":"roles/suite_manage_customer_files_config/#custfiles_bucketname","text":"The main customer files bucket name. Optional Environment Variable: MANAGE_CUSTFILES_BUCKET_NAME Default Value: {{ mas_instance_id }}-{{ mas_workspace_id }}-custfiles","title":"custfiles_bucketname"},{"location":"roles/suite_manage_customer_files_config/#custfiles_bucketname_backup","text":"The customer files bucket name used for backup. Optional Environment Variable: MANAGE_CUSTFILES_BACKUP_BUCKET_NAME Default Value: {{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesbackup","title":"custfiles_bucketname_backup"},{"location":"roles/suite_manage_customer_files_config/#custfiles_bucketname_recovery","text":"The customer files bucket name used for recovery. Optional Environment Variable: MANAGE_CUSTFILES_RECOVERY_BUCKET_NAME Default Value: {{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesrecovery","title":"custfiles_bucketname_recovery"},{"location":"roles/suite_manage_customer_files_config/#mas-configuration","text":"","title":"MAS Configuration"},{"location":"roles/suite_manage_customer_files_config/#mas_instance_id","text":"The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_customer_files_config/#mas_workspace_id","text":"The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/suite_manage_customer_files_config/#manage_workspace_cr_name","text":"Name of the ManageWorkspace Custom Resource that will be targeted to configure the new customer files definitions. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID","title":"manage_workspace_cr_name"},{"location":"roles/suite_manage_customer_files_config/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_manage_customer_files_config/#configure-cos-for-existing-manage-instance","text":"The following sample can be used to configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx roles: - ibm.mas_devops.suite_manage_customer_files_config","title":"Configure COS for Existing Manage Instance"},{"location":"roles/suite_manage_customer_files_config/#provision-and-configure-ibm-cloud-cos","text":"The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_customer_files_config","title":"Provision and Configure IBM Cloud COS"},{"location":"roles/suite_manage_customer_files_config/#configure-aws-s3-buckets","text":"The following sample can be used to configure AWS S3 buckets for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: aws roles: - ibm.mas_devops.suite_manage_customer_files_config","title":"Configure AWS S3 Buckets"},{"location":"roles/suite_manage_customer_files_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_imagestitching_config/","text":"suite_manage_imagestitching_config \u00a4 This role configures the manage workspace to autodeploy the image stitching application. The role will only make changes if the Civil component has been installed. It will configure a PVC, patch the ManageWorkspace CR with the name of the PVC and also patch PV specifications based on the PVC. It also sets two key system properties required by the image stitching application: 'mci.imagestitching.apiurl' and 'imagestitching.dataInputPath'. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running in order to set system properties. Role Variables \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_INSTANCE_ID Default: None mas_workspace_id \u00a4 The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_WORKSPACE_ID Default: None mas_domain \u00a4 The domain name for the Manage cluster. Required Environment Variable: MAS_DOMAIN Default: Discovered from Suite CR stitching_pvcname \u00a4 The postfix for the PVC created for image stitching. The ManageWorkspace CR will prepend 'mas_instance_id-mas_workspace_id-' to generate the actual name of the PVC. Required Environment Variable: IMAGESTITCHING_PVCNAME Default: manage-imagestitching stitching_storage_class \u00a4 The storage class used for the PVC. If not specified the default value will be found by discovery. The storage class must support ReadWriteMany(RWX) access mode. Optional Environment Variable: IMAGESTITCHING_STORAGE_CLASS Default: None stitching_storage_size \u00a4 The size of the persistent volume claim. Required Environment Variable: IMAGESTITCHING_STORAGE_SIZE Default: 20Gi stitching_storage_mode \u00a4 The access mode for the PVC. Required Environment Variable: IMAGESTITCHING_STORAGE_MODE Default: ReadWriteMany stitching_storage_mountpath \u00a4 The mount path of the Persistent Volume. Required Environment Variable: IMAGESTITCHING_STORAGE_MOUNTPATH Default: imagestitching Example Playbook \u00a4 The following sample will configure image stitching for an existing Manage application instance via ManageWorkspace CR update: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: civil mas_workspace_id: masdev mas_domain: civil.ibmmasdev.com stitching_pvcname: manage-imagestitching stitching_storage_class: nfs-client stitching_storage_size: 20Gi stitching_storage_mode: ReadWriteMany stitching_storage_mountpath: imagestitching roles: - ibm.mas_devops.suite_manage_imagestitching_config License \u00a4 EPL-2.0","title":"suite_manage_imagestitching_config"},{"location":"roles/suite_manage_imagestitching_config/#suite_manage_imagestitching_config","text":"This role configures the manage workspace to autodeploy the image stitching application. The role will only make changes if the Civil component has been installed. It will configure a PVC, patch the ManageWorkspace CR with the name of the PVC and also patch PV specifications based on the PVC. It also sets two key system properties required by the image stitching application: 'mci.imagestitching.apiurl' and 'imagestitching.dataInputPath'. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running in order to set system properties.","title":"suite_manage_imagestitching_config"},{"location":"roles/suite_manage_imagestitching_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_imagestitching_config/#mas_instance_id","text":"The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_manage_imagestitching_config/#mas_workspace_id","text":"The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/suite_manage_imagestitching_config/#mas_domain","text":"The domain name for the Manage cluster. Required Environment Variable: MAS_DOMAIN Default: Discovered from Suite CR","title":"mas_domain"},{"location":"roles/suite_manage_imagestitching_config/#stitching_pvcname","text":"The postfix for the PVC created for image stitching. The ManageWorkspace CR will prepend 'mas_instance_id-mas_workspace_id-' to generate the actual name of the PVC. Required Environment Variable: IMAGESTITCHING_PVCNAME Default: manage-imagestitching","title":"stitching_pvcname"},{"location":"roles/suite_manage_imagestitching_config/#stitching_storage_class","text":"The storage class used for the PVC. If not specified the default value will be found by discovery. The storage class must support ReadWriteMany(RWX) access mode. Optional Environment Variable: IMAGESTITCHING_STORAGE_CLASS Default: None","title":"stitching_storage_class"},{"location":"roles/suite_manage_imagestitching_config/#stitching_storage_size","text":"The size of the persistent volume claim. Required Environment Variable: IMAGESTITCHING_STORAGE_SIZE Default: 20Gi","title":"stitching_storage_size"},{"location":"roles/suite_manage_imagestitching_config/#stitching_storage_mode","text":"The access mode for the PVC. Required Environment Variable: IMAGESTITCHING_STORAGE_MODE Default: ReadWriteMany","title":"stitching_storage_mode"},{"location":"roles/suite_manage_imagestitching_config/#stitching_storage_mountpath","text":"The mount path of the Persistent Volume. Required Environment Variable: IMAGESTITCHING_STORAGE_MOUNTPATH Default: imagestitching","title":"stitching_storage_mountpath"},{"location":"roles/suite_manage_imagestitching_config/#example-playbook","text":"The following sample will configure image stitching for an existing Manage application instance via ManageWorkspace CR update: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: civil mas_workspace_id: masdev mas_domain: civil.ibmmasdev.com stitching_pvcname: manage-imagestitching stitching_storage_class: nfs-client stitching_storage_size: 20Gi stitching_storage_mode: ReadWriteMany stitching_storage_mountpath: imagestitching roles: - ibm.mas_devops.suite_manage_imagestitching_config","title":"Example Playbook"},{"location":"roles/suite_manage_imagestitching_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_import_certs_config/","text":"suite_manage_import_certs_config \u00a4 This role extends support for importing certificates into Manage application's workspace. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior importing new certificates. You can run this as standalone role, providing a local path for a file that contains the Manage certificates definition ( manage_certificates_file_path_local variable). Or you can invoke this role inside another playbook/role, passing the Manage certificates content as a list variable ( manage_certificates ) and an alias prefix as a string variable ( manage_certificates_alias_prefix ). The certificate alias name will be concatenated with the alias prefix plus auto incremented accordingly to the number of certificates provided i.e If you provide a list with 3 certificates, and define manage_certificates_alias_prefix: myaliasprefixpart , then the alias name will be myaliasprefixpart1; myaliasprefixpart2; myaliasprefixpart3 Role Variables \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_INSTANCE_ID Default: None mas_workspace_id \u00a4 The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_WORKSPACE_ID Default: None manage_workspace_cr_name \u00a4 Name of the ManageWorkspace Custom Resource that will be targeted to import the new certificates. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID manage_certificates_file_path_local \u00a4 This defines a local path pointing the certificates definition from a custom file. Sample file definition can be found in files/manage-certs-sample.yml . Required if running as standalone role Environment Variable: MANAGE_CERTIFICATES_FILE_PATH_LOCAL Default: None manage_certificates \u00a4 List of certificate contents to import. Used when invoking this role from another playbook/role. Optional Environment Variable: MANAGE_CERTIFICATES Default: None manage_certificates_alias_prefix \u00a4 Alias prefix for certificate names. The certificate alias name will be concatenated with this prefix plus auto incremented number. Optional Environment Variable: MANAGE_CERTIFICATES_ALIAS_PREFIX Default: None Example Playbook \u00a4 Using Local File Path \u00a4 The following sample can be used to import Manage certificates for an existing Manage instance, using a local path pointing the certificates definition from a custom file. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev manage_certificates_file_path_local: /my-path/manage-certs.yml roles: - ibm.mas_devops.suite_manage_import_certs_config Using Variables \u00a4 The following sample can be used to import Manage certificates for an existing Manage instance, passing the certificates and prefix from a variable. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev manage_certificates: ['-----BEGIN CERTIFICATE----- << your-cert-content >> -----END CERTIFICATE-----'] manage_certificates_alias_prefix: \"myaliasprefixpart\" roles: - ibm.mas_devops.suite_manage_import_certs_config Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export MAS_INSTANCE_ID=masinst1 export MAS_WORKSPACE_ID=masdev export MANAGE_CERTIFICATES_FILE_PATH_LOCAL=/my-path/manage-certs.yml ROLE_NAME='suite_manage_import_certs_config' ansible-playbook playbooks/run_role.yml License \u00a4 EPL-2.0","title":"suite_manage_import_certs_config"},{"location":"roles/suite_manage_import_certs_config/#suite_manage_import_certs_config","text":"This role extends support for importing certificates into Manage application's workspace. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior importing new certificates. You can run this as standalone role, providing a local path for a file that contains the Manage certificates definition ( manage_certificates_file_path_local variable). Or you can invoke this role inside another playbook/role, passing the Manage certificates content as a list variable ( manage_certificates ) and an alias prefix as a string variable ( manage_certificates_alias_prefix ). The certificate alias name will be concatenated with the alias prefix plus auto incremented accordingly to the number of certificates provided i.e If you provide a list with 3 certificates, and define manage_certificates_alias_prefix: myaliasprefixpart , then the alias name will be myaliasprefixpart1; myaliasprefixpart2; myaliasprefixpart3","title":"suite_manage_import_certs_config"},{"location":"roles/suite_manage_import_certs_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_import_certs_config/#mas_instance_id","text":"The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_manage_import_certs_config/#mas_workspace_id","text":"The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/suite_manage_import_certs_config/#manage_workspace_cr_name","text":"Name of the ManageWorkspace Custom Resource that will be targeted to import the new certificates. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID","title":"manage_workspace_cr_name"},{"location":"roles/suite_manage_import_certs_config/#manage_certificates_file_path_local","text":"This defines a local path pointing the certificates definition from a custom file. Sample file definition can be found in files/manage-certs-sample.yml . Required if running as standalone role Environment Variable: MANAGE_CERTIFICATES_FILE_PATH_LOCAL Default: None","title":"manage_certificates_file_path_local"},{"location":"roles/suite_manage_import_certs_config/#manage_certificates","text":"List of certificate contents to import. Used when invoking this role from another playbook/role. Optional Environment Variable: MANAGE_CERTIFICATES Default: None","title":"manage_certificates"},{"location":"roles/suite_manage_import_certs_config/#manage_certificates_alias_prefix","text":"Alias prefix for certificate names. The certificate alias name will be concatenated with this prefix plus auto incremented number. Optional Environment Variable: MANAGE_CERTIFICATES_ALIAS_PREFIX Default: None","title":"manage_certificates_alias_prefix"},{"location":"roles/suite_manage_import_certs_config/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_manage_import_certs_config/#using-local-file-path","text":"The following sample can be used to import Manage certificates for an existing Manage instance, using a local path pointing the certificates definition from a custom file. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev manage_certificates_file_path_local: /my-path/manage-certs.yml roles: - ibm.mas_devops.suite_manage_import_certs_config","title":"Using Local File Path"},{"location":"roles/suite_manage_import_certs_config/#using-variables","text":"The following sample can be used to import Manage certificates for an existing Manage instance, passing the certificates and prefix from a variable. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev manage_certificates: ['-----BEGIN CERTIFICATE----- << your-cert-content >> -----END CERTIFICATE-----'] manage_certificates_alias_prefix: \"myaliasprefixpart\" roles: - ibm.mas_devops.suite_manage_import_certs_config","title":"Using Variables"},{"location":"roles/suite_manage_import_certs_config/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export MAS_INSTANCE_ID=masinst1 export MAS_WORKSPACE_ID=masdev export MANAGE_CERTIFICATES_FILE_PATH_LOCAL=/my-path/manage-certs.yml ROLE_NAME='suite_manage_import_certs_config' ansible-playbook playbooks/run_role.yml","title":"Run Role Playbook"},{"location":"roles/suite_manage_import_certs_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_load_dbc_scripts/","text":"suite_manage_load_dbc_scripts \u00a4 This role allows to load and execute one or more ad-hoc DBC script files into Manage/Health server. Only dbc format files will be accepted. The role will assert if each script executed successfully and fail in case of errors while locating the DBC scripts or executing them against the Manage/Health server. Role Variables \u00a4 MAS Configuration \u00a4 mas_instance_id \u00a4 Defines the instance id that was used for the MAS installation. It is used to lookup the Manage/Health namespace. Required Environment Variable: MAS_INSTANCE_ID Default Value: None mas_app_id \u00a4 The MAS application ID. Must be either health or manage . Required Environment Variable: MAS_APP_ID Default Value: None Script Configuration \u00a4 dbc_script_path_local \u00a4 Defines the local path/folder where the DBC script files should be located in order to be loaded onto the Manage/Health server. Optional Environment Variable: DBC_SCRIPT_PATH_LOCAL Default Value: suite_manage_load_dbc_scripts/files Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" mas_app_id: manage dbc_script_path_local: \"{{ lookup('env', 'DBC_SCRIPT_PATH_LOCAL') }}\" roles: - ibm.mas_devops.suite_manage_load_dbc_scripts License \u00a4 EPL-2.0","title":"suite_manage_load_dbc_scripts"},{"location":"roles/suite_manage_load_dbc_scripts/#suite_manage_load_dbc_scripts","text":"This role allows to load and execute one or more ad-hoc DBC script files into Manage/Health server. Only dbc format files will be accepted. The role will assert if each script executed successfully and fail in case of errors while locating the DBC scripts or executing them against the Manage/Health server.","title":"suite_manage_load_dbc_scripts"},{"location":"roles/suite_manage_load_dbc_scripts/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_load_dbc_scripts/#mas-configuration","text":"","title":"MAS Configuration"},{"location":"roles/suite_manage_load_dbc_scripts/#mas_instance_id","text":"Defines the instance id that was used for the MAS installation. It is used to lookup the Manage/Health namespace. Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_load_dbc_scripts/#mas_app_id","text":"The MAS application ID. Must be either health or manage . Required Environment Variable: MAS_APP_ID Default Value: None","title":"mas_app_id"},{"location":"roles/suite_manage_load_dbc_scripts/#script-configuration","text":"","title":"Script Configuration"},{"location":"roles/suite_manage_load_dbc_scripts/#dbc_script_path_local","text":"Defines the local path/folder where the DBC script files should be located in order to be loaded onto the Manage/Health server. Optional Environment Variable: DBC_SCRIPT_PATH_LOCAL Default Value: suite_manage_load_dbc_scripts/files","title":"dbc_script_path_local"},{"location":"roles/suite_manage_load_dbc_scripts/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" mas_app_id: manage dbc_script_path_local: \"{{ lookup('env', 'DBC_SCRIPT_PATH_LOCAL') }}\" roles: - ibm.mas_devops.suite_manage_load_dbc_scripts","title":"Example Playbook"},{"location":"roles/suite_manage_load_dbc_scripts/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_logging_config/","text":"suite_manage_logging_config \u00a4 This role extends support for configuring IBM Cloud Object Storage to store Manage application server logs. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior configuring logging features. The default for Manage logging configuration is to use IBM Cloud Object Storage as persistent storage for Manage logging. You can run cos role to provision an IBM Cloud Object Storage or you can provide existing IBM Cloud Object Storage information to use it as storage for Manage application logs. In addition, you can also define an AWS S3 bucket as storage system for Manage logs. Role Variables \u00a4 Storage Configuration \u00a4 cos_type \u00a4 Defines the storage provider type to be used to store Manage application's logs. Available options are ibm or aws . Required Environment Variable: COS_TYPE Default Value: None Provider Options: - ibm : Configures IBM Cloud Object Storage as storage system for Manage logging. - aws : Configures AWS S3 buckets as storage system for Manage logging. Note: When running this role, the cos_bucket role will be executed underneath the covers to setup a new or existing targeted IBM Cloud object or AWS S3 storage bucket to be used to store Manage logs, therefore make sure you set the expected variables to customize your Object Storage bucket accordingly to the desired provider. MAS Configuration \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \u00a4 The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_WORKSPACE_ID Default Value: None Database Configuration \u00a4 db2_instance_name \u00a4 The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the Object Storage configuration. Required Environment Variable: DB2_INSTANCE_NAME Default Value: None db2_namespace \u00a4 The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the Object Storage configuration. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Optional Environment Variable: DB2_NAMESPACE Default Value: db2u db2_dbname \u00a4 Name of the database within the instance. Optional Environment Variable: DB2_DBNAME Default Value: BLUDB Example Playbook \u00a4 Configure IBM Cloud COS for Existing Manage Instance \u00a4 The following sample can be used to configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm db2_instance_name: db2w-manage cos_instance_name: cos-masinst1 cos_bucket_name: manage-logs-bucket ibmcloud_apikey: xxxx roles: - ibm.mas_devops.suite_manage_logging_config Configure AWS S3 for Existing Manage Instance \u00a4 The following sample can be used to configure AWS S3 for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: aws cos_bucket_action: create aws_bucket_name: manage-logs-bucket aws_region: us-east-2 aws_bucket_versioning_flag: True aws_bucket_encryption: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}' db2_instance_name: db2w-manage roles: - ibm.mas_devops.suite_manage_logging_config Provision and Configure IBM Cloud COS \u00a4 The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_type: ibm cos_instance_name: cos-masinst1 cos_bucket_name: manage-logs-bucket ibmcloud_apikey: xxxx roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_logging_config License \u00a4 EPL-2.0","title":"suite_manage_logging_config"},{"location":"roles/suite_manage_logging_config/#suite_manage_logging_config","text":"This role extends support for configuring IBM Cloud Object Storage to store Manage application server logs. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior configuring logging features. The default for Manage logging configuration is to use IBM Cloud Object Storage as persistent storage for Manage logging. You can run cos role to provision an IBM Cloud Object Storage or you can provide existing IBM Cloud Object Storage information to use it as storage for Manage application logs. In addition, you can also define an AWS S3 bucket as storage system for Manage logs.","title":"suite_manage_logging_config"},{"location":"roles/suite_manage_logging_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_logging_config/#storage-configuration","text":"","title":"Storage Configuration"},{"location":"roles/suite_manage_logging_config/#cos_type","text":"Defines the storage provider type to be used to store Manage application's logs. Available options are ibm or aws . Required Environment Variable: COS_TYPE Default Value: None Provider Options: - ibm : Configures IBM Cloud Object Storage as storage system for Manage logging. - aws : Configures AWS S3 buckets as storage system for Manage logging. Note: When running this role, the cos_bucket role will be executed underneath the covers to setup a new or existing targeted IBM Cloud object or AWS S3 storage bucket to be used to store Manage logs, therefore make sure you set the expected variables to customize your Object Storage bucket accordingly to the desired provider.","title":"cos_type"},{"location":"roles/suite_manage_logging_config/#mas-configuration","text":"","title":"MAS Configuration"},{"location":"roles/suite_manage_logging_config/#mas_instance_id","text":"The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_logging_config/#mas_workspace_id","text":"The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Required Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/suite_manage_logging_config/#database-configuration","text":"","title":"Database Configuration"},{"location":"roles/suite_manage_logging_config/#db2_instance_name","text":"The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the Object Storage configuration. Required Environment Variable: DB2_INSTANCE_NAME Default Value: None","title":"db2_instance_name"},{"location":"roles/suite_manage_logging_config/#db2_namespace","text":"The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the Object Storage configuration. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Optional Environment Variable: DB2_NAMESPACE Default Value: db2u","title":"db2_namespace"},{"location":"roles/suite_manage_logging_config/#db2_dbname","text":"Name of the database within the instance. Optional Environment Variable: DB2_DBNAME Default Value: BLUDB","title":"db2_dbname"},{"location":"roles/suite_manage_logging_config/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_manage_logging_config/#configure-ibm-cloud-cos-for-existing-manage-instance","text":"The following sample can be used to configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm db2_instance_name: db2w-manage cos_instance_name: cos-masinst1 cos_bucket_name: manage-logs-bucket ibmcloud_apikey: xxxx roles: - ibm.mas_devops.suite_manage_logging_config","title":"Configure IBM Cloud COS for Existing Manage Instance"},{"location":"roles/suite_manage_logging_config/#configure-aws-s3-for-existing-manage-instance","text":"The following sample can be used to configure AWS S3 for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: aws cos_bucket_action: create aws_bucket_name: manage-logs-bucket aws_region: us-east-2 aws_bucket_versioning_flag: True aws_bucket_encryption: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}' db2_instance_name: db2w-manage roles: - ibm.mas_devops.suite_manage_logging_config","title":"Configure AWS S3 for Existing Manage Instance"},{"location":"roles/suite_manage_logging_config/#provision-and-configure-ibm-cloud-cos","text":"The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_type: ibm cos_instance_name: cos-masinst1 cos_bucket_name: manage-logs-bucket ibmcloud_apikey: xxxx roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_logging_config","title":"Provision and Configure IBM Cloud COS"},{"location":"roles/suite_manage_logging_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_pvc_config/","text":"suite_manage_pvc_config \u00a4 This role extends support for configuring persistent volume claims for Manage application. Note This role should be executed after Manage application is deployed and activated because it needs Manage up and running prior to configuring the additional persistent volume claims. There are two options to setup new Manage PVCs: Exporting Manage PVCs variables Loading Manage PVCs variables from a file Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier for locating Manage resources. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application to configure. Used to locate the correct ManageWorkspace resource. When to use : - Always required when configuring Manage PVCs - Must match the instance ID from MAS installation - Used to construct resource names and locate Manage workspace Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance's Manage workspace will be configured with additional PVCs. Incorrect instance ID will cause configuration to fail. Related variables : - mas_workspace_id : Workspace within this instance - manage_workspace_cr_name : Defaults to {instance_id}-{workspace_id} Note : Manage application must already be deployed and activated in this instance before configuring PVCs. mas_workspace_id \u00a4 Workspace identifier for locating Manage resources. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application to configure. Used to locate the correct ManageWorkspace resource. When to use : - Always required when configuring Manage PVCs - Must match the workspace ID from Manage installation - Used to construct resource names and locate Manage workspace Valid values : Lowercase alphanumeric string (e.g., masdev , prod , test ) Impact : Determines which Manage workspace will be configured with additional PVCs. Incorrect workspace ID will cause configuration to fail. Related variables : - mas_instance_id : Instance containing this workspace - manage_workspace_cr_name : Defaults to {instance_id}-{workspace_id} Note : Manage application must already be deployed and activated in this workspace before configuring PVCs. manage_workspace_cr_name \u00a4 ManageWorkspace custom resource name to configure. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: {mas_instance_id}-{mas_workspace_id} Purpose : Specifies the exact name of the ManageWorkspace custom resource to modify with new PVC definitions. When to use : - Leave as default for standard deployments - Set explicitly if ManageWorkspace CR has a custom name - Useful when workspace CR name doesn't follow default pattern Valid values : Valid Kubernetes resource name matching an existing ManageWorkspace CR Impact : Determines which ManageWorkspace CR will be updated with PVC configuration. Incorrect name will cause configuration to fail. Related variables : - mas_instance_id : Used in default name - mas_workspace_id : Used in default name Note : The default value follows the standard naming pattern {instance_id}-{workspace_id} . Only override if your ManageWorkspace CR uses a different name. mas_app_settings_custom_persistent_volume_pvc_name \u00a4 Name for the new Persistent Volume Claim. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PVC_NAME Default: {mas_instance_id}-{mas_workspace_id}-cust-files-pvc Purpose : Specifies the name of the PVC to create for additional Manage storage. This PVC will be mounted in Manage pods. When to use : - Use default for standard custom files storage - Set custom name for specific storage purposes - Must be unique within the namespace Valid values : Valid Kubernetes PVC name (lowercase alphanumeric with hyphens) Impact : Creates a PVC with this name that will be mounted in Manage pods at the specified mount path. Related variables : - mas_app_settings_custom_persistent_volume_mount_path : Where this PVC is mounted - mas_app_settings_custom_persistent_volume_pvc_size : Size of this PVC - mas_app_settings_custom_persistent_volume_sc_name : Storage class for this PVC Note : The default name follows the pattern {instance}-{workspace}-cust-files-pvc . Choose descriptive names for multiple PVCs. mas_app_settings_custom_persistent_volume_pv_name \u00a4 Name for the Persistent Volume (storage provider dependent). Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_NAME Default: None (random name generated) Purpose : Specifies the name of the underlying Persistent Volume. Some storage providers allow or require specific PV names. When to use : - Leave unset (recommended) for automatic random name generation - Set only if your storage provider requires specific PV naming - Verify your storage class supports custom PV names before setting Valid values : Valid PV name if supported by storage provider, or leave unset Impact : If set and storage provider doesn't support custom PV names, provisioning may fail. When unset, a random name is generated automatically. Related variables : - mas_app_settings_custom_persistent_volume_sc_name : Storage class that provisions this PV - mas_app_settings_custom_persistent_volume_pvc_name : PVC that binds to this PV Note : Most storage classes use dynamic provisioning with auto-generated PV names. Only set this if you have specific requirements or your storage provider requires it. mas_app_settings_custom_persistent_volume_pvc_size \u00a4 Size of the Persistent Volume Claim. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PVC_SIZE Default: 100Gi Purpose : Specifies the storage capacity for the PVC. This determines how much data can be stored in the mounted volume. When to use : - Use default ( 100Gi ) for standard custom files storage - Increase for larger storage requirements - Consider your data volume and growth projections - Ensure sufficient space for your use case Valid values : Kubernetes storage size format (e.g., 50Gi , 100Gi , 500Gi , 1Ti ) Impact : Determines the storage capacity available to Manage. Insufficient size will cause storage full errors. Some storage classes may not support resizing after creation. Related variables : - mas_app_settings_custom_persistent_volume_pvc_name : PVC with this size - mas_app_settings_custom_persistent_volume_sc_name : Storage class providing this capacity Note : The default 100Gi is suitable for moderate custom files storage. Plan capacity based on expected data volume. Check if your storage class supports volume expansion for future growth. mas_app_settings_custom_persistent_volume_mount_path \u00a4 Mount path for the PVC in Manage containers. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_MOUNT_PATH Default: /MeaGlobalDirs Purpose : Specifies where the PVC will be mounted inside Manage server containers. This is the directory path where the persistent storage will be accessible. When to use : - Use default ( /MeaGlobalDirs ) for standard Manage global directories - Set custom path for specific storage purposes - Ensure path doesn't conflict with existing Manage directories - Path must be absolute (start with / ) Valid values : Absolute Linux filesystem path (e.g., /MeaGlobalDirs , /MyCustomDir , /opt/data ) Impact : Determines where Manage can access the persistent storage. Applications and configurations must reference this path to use the storage. Related variables : - mas_app_settings_custom_persistent_volume_pvc_name : PVC mounted at this path - mas_app_settings_custom_persistent_volume_pvc_size : Size available at this path Note : The default /MeaGlobalDirs is the standard location for Manage global directories. Ensure the path doesn't conflict with existing Manage directories or mount points. mas_app_settings_custom_persistent_volume_sc_name \u00a4 Storage class for the PVC (supports RWX or RWO). Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_STORAGE_CLASS Default: Auto-detected from available storage classes Purpose : Specifies which storage class to use for provisioning the PVC. The storage class determines the underlying storage technology and performance characteristics. When to use : - Leave unset for automatic detection (recommended) - Set explicitly when you need a specific storage class - Both ReadWriteMany (RWX) and ReadWriteOnce (RWO) access modes are supported Valid values : Any storage class name available in your cluster Impact : Determines the storage technology, performance, and access mode for the PVC. Incorrect storage class will cause provisioning to fail. Related variables : - mas_app_settings_custom_persistent_volume_pvc_size : Size to provision from this storage class - mas_app_settings_custom_persistent_volume_pv_name : PV name (if storage class supports it) Note : Both RWX (ReadWriteMany) and RWO (ReadWriteOnce) storage classes are supported. RWX allows multiple pods to access the volume simultaneously, while RWO restricts to a single pod. Choose based on your access requirements. mas_app_settings_custom_persistent_volume_file_path \u00a4 Path to custom PVC definition file (alternative to individual variables). Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_FILE_PATH Default: None Purpose : Provides an alternative method to configure PVCs using a complete YAML definition file instead of individual variables. Useful for complex or multiple PVC configurations. When to use : - Use when you have complex PVC requirements - Use when configuring multiple PVCs at once - Use when you need full control over PVC specification - Alternative to setting individual PVC variables Valid values : Absolute path to YAML file containing PVC definitions Impact : When set, the role uses this file instead of individual PVC variables. The file must contain valid PVC definitions matching the expected format. Related variables : Overrides all individual mas_app_settings_custom_persistent_volume_* variables when set Note : See files/manage-persistent-volumes-sample.yml for file format example. This approach is more flexible but requires understanding of PVC YAML structure. For simple single-PVC scenarios, use individual variables instead. Example Playbook \u00a4 Using Variables \u00a4 The following sample can be used to configure new PVCs for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev mas_app_settings_custom_persistent_volume_sc_name: \"ibmc-file-gold-gid\" mas_app_settings_custom_persistent_volume_pvc_name: \"my-manage-pvc\" mas_app_settings_custom_persistent_volume_pvc_size: \"20Gi\" mas_app_settings_custom_persistent_volume_mount_path: \"/MyOwnFolder\" roles: - ibm.mas_devops.suite_manage_pvc_config Using File Definition \u00a4 The following sample can be used to configure new PVCs for an existing Manage application instance from a custom file definition. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev mas_app_settings_custom_persistent_volume_file_path: \"/my-path/manage-pv.yml\" roles: - ibm.mas_devops.suite_manage_pvc_config Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export MAS_APP_SETTINGS_CUSTOM_PV_STORAGE_CLASS=ibmc-file-silver-gid export MAS_APP_SETTINGS_CUSTOM_PVC_NAME=my-manage-pvc export MAS_APP_SETTINGS_CUSTOM_PVC_SIZE=20Gi export MAS_APP_SETTINGS_CUSTOM_MOUNT_PATH=/MyOwnDir export MAS_APP_SETTINGS_CUSTOM_PV_FILE_PATH=/my-path/manage-pv.yml ROLE_NAME='suite_manage_pvc_config' ansible-playbook playbooks/run_role.yml License \u00a4 EPL-2.0","title":"suite_manage_pvc_config"},{"location":"roles/suite_manage_pvc_config/#suite_manage_pvc_config","text":"This role extends support for configuring persistent volume claims for Manage application. Note This role should be executed after Manage application is deployed and activated because it needs Manage up and running prior to configuring the additional persistent volume claims. There are two options to setup new Manage PVCs: Exporting Manage PVCs variables Loading Manage PVCs variables from a file","title":"suite_manage_pvc_config"},{"location":"roles/suite_manage_pvc_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_pvc_config/#mas_instance_id","text":"MAS instance identifier for locating Manage resources. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance contains the Manage application to configure. Used to locate the correct ManageWorkspace resource. When to use : - Always required when configuring Manage PVCs - Must match the instance ID from MAS installation - Used to construct resource names and locate Manage workspace Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance's Manage workspace will be configured with additional PVCs. Incorrect instance ID will cause configuration to fail. Related variables : - mas_workspace_id : Workspace within this instance - manage_workspace_cr_name : Defaults to {instance_id}-{workspace_id} Note : Manage application must already be deployed and activated in this instance before configuring PVCs.","title":"mas_instance_id"},{"location":"roles/suite_manage_pvc_config/#mas_workspace_id","text":"Workspace identifier for locating Manage resources. Required Environment Variable: MAS_WORKSPACE_ID Default: None Purpose : Identifies which workspace within the MAS instance contains the Manage application to configure. Used to locate the correct ManageWorkspace resource. When to use : - Always required when configuring Manage PVCs - Must match the workspace ID from Manage installation - Used to construct resource names and locate Manage workspace Valid values : Lowercase alphanumeric string (e.g., masdev , prod , test ) Impact : Determines which Manage workspace will be configured with additional PVCs. Incorrect workspace ID will cause configuration to fail. Related variables : - mas_instance_id : Instance containing this workspace - manage_workspace_cr_name : Defaults to {instance_id}-{workspace_id} Note : Manage application must already be deployed and activated in this workspace before configuring PVCs.","title":"mas_workspace_id"},{"location":"roles/suite_manage_pvc_config/#manage_workspace_cr_name","text":"ManageWorkspace custom resource name to configure. Optional Environment Variable: MANAGE_WORKSPACE_CR_NAME Default: {mas_instance_id}-{mas_workspace_id} Purpose : Specifies the exact name of the ManageWorkspace custom resource to modify with new PVC definitions. When to use : - Leave as default for standard deployments - Set explicitly if ManageWorkspace CR has a custom name - Useful when workspace CR name doesn't follow default pattern Valid values : Valid Kubernetes resource name matching an existing ManageWorkspace CR Impact : Determines which ManageWorkspace CR will be updated with PVC configuration. Incorrect name will cause configuration to fail. Related variables : - mas_instance_id : Used in default name - mas_workspace_id : Used in default name Note : The default value follows the standard naming pattern {instance_id}-{workspace_id} . Only override if your ManageWorkspace CR uses a different name.","title":"manage_workspace_cr_name"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_pvc_name","text":"Name for the new Persistent Volume Claim. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PVC_NAME Default: {mas_instance_id}-{mas_workspace_id}-cust-files-pvc Purpose : Specifies the name of the PVC to create for additional Manage storage. This PVC will be mounted in Manage pods. When to use : - Use default for standard custom files storage - Set custom name for specific storage purposes - Must be unique within the namespace Valid values : Valid Kubernetes PVC name (lowercase alphanumeric with hyphens) Impact : Creates a PVC with this name that will be mounted in Manage pods at the specified mount path. Related variables : - mas_app_settings_custom_persistent_volume_mount_path : Where this PVC is mounted - mas_app_settings_custom_persistent_volume_pvc_size : Size of this PVC - mas_app_settings_custom_persistent_volume_sc_name : Storage class for this PVC Note : The default name follows the pattern {instance}-{workspace}-cust-files-pvc . Choose descriptive names for multiple PVCs.","title":"mas_app_settings_custom_persistent_volume_pvc_name"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_pv_name","text":"Name for the Persistent Volume (storage provider dependent). Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_NAME Default: None (random name generated) Purpose : Specifies the name of the underlying Persistent Volume. Some storage providers allow or require specific PV names. When to use : - Leave unset (recommended) for automatic random name generation - Set only if your storage provider requires specific PV naming - Verify your storage class supports custom PV names before setting Valid values : Valid PV name if supported by storage provider, or leave unset Impact : If set and storage provider doesn't support custom PV names, provisioning may fail. When unset, a random name is generated automatically. Related variables : - mas_app_settings_custom_persistent_volume_sc_name : Storage class that provisions this PV - mas_app_settings_custom_persistent_volume_pvc_name : PVC that binds to this PV Note : Most storage classes use dynamic provisioning with auto-generated PV names. Only set this if you have specific requirements or your storage provider requires it.","title":"mas_app_settings_custom_persistent_volume_pv_name"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_pvc_size","text":"Size of the Persistent Volume Claim. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PVC_SIZE Default: 100Gi Purpose : Specifies the storage capacity for the PVC. This determines how much data can be stored in the mounted volume. When to use : - Use default ( 100Gi ) for standard custom files storage - Increase for larger storage requirements - Consider your data volume and growth projections - Ensure sufficient space for your use case Valid values : Kubernetes storage size format (e.g., 50Gi , 100Gi , 500Gi , 1Ti ) Impact : Determines the storage capacity available to Manage. Insufficient size will cause storage full errors. Some storage classes may not support resizing after creation. Related variables : - mas_app_settings_custom_persistent_volume_pvc_name : PVC with this size - mas_app_settings_custom_persistent_volume_sc_name : Storage class providing this capacity Note : The default 100Gi is suitable for moderate custom files storage. Plan capacity based on expected data volume. Check if your storage class supports volume expansion for future growth.","title":"mas_app_settings_custom_persistent_volume_pvc_size"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_mount_path","text":"Mount path for the PVC in Manage containers. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_MOUNT_PATH Default: /MeaGlobalDirs Purpose : Specifies where the PVC will be mounted inside Manage server containers. This is the directory path where the persistent storage will be accessible. When to use : - Use default ( /MeaGlobalDirs ) for standard Manage global directories - Set custom path for specific storage purposes - Ensure path doesn't conflict with existing Manage directories - Path must be absolute (start with / ) Valid values : Absolute Linux filesystem path (e.g., /MeaGlobalDirs , /MyCustomDir , /opt/data ) Impact : Determines where Manage can access the persistent storage. Applications and configurations must reference this path to use the storage. Related variables : - mas_app_settings_custom_persistent_volume_pvc_name : PVC mounted at this path - mas_app_settings_custom_persistent_volume_pvc_size : Size available at this path Note : The default /MeaGlobalDirs is the standard location for Manage global directories. Ensure the path doesn't conflict with existing Manage directories or mount points.","title":"mas_app_settings_custom_persistent_volume_mount_path"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_sc_name","text":"Storage class for the PVC (supports RWX or RWO). Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_STORAGE_CLASS Default: Auto-detected from available storage classes Purpose : Specifies which storage class to use for provisioning the PVC. The storage class determines the underlying storage technology and performance characteristics. When to use : - Leave unset for automatic detection (recommended) - Set explicitly when you need a specific storage class - Both ReadWriteMany (RWX) and ReadWriteOnce (RWO) access modes are supported Valid values : Any storage class name available in your cluster Impact : Determines the storage technology, performance, and access mode for the PVC. Incorrect storage class will cause provisioning to fail. Related variables : - mas_app_settings_custom_persistent_volume_pvc_size : Size to provision from this storage class - mas_app_settings_custom_persistent_volume_pv_name : PV name (if storage class supports it) Note : Both RWX (ReadWriteMany) and RWO (ReadWriteOnce) storage classes are supported. RWX allows multiple pods to access the volume simultaneously, while RWO restricts to a single pod. Choose based on your access requirements.","title":"mas_app_settings_custom_persistent_volume_sc_name"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_file_path","text":"Path to custom PVC definition file (alternative to individual variables). Optional Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_FILE_PATH Default: None Purpose : Provides an alternative method to configure PVCs using a complete YAML definition file instead of individual variables. Useful for complex or multiple PVC configurations. When to use : - Use when you have complex PVC requirements - Use when configuring multiple PVCs at once - Use when you need full control over PVC specification - Alternative to setting individual PVC variables Valid values : Absolute path to YAML file containing PVC definitions Impact : When set, the role uses this file instead of individual PVC variables. The file must contain valid PVC definitions matching the expected format. Related variables : Overrides all individual mas_app_settings_custom_persistent_volume_* variables when set Note : See files/manage-persistent-volumes-sample.yml for file format example. This approach is more flexible but requires understanding of PVC YAML structure. For simple single-PVC scenarios, use individual variables instead.","title":"mas_app_settings_custom_persistent_volume_file_path"},{"location":"roles/suite_manage_pvc_config/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_manage_pvc_config/#using-variables","text":"The following sample can be used to configure new PVCs for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev mas_app_settings_custom_persistent_volume_sc_name: \"ibmc-file-gold-gid\" mas_app_settings_custom_persistent_volume_pvc_name: \"my-manage-pvc\" mas_app_settings_custom_persistent_volume_pvc_size: \"20Gi\" mas_app_settings_custom_persistent_volume_mount_path: \"/MyOwnFolder\" roles: - ibm.mas_devops.suite_manage_pvc_config","title":"Using Variables"},{"location":"roles/suite_manage_pvc_config/#using-file-definition","text":"The following sample can be used to configure new PVCs for an existing Manage application instance from a custom file definition. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev mas_app_settings_custom_persistent_volume_file_path: \"/my-path/manage-pv.yml\" roles: - ibm.mas_devops.suite_manage_pvc_config","title":"Using File Definition"},{"location":"roles/suite_manage_pvc_config/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export MAS_APP_SETTINGS_CUSTOM_PV_STORAGE_CLASS=ibmc-file-silver-gid export MAS_APP_SETTINGS_CUSTOM_PVC_NAME=my-manage-pvc export MAS_APP_SETTINGS_CUSTOM_PVC_SIZE=20Gi export MAS_APP_SETTINGS_CUSTOM_MOUNT_PATH=/MyOwnDir export MAS_APP_SETTINGS_CUSTOM_PV_FILE_PATH=/my-path/manage-pv.yml ROLE_NAME='suite_manage_pvc_config' ansible-playbook playbooks/run_role.yml","title":"Run Role Playbook"},{"location":"roles/suite_manage_pvc_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_rollback/","text":"suite_rollback \u00a4 This role is to roll back Maximo Application Suite to an earlier version. Rollback is possible only in 8.11 and later. From 8.11 onwards, every version comes with a set of supported versions to which Suite can be rolled back. For example, you can roll back Maximo Application Suite from 8.11.x to 8.11.0. This role validates given MAS installation is ready for the core platform to be rolled back to a specific MAS core version, and (as long as dry run mode is not enabled) will execute the rollback. It will validate that the specified version is compatible to rollback from the current version. It will validate that the core is already running at the targetted version. It will rollback the MAS core platform to the desired version (as long as dry run is not enabled). It will validate that the core platform has been successfully reconciled at the rolled back version. It will not validate that all core services successfully deploy after the reconcile (but we will be working on this limitation). Role Variables \u00a4 mas_instance_id \u00a4 The ID of the MAS instance to rollback. Required Environment Variable: MAS_INSTANCE_ID Default: None mas_core_version \u00a4 The version of the MAS core that you want to rollback to or to validate current version. It is required when any of the ROLLBACK_MAS_CORE and VERIFY_CORE_VERSION variables is set to true . Required Environment Variable: MAS_CORE_VERSION Default: None rollback_mas_core \u00a4 When set to true will ensure that the role performs rollback operation. Optional Environment Variable: ROLLBACK_MAS_CORE Default: True verify_core_version \u00a4 When set to true will ensure that the role checks the current MAS core version matches with specified version. Optional Environment Variable: VERIFY_CORE_VERSION Default: False mas_rollback_dryrun \u00a4 When set to true will ensure that the role only performs rollback validation checks and does not make any changes to the target installation. Optional Environment Variable: MAS_ROLLBACK_DRYRUN Default: False skip_compatibility_check \u00a4 When set to true will skip compatibility check before the rollback. This is meant only for development mode. In dev mode, rollback request is accepted from one pre built version to another on the same base version though it is not listed under supported versions. For example: 8.11.0-pre.dev to 8.11.0-pre.stable. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: False Example Playbook \u00a4 Rollback to Specified Version \u00a4 Running this playbook will rollback MAS core to the specified version. If you run this playbook when you are already on the same version it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_core_version: 8.11.0 mas_rollback_dryrun: False roles: - ibm.mas_devops.suite_rollback Verify MAS Core Version \u00a4 Running this playbook will attempt to verify the current version of MAS core matches with the specified version. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_core_version: 8.11.0 mas_upgrade_dryrun: False rollback_mas_core: False verify_core_version: True roles: - ibm.mas_devops.suite_rollback License \u00a4 EPL-2.0","title":"suite_rollback"},{"location":"roles/suite_rollback/#suite_rollback","text":"This role is to roll back Maximo Application Suite to an earlier version. Rollback is possible only in 8.11 and later. From 8.11 onwards, every version comes with a set of supported versions to which Suite can be rolled back. For example, you can roll back Maximo Application Suite from 8.11.x to 8.11.0. This role validates given MAS installation is ready for the core platform to be rolled back to a specific MAS core version, and (as long as dry run mode is not enabled) will execute the rollback. It will validate that the specified version is compatible to rollback from the current version. It will validate that the core is already running at the targetted version. It will rollback the MAS core platform to the desired version (as long as dry run is not enabled). It will validate that the core platform has been successfully reconciled at the rolled back version. It will not validate that all core services successfully deploy after the reconcile (but we will be working on this limitation).","title":"suite_rollback"},{"location":"roles/suite_rollback/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_rollback/#mas_instance_id","text":"The ID of the MAS instance to rollback. Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_rollback/#mas_core_version","text":"The version of the MAS core that you want to rollback to or to validate current version. It is required when any of the ROLLBACK_MAS_CORE and VERIFY_CORE_VERSION variables is set to true . Required Environment Variable: MAS_CORE_VERSION Default: None","title":"mas_core_version"},{"location":"roles/suite_rollback/#rollback_mas_core","text":"When set to true will ensure that the role performs rollback operation. Optional Environment Variable: ROLLBACK_MAS_CORE Default: True","title":"rollback_mas_core"},{"location":"roles/suite_rollback/#verify_core_version","text":"When set to true will ensure that the role checks the current MAS core version matches with specified version. Optional Environment Variable: VERIFY_CORE_VERSION Default: False","title":"verify_core_version"},{"location":"roles/suite_rollback/#mas_rollback_dryrun","text":"When set to true will ensure that the role only performs rollback validation checks and does not make any changes to the target installation. Optional Environment Variable: MAS_ROLLBACK_DRYRUN Default: False","title":"mas_rollback_dryrun"},{"location":"roles/suite_rollback/#skip_compatibility_check","text":"When set to true will skip compatibility check before the rollback. This is meant only for development mode. In dev mode, rollback request is accepted from one pre built version to another on the same base version though it is not listed under supported versions. For example: 8.11.0-pre.dev to 8.11.0-pre.stable. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: False","title":"skip_compatibility_check"},{"location":"roles/suite_rollback/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_rollback/#rollback-to-specified-version","text":"Running this playbook will rollback MAS core to the specified version. If you run this playbook when you are already on the same version it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_core_version: 8.11.0 mas_rollback_dryrun: False roles: - ibm.mas_devops.suite_rollback","title":"Rollback to Specified Version"},{"location":"roles/suite_rollback/#verify-mas-core-version","text":"Running this playbook will attempt to verify the current version of MAS core matches with the specified version. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_core_version: 8.11.0 mas_upgrade_dryrun: False rollback_mas_core: False verify_core_version: True roles: - ibm.mas_devops.suite_rollback","title":"Verify MAS Core Version"},{"location":"roles/suite_rollback/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_uninstall/","text":"suite_uninstall \u00a4 This role removes Maximo Application Suite Core Platform. Note that it does not remove any data from MongoDb, and does not remove any applications from the MAS install, generally it should be used after suite_app_uninstall to remove all installed Maximo Application Suite applications. Role Variables \u00a4 mas_instance_id \u00a4 Defines the MAS instance to be removed from the cluster. Required Environment Variable: MAS_INSTANCE_ID Default: None mas_wipe_mongo_data \u00a4 Defines whether Mongo databases should be deleted along with MAS uninstall. Optional Environment Variable: MAS_WIPE_MONGO_DATA Default: false Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" roles: - ibm.mas_devops.suite_uninstall License \u00a4 EPL-2.0","title":"suite_uninstall"},{"location":"roles/suite_uninstall/#suite_uninstall","text":"This role removes Maximo Application Suite Core Platform. Note that it does not remove any data from MongoDb, and does not remove any applications from the MAS install, generally it should be used after suite_app_uninstall to remove all installed Maximo Application Suite applications.","title":"suite_uninstall"},{"location":"roles/suite_uninstall/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_uninstall/#mas_instance_id","text":"Defines the MAS instance to be removed from the cluster. Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_uninstall/#mas_wipe_mongo_data","text":"Defines whether Mongo databases should be deleted along with MAS uninstall. Optional Environment Variable: MAS_WIPE_MONGO_DATA Default: false","title":"mas_wipe_mongo_data"},{"location":"roles/suite_uninstall/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" roles: - ibm.mas_devops.suite_uninstall","title":"Example Playbook"},{"location":"roles/suite_uninstall/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_upgrade/","text":"suite_upgrade \u00a4 This role validates if a given MAS installation is ready for the core platform to be upgraded to a specific subscription channel, and (as long as dry run mode is not enabled) will execute the upgrade. It will validate that the current subscription channel is able to be upgraded to the target channel. It will validate that all installed applications have already been upgraded to versions compatible with the new version of the Core Platform. It will upgrade the MAS core platform to the desired channel (as long as dry run is not enabled). It will validate that the core platform has been successfully reconciled at the upgraded version. It will not validate that all core services successfully deploy after the reconcile (but we will be working on this limitation). Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier for core platform upgrade. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance's core platform to upgrade. Used to locate and validate the MAS installation. When to use : - Always required for MAS core platform upgrades - Must match the instance ID from MAS installation - Used to validate upgrade readiness and application compatibility Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance will be upgraded. Incorrect instance ID will cause upgrade to fail. Related variables : - mas_channel : Target upgrade channel for this instance Note : The role validates that all installed applications are compatible with the target MAS version before proceeding with the upgrade. mas_channel \u00a4 Target subscription channel for MAS core platform upgrade. Optional Environment Variable: MAS_CHANNEL Default: Auto-selected based on current version Purpose : Specifies the target subscription channel for MAS core platform upgrade. If not provided, the role automatically selects the next appropriate version. When to use : - Leave unset for automatic upgrade to next release - Set explicitly when you need a specific target version - Must be a valid upgrade path from current version Valid values : Valid MAS subscription channel (e.g., 8.8.x , 8.9.x , 8.10.x , 8.11.x ) Impact : Determines the target version for MAS core platform upgrade. The role validates: - Upgrade path compatibility (can upgrade from current to target) - Application compatibility (all apps support target MAS version) - If validation fails, no upgrade is performed Related variables : - mas_instance_id : Instance being upgraded - skip_compatibility_check : Whether to skip validation (not recommended) Note : When unset, the role automatically selects the next release. If already on the latest release, no action is taken. The role validates that all installed applications are compatible with the target MAS version before upgrading. mas_upgrade_dryrun \u00a4 Dry-run mode for upgrade validation only. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: false Purpose : Enables dry-run mode where the role performs all validation checks without making any changes to the MAS installation. Useful for testing upgrade paths. When to use : - Set to true to validate upgrade without executing it - Use for testing and planning upgrade paths - Recommended before production upgrades - Leave as false (default) to perform actual upgrade Valid values : true , false Impact : - true : Performs validation only (upgrade path check, application compatibility check) without modifying the installation - false : Performs validation and executes the upgrade if validation passes Related variables : - skip_compatibility_check : Controls whether compatibility validation is performed Note : Dry-run mode is highly recommended before production upgrades to identify potential issues. All validation checks are performed, including application compatibility, but no changes are made to the subscription channel or MAS core platform. skip_compatibility_check \u00a4 Skip compatibility validation before upgrade. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: false Purpose : Controls whether compatibility validation is performed before MAS core platform upgrade. Validation checks if the target channel is compatible with current MAS version and all installed applications. When to use : - Leave as false (default) for safe upgrades with validation - Set to true only in exceptional cases (not recommended) - Use only when you have verified compatibility manually Valid values : true , false Impact : - false (default): Performs comprehensive compatibility validation before upgrade (recommended) - true : Skips compatibility validation, allowing potentially incompatible upgrades Related variables : - mas_upgrade_dryrun : Controls whether upgrade is executed or only validated - mas_channel : Target channel being validated Note : WARNING - Skipping compatibility checks can lead to failed upgrades, application incompatibilities, or unstable installations. Only skip validation if you have manually verified that: 1. The upgrade path from current to target MAS version is supported 2. All installed applications are compatible with the target MAS version The default validation protects against incompatible upgrades and application version mismatches. Example Playbook \u00a4 Automatic Target Selection \u00a4 Running this playbook will upgrade MAS to the next release. If you run this playbook when you are already on the latest release then it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade Explicit Upgrade Target \u00a4 Running this playbook will attempt to upgrade MAS to the specified release. If the specified release cannot be upgraded to from the installed version of MAS then no action will be taken. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_channel: 8.8.x mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade License \u00a4 EPL-2.0","title":"suite_upgrade"},{"location":"roles/suite_upgrade/#suite_upgrade","text":"This role validates if a given MAS installation is ready for the core platform to be upgraded to a specific subscription channel, and (as long as dry run mode is not enabled) will execute the upgrade. It will validate that the current subscription channel is able to be upgraded to the target channel. It will validate that all installed applications have already been upgraded to versions compatible with the new version of the Core Platform. It will upgrade the MAS core platform to the desired channel (as long as dry run is not enabled). It will validate that the core platform has been successfully reconciled at the upgraded version. It will not validate that all core services successfully deploy after the reconcile (but we will be working on this limitation).","title":"suite_upgrade"},{"location":"roles/suite_upgrade/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_upgrade/#mas_instance_id","text":"MAS instance identifier for core platform upgrade. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance's core platform to upgrade. Used to locate and validate the MAS installation. When to use : - Always required for MAS core platform upgrades - Must match the instance ID from MAS installation - Used to validate upgrade readiness and application compatibility Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , inst1 ) Impact : Determines which MAS instance will be upgraded. Incorrect instance ID will cause upgrade to fail. Related variables : - mas_channel : Target upgrade channel for this instance Note : The role validates that all installed applications are compatible with the target MAS version before proceeding with the upgrade.","title":"mas_instance_id"},{"location":"roles/suite_upgrade/#mas_channel","text":"Target subscription channel for MAS core platform upgrade. Optional Environment Variable: MAS_CHANNEL Default: Auto-selected based on current version Purpose : Specifies the target subscription channel for MAS core platform upgrade. If not provided, the role automatically selects the next appropriate version. When to use : - Leave unset for automatic upgrade to next release - Set explicitly when you need a specific target version - Must be a valid upgrade path from current version Valid values : Valid MAS subscription channel (e.g., 8.8.x , 8.9.x , 8.10.x , 8.11.x ) Impact : Determines the target version for MAS core platform upgrade. The role validates: - Upgrade path compatibility (can upgrade from current to target) - Application compatibility (all apps support target MAS version) - If validation fails, no upgrade is performed Related variables : - mas_instance_id : Instance being upgraded - skip_compatibility_check : Whether to skip validation (not recommended) Note : When unset, the role automatically selects the next release. If already on the latest release, no action is taken. The role validates that all installed applications are compatible with the target MAS version before upgrading.","title":"mas_channel"},{"location":"roles/suite_upgrade/#mas_upgrade_dryrun","text":"Dry-run mode for upgrade validation only. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: false Purpose : Enables dry-run mode where the role performs all validation checks without making any changes to the MAS installation. Useful for testing upgrade paths. When to use : - Set to true to validate upgrade without executing it - Use for testing and planning upgrade paths - Recommended before production upgrades - Leave as false (default) to perform actual upgrade Valid values : true , false Impact : - true : Performs validation only (upgrade path check, application compatibility check) without modifying the installation - false : Performs validation and executes the upgrade if validation passes Related variables : - skip_compatibility_check : Controls whether compatibility validation is performed Note : Dry-run mode is highly recommended before production upgrades to identify potential issues. All validation checks are performed, including application compatibility, but no changes are made to the subscription channel or MAS core platform.","title":"mas_upgrade_dryrun"},{"location":"roles/suite_upgrade/#skip_compatibility_check","text":"Skip compatibility validation before upgrade. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: false Purpose : Controls whether compatibility validation is performed before MAS core platform upgrade. Validation checks if the target channel is compatible with current MAS version and all installed applications. When to use : - Leave as false (default) for safe upgrades with validation - Set to true only in exceptional cases (not recommended) - Use only when you have verified compatibility manually Valid values : true , false Impact : - false (default): Performs comprehensive compatibility validation before upgrade (recommended) - true : Skips compatibility validation, allowing potentially incompatible upgrades Related variables : - mas_upgrade_dryrun : Controls whether upgrade is executed or only validated - mas_channel : Target channel being validated Note : WARNING - Skipping compatibility checks can lead to failed upgrades, application incompatibilities, or unstable installations. Only skip validation if you have manually verified that: 1. The upgrade path from current to target MAS version is supported 2. All installed applications are compatible with the target MAS version The default validation protects against incompatible upgrades and application version mismatches.","title":"skip_compatibility_check"},{"location":"roles/suite_upgrade/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_upgrade/#automatic-target-selection","text":"Running this playbook will upgrade MAS to the next release. If you run this playbook when you are already on the latest release then it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade","title":"Automatic Target Selection"},{"location":"roles/suite_upgrade/#explicit-upgrade-target","text":"Running this playbook will attempt to upgrade MAS to the specified release. If the specified release cannot be upgraded to from the installed version of MAS then no action will be taken. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_channel: 8.8.x mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade","title":"Explicit Upgrade Target"},{"location":"roles/suite_upgrade/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_verify/","text":"suite_verify \u00a4 Verify a MAS installation is ready to use. This role will also print out the Admin Dashboard URL and the username and password of the superuser. If you want to disable these credentials being written to the output set the mas_hide_superuser_credentials to True . Role Variables \u00a4 mas_instance_id \u00a4 MAS instance identifier to verify. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to verify. The role checks that the instance is ready to use and retrieves access credentials. When to use : - Always required for verification operations - Must match the instance ID from MAS installation - Used after installation to confirm readiness Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance is verified. The role checks instance status and retrieves the Admin Dashboard URL and superuser credentials. Related variables : - mas_hide_superuser_credentials : Controls credential display in output Note : This role verifies that the MAS instance is ready to use and provides access information. Run after installation or upgrade to confirm successful deployment. mas_hide_superuser_credentials \u00a4 Hide superuser credentials in output. Optional Environment Variable: MAS_HIDE_SUPERUSER_CREDENTIALS Default: true Purpose : Controls whether superuser credentials are displayed in the verification output. When enabled, only the secret name is shown instead of actual credentials. When to use : - Leave as true (default) for security (recommended) - Set to false only when you need to see credentials in output - Use true in CI/CD pipelines and shared environments Valid values : true , false Impact : - true : Displays only the secret name containing credentials (secure) - false : Displays actual username and password in output (insecure) Related variables : - mas_instance_id : Instance whose credentials are being verified Note : SECURITY - The default true is recommended for security. Only set to false in secure, private environments where you need immediate access to credentials. Credentials can always be retrieved from the Kubernetes secret. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_hide_superuser_credentials: True roles: - ibm.mas_devops.suite_verify License \u00a4 EPL-2.0","title":"suite_verify"},{"location":"roles/suite_verify/#suite_verify","text":"Verify a MAS installation is ready to use. This role will also print out the Admin Dashboard URL and the username and password of the superuser. If you want to disable these credentials being written to the output set the mas_hide_superuser_credentials to True .","title":"suite_verify"},{"location":"roles/suite_verify/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_verify/#mas_instance_id","text":"MAS instance identifier to verify. Required Environment Variable: MAS_INSTANCE_ID Default: None Purpose : Identifies which MAS instance to verify. The role checks that the instance is ready to use and retrieves access credentials. When to use : - Always required for verification operations - Must match the instance ID from MAS installation - Used after installation to confirm readiness Valid values : Lowercase alphanumeric string, 3-12 characters (e.g., prod , dev , masinst1 ) Impact : Determines which MAS instance is verified. The role checks instance status and retrieves the Admin Dashboard URL and superuser credentials. Related variables : - mas_hide_superuser_credentials : Controls credential display in output Note : This role verifies that the MAS instance is ready to use and provides access information. Run after installation or upgrade to confirm successful deployment.","title":"mas_instance_id"},{"location":"roles/suite_verify/#mas_hide_superuser_credentials","text":"Hide superuser credentials in output. Optional Environment Variable: MAS_HIDE_SUPERUSER_CREDENTIALS Default: true Purpose : Controls whether superuser credentials are displayed in the verification output. When enabled, only the secret name is shown instead of actual credentials. When to use : - Leave as true (default) for security (recommended) - Set to false only when you need to see credentials in output - Use true in CI/CD pipelines and shared environments Valid values : true , false Impact : - true : Displays only the secret name containing credentials (secure) - false : Displays actual username and password in output (insecure) Related variables : - mas_instance_id : Instance whose credentials are being verified Note : SECURITY - The default true is recommended for security. Only set to false in secure, private environments where you need immediate access to credentials. Credentials can always be retrieved from the Kubernetes secret.","title":"mas_hide_superuser_credentials"},{"location":"roles/suite_verify/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_hide_superuser_credentials: True roles: - ibm.mas_devops.suite_verify","title":"Example Playbook"},{"location":"roles/suite_verify/#license","text":"EPL-2.0","title":"License"},{"location":"roles/turbonomic/","text":"Turbonomic \u00a4 Installs kubeturbo from any available CatalogSource, and automatically configures it to connect to a defined Turbonomic server. Note The Turbonomic Kubernetes Operator does not support disconnected installation. The kubeturbo deployment will be created using a tag rather than a digest, which prevents the use of an ImageContentSourcePolicy to configure a mirror registry for this image. Role Variables - KubeTurbo Configuration \u00a4 kubeturbo_namespace \u00a4 Set the namespace where the KubeTurbo operator will be installed. Optional Environment Variable: KUBETURBO_NAMESPACE Default: kubeturbo Role Variables - Turbonomic Server Configuration \u00a4 turbonomic_target_name \u00a4 The cluster name is required to install Kubeturbo. The agent component is deployed onto target Kubernetes and OpenShift cluster which then send data to the Turbonomic ARM server. Required Environment Variable: TURBONOMIC_TARGET_NAME Default: None turbonomic_server_url \u00a4 The route is required to access the Turbonomics instance. Kubeturbo communicates with the Turbo Server using the supplied turbonomic route as the Turbonomic Server endpoint while configuring kubeturbo. Required Environment Variable: TURBONOMIC_SERVER_URL Default: None turbonomic_server_version \u00a4 The version of the Turbonomic server you are connecting to. Optional Environment Variable: TURBONOMIC_SERVER_VERSION Default: None turbonomic_username \u00a4 The username to authenticate with the Turbonomic server. Required Environment Variable: TURBONOMIC_USERNAME Default: None turbonomic_password \u00a4 The password to authenticate with the Turbonomic server. Required Environment Variable: TURBONOMIC_PASSWORD Default: None Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: turbonomic_server_url: https://myturbonomicserver.com turbonomic_server_version: \"8.9.4\" turbonomic_username: user turbonomic_password: passw0rd turbonomic_target_name: myocp roles: - ibm.mas_devops.turbonomic License \u00a4 EPL-2.0","title":"turbonomic"},{"location":"roles/turbonomic/#turbonomic","text":"Installs kubeturbo from any available CatalogSource, and automatically configures it to connect to a defined Turbonomic server. Note The Turbonomic Kubernetes Operator does not support disconnected installation. The kubeturbo deployment will be created using a tag rather than a digest, which prevents the use of an ImageContentSourcePolicy to configure a mirror registry for this image.","title":"Turbonomic"},{"location":"roles/turbonomic/#role-variables-kubeturbo-configuration","text":"","title":"Role Variables - KubeTurbo Configuration"},{"location":"roles/turbonomic/#kubeturbo_namespace","text":"Set the namespace where the KubeTurbo operator will be installed. Optional Environment Variable: KUBETURBO_NAMESPACE Default: kubeturbo","title":"kubeturbo_namespace"},{"location":"roles/turbonomic/#role-variables-turbonomic-server-configuration","text":"","title":"Role Variables - Turbonomic Server Configuration"},{"location":"roles/turbonomic/#turbonomic_target_name","text":"The cluster name is required to install Kubeturbo. The agent component is deployed onto target Kubernetes and OpenShift cluster which then send data to the Turbonomic ARM server. Required Environment Variable: TURBONOMIC_TARGET_NAME Default: None","title":"turbonomic_target_name"},{"location":"roles/turbonomic/#turbonomic_server_url","text":"The route is required to access the Turbonomics instance. Kubeturbo communicates with the Turbo Server using the supplied turbonomic route as the Turbonomic Server endpoint while configuring kubeturbo. Required Environment Variable: TURBONOMIC_SERVER_URL Default: None","title":"turbonomic_server_url"},{"location":"roles/turbonomic/#turbonomic_server_version","text":"The version of the Turbonomic server you are connecting to. Optional Environment Variable: TURBONOMIC_SERVER_VERSION Default: None","title":"turbonomic_server_version"},{"location":"roles/turbonomic/#turbonomic_username","text":"The username to authenticate with the Turbonomic server. Required Environment Variable: TURBONOMIC_USERNAME Default: None","title":"turbonomic_username"},{"location":"roles/turbonomic/#turbonomic_password","text":"The password to authenticate with the Turbonomic server. Required Environment Variable: TURBONOMIC_PASSWORD Default: None","title":"turbonomic_password"},{"location":"roles/turbonomic/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: turbonomic_server_url: https://myturbonomicserver.com turbonomic_server_version: \"8.9.4\" turbonomic_username: user turbonomic_password: passw0rd turbonomic_target_name: myocp roles: - ibm.mas_devops.turbonomic","title":"Example Playbook"},{"location":"roles/turbonomic/#license","text":"EPL-2.0","title":"License"}]}