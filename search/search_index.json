{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MAS DevOps Ansible Collection \u00a4 The ibm.mas_devops Ansible Collection is published on Ansible Galaxy and works with all supported releases of IBM Maximo Application Suite. Release information for the collection can be found in GitHub . Usage \u00a4 Run a Playbook \u00a4 The collection includes a number of playbooks that string together multiple roles, you can directly invoke them after installing the collection: ansible-playbook ibm.mas_devops.lite_core_roks Run a Role \u00a4 If you only want to perform a single action, you can directly invoke one of our roles from the command line without the need to build a playbook: ansible localhost -m include_role -a name=ibm.mas_devops.ocp_verify You can also use the run_role playbook: ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role Running in Docker \u00a4 The easiest way to use this collection is to take advantage of the ibmmas/cli container image, this negates the need to install anything on your local machine (other than docker - or podman if you prefer). docker run -ti --rm --pull always quay.io/ibmmas/cli Local Install \u00a4 Install Python & Ansible \u00a4 Python 3.9 is recommended as it is the most widely used version of Python within our development team, but any in-support 3.x version of Python should work fine. python3 --version python3 -m pip install ansible junit_xml pymongo xmljson jmespath kubernetes==12.0.1 openshift==0.12.1 ansible --version ansible-playbook --version We recommend using the latest version of ansible-core at all times (at time of writing this was v2.12.3) and the collection has a minimum supported version of ansible-core v2.10.3 which is enforced by the ibm.mas_devops.ansible_version_check role. Install OpenShift CLI \u00a4 If you do not already have the oc command line tool, you can download it as below: wget -q https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.14.7/openshift-client-linux-4.14.7.tar.gz tar -zxf openshift-client-linux.tar.gz mv oc kubectl /usr/local/bin/ rm -rf openshift-client-linux.tar.gz oc version Install IBM Cloud CLI \u00a4 If you are using this collection to manage an OpenShift cluster in IBM Cloud RedHat OpenShift Kubernetes Service (ROKS), then you must also install the IBM Cloud CLI: curl -sL https://raw.githubusercontent.com/IBM-Cloud/ibm-cloud-developer-tools/master/linux-installer/idt-installer | bash ibmcloud version` Install the Ansible Collection \u00a4 Install the collection direct from Ansible Galaxy ansible-galaxy collection install ibm.mas_devops Optionally, you can also pin the version of the collection that you install, allowing you to control exactly what version of the collection is in use in your automation: ansible-galaxy collection install ibm.mas_devops:18.10.4 Ansible Automation Platform \u00a4 If you wish to use Red Hat Ansible Automation Platform then a Automation Execution Environment image is available at quay.io/ibmmas/ansible-devops-ee that contains the ibm.mas_devops collection at the same release level, plus required client packages and access to the automation content collections supported by Red Hat. More details on how to use the ansible-devops execution environment can be found here Support \u00a4 This Ansible collection is developed by the IBM Maximo Application Suite development team, customers may raise support tickets via the same routes they would an issue with the product itself, or raise an issue directly in the GitHub repository .","title":"Home"},{"location":"#mas-devops-ansible-collection","text":"The ibm.mas_devops Ansible Collection is published on Ansible Galaxy and works with all supported releases of IBM Maximo Application Suite. Release information for the collection can be found in GitHub .","title":"MAS DevOps Ansible Collection"},{"location":"#usage","text":"","title":"Usage"},{"location":"#run-a-playbook","text":"The collection includes a number of playbooks that string together multiple roles, you can directly invoke them after installing the collection: ansible-playbook ibm.mas_devops.lite_core_roks","title":"Run a Playbook"},{"location":"#run-a-role","text":"If you only want to perform a single action, you can directly invoke one of our roles from the command line without the need to build a playbook: ansible localhost -m include_role -a name=ibm.mas_devops.ocp_verify You can also use the run_role playbook: ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role","title":"Run a Role"},{"location":"#running-in-docker","text":"The easiest way to use this collection is to take advantage of the ibmmas/cli container image, this negates the need to install anything on your local machine (other than docker - or podman if you prefer). docker run -ti --rm --pull always quay.io/ibmmas/cli","title":"Running in Docker"},{"location":"#local-install","text":"","title":"Local Install"},{"location":"#install-python-ansible","text":"Python 3.9 is recommended as it is the most widely used version of Python within our development team, but any in-support 3.x version of Python should work fine. python3 --version python3 -m pip install ansible junit_xml pymongo xmljson jmespath kubernetes==12.0.1 openshift==0.12.1 ansible --version ansible-playbook --version We recommend using the latest version of ansible-core at all times (at time of writing this was v2.12.3) and the collection has a minimum supported version of ansible-core v2.10.3 which is enforced by the ibm.mas_devops.ansible_version_check role.","title":"Install Python &amp; Ansible"},{"location":"#install-openshift-cli","text":"If you do not already have the oc command line tool, you can download it as below: wget -q https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.14.7/openshift-client-linux-4.14.7.tar.gz tar -zxf openshift-client-linux.tar.gz mv oc kubectl /usr/local/bin/ rm -rf openshift-client-linux.tar.gz oc version","title":"Install OpenShift CLI"},{"location":"#install-ibm-cloud-cli","text":"If you are using this collection to manage an OpenShift cluster in IBM Cloud RedHat OpenShift Kubernetes Service (ROKS), then you must also install the IBM Cloud CLI: curl -sL https://raw.githubusercontent.com/IBM-Cloud/ibm-cloud-developer-tools/master/linux-installer/idt-installer | bash ibmcloud version`","title":"Install IBM Cloud CLI"},{"location":"#install-the-ansible-collection","text":"Install the collection direct from Ansible Galaxy ansible-galaxy collection install ibm.mas_devops Optionally, you can also pin the version of the collection that you install, allowing you to control exactly what version of the collection is in use in your automation: ansible-galaxy collection install ibm.mas_devops:18.10.4","title":"Install the Ansible Collection"},{"location":"#ansible-automation-platform","text":"If you wish to use Red Hat Ansible Automation Platform then a Automation Execution Environment image is available at quay.io/ibmmas/ansible-devops-ee that contains the ibm.mas_devops collection at the same release level, plus required client packages and access to the automation content collections supported by Red Hat. More details on how to use the ansible-devops execution environment can be found here","title":"Ansible Automation Platform"},{"location":"#support","text":"This Ansible collection is developed by the IBM Maximo Application Suite development team, customers may raise support tickets via the same routes they would an issue with the product itself, or raise an issue directly in the GitHub repository .","title":"Support"},{"location":"execution-environment/","text":"Execution Environment \u00a4 Details on the Red Hat Ansible Automation Platform Execution Environment for the ibm.mas_devops Ansible Collection. Execution Environment Image \u00a4 The execution environment image for ansible-devops builds on the latest ansible-automation-platform-24/ee-supported-rhel9 image from Red Hat that provides the ansible-core and Red Hat supported collections. The ansible-devops-ee image includes the ibm.mas_devops collection and all required client libraries to function. The image is uploaded to quay.io at quay.io/ibmmas/ansible-devops-ee How to setup Anisble Automation Platform \u00a4 Organization \u00a4 An Organization is a logical collection of Users, Teams, Projects, and Inventories, and is the highest level in the automation controller object hierarchy. Create an organization if you don't already have one: Inventory \u00a4 An Inventory is a collection of hosts against which jobs may be launched, the same as an Ansible inventory file. The ibm.mas_devops collection runs against localhost so an Inventory of hosts just requires the one host of localhost to be added but this might be important for any other roles you might want to execute outside of this collection but within this organization. Create an initial inventory if one doesn't exist yet: Ensure that the host entry has the following variables set to ensure a local connection: Execution Environment \u00a4 In Ansible Automation Platform (AAP) you can setup a new Execution Environment by specifying the image and tag. You can use either a versioned tag or latest such as quay.io/ibmmas/ansible-devops-ee:latest or quay.io/ibmmas/ansible-devops-ee:24.0.0 Credentials \u00a4 To use your own playbooks that are in source control management (SCM) you will need to setup credentials in AAP to fetch these (unless the repo is public). An example of this is providing a GitHub fine-grained access token which will just allow the token to read the contents of the repo. \u00a1 creds-2 Depending on if you have an exiting cluster or not, then you will need to setup the OpenShift or Kubernetes API Bearer Token credentials to access the Openshift cluster, which will be required on any Jobs that interect with the cluster. Project \u00a4 Once you have the execution environment setup you can now create a Project that will point to the source of your playbooks. It is recommended that you use your own source of playbooks rather than the playbooks contained in this repo, to give you full control over what is run to suit your needs. Create the Project and set the Execution Environment to be the one created eariler, and set the source control details and credentials needed for AAP to read your playbooks. Job Templates \u00a4 The Job Templates are what each Job is launched from and contains the details of what Playbook to execute. The Job is the executed instance of a Template that contains any specific values required. To create the Template add the Inventory, Project and Execution Environment setup previously. The playbook dropdown should contain all the playbooks that are found in your SCM (if no playbooks are shown then check the Troubleshooting section). At this point you can choose any other settings that you might want to use, and it is recommended to check the AAP documentation on this. Once the Job template is created then you can click on the created template and navigate to Survey . This is where you can provide any secure variables for your playbook that you don't want to keep in source control. You can choose the questions to ask and what type the variable should be. The variable name should make the variable name that the corresponding role is expecting. Finally, ensure that the Survey is Enabled: Workflow Templates \u00a4 The Workflow Templates allow you to configure a number of job templates (or other workflow templates) together. This can be useful when you want to use other roles from other collections (see the AAP Certified Content here ) as well as the ansible-devops collection. For example, you might want to run some AWS or VMWare roles to configure resources after or before the ansible-devops collection roles. How to run a Job \u00a4 Now you have the Job Template setup, you can launch the Job from that Template. Navigate to the Templates view and click the launch icon: Enter any survey questions you have configured: The Job now launches and you can see the output. The list of executing and executed jobs can be seen from the Jobs seciton Examples of playbooks \u00a4 You can use the ansbile-devops playbooks as a reference on how to setup your own playbooks. Defining your own playbooks gives you full control over what roles can be run and also include your own roles or roles from the supported Red Hat collection. If you are using the playbooks as a starting point then please note the following changes that would be required: Remove any pre_tasks related to environment variables Remove any lookups of environment variables from the playbook oneclick_core.yml \u00a4 An example of this is taking the oneclick_core.yml playbook: --- - hosts: localhost any_errors_fatal: true vars: # Install SLS # Note: We need to create some intermediate variables to construct sls_mongodb_cfg_file, # This is the only reason they feature here, all the roles that use these variables would also # load them directly from the same environment variables if they were not defined here. mas_config_dir: \"{{ lookup('env', 'MAS_CONFIG_DIR') }}\" mongodb_namespace: \"{{ lookup('env', 'MONGODB_NAMESPACE') | default('mongoce', True) }}\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" # Core Services Configuration mas_channel: \"{{ lookup('env', 'MAS_CHANNEL') | default('9.0.x', true) }}\" # Workspace Configuration mas_workspace_name: \"{{ lookup('env', 'MAS_WORKSPACE_NAME') | default('MAS Development', true) }}\" mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') | default('masdev', true) }}\" pre_tasks: # For the full set of supported environment variables refer to the playbook documentation - name: Check for required environment variables assert: that: # IBM - lookup('env', 'IBM_ENTITLEMENT_KEY') != \"\" # MAS - lookup('env', 'MAS_INSTANCE_ID') != \"\" - lookup('env', 'MAS_CONFIG_DIR') != \"\" # SLS - (lookup('env', 'SLS_LICENSE_ID') != \"\" and lookup('env', 'SLS_LICENSE_FILE') != \"\") or (lookup('env', 'SLS_ENTITLEMENT_FILE') != \"\") # DRO - lookup('env', 'DRO_CONTACT_EMAIL') != \"\" - lookup('env', 'DRO_CONTACT_FIRSTNAME') != \"\" - lookup('env', 'DRO_CONTACT_LASTNAME') != \"\" fail_msg: \"One or more required environment variables are not defined\" roles: # 1. Install cluster-scoped dependencies (e.g. Cert-Manager, Operator Catalogs) & Grafana - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.cert_manager - ibm.mas_devops.grafana # 2. Install MongoDb - ibm.mas_devops.mongodb # 3. Install SLS # Set sls_url, sls_tls_crt_local_file_path, sls_registration_key variables to skip install and set up SLSCfg for # an existing installation of SLS - ibm.mas_devops.sls # 4 Install DRO - ibm.mas_devops.dro # 5. Generate a Workspace - ibm.mas_devops.gencfg_workspace # 6. Install & configure MAS - ibm.mas_devops.suite_dns - ibm.mas_devops.suite_certs - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify and changing it to look like the following: --- - name: \"oneclick-core\" hosts: localhost any_errors_fatal: true vars: mas_config_dir: \"/tmp\" mongodb_namespace: \"mongoce\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" # Core Services Configuration mas_channel: \"9.0.x\" mas_instance_id: \"aap1\" # Workspace Configuration mas_workspace_name: \"MAS Development\" mas_workspace_id: \"masdev\" entitlement_file: \"license_file/entitlement.lic\" dro_contact: email: \"whitfiea@uk.ibm.com\" first_name: \"Andrew\" last_name: \"Whitfield\" roles: # 1. Install cluster-scoped dependencies (e.g. Cert-Manager, Operator Catalogs) & Grafana - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.cert_manager # 2. Install MongoDb - ibm.mas_devops.mongodb # 3. Install SLS - ibm.mas_devops.sls # 4 Install DRO - ibm.mas_devops.dro # 5. Generate a Workspace - ibm.mas_devops.gencfg_workspace # 6. Install & configure MAS - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify The above has removed any environment variable lookups in the playbook itself, and also modified the roles so it doesn't run the grafana role. ocp-provision \u00a4 If you need to set an environment variable then you can do this in the playbook as well, this would be needed in certain cases such as setting the AWS cli details. The aws_access_key_id and aws_secret_access_key are variables set by AAP as a result of the Survey questions: --- - name: \"ocp-provision\" hosts: localhost vars: cluster_name: testcluster cluster_type: rosa ocp_version: 4.14.35 rosa_compute_nodes: 3 environment: AWS_DEFAULT_REGION: us-east-1 AWS_ACCESS_KEY_ID: \"{{ aws_access_key_id }}\" AWS_SECRET_ACCESS_KEY: \"{{ aws_secret_access_key }}\" roles: # 1. Provision the ROSA cluster - ibm.mas_devops.ocp_provision # 2. Login and verify the cluster is ready - ibm.mas_devops.ocp_login - ibm.mas_devops.ocp_verify # 3. Set up storage classes - ibm.mas_devops.ocp_efs backup/restore \u00a4 The backup and restore tasks and playbooks provided in the ibm.mas_devops collection has a number of differences with other roles/playbooks that can make creating the playbook difficult. The following playbook is an example of how a backup for core playbook can be written to work with AAP, along with a rclone.conf.j2 file that should be stored in the same location as the playbook in your SCM. The playbook needs to set both the environment so that the ansible_env var is set, and for task vars to be set to override the common_vars that are looking for environment variables on the local controller (note that environment does not update the local controller environment so lookups using the ansible builtin env won't pick up those values). This playbook requires the following survey questions to be setup: { \"aws_access_key_id\": \"$encrypted$\", \"aws_secret_access_key\": \"$encrypted$\", \"ocp_token\": \"$encrypted$\", \"ocp_server\": \"https://api.your_ocp_cluster:6443\" } --- - name: \"Backup/Restore MAS Core\" hosts: localhost any_errors_fatal: true vars: # Define the target for backup/restore mas_instance_id: aap1 # Define what action to perform masbr_action: backup # Define storage type masbr_storage_type: cloud masbr_storage_cloud_rclone_file: rclone.conf masbr_storage_cloud_rclone_name: masbr masbr_storage_cloud_bucket: mas-backup # Define what to backup/restore masbr_job_component: name: \"core\" instance: \"{{ mas_instance_id }}\" namespace: \"mas-{{ mas_instance_id }}-core\" # Configure path to backup_restore tasks that should be present in the execution environment role_path: \"/usr/share/ansible/collections/ansible_collections/ibm/mas_devops/roles/suite_backup_restore\" # Set here to get into ansible_env environment: MASBR_STORAGE_TYPE: cloud MASBR_STORAGE_CLOUD_RCLONE_FILE: rclone.conf MASBR_STORAGE_CLOUD_RCLONE_NAME: masbr MASBR_STORAGE_CLOUD_BUCKET: mas-backup pre_tasks: - name: \"Asserts aws_access_key_id secret defined\" assert: that: aws_access_key_id is defined fail_msg: \"aws_access_key_id not defined\" - name: \"Asserts aws_secret_access_key secret defined\" assert: that: aws_secret_access_key is defined fail_msg: \"aws_secret_access_key not defined\" # Template out the rclone.conf so no credentials are stored in git - name: \"Create rclone.conf\" template: src: \"{{ masbr_storage_cloud_rclone_file }}.j2\" dest: \"{{ masbr_storage_cloud_rclone_file }}\" roles: - ibm.mas_devops.ocp_login tasks: # Common checks before run tasks # ------------------------------------------------------------------------- - name: \"Before run tasks\" include_tasks: file: \"{{ role_path }}/../../common_tasks/backup_restore/before_run_tasks.yml\" vars: _job_type: \"{{ masbr_action }}\" masbr_storage_type: cloud masbr_storage_cloud_rclone_file: rclone.conf masbr_storage_cloud_rclone_name: masbr masbr_storage_cloud_bucket: mas-backup # Create k8s Job to run backup/restore tasks # ------------------------------------------------------------------------- - name: \"Create k8s Job to run {{ masbr_action }} tasks\" when: masbr_create_task_job include_tasks: file: \"{{ role_path }}/../../common_tasks/backup_restore/create_run_tasks_job.yml\" vars: _rt_playbook_name: \"br_core\" _rt_env: - name: \"MASBR_ACTION\" value: \"{{ masbr_action }}\" - name: \"MASBR_JOB_VERSION\" value: \"{{ masbr_job_version }}\" - name: \"MAS_INSTANCE_ID\" value: \"{{ mas_instance_id }}\" masbr_storage_type: cloud masbr_storage_cloud_rclone_file: rclone.conf masbr_storage_cloud_rclone_name: masbr masbr_storage_cloud_bucket: mas-backup # Run backup/restore tasks locally # ------------------------------------------------------------------------- - name: \"Run {{ masbr_action }} tasks\" when: not masbr_create_task_job block: - name: \"MongoDB: {{ masbr_action }}\" include_role: name: ibm.mas_devops.mongodb vars: mongodb_action: \"{{ masbr_action }}\" mas_app_id: \"core\" masbr_storage_type: cloud masbr_storage_cloud_rclone_file: rclone.conf masbr_storage_cloud_rclone_name: masbr masbr_storage_cloud_bucket: mas-backup - name: \"MAS Core namespace: {{ masbr_action }}\" include_role: name: ibm.mas_devops.suite_backup_restore vars: masbr_storage_type: cloud masbr_storage_cloud_rclone_file: rclone.conf masbr_storage_cloud_rclone_name: masbr masbr_storage_cloud_bucket: mas-backup The following file is stored in your SCM and is a template to be injected with the credentials needed during the play. This allows the file to be stored in SCM without exposing credentials. rclone.conf.j2 [masbr] type = s3 provider = Minio endpoint = http://minio-api.apps.mydomain.com access_key_id = \"{{ aws_access_key_id }}\" secret_access_key = \"{{ aws_secret_access_key }}\" region = minio Troubleshooting \u00a4 Helpful Links \u00a4 Red Hat AAP documentation Installing AAP in OCP Other install methods of AAP AAP Certified Content Can't see all output in log file \u00a4 The output for the job doesn't output all the data in the standard view. To see the expanded data you must click on the entry to see the details and then select JSON: My playbook or updated playbook can't be found \u00a4 If you have updated your Playbook in your SCM but it is not reflected in AAP then you can sync the Project. Navigate to the Project view and click the sync icon so the Project gets the latest revision. Job failed can I re-launch it? \u00a4 If the job fails and you want to relaunch with the same variables (from both hosts and survey), then you can re-launch the job from the Job page or the Jobs list page How to provide supporting files? \u00a4 If you have a file that is not a playbook but you want to reference it, then you can include this in the same repo as your playbooks as AAP will be syncing the whole repo. You can then reference these files from your playbook or roles, using a pth relative to the playbook being run. An exmaple of this is the entitlement_file which can be contained in your repo: and then referenced in your playbook in the path \"license_file/entitlement.lic\" : --- - hosts: localhost any_errors_fatal: true vars: mas_config_dir: \"/tmp\" mongodb_namespace: \"mongoce\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" entitlement_file: \"license_file/entitlement.lic\" roles: - ibm.mas_devops.mongodb - ibm.mas_devops.sls How do I set environment variables? \u00a4 Running in AAP means you can't set environment variables on the ansible controller before a playbook is executed. If a role in the ibm.mas_devops collection has an environment variable to be set there is normally a corresponding ansible variable. For exmaple, in the documentation for (kafka_action)[https://ibm-mas.github.io/ansible-devops/roles/kafka/#kafka_action] in the kafka role it says to set the environment variable KAFKA_VERSION , this is just setting the ansible role varible called kafka_version which you can set in your playbook. With ansible predence in place, the play vars or role vars will override the default vars that use the environment variable. Example setting play vars: --- - hosts: localhost any_errors_fatal: true vars: kafka_version: 3.7.0 roles: - ibm.mas_devops.kafka Example setting role vars: --- - hosts: localhost any_errors_fatal: true roles: - role: ibm.mas_devops.kafka vars: kafka_version: 3.7.0 One caveat to the above is the backup/restore tasks as these use a combination of ansible vars and expecting environment variables to be set in the ansible_env variable. In order to work with the backup/restore tasks you need to set both environment and role/task vars (not play vars). Example, showing environment and task vars set. See backup exmaple for more complete play: - name: \"Backup/Restore MAS Core\" hosts: localhost any_errors_fatal: true vars: # Configure path to backup_restore tasks role_path: \"/usr/share/ansible/collections/ansible_collections/ibm/mas_devops/roles/suite_backup_restore\" # Set here to get into ansible_env environment: MASBR_STORAGE_TYPE: cloud MASBR_STORAGE_CLOUD_RCLONE_FILE: rclone.conf MASBR_STORAGE_CLOUD_RCLONE_NAME: masbr MASBR_STORAGE_CLOUD_BUCKET: mas-backup tasks: # Common checks before run tasks # ------------------------------------------------------------------------- - name: \"Before run tasks\" include_tasks: file: \"{{ role_path }}/../../common_tasks/backup_restore/before_run_tasks.yml\" vars: _job_type: \"{{ masbr_action }}\" masbr_storage_type: cloud masbr_storage_cloud_rclone_file: rclone.conf masbr_storage_cloud_rclone_name: masbr masbr_storage_cloud_bucket: mas-backup","title":"Ansible Automation Platform"},{"location":"execution-environment/#execution-environment","text":"Details on the Red Hat Ansible Automation Platform Execution Environment for the ibm.mas_devops Ansible Collection.","title":"Execution Environment"},{"location":"execution-environment/#execution-environment-image","text":"The execution environment image for ansible-devops builds on the latest ansible-automation-platform-24/ee-supported-rhel9 image from Red Hat that provides the ansible-core and Red Hat supported collections. The ansible-devops-ee image includes the ibm.mas_devops collection and all required client libraries to function. The image is uploaded to quay.io at quay.io/ibmmas/ansible-devops-ee","title":"Execution Environment Image"},{"location":"execution-environment/#how-to-setup-anisble-automation-platform","text":"","title":"How to setup Anisble Automation Platform"},{"location":"execution-environment/#organization","text":"An Organization is a logical collection of Users, Teams, Projects, and Inventories, and is the highest level in the automation controller object hierarchy. Create an organization if you don't already have one:","title":"Organization"},{"location":"execution-environment/#inventory","text":"An Inventory is a collection of hosts against which jobs may be launched, the same as an Ansible inventory file. The ibm.mas_devops collection runs against localhost so an Inventory of hosts just requires the one host of localhost to be added but this might be important for any other roles you might want to execute outside of this collection but within this organization. Create an initial inventory if one doesn't exist yet: Ensure that the host entry has the following variables set to ensure a local connection:","title":"Inventory"},{"location":"execution-environment/#execution-environment_1","text":"In Ansible Automation Platform (AAP) you can setup a new Execution Environment by specifying the image and tag. You can use either a versioned tag or latest such as quay.io/ibmmas/ansible-devops-ee:latest or quay.io/ibmmas/ansible-devops-ee:24.0.0","title":"Execution Environment"},{"location":"execution-environment/#credentials","text":"To use your own playbooks that are in source control management (SCM) you will need to setup credentials in AAP to fetch these (unless the repo is public). An example of this is providing a GitHub fine-grained access token which will just allow the token to read the contents of the repo. \u00a1 creds-2 Depending on if you have an exiting cluster or not, then you will need to setup the OpenShift or Kubernetes API Bearer Token credentials to access the Openshift cluster, which will be required on any Jobs that interect with the cluster.","title":"Credentials"},{"location":"execution-environment/#project","text":"Once you have the execution environment setup you can now create a Project that will point to the source of your playbooks. It is recommended that you use your own source of playbooks rather than the playbooks contained in this repo, to give you full control over what is run to suit your needs. Create the Project and set the Execution Environment to be the one created eariler, and set the source control details and credentials needed for AAP to read your playbooks.","title":"Project"},{"location":"execution-environment/#job-templates","text":"The Job Templates are what each Job is launched from and contains the details of what Playbook to execute. The Job is the executed instance of a Template that contains any specific values required. To create the Template add the Inventory, Project and Execution Environment setup previously. The playbook dropdown should contain all the playbooks that are found in your SCM (if no playbooks are shown then check the Troubleshooting section). At this point you can choose any other settings that you might want to use, and it is recommended to check the AAP documentation on this. Once the Job template is created then you can click on the created template and navigate to Survey . This is where you can provide any secure variables for your playbook that you don't want to keep in source control. You can choose the questions to ask and what type the variable should be. The variable name should make the variable name that the corresponding role is expecting. Finally, ensure that the Survey is Enabled:","title":"Job Templates"},{"location":"execution-environment/#workflow-templates","text":"The Workflow Templates allow you to configure a number of job templates (or other workflow templates) together. This can be useful when you want to use other roles from other collections (see the AAP Certified Content here ) as well as the ansible-devops collection. For example, you might want to run some AWS or VMWare roles to configure resources after or before the ansible-devops collection roles.","title":"Workflow Templates"},{"location":"execution-environment/#how-to-run-a-job","text":"Now you have the Job Template setup, you can launch the Job from that Template. Navigate to the Templates view and click the launch icon: Enter any survey questions you have configured: The Job now launches and you can see the output. The list of executing and executed jobs can be seen from the Jobs seciton","title":"How to run a Job"},{"location":"execution-environment/#examples-of-playbooks","text":"You can use the ansbile-devops playbooks as a reference on how to setup your own playbooks. Defining your own playbooks gives you full control over what roles can be run and also include your own roles or roles from the supported Red Hat collection. If you are using the playbooks as a starting point then please note the following changes that would be required: Remove any pre_tasks related to environment variables Remove any lookups of environment variables from the playbook","title":"Examples of playbooks"},{"location":"execution-environment/#oneclick_coreyml","text":"An example of this is taking the oneclick_core.yml playbook: --- - hosts: localhost any_errors_fatal: true vars: # Install SLS # Note: We need to create some intermediate variables to construct sls_mongodb_cfg_file, # This is the only reason they feature here, all the roles that use these variables would also # load them directly from the same environment variables if they were not defined here. mas_config_dir: \"{{ lookup('env', 'MAS_CONFIG_DIR') }}\" mongodb_namespace: \"{{ lookup('env', 'MONGODB_NAMESPACE') | default('mongoce', True) }}\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" # Core Services Configuration mas_channel: \"{{ lookup('env', 'MAS_CHANNEL') | default('9.0.x', true) }}\" # Workspace Configuration mas_workspace_name: \"{{ lookup('env', 'MAS_WORKSPACE_NAME') | default('MAS Development', true) }}\" mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') | default('masdev', true) }}\" pre_tasks: # For the full set of supported environment variables refer to the playbook documentation - name: Check for required environment variables assert: that: # IBM - lookup('env', 'IBM_ENTITLEMENT_KEY') != \"\" # MAS - lookup('env', 'MAS_INSTANCE_ID') != \"\" - lookup('env', 'MAS_CONFIG_DIR') != \"\" # SLS - (lookup('env', 'SLS_LICENSE_ID') != \"\" and lookup('env', 'SLS_LICENSE_FILE') != \"\") or (lookup('env', 'SLS_ENTITLEMENT_FILE') != \"\") # DRO - lookup('env', 'DRO_CONTACT_EMAIL') != \"\" - lookup('env', 'DRO_CONTACT_FIRSTNAME') != \"\" - lookup('env', 'DRO_CONTACT_LASTNAME') != \"\" fail_msg: \"One or more required environment variables are not defined\" roles: # 1. Install cluster-scoped dependencies (e.g. Cert-Manager, Operator Catalogs) & Grafana - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.cert_manager - ibm.mas_devops.grafana # 2. Install MongoDb - ibm.mas_devops.mongodb # 3. Install SLS # Set sls_url, sls_tls_crt_local_file_path, sls_registration_key variables to skip install and set up SLSCfg for # an existing installation of SLS - ibm.mas_devops.sls # 4 Install DRO - ibm.mas_devops.dro # 5. Generate a Workspace - ibm.mas_devops.gencfg_workspace # 6. Install & configure MAS - ibm.mas_devops.suite_dns - ibm.mas_devops.suite_certs - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify and changing it to look like the following: --- - name: \"oneclick-core\" hosts: localhost any_errors_fatal: true vars: mas_config_dir: \"/tmp\" mongodb_namespace: \"mongoce\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" # Core Services Configuration mas_channel: \"9.0.x\" mas_instance_id: \"aap1\" # Workspace Configuration mas_workspace_name: \"MAS Development\" mas_workspace_id: \"masdev\" entitlement_file: \"license_file/entitlement.lic\" dro_contact: email: \"whitfiea@uk.ibm.com\" first_name: \"Andrew\" last_name: \"Whitfield\" roles: # 1. Install cluster-scoped dependencies (e.g. Cert-Manager, Operator Catalogs) & Grafana - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.cert_manager # 2. Install MongoDb - ibm.mas_devops.mongodb # 3. Install SLS - ibm.mas_devops.sls # 4 Install DRO - ibm.mas_devops.dro # 5. Generate a Workspace - ibm.mas_devops.gencfg_workspace # 6. Install & configure MAS - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify The above has removed any environment variable lookups in the playbook itself, and also modified the roles so it doesn't run the grafana role.","title":"oneclick_core.yml"},{"location":"execution-environment/#ocp-provision","text":"If you need to set an environment variable then you can do this in the playbook as well, this would be needed in certain cases such as setting the AWS cli details. The aws_access_key_id and aws_secret_access_key are variables set by AAP as a result of the Survey questions: --- - name: \"ocp-provision\" hosts: localhost vars: cluster_name: testcluster cluster_type: rosa ocp_version: 4.14.35 rosa_compute_nodes: 3 environment: AWS_DEFAULT_REGION: us-east-1 AWS_ACCESS_KEY_ID: \"{{ aws_access_key_id }}\" AWS_SECRET_ACCESS_KEY: \"{{ aws_secret_access_key }}\" roles: # 1. Provision the ROSA cluster - ibm.mas_devops.ocp_provision # 2. Login and verify the cluster is ready - ibm.mas_devops.ocp_login - ibm.mas_devops.ocp_verify # 3. Set up storage classes - ibm.mas_devops.ocp_efs","title":"ocp-provision"},{"location":"execution-environment/#backuprestore","text":"The backup and restore tasks and playbooks provided in the ibm.mas_devops collection has a number of differences with other roles/playbooks that can make creating the playbook difficult. The following playbook is an example of how a backup for core playbook can be written to work with AAP, along with a rclone.conf.j2 file that should be stored in the same location as the playbook in your SCM. The playbook needs to set both the environment so that the ansible_env var is set, and for task vars to be set to override the common_vars that are looking for environment variables on the local controller (note that environment does not update the local controller environment so lookups using the ansible builtin env won't pick up those values). This playbook requires the following survey questions to be setup: { \"aws_access_key_id\": \"$encrypted$\", \"aws_secret_access_key\": \"$encrypted$\", \"ocp_token\": \"$encrypted$\", \"ocp_server\": \"https://api.your_ocp_cluster:6443\" } --- - name: \"Backup/Restore MAS Core\" hosts: localhost any_errors_fatal: true vars: # Define the target for backup/restore mas_instance_id: aap1 # Define what action to perform masbr_action: backup # Define storage type masbr_storage_type: cloud masbr_storage_cloud_rclone_file: rclone.conf masbr_storage_cloud_rclone_name: masbr masbr_storage_cloud_bucket: mas-backup # Define what to backup/restore masbr_job_component: name: \"core\" instance: \"{{ mas_instance_id }}\" namespace: \"mas-{{ mas_instance_id }}-core\" # Configure path to backup_restore tasks that should be present in the execution environment role_path: \"/usr/share/ansible/collections/ansible_collections/ibm/mas_devops/roles/suite_backup_restore\" # Set here to get into ansible_env environment: MASBR_STORAGE_TYPE: cloud MASBR_STORAGE_CLOUD_RCLONE_FILE: rclone.conf MASBR_STORAGE_CLOUD_RCLONE_NAME: masbr MASBR_STORAGE_CLOUD_BUCKET: mas-backup pre_tasks: - name: \"Asserts aws_access_key_id secret defined\" assert: that: aws_access_key_id is defined fail_msg: \"aws_access_key_id not defined\" - name: \"Asserts aws_secret_access_key secret defined\" assert: that: aws_secret_access_key is defined fail_msg: \"aws_secret_access_key not defined\" # Template out the rclone.conf so no credentials are stored in git - name: \"Create rclone.conf\" template: src: \"{{ masbr_storage_cloud_rclone_file }}.j2\" dest: \"{{ masbr_storage_cloud_rclone_file }}\" roles: - ibm.mas_devops.ocp_login tasks: # Common checks before run tasks # ------------------------------------------------------------------------- - name: \"Before run tasks\" include_tasks: file: \"{{ role_path }}/../../common_tasks/backup_restore/before_run_tasks.yml\" vars: _job_type: \"{{ masbr_action }}\" masbr_storage_type: cloud masbr_storage_cloud_rclone_file: rclone.conf masbr_storage_cloud_rclone_name: masbr masbr_storage_cloud_bucket: mas-backup # Create k8s Job to run backup/restore tasks # ------------------------------------------------------------------------- - name: \"Create k8s Job to run {{ masbr_action }} tasks\" when: masbr_create_task_job include_tasks: file: \"{{ role_path }}/../../common_tasks/backup_restore/create_run_tasks_job.yml\" vars: _rt_playbook_name: \"br_core\" _rt_env: - name: \"MASBR_ACTION\" value: \"{{ masbr_action }}\" - name: \"MASBR_JOB_VERSION\" value: \"{{ masbr_job_version }}\" - name: \"MAS_INSTANCE_ID\" value: \"{{ mas_instance_id }}\" masbr_storage_type: cloud masbr_storage_cloud_rclone_file: rclone.conf masbr_storage_cloud_rclone_name: masbr masbr_storage_cloud_bucket: mas-backup # Run backup/restore tasks locally # ------------------------------------------------------------------------- - name: \"Run {{ masbr_action }} tasks\" when: not masbr_create_task_job block: - name: \"MongoDB: {{ masbr_action }}\" include_role: name: ibm.mas_devops.mongodb vars: mongodb_action: \"{{ masbr_action }}\" mas_app_id: \"core\" masbr_storage_type: cloud masbr_storage_cloud_rclone_file: rclone.conf masbr_storage_cloud_rclone_name: masbr masbr_storage_cloud_bucket: mas-backup - name: \"MAS Core namespace: {{ masbr_action }}\" include_role: name: ibm.mas_devops.suite_backup_restore vars: masbr_storage_type: cloud masbr_storage_cloud_rclone_file: rclone.conf masbr_storage_cloud_rclone_name: masbr masbr_storage_cloud_bucket: mas-backup The following file is stored in your SCM and is a template to be injected with the credentials needed during the play. This allows the file to be stored in SCM without exposing credentials. rclone.conf.j2 [masbr] type = s3 provider = Minio endpoint = http://minio-api.apps.mydomain.com access_key_id = \"{{ aws_access_key_id }}\" secret_access_key = \"{{ aws_secret_access_key }}\" region = minio","title":"backup/restore"},{"location":"execution-environment/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"execution-environment/#helpful-links","text":"Red Hat AAP documentation Installing AAP in OCP Other install methods of AAP AAP Certified Content","title":"Helpful Links"},{"location":"execution-environment/#cant-see-all-output-in-log-file","text":"The output for the job doesn't output all the data in the standard view. To see the expanded data you must click on the entry to see the details and then select JSON:","title":"Can't see all output in log file"},{"location":"execution-environment/#my-playbook-or-updated-playbook-cant-be-found","text":"If you have updated your Playbook in your SCM but it is not reflected in AAP then you can sync the Project. Navigate to the Project view and click the sync icon so the Project gets the latest revision.","title":"My playbook or updated playbook can't be found"},{"location":"execution-environment/#job-failed-can-i-re-launch-it","text":"If the job fails and you want to relaunch with the same variables (from both hosts and survey), then you can re-launch the job from the Job page or the Jobs list page","title":"Job failed can I re-launch it?"},{"location":"execution-environment/#how-to-provide-supporting-files","text":"If you have a file that is not a playbook but you want to reference it, then you can include this in the same repo as your playbooks as AAP will be syncing the whole repo. You can then reference these files from your playbook or roles, using a pth relative to the playbook being run. An exmaple of this is the entitlement_file which can be contained in your repo: and then referenced in your playbook in the path \"license_file/entitlement.lic\" : --- - hosts: localhost any_errors_fatal: true vars: mas_config_dir: \"/tmp\" mongodb_namespace: \"mongoce\" sls_mongodb_cfg_file: \"{{ mas_config_dir }}/mongo-{{ mongodb_namespace }}.yml\" entitlement_file: \"license_file/entitlement.lic\" roles: - ibm.mas_devops.mongodb - ibm.mas_devops.sls","title":"How to provide supporting files?"},{"location":"execution-environment/#how-do-i-set-environment-variables","text":"Running in AAP means you can't set environment variables on the ansible controller before a playbook is executed. If a role in the ibm.mas_devops collection has an environment variable to be set there is normally a corresponding ansible variable. For exmaple, in the documentation for (kafka_action)[https://ibm-mas.github.io/ansible-devops/roles/kafka/#kafka_action] in the kafka role it says to set the environment variable KAFKA_VERSION , this is just setting the ansible role varible called kafka_version which you can set in your playbook. With ansible predence in place, the play vars or role vars will override the default vars that use the environment variable. Example setting play vars: --- - hosts: localhost any_errors_fatal: true vars: kafka_version: 3.7.0 roles: - ibm.mas_devops.kafka Example setting role vars: --- - hosts: localhost any_errors_fatal: true roles: - role: ibm.mas_devops.kafka vars: kafka_version: 3.7.0 One caveat to the above is the backup/restore tasks as these use a combination of ansible vars and expecting environment variables to be set in the ansible_env variable. In order to work with the backup/restore tasks you need to set both environment and role/task vars (not play vars). Example, showing environment and task vars set. See backup exmaple for more complete play: - name: \"Backup/Restore MAS Core\" hosts: localhost any_errors_fatal: true vars: # Configure path to backup_restore tasks role_path: \"/usr/share/ansible/collections/ansible_collections/ibm/mas_devops/roles/suite_backup_restore\" # Set here to get into ansible_env environment: MASBR_STORAGE_TYPE: cloud MASBR_STORAGE_CLOUD_RCLONE_FILE: rclone.conf MASBR_STORAGE_CLOUD_RCLONE_NAME: masbr MASBR_STORAGE_CLOUD_BUCKET: mas-backup tasks: # Common checks before run tasks # ------------------------------------------------------------------------- - name: \"Before run tasks\" include_tasks: file: \"{{ role_path }}/../../common_tasks/backup_restore/before_run_tasks.yml\" vars: _job_type: \"{{ masbr_action }}\" masbr_storage_type: cloud masbr_storage_cloud_rclone_file: rclone.conf masbr_storage_cloud_rclone_name: masbr masbr_storage_cloud_bucket: mas-backup","title":"How do I set environment variables?"},{"location":"playbooks/backup-restore/","text":"Backup and Restore \u00a4 Overview \u00a4 MAS Devops Collection includes playbooks for backing up and restoring of the following MAS components and their dependencies: MongoDB Db2 MAS Core Manage IoT Monitor Health Optimizer Visual Inspection Creation of both full and incremental backups are supported. Backup and restore actions can be executed locally, or by generating on-demand or scheduled jobs that will allow the work to be performed on your OpenShift cluster using the MAS CLI container image . Tip The backup and restore Ansible roles can also be used individually, allowing you to build your own customized backup and restore playbook covering exactly what you need. For example, you can only backup/restore Manage attachments . For more information about backup and restore for Maximo Application Suite, please refer to Backing up and restoring Maximo Application Suite in the product documentation. Configuration \u00a4 Storage \u00a4 Envrionment variable Required (Default Value) Description MASBR_STORAGE_TYPE Yes Type of storage system for saving the backup files MASBR_STORAGE_LOCAL_FOLDER Yes , if MASBR_STORAGE_TYPE=local The local path to save the backup files MASBR_STORAGE_CLOUD_RCLONE_FILE Yes , if MASBR_STORAGE_TYPE=cloud The path of rclone.conf file MASBR_STORAGE_CLOUD_RCLONE_NAME Yes if MASBR_STORAGE_TYPE=cloud The configuration name defined in rclone.conf file MASBR_STORAGE_CLOUD_BUCKET Yes , if MASBR_STORAGE_TYPE=cloud Object storage bucket for saving backup files MASBR_LOCAL_TEMP_FOLDER No ( /tmp/masbr ) Local folder for saving the temporary backup/restore data, the data in this folder will be deleted after the backup/restore job completed. You need to set the environment variable MASBR_STORAGE_TYPE before you perform a backup or restore job. This variable indicates what type of storage systems that you are using for saving the backup files. Currently, it supports below types: local : use the local file system, e.g. a folder on your laptop or workstation. cloud : use the cloud object storage, such as IBM Cloud Object Storage, AWS S3, etc. Use Local Folder \u00a4 You can save the backup files to a folder on your local file system by setting the following environment variables: MASBR_STORAGE_TYPE=local MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup MASBR_STORAGE_LOCAL_FOLDER : the path for saving the backup files Use Cloud Object Storage \u00a4 The backup playbooks use Rclone to copy backup files from data stores to cloud object storage. It requires a Rclone configuration file which you can either create it manually, or you can install the Rclone tool and create the configuration file by running the rclone config command. For more information about the rclone config command and configuration file format, please refer to the Rclone documentation . Below is a sample Rclone configuration file that using MinIO object storage: [masbr] type = s3 provider = Minio endpoint = http://minio-api.apps.mydomain.com access_key_id = Qfx9YGnykJapxL7pzUyA secret_access_key = qKRGSnxsJ7z6pIA74sVxJ6fkEh4Fq5m4fo0inDuJ region = minio Set the following environment variables to indicate the playbooks to use cloud object storage for saving backup files: MASBR_STORAGE_TYPE=cloud MASBR_STORAGE_CLOUD_RCLONE_FILE=/mnt/configmap/rclone.conf MASBR_STORAGE_CLOUD_RCLONE_NAME=masbr MASBR_STORAGE_CLOUD_BUCKET=mas-backup MASBR_STORAGE_CLOUD_RCLONE_FILE : the path where your rclone.conf file is located MASBR_STORAGE_CLOUD_RCLONE_NAME : the Rclone configuration name ( [masbr] from above sample) defined in the rclone.conf file MASBR_STORAGE_CLOUD_BUCKET : the bucket name you created on the object storage for saving the backup files Kubernetes Jobs \u00a4 Envrionment variable Required (Default Value) Description MASBR_CONFIRM_CLUSTER No ( false ) Set true or false to indicate the playbook whether to confirm the currently connected cluster before running the backup or restore job MASBR_CREATE_TASK_JOB No ( true ) Whether to run backup/restore process in kubernetes Job MASBR_COPY_TIMEOUT_SEC No ( 43200 ) The transfer files timeout in seconds, default timeout value is 12 hours. MASBR_JOB_TIMEZONE No The time zone for creating scheduled job. If not set a value for this variable, this role will use UTC time zone when creating a CronJob for running scheduled job. MASBR_CLEANUP_SCHEDULE No ( 0 1 * * * ) Cron expression of cleanup Job (default to run at 1:00 every day) MASBR_CLEANUP_TTL_SEC No ( 604800 ) All completed jobs that exceed this TTL(time-to-live) in seconds will be deleted (default TTL is 1 week: 3600 * 24 * 7) MASBR_MASCLI_IMAGE_TAG No ( latest ) MAS CLI docker image tag MASBR_MASCLI_IMAGE_PULL_POLICY No MAS CLI docker image pull policy When the playbook starts running, it will first perform some checks, such as checking the required environment variables, get the source or target cluster information, etc. Then, depending on the value of MASBR_CREATE_TASK_JOB , the remaining backup/restore process can be ran in different ways: MASBR_CREATE_TASK_JOB=false , run backup/restore process in your current terminal, and you can view the terminal output to get the progress of the backup/restore. MASBR_CREATE_TASK_JOB=true , a new kubernetes Job will be created to run the backup/restore process in the cluster, then you can check the log of the created kubernetes Job in the cluster to monitor the backup/restore progress. The environment variable MASBR_CREATE_TASK_JOB is only valid when using cloud object storage ( MASBR_STORAGE_TYPE=cloud ). The playbooks will always run backup/restore process in your local terminal when using local storage system ( MASBR_STORAGE_TYPE=local ). During the backup/restore process, the playbook will copy backup files between different data stores and specified backup storage systems. Set a suitable value for the environment variable MASBR_COPY_TIMEOUT_SEC to avoid the playbook entering a waiting state due to some errors, such as the specified storage system network speed is too slow or cannot be connected. Warning Set a suitable value for MASBR_COPY_TIMEOUT_SEC based on the estimated size of the backup/restore data and the network conditions, setting it too low can result in the data copying process being interrupted. The playbook will create an additional CronJob masbr-cleanup for each namespace that has backup/restore jobs created. This cleanup CronJob will periodically delete the completed jobs that exceed a certain priod of time which specified by MASBR_CLEANUP_TTL_SEC . You can also specify when to run the cleanup CronJob by setting Cron expression for MASBR_CLEANUP_SCHEDULE . Backups \u00a4 Envrionment variable Required (Default Value) Description MASBR_ACTION Yes Whether to run the playbook to perform a backup or a restore MASBR_BACKUP_TYPE No ( full ) Set full or incr to indicate the playbook to create a full backup or incremental backup. MASBR_BACKUP_FROM_VERSION No Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). MASBR_BACKUP_SCHEDULE No Set Cron expression to create a scheduled backup. If not set a value for this varialbe, the playbook will create an on-demand backup. The playbooks are switched to backup mode by setting MASBR_ACTION to backup . Full Backups \u00a4 If you set environment variable MASBR_BACKUP_TYPE=full or do not specify a value for this variable, the playbook will take a full backup. Incremental Backups \u00a4 You can set environment variable MASBR_BACKUP_TYPE=incr to indicate the playbook to take an incremental backup. Important Only supports creating incremental backup for MonogDB, Db2 and persistent volume data. The playbook will always create a full backup for other type of data regardless of whether this variable be set to incr . The environment variable MASBR_BACKUP_FROM_VERSION is only valid if MASBR_BACKUP_TYPE=incr . It indicates which backup version that the incremental backup to based on. If you do not set a value for this variable, the playbook will try to find the latest Completed Full backup from the specified storage location, and then take an incremental backup based on it. Important The backup files you specified by MASBR_BACKUP_FROM_VERSION must be a Full backup. And the component name and data types in the specified Full backup file must be same as the current incremental backup job. Scheduled Backups \u00a4 In addition to create an on-demand backup job, you can also set environment variable MASBR_BACKUP_SCHEDULE to indicate the playbook to create a kubernetes CronJob to run the backup process periodically. The value of MASBR_BACKUP_SCHEDULE is a Cron expression : \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0\u201359) \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0\u201323) \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1\u201331) \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1\u201312) \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0\u20136) (Sunday to Saturday; \u2502 \u2502 \u2502 \u2502 \u2502 7 is also Sunday on some systems) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 * * * * * For example, set below value to create a scheduled backup job that will run at 1:00 a.m. from Monday to Friday: MASBR_BACKUP_SCHEDULE=\"0 1 * * 1-5\" By default, the kubernetes CronJob use UTC time zone, so maybe you want to set environment variable MASBR_JOB_TIMEZONE with the Cron expression based on your local time zone Restore \u00a4 Envrionment variable Required (Default Value) Description MASBR_ACTION Yes Whether to run the playbook to perform a backup or a restore MASBR_RESTORE_FROM_VERSION Yes Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) The playbooks are switched to restore mode by setting MASBR_ACTION to restore . You must specify the MASBR_RESTORE_FROM_VERSION environment variable to indicate which version of the backup files to use. In the case of restoring from an incremental backup, the corresponding full backup will be restored first before continuing to restore the incremental backup. Slack Notifications \u00a4 Envrionment variable Required (Default Value) Description MASBR_SLACK_ENABLED No ( false ) Set true or false to indicate whether the playbook will send Slack notification messages of the backup and restore progress MASBR_SLACK_LEVEL No ( info ) Set failure , info or verbose to indicate the playbook to send Slack notification messages in which backup and resore phases MASBR_SLACK_TOKEN Yes, if MASBR_SLACK_ENABLED=true Slack integration token MASBR_SLACK_CHANNEL Yes, if MASBR_SLACK_ENABLED=true Channel to send the message to MASBR_SLACK_USER No ( MASBR ) Sender of the message Integration with Slack is supported with below notification levels: verbose : send notifications when job in the phase InProgress , Completed , Failed or PartiallyFailed . info : send notifications when job in the phase Completed , Failed or PartiallyFailed . failure : send notifications only when job in the phase Failed or PartiallyFailed Backup/Restore for MongoDB \u00a4 This playbook ibm.mas_devops.br_mongodb will invoke the role mongodb to backup/restore the MongoDB databases. This playbook supports backing up and restoring databases for an in-cluster MongoDB CE instance. If you are using other MongoDB venders, such as IBM Cloud Databases for MongoDB, Amazon DocumentDB or MongoDB Altas Database, please refer to the corresponding vender's documentation for more information about their provided backup/restore service. Environment Variables \u00a4 MAS_INSTANCE_ID : Required . This playbook supports backup/restore MongoDB databases that belong to a specific MAS instance, call the playbook multiple times with different values for MAS_INSTANCE_ID if you wish to back up multiple MAS instances that use the same MongoDB CE instance. MAS_APP_ID : Optional . By default, this playbook will backup all databases belonging to the specified MAS instance. You can backup the databases only belong to a specific MAS application by setting this environment variable to a supported MAS application id core , manage , iot , monitor , health , optimizer or visualinspection . Examples \u00a4 # Full backup all MongoDB data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_mongodb # Incremental backup all MongoDB data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_mongodb # Create a scheduled backup Job for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_SCHEDULE=\"50 0 * * *\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_mongodb # Restore all MongoDB data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_mongodb # Backup just the IoT MongoDB data for the dev2 instance export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev2 export MAS_APP_ID=iot ansible-playbook ibm.mas_devops.br_mongodb Backup/Restore for Db2 \u00a4 This playbook ibm.mas_devops.br_db2 will invoke the role db2 to backup/restore Db2 instance. Environment Variables \u00a4 DB2_INSTANCE_NAME : Required . This playbook only supports backing up specific Db2 instance at a time. If you want to backup all Db2 instances in the Db2 cluster, you need to run this playbook multiple times with different value of this environment variable. Examples \u00a4 # Full backup for the db2w-shared Db2 instance export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 # Incremental backup for the db2w-shared Db2 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 # Create a scheduled backup Job for the db2w-shared Db2 instance export MASBR_ACTION=backup export MASBR_BACKUP_SCHEDULE=\"50 0 * * *\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 # Restore for the db2w-shared Db2 instance export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 Backup/Restore for MAS Core \u00a4 This playbook ibm.mas_devops.br_core will backup the following components that MAS Core depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core core suite_backup_restore MAS Core namespace resources Environment Variables \u00a4 MAS_INSTANCE_ID Required . The MAS instance ID to perform a backup for. Examples \u00a4 # Full backup all core data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_core # Incremental backup all core data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_core # Create a scheduled backup Job for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_SCHEDULE=\"50 0 * * *\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_core # Restore all core data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_core Backup/Restore for Manage \u00a4 This playbook ibm.mas_devops.br_manage will backup the following components that Manage depends on in order: Component Role Data included mongodb mongodb MongoDB databases used by MAS Core db2 db2 Db2 instance used by Manage core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Optional . When defined, this playbook will backup the Db2 instance used by Manage. DB2 role is skipped when environment variable is not defined.. Examples \u00a4 # Full backup all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage # Incremental backup all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage # Restore all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 restore role ansible-playbook ibm.mas_devops.br_manage # Create a scheduled backup of all manage data for the dev1 instance and ws1 workspace # This will run at 01:00, Monday through Friday export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_BACKUP_SCHEDULE=\"0 1 * * 1-5\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage Backup/Restore for IoT \u00a4 This playbook ibm.mas_devops.br_iot will backup the following components that IoT depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and IoT db2 db2 Db2 instance used by IoT core suite_backup_restore MAS Core namespace resources iot suite_app_backup_restore IoT namespace resources Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by IoT, you need to set the correct Db2 instance name for this environment variable. Examples \u00a4 # Full backup all iot data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot # Incremental backup all iot data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot # Restore all iot data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot # Create a scheduled backup of all iot data for the dev1 instance # This will run at 01:00, Monday through Friday export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_BACKUP_SCHEDULE=\"0 1 * * 1-5\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot Backup/Restore for Monitor \u00a4 This playbook ibm.mas_devops.br_monitor will backup the following components that Monitor depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core, IoT and Monitor db2 db2 Db2 instance used by IoT and Monitor core suite_backup_restore MAS Core namespace resources iot suite_app_backup_restore IoT namespace resources monitor suite_app_backup_restore Monitor namespace resources Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by IoT and Monitor, you need to set the correct Db2 instance name for this environment variable. Examples \u00a4 # Full backup all monitor data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor # Incremental backup all monitor data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor # Restore all monitor data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor # Create a scheduled backup of all monitor data for the dev1 instance # This will run at 01:00, Monday through Friday export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_BACKUP_SCHEDULE=\"0 1 * * 1-5\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor Backup/Restore for Health \u00a4 This playbook ibm.mas_devops.br_health will backup the following components that Health depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core db2 db2 Db2 instance used by Manage and Health core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments health suite_backup_restore Health namespace resources Watson Studio project assets Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by Manage and Health, you need to set the correct Db2 instance name for this environment variable. Examples \u00a4 # Full backup all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health # Incremental backup all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health # Restore all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health # Create a scheduled backup of all health data for the dev1 instance and ws1 workspace # This will run at 01:00, Monday through Friday export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_BACKUP_SCHEDULE=\"0 1 * * 1-5\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health Backup/Restore for Optimizer \u00a4 This playbook ibm.mas_devops.br_optimizer will backup the following components that Optimizer depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and Optimizer db2 db2 Db2 instance used by Manage core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments optimizer suite_backup_restore Optimizer namespace resources Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by Manage, you need to set the correct Db2 instance name for this environment variable. Examples \u00a4 # Full backup all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer # Incremental backup all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer # Restore all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer # Create a scheduled backup of all optimizer data for the dev1 instance and ws1 workspace # This will run at 01:00, Monday through Friday export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_BACKUP_SCHEDULE=\"0 1 * * 1-5\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer Backup/Restore for Visual Inspection \u00a4 This playbook ibm.mas_devops.br_visualinspection will backup the following components that Visual Inspection depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and Visual Inspection core suite_backup_restore MAS Core namespace resources visualinspection suite_app_backup_restore Visual Inspection namespace resources Persistent volume data, such as images and models Environment Variables \u00a4 MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. Examples \u00a4 # Full backup all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection # Incremental backup all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection # Restore all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection # Create a scheduled backup of all visual inspection data for the dev1 instance and ws1 workspace # This will run at 01:00, Monday through Friday export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_BACKUP_SCHEDULE=\"0 1 * * 1-5\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection Reference \u00a4 Directory Structure \u00a4 No matter what kind of storage systems you choose, the folder structure created in the storage system is same. Below is the sample folder structure for saving backup jobs: <root_folder>/backups/mongodb-main-full-20240621122530 \u251c\u2500\u2500 backup.yml \u251c\u2500\u2500 database \u2502 \u251c\u2500\u2500 mongodb-main-full-20240621122530.tar.gz \u2502 \u2514\u2500\u2500 query.json \u2514\u2500\u2500 log \u251c\u2500\u2500 mongodb-main-full-20240621122530-backup-log.tar.gz \u2514\u2500\u2500 mongodb-main-full-20240621122530-ansible-log.tar.gz <root_folder>/backups/core-main-full-20240621122530 \u251c\u2500\u2500 backup.yml \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 core-main-full-20240621122530-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 core-main-full-20240621122530-namespace-log.tar.gz \u2514\u2500\u2500 namespace \u2514\u2500\u2500 core-main-full-20240621122530-namespace.tar.gz <root_folder> : the root folder is specified by MASBR_STORAGE_LOCAL_FOLDER or MASBR_STORAGE_CLOUD_BUCKET The backup playbooks will create a seperated backup job folder under the backups folder for each component. The backup job folder is named by following this format: {BACKUP COMPONENT}-{INSTANCE ID}-{BACKUP TYPE}-{BACKUP VERSION} . When using playbook to backup multiple components at once, all backup job folders will be assigned to the same backup version. In above example, the same backup version 20240621122530 for backing up mongodb and core components. backup.yml : keep the backup job information database : data type for database. This folder save the backup files of MongoDB database, Db2 database. namespace : data type for namespace resources. This folder save the exported namespace resources. pv : data type for persistent volume. This folder save the persistent volume data, e.g. the Manage attachments, VI images and models. log : this folder save all job running log files In addition to the backup jobs, we also save restore jobs in the specified storage location. For example: <root_folder>/restores/mongodb-main-incr-20240622040201-20240622075501 \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 mongodb-main-incr-20240622040201-20240622075501-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 mongodb-main-incr-20240622040201-20240622075501-restore-log.tar.gz \u2514\u2500\u2500 restore.yml <root_folder>/restores/core-main-incr-20240622040201-20240622075501 \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 core-main-incr-20240622040201-20240622075501-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 core-main-incr-20240622040201-20240622075501-namespace-log.tar.gz \u2514\u2500\u2500 restore.yml The restore playbooks will create a seperated restore job folder under the restores folder for each component. The restore job folder is named by following this format: {BACKUP JOB NAME}-{RESTORE VERSION} . restore.yml : keep the restore job information log : this folder save all job running log files Data Model \u00a4 backup.yml \u00a4 kind: Backup name: \"core-main-incr-20240622040201\" version: \"20240622040201\" type: \"incr\" from: \"core-main-full-20240621122530\" source: domain: \"source-cluster.mydomain.com\" suite: \"8.11.11\" instance: \"main\" workspace: \"\" component: name: \"core\" instance: \"main\" namespace: \"mas-main-core\" data: - seq: \"1\" type: \"namespace\" phase: \"Completed\" status: phase: \"Completed\" startTimestamp: \"2024-06-22T04:05:22\" completionTimestamp: \"2024-06-22T04:06:04\" sentNotifications: - type: \"Slack\" channel: \"#ansible-slack-dev\" timestamp: \"2024-06-22T04:05:34\" phase: \"InProgress\" - type: \"Slack\" channel: \"#ansible-slack-dev\" timestamp: \"2024-06-22T04:06:10\" phase: \"Completed\" restore.yml \u00a4 kind: Restore name: \"core-main-incr-20240622040201-20240622075501\" version: \"20240622075501\" from: \"core-main-incr-20240622040201\" target: domain: \"target-cluster.mydomain.com\" component: name: \"core\" instance: \"main\" namespace: \"mas-main-core\" data: - seq: 1 type: \"namespace\" phase: \"Completed\" status: phase: \"Completed\" startTimestamp: \"2024-06-22T08:04:19\" completionTimestamp: \"2024-06-22T08:04:33\"","title":"Backup & Restore"},{"location":"playbooks/backup-restore/#backup-and-restore","text":"","title":"Backup and Restore"},{"location":"playbooks/backup-restore/#overview","text":"MAS Devops Collection includes playbooks for backing up and restoring of the following MAS components and their dependencies: MongoDB Db2 MAS Core Manage IoT Monitor Health Optimizer Visual Inspection Creation of both full and incremental backups are supported. Backup and restore actions can be executed locally, or by generating on-demand or scheduled jobs that will allow the work to be performed on your OpenShift cluster using the MAS CLI container image . Tip The backup and restore Ansible roles can also be used individually, allowing you to build your own customized backup and restore playbook covering exactly what you need. For example, you can only backup/restore Manage attachments . For more information about backup and restore for Maximo Application Suite, please refer to Backing up and restoring Maximo Application Suite in the product documentation.","title":"Overview"},{"location":"playbooks/backup-restore/#configuration","text":"","title":"Configuration"},{"location":"playbooks/backup-restore/#storage","text":"Envrionment variable Required (Default Value) Description MASBR_STORAGE_TYPE Yes Type of storage system for saving the backup files MASBR_STORAGE_LOCAL_FOLDER Yes , if MASBR_STORAGE_TYPE=local The local path to save the backup files MASBR_STORAGE_CLOUD_RCLONE_FILE Yes , if MASBR_STORAGE_TYPE=cloud The path of rclone.conf file MASBR_STORAGE_CLOUD_RCLONE_NAME Yes if MASBR_STORAGE_TYPE=cloud The configuration name defined in rclone.conf file MASBR_STORAGE_CLOUD_BUCKET Yes , if MASBR_STORAGE_TYPE=cloud Object storage bucket for saving backup files MASBR_LOCAL_TEMP_FOLDER No ( /tmp/masbr ) Local folder for saving the temporary backup/restore data, the data in this folder will be deleted after the backup/restore job completed. You need to set the environment variable MASBR_STORAGE_TYPE before you perform a backup or restore job. This variable indicates what type of storage systems that you are using for saving the backup files. Currently, it supports below types: local : use the local file system, e.g. a folder on your laptop or workstation. cloud : use the cloud object storage, such as IBM Cloud Object Storage, AWS S3, etc.","title":"Storage"},{"location":"playbooks/backup-restore/#use-local-folder","text":"You can save the backup files to a folder on your local file system by setting the following environment variables: MASBR_STORAGE_TYPE=local MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup MASBR_STORAGE_LOCAL_FOLDER : the path for saving the backup files","title":"Use Local Folder"},{"location":"playbooks/backup-restore/#use-cloud-object-storage","text":"The backup playbooks use Rclone to copy backup files from data stores to cloud object storage. It requires a Rclone configuration file which you can either create it manually, or you can install the Rclone tool and create the configuration file by running the rclone config command. For more information about the rclone config command and configuration file format, please refer to the Rclone documentation . Below is a sample Rclone configuration file that using MinIO object storage: [masbr] type = s3 provider = Minio endpoint = http://minio-api.apps.mydomain.com access_key_id = Qfx9YGnykJapxL7pzUyA secret_access_key = qKRGSnxsJ7z6pIA74sVxJ6fkEh4Fq5m4fo0inDuJ region = minio Set the following environment variables to indicate the playbooks to use cloud object storage for saving backup files: MASBR_STORAGE_TYPE=cloud MASBR_STORAGE_CLOUD_RCLONE_FILE=/mnt/configmap/rclone.conf MASBR_STORAGE_CLOUD_RCLONE_NAME=masbr MASBR_STORAGE_CLOUD_BUCKET=mas-backup MASBR_STORAGE_CLOUD_RCLONE_FILE : the path where your rclone.conf file is located MASBR_STORAGE_CLOUD_RCLONE_NAME : the Rclone configuration name ( [masbr] from above sample) defined in the rclone.conf file MASBR_STORAGE_CLOUD_BUCKET : the bucket name you created on the object storage for saving the backup files","title":"Use Cloud Object Storage"},{"location":"playbooks/backup-restore/#kubernetes-jobs","text":"Envrionment variable Required (Default Value) Description MASBR_CONFIRM_CLUSTER No ( false ) Set true or false to indicate the playbook whether to confirm the currently connected cluster before running the backup or restore job MASBR_CREATE_TASK_JOB No ( true ) Whether to run backup/restore process in kubernetes Job MASBR_COPY_TIMEOUT_SEC No ( 43200 ) The transfer files timeout in seconds, default timeout value is 12 hours. MASBR_JOB_TIMEZONE No The time zone for creating scheduled job. If not set a value for this variable, this role will use UTC time zone when creating a CronJob for running scheduled job. MASBR_CLEANUP_SCHEDULE No ( 0 1 * * * ) Cron expression of cleanup Job (default to run at 1:00 every day) MASBR_CLEANUP_TTL_SEC No ( 604800 ) All completed jobs that exceed this TTL(time-to-live) in seconds will be deleted (default TTL is 1 week: 3600 * 24 * 7) MASBR_MASCLI_IMAGE_TAG No ( latest ) MAS CLI docker image tag MASBR_MASCLI_IMAGE_PULL_POLICY No MAS CLI docker image pull policy When the playbook starts running, it will first perform some checks, such as checking the required environment variables, get the source or target cluster information, etc. Then, depending on the value of MASBR_CREATE_TASK_JOB , the remaining backup/restore process can be ran in different ways: MASBR_CREATE_TASK_JOB=false , run backup/restore process in your current terminal, and you can view the terminal output to get the progress of the backup/restore. MASBR_CREATE_TASK_JOB=true , a new kubernetes Job will be created to run the backup/restore process in the cluster, then you can check the log of the created kubernetes Job in the cluster to monitor the backup/restore progress. The environment variable MASBR_CREATE_TASK_JOB is only valid when using cloud object storage ( MASBR_STORAGE_TYPE=cloud ). The playbooks will always run backup/restore process in your local terminal when using local storage system ( MASBR_STORAGE_TYPE=local ). During the backup/restore process, the playbook will copy backup files between different data stores and specified backup storage systems. Set a suitable value for the environment variable MASBR_COPY_TIMEOUT_SEC to avoid the playbook entering a waiting state due to some errors, such as the specified storage system network speed is too slow or cannot be connected. Warning Set a suitable value for MASBR_COPY_TIMEOUT_SEC based on the estimated size of the backup/restore data and the network conditions, setting it too low can result in the data copying process being interrupted. The playbook will create an additional CronJob masbr-cleanup for each namespace that has backup/restore jobs created. This cleanup CronJob will periodically delete the completed jobs that exceed a certain priod of time which specified by MASBR_CLEANUP_TTL_SEC . You can also specify when to run the cleanup CronJob by setting Cron expression for MASBR_CLEANUP_SCHEDULE .","title":"Kubernetes Jobs"},{"location":"playbooks/backup-restore/#backups","text":"Envrionment variable Required (Default Value) Description MASBR_ACTION Yes Whether to run the playbook to perform a backup or a restore MASBR_BACKUP_TYPE No ( full ) Set full or incr to indicate the playbook to create a full backup or incremental backup. MASBR_BACKUP_FROM_VERSION No Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). MASBR_BACKUP_SCHEDULE No Set Cron expression to create a scheduled backup. If not set a value for this varialbe, the playbook will create an on-demand backup. The playbooks are switched to backup mode by setting MASBR_ACTION to backup .","title":"Backups"},{"location":"playbooks/backup-restore/#full-backups","text":"If you set environment variable MASBR_BACKUP_TYPE=full or do not specify a value for this variable, the playbook will take a full backup.","title":"Full Backups"},{"location":"playbooks/backup-restore/#incremental-backups","text":"You can set environment variable MASBR_BACKUP_TYPE=incr to indicate the playbook to take an incremental backup. Important Only supports creating incremental backup for MonogDB, Db2 and persistent volume data. The playbook will always create a full backup for other type of data regardless of whether this variable be set to incr . The environment variable MASBR_BACKUP_FROM_VERSION is only valid if MASBR_BACKUP_TYPE=incr . It indicates which backup version that the incremental backup to based on. If you do not set a value for this variable, the playbook will try to find the latest Completed Full backup from the specified storage location, and then take an incremental backup based on it. Important The backup files you specified by MASBR_BACKUP_FROM_VERSION must be a Full backup. And the component name and data types in the specified Full backup file must be same as the current incremental backup job.","title":"Incremental Backups"},{"location":"playbooks/backup-restore/#scheduled-backups","text":"In addition to create an on-demand backup job, you can also set environment variable MASBR_BACKUP_SCHEDULE to indicate the playbook to create a kubernetes CronJob to run the backup process periodically. The value of MASBR_BACKUP_SCHEDULE is a Cron expression : \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0\u201359) \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0\u201323) \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1\u201331) \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1\u201312) \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0\u20136) (Sunday to Saturday; \u2502 \u2502 \u2502 \u2502 \u2502 7 is also Sunday on some systems) \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 * * * * * For example, set below value to create a scheduled backup job that will run at 1:00 a.m. from Monday to Friday: MASBR_BACKUP_SCHEDULE=\"0 1 * * 1-5\" By default, the kubernetes CronJob use UTC time zone, so maybe you want to set environment variable MASBR_JOB_TIMEZONE with the Cron expression based on your local time zone","title":"Scheduled Backups"},{"location":"playbooks/backup-restore/#restore","text":"Envrionment variable Required (Default Value) Description MASBR_ACTION Yes Whether to run the playbook to perform a backup or a restore MASBR_RESTORE_FROM_VERSION Yes Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) The playbooks are switched to restore mode by setting MASBR_ACTION to restore . You must specify the MASBR_RESTORE_FROM_VERSION environment variable to indicate which version of the backup files to use. In the case of restoring from an incremental backup, the corresponding full backup will be restored first before continuing to restore the incremental backup.","title":"Restore"},{"location":"playbooks/backup-restore/#slack-notifications","text":"Envrionment variable Required (Default Value) Description MASBR_SLACK_ENABLED No ( false ) Set true or false to indicate whether the playbook will send Slack notification messages of the backup and restore progress MASBR_SLACK_LEVEL No ( info ) Set failure , info or verbose to indicate the playbook to send Slack notification messages in which backup and resore phases MASBR_SLACK_TOKEN Yes, if MASBR_SLACK_ENABLED=true Slack integration token MASBR_SLACK_CHANNEL Yes, if MASBR_SLACK_ENABLED=true Channel to send the message to MASBR_SLACK_USER No ( MASBR ) Sender of the message Integration with Slack is supported with below notification levels: verbose : send notifications when job in the phase InProgress , Completed , Failed or PartiallyFailed . info : send notifications when job in the phase Completed , Failed or PartiallyFailed . failure : send notifications only when job in the phase Failed or PartiallyFailed","title":"Slack Notifications"},{"location":"playbooks/backup-restore/#backuprestore-for-mongodb","text":"This playbook ibm.mas_devops.br_mongodb will invoke the role mongodb to backup/restore the MongoDB databases. This playbook supports backing up and restoring databases for an in-cluster MongoDB CE instance. If you are using other MongoDB venders, such as IBM Cloud Databases for MongoDB, Amazon DocumentDB or MongoDB Altas Database, please refer to the corresponding vender's documentation for more information about their provided backup/restore service.","title":"Backup/Restore for MongoDB"},{"location":"playbooks/backup-restore/#environment-variables","text":"MAS_INSTANCE_ID : Required . This playbook supports backup/restore MongoDB databases that belong to a specific MAS instance, call the playbook multiple times with different values for MAS_INSTANCE_ID if you wish to back up multiple MAS instances that use the same MongoDB CE instance. MAS_APP_ID : Optional . By default, this playbook will backup all databases belonging to the specified MAS instance. You can backup the databases only belong to a specific MAS application by setting this environment variable to a supported MAS application id core , manage , iot , monitor , health , optimizer or visualinspection .","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples","text":"# Full backup all MongoDB data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_mongodb # Incremental backup all MongoDB data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_mongodb # Create a scheduled backup Job for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_SCHEDULE=\"50 0 * * *\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_mongodb # Restore all MongoDB data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_mongodb # Backup just the IoT MongoDB data for the dev2 instance export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev2 export MAS_APP_ID=iot ansible-playbook ibm.mas_devops.br_mongodb","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-db2","text":"This playbook ibm.mas_devops.br_db2 will invoke the role db2 to backup/restore Db2 instance.","title":"Backup/Restore for Db2"},{"location":"playbooks/backup-restore/#environment-variables_1","text":"DB2_INSTANCE_NAME : Required . This playbook only supports backing up specific Db2 instance at a time. If you want to backup all Db2 instances in the Db2 cluster, you need to run this playbook multiple times with different value of this environment variable.","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_1","text":"# Full backup for the db2w-shared Db2 instance export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 # Incremental backup for the db2w-shared Db2 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 # Create a scheduled backup Job for the db2w-shared Db2 instance export MASBR_ACTION=backup export MASBR_BACKUP_SCHEDULE=\"50 0 * * *\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2 # Restore for the db2w-shared Db2 instance export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_db2","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-mas-core","text":"This playbook ibm.mas_devops.br_core will backup the following components that MAS Core depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core core suite_backup_restore MAS Core namespace resources","title":"Backup/Restore for MAS Core"},{"location":"playbooks/backup-restore/#environment-variables_2","text":"MAS_INSTANCE_ID Required . The MAS instance ID to perform a backup for.","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_2","text":"# Full backup all core data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_core # Incremental backup all core data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_core # Create a scheduled backup Job for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_SCHEDULE=\"50 0 * * *\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_core # Restore all core data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 ansible-playbook ibm.mas_devops.br_core","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-manage","text":"This playbook ibm.mas_devops.br_manage will backup the following components that Manage depends on in order: Component Role Data included mongodb mongodb MongoDB databases used by MAS Core db2 db2 Db2 instance used by Manage core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments","title":"Backup/Restore for Manage"},{"location":"playbooks/backup-restore/#environment-variables_3","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Optional . When defined, this playbook will backup the Db2 instance used by Manage. DB2 role is skipped when environment variable is not defined..","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_3","text":"# Full backup all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage # Incremental backup all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage # Restore all manage data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 restore role ansible-playbook ibm.mas_devops.br_manage # Create a scheduled backup of all manage data for the dev1 instance and ws1 workspace # This will run at 01:00, Monday through Friday export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_BACKUP_SCHEDULE=\"0 1 * * 1-5\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage # set this to execute db2 backup role ansible-playbook ibm.mas_devops.br_manage","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-iot","text":"This playbook ibm.mas_devops.br_iot will backup the following components that IoT depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and IoT db2 db2 Db2 instance used by IoT core suite_backup_restore MAS Core namespace resources iot suite_app_backup_restore IoT namespace resources","title":"Backup/Restore for IoT"},{"location":"playbooks/backup-restore/#environment-variables_4","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by IoT, you need to set the correct Db2 instance name for this environment variable.","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_4","text":"# Full backup all iot data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot # Incremental backup all iot data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot # Restore all iot data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot # Create a scheduled backup of all iot data for the dev1 instance # This will run at 01:00, Monday through Friday export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_BACKUP_SCHEDULE=\"0 1 * * 1-5\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_iot","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-monitor","text":"This playbook ibm.mas_devops.br_monitor will backup the following components that Monitor depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core, IoT and Monitor db2 db2 Db2 instance used by IoT and Monitor core suite_backup_restore MAS Core namespace resources iot suite_app_backup_restore IoT namespace resources monitor suite_app_backup_restore Monitor namespace resources","title":"Backup/Restore for Monitor"},{"location":"playbooks/backup-restore/#environment-variables_5","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by IoT and Monitor, you need to set the correct Db2 instance name for this environment variable.","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_5","text":"# Full backup all monitor data for the dev1 instance export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor # Incremental backup all monitor data for the dev1 instance export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor # Restore all monitor data for the dev1 instance export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor # Create a scheduled backup of all monitor data for the dev1 instance # This will run at 01:00, Monday through Friday export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_BACKUP_SCHEDULE=\"0 1 * * 1-5\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=db2w-shared ansible-playbook ibm.mas_devops.br_monitor","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-health","text":"This playbook ibm.mas_devops.br_health will backup the following components that Health depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core db2 db2 Db2 instance used by Manage and Health core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments health suite_backup_restore Health namespace resources Watson Studio project assets","title":"Backup/Restore for Health"},{"location":"playbooks/backup-restore/#environment-variables_6","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by Manage and Health, you need to set the correct Db2 instance name for this environment variable.","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_6","text":"# Full backup all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health # Incremental backup all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health # Restore all health data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health # Create a scheduled backup of all health data for the dev1 instance and ws1 workspace # This will run at 01:00, Monday through Friday export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_BACKUP_SCHEDULE=\"0 1 * * 1-5\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_health","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-optimizer","text":"This playbook ibm.mas_devops.br_optimizer will backup the following components that Optimizer depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and Optimizer db2 db2 Db2 instance used by Manage core suite_backup_restore MAS Core namespace resources manage suite_app_backup_restore Manage namespace resources Persistent volume data, such as attachments optimizer suite_backup_restore Optimizer namespace resources","title":"Backup/Restore for Optimizer"},{"location":"playbooks/backup-restore/#environment-variables_7","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. DB2_INSTANCE_NAME Required . This playbook will backup the the Db2 instance used by Manage, you need to set the correct Db2 instance name for this environment variable.","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_7","text":"# Full backup all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer # Incremental backup all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer # Restore all optimizer data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer # Create a scheduled backup of all optimizer data for the dev1 instance and ws1 workspace # This will run at 01:00, Monday through Friday export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_BACKUP_SCHEDULE=\"0 1 * * 1-5\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 export DB2_INSTANCE_NAME=mas-dev1-ws1-manage ansible-playbook ibm.mas_devops.br_optimizer","title":"Examples"},{"location":"playbooks/backup-restore/#backuprestore-for-visual-inspection","text":"This playbook ibm.mas_devops.br_visualinspection will backup the following components that Visual Inspection depends on in order: Component Ansible Role Data included mongodb mongodb MongoDB databases used by MAS Core and Visual Inspection core suite_backup_restore MAS Core namespace resources visualinspection suite_app_backup_restore Visual Inspection namespace resources Persistent volume data, such as images and models","title":"Backup/Restore for Visual Inspection"},{"location":"playbooks/backup-restore/#environment-variables_8","text":"MAS_INSTANCE_ID Required . This playbook only supports backing up components belong to a specific MAS instance at a time. If you have multiple MAS instances in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable. MAS_WORKSPACE_ID Required . This playbook only supports backing up components belong to a specific MAS workspace at a time. If you have multiple MAS workspaces in the cluster to be backed up, you need to run this playbook multiple times with different value of this environment variable.","title":"Environment Variables"},{"location":"playbooks/backup-restore/#examples_8","text":"# Full backup all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection # Incremental backup all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection # Restore all visual inspection data for the dev1 instance and ws1 workspace export MASBR_ACTION=restore export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MASBR_RESTORE_FROM_VERSION=20240630132439 export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection # Create a scheduled backup of all visual inspection data for the dev1 instance and ws1 workspace # This will run at 01:00, Monday through Friday export MASBR_ACTION=backup export MASBR_BACKUP_TYPE=incr export MASBR_BACKUP_SCHEDULE=\"0 1 * * 1-5\" export MASBR_JOB_TIMEZONE=\"Asia/Shanghai\" export MASBR_STORAGE_TYPE=local export MASBR_STORAGE_LOCAL_FOLDER=/tmp/backup export MAS_INSTANCE_ID=dev1 export MAS_WORKSPACE_ID=ws1 ansible-playbook ibm.mas_devops.br_visualinspection","title":"Examples"},{"location":"playbooks/backup-restore/#reference","text":"","title":"Reference"},{"location":"playbooks/backup-restore/#directory-structure","text":"No matter what kind of storage systems you choose, the folder structure created in the storage system is same. Below is the sample folder structure for saving backup jobs: <root_folder>/backups/mongodb-main-full-20240621122530 \u251c\u2500\u2500 backup.yml \u251c\u2500\u2500 database \u2502 \u251c\u2500\u2500 mongodb-main-full-20240621122530.tar.gz \u2502 \u2514\u2500\u2500 query.json \u2514\u2500\u2500 log \u251c\u2500\u2500 mongodb-main-full-20240621122530-backup-log.tar.gz \u2514\u2500\u2500 mongodb-main-full-20240621122530-ansible-log.tar.gz <root_folder>/backups/core-main-full-20240621122530 \u251c\u2500\u2500 backup.yml \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 core-main-full-20240621122530-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 core-main-full-20240621122530-namespace-log.tar.gz \u2514\u2500\u2500 namespace \u2514\u2500\u2500 core-main-full-20240621122530-namespace.tar.gz <root_folder> : the root folder is specified by MASBR_STORAGE_LOCAL_FOLDER or MASBR_STORAGE_CLOUD_BUCKET The backup playbooks will create a seperated backup job folder under the backups folder for each component. The backup job folder is named by following this format: {BACKUP COMPONENT}-{INSTANCE ID}-{BACKUP TYPE}-{BACKUP VERSION} . When using playbook to backup multiple components at once, all backup job folders will be assigned to the same backup version. In above example, the same backup version 20240621122530 for backing up mongodb and core components. backup.yml : keep the backup job information database : data type for database. This folder save the backup files of MongoDB database, Db2 database. namespace : data type for namespace resources. This folder save the exported namespace resources. pv : data type for persistent volume. This folder save the persistent volume data, e.g. the Manage attachments, VI images and models. log : this folder save all job running log files In addition to the backup jobs, we also save restore jobs in the specified storage location. For example: <root_folder>/restores/mongodb-main-incr-20240622040201-20240622075501 \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 mongodb-main-incr-20240622040201-20240622075501-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 mongodb-main-incr-20240622040201-20240622075501-restore-log.tar.gz \u2514\u2500\u2500 restore.yml <root_folder>/restores/core-main-incr-20240622040201-20240622075501 \u251c\u2500\u2500 log \u2502 \u251c\u2500\u2500 core-main-incr-20240622040201-20240622075501-ansible-log.tar.gz \u2502 \u2514\u2500\u2500 core-main-incr-20240622040201-20240622075501-namespace-log.tar.gz \u2514\u2500\u2500 restore.yml The restore playbooks will create a seperated restore job folder under the restores folder for each component. The restore job folder is named by following this format: {BACKUP JOB NAME}-{RESTORE VERSION} . restore.yml : keep the restore job information log : this folder save all job running log files","title":"Directory Structure"},{"location":"playbooks/backup-restore/#data-model","text":"","title":"Data Model"},{"location":"playbooks/backup-restore/#backupyml","text":"kind: Backup name: \"core-main-incr-20240622040201\" version: \"20240622040201\" type: \"incr\" from: \"core-main-full-20240621122530\" source: domain: \"source-cluster.mydomain.com\" suite: \"8.11.11\" instance: \"main\" workspace: \"\" component: name: \"core\" instance: \"main\" namespace: \"mas-main-core\" data: - seq: \"1\" type: \"namespace\" phase: \"Completed\" status: phase: \"Completed\" startTimestamp: \"2024-06-22T04:05:22\" completionTimestamp: \"2024-06-22T04:06:04\" sentNotifications: - type: \"Slack\" channel: \"#ansible-slack-dev\" timestamp: \"2024-06-22T04:05:34\" phase: \"InProgress\" - type: \"Slack\" channel: \"#ansible-slack-dev\" timestamp: \"2024-06-22T04:06:10\" phase: \"Completed\"","title":"backup.yml"},{"location":"playbooks/backup-restore/#restoreyml","text":"kind: Restore name: \"core-main-incr-20240622040201-20240622075501\" version: \"20240622075501\" from: \"core-main-incr-20240622040201\" target: domain: \"target-cluster.mydomain.com\" component: name: \"core\" instance: \"main\" namespace: \"mas-main-core\" data: - seq: 1 type: \"namespace\" phase: \"Completed\" status: phase: \"Completed\" startTimestamp: \"2024-06-22T08:04:19\" completionTimestamp: \"2024-06-22T08:04:33\"","title":"restore.yml"},{"location":"playbooks/cp4d/","text":"Install Cloud Pak for Data \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift v4.10 cluster with IBM Maximo Application Suite Core v8.10 already installed, the oneclick-core playbook can be used to set this up. Overview \u00a4 This playbook will add Cloud Pak for Data 4.x to an existing cluster installation. It could also install additional CP4D services. This playbook can be run against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install CP4D ControlPlane (~1 hour) Install CP4D Services (~30 Minutes - 1 hour for each service) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \u00a4 MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) CPD_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry CPD_ENTITLEMENT_USERNAME Your user name to access the IBM Container Registr CPD_INSTALL_PLATFORM True/False - If you HAVE CP4D already installed in your cluster, then set it to \"false\" CPD_PRODUCT_VERSION (Required) Cloud Pak for Data version installed in the cluster in 4.X format. These variables are required only if you set CP4D_INSTALL_WSL to false in optional varibles, otherwise don't set it. \u00a4 CPD_ADMIN_USERNAME CP4D Username CPD_ADMIN_PASSWORD CP4D Password CPD_ADMIN_URL CP4D Base URL Optional environment variables \u00a4 CPD_INSTALL_COGNOS True/False - Set to true to install Cognos Analytics CPD_INSTALL_WSL True/False - Set to true to install Watson Studio CPD_INSTALL_WML True/False - Set to true to install Watson Machine Learning CPD_INSTALL_SPARK True/False - Set to true to install Analytics Engine \"Spark\" CPD_INSTALL_OPENSCALE True/False - Set to true to install AI Openscale CPD_INSTALL_DISCOVERY True/False - Set to true to install Watson Discovery CPD_INSTALL_SPSS True/False - Set to true to install SPSS Modeler Usage when you already HAVE CP4D installed \u00a4 export MAS_CONFIG_DIR=~/masconfig export CPD_INSTALL_COGNOS=\"true\" export CPD_ENTITLEMENT_KEY=xxx export CPD_ENTITLEMENT_USERNAME=xxx export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_URL=\"https://mycp4durl\" export CPD_PRODUCT_VERSION=\"4.6.6\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.cp4d Usage when you DON'T HAVE CP4D installed \u00a4 export MAS_CONFIG_DIR=~/masconfig export CPD_ENTITLEMENT_KEY=xxx export CPD_ENTITLEMENT_USERNAME=xxx export CPD_INSTALL_PLATFORM=\"true\" export CPD_PRODUCT_VERSION=\"4.6.6\" ## To install additional CP4D services, add one or many of these environment variables: export CPD_INSTALL_COGNOS=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_INSTALL_WML=\"true\" export CPD_INSTALL_SPARK=\"true\" export CPD_INSTALL_OPENSCALE=\"true\" export CPD_INSTALL_DISCOVERY=\"true\" export CPD_INSTALL_SPSS=\"true\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.cp4d Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Install Cloud Pak For Data"},{"location":"playbooks/cp4d/#install-cloud-pak-for-data","text":"","title":"Install Cloud Pak for Data"},{"location":"playbooks/cp4d/#prerequisites","text":"You will need a RedHat OpenShift v4.10 cluster with IBM Maximo Application Suite Core v8.10 already installed, the oneclick-core playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/cp4d/#overview","text":"This playbook will add Cloud Pak for Data 4.x to an existing cluster installation. It could also install additional CP4D services. This playbook can be run against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install CP4D ControlPlane (~1 hour) Install CP4D Services (~30 Minutes - 1 hour for each service) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/cp4d/#required-environment-variables","text":"MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) CPD_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry CPD_ENTITLEMENT_USERNAME Your user name to access the IBM Container Registr CPD_INSTALL_PLATFORM True/False - If you HAVE CP4D already installed in your cluster, then set it to \"false\" CPD_PRODUCT_VERSION (Required) Cloud Pak for Data version installed in the cluster in 4.X format.","title":"Required environment variables"},{"location":"playbooks/cp4d/#these-variables-are-required-only-if-you-set-cp4d_install_wsl-to-false-in-optional-varibles-otherwise-dont-set-it","text":"CPD_ADMIN_USERNAME CP4D Username CPD_ADMIN_PASSWORD CP4D Password CPD_ADMIN_URL CP4D Base URL","title":"These variables are required only if you set CP4D_INSTALL_WSL to false in optional varibles, otherwise don't set it."},{"location":"playbooks/cp4d/#optional-environment-variables","text":"CPD_INSTALL_COGNOS True/False - Set to true to install Cognos Analytics CPD_INSTALL_WSL True/False - Set to true to install Watson Studio CPD_INSTALL_WML True/False - Set to true to install Watson Machine Learning CPD_INSTALL_SPARK True/False - Set to true to install Analytics Engine \"Spark\" CPD_INSTALL_OPENSCALE True/False - Set to true to install AI Openscale CPD_INSTALL_DISCOVERY True/False - Set to true to install Watson Discovery CPD_INSTALL_SPSS True/False - Set to true to install SPSS Modeler","title":"Optional environment variables"},{"location":"playbooks/cp4d/#usage-when-you-already-have-cp4d-installed","text":"export MAS_CONFIG_DIR=~/masconfig export CPD_INSTALL_COGNOS=\"true\" export CPD_ENTITLEMENT_KEY=xxx export CPD_ENTITLEMENT_USERNAME=xxx export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_URL=\"https://mycp4durl\" export CPD_PRODUCT_VERSION=\"4.6.6\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.cp4d","title":"Usage when you already HAVE CP4D installed"},{"location":"playbooks/cp4d/#usage-when-you-dont-have-cp4d-installed","text":"export MAS_CONFIG_DIR=~/masconfig export CPD_ENTITLEMENT_KEY=xxx export CPD_ENTITLEMENT_USERNAME=xxx export CPD_INSTALL_PLATFORM=\"true\" export CPD_PRODUCT_VERSION=\"4.6.6\" ## To install additional CP4D services, add one or many of these environment variables: export CPD_INSTALL_COGNOS=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_INSTALL_WML=\"true\" export CPD_INSTALL_SPARK=\"true\" export CPD_INSTALL_OPENSCALE=\"true\" export CPD_INSTALL_DISCOVERY=\"true\" export CPD_INSTALL_SPSS=\"true\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.cp4d Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage when you DON'T HAVE CP4D installed"},{"location":"playbooks/ocp/","text":"OCP Playbooks \u00a4 Provision \u00a4 Refer to the ocp_provision role documentation for more information. Provision on AWS ROSA \u00a4 This playbook uses your ROSA API Token to provision a brand new OCP cluster, provision an instance of EFS and set up the cluster with a ReadWriteMany storage class named efs utilizing that instance. To obtain your API token login to the OpenShift cluster manager . export AWS_ACCESS_KEY_ID=xxx export AWS_SECRET_ACCESS_KEY=xxx export ROSA_TOKEN=xxx export CLUSTER_NAME=masonrosa export OCP_VERSION=4.15 export ROSA_COMPUTE_NODES=5 export ROSA_CLUSTER_ADMIN_PASSWORD=xxx ansible-playbook ibm.mas_devops.ocp_rosa_provision Provision on IBMCloud ROKS \u00a4 This playbook uses your IBMCloud API key to provision a brand new OCP cluster. The playbook supports installing an IBM entitlement key as a cluster-wide image pull secret and reboot all worker nodes, which is required for IBM Cloud Pak for Data v4; this can be enabled by setting REBOOT_WORKER_NODES to true and providing the entitlement key with CPD_ENTITLEMENT_KEY . This also supports upgrading the storage volume used for the cluster's internal image registry from 100Gb to 400Gb, this must be enabled by setting UPGRADE_IMAGE_REGISTRY_STORAGE to true . This option is stringly recommended if you intend to install the Watson services from Cloud Pak for Data as the default volume size is too small. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.15_openshift export IBMCLOUD_APIKEY=xxx export REBOOT_WORKER_NODES=true export CPD_ENTITLEMENT_KEY=xxx export UPGRADE_IMAGE_REGISTRY_STORAGE=true ansible-playbook ibm.mas_devops.ocp_roks_provision Provision on IBM DevIT Fyre \u00a4 This playbook will provision a QuickBurn OCP cluster in IBM DevIT Fyre service, QuickBurn clusters will be automatically deprovisioned after 36 hours and are only suitable for small scale deployments for local development and demostration systems. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.15 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx export FYRE_PRODUCT_ID=xxx ansible-playbook ibm.mas_devops.ocp_fyre_provision Deprovision \u00a4 Refer to the ocp_deprovision role documentation for more information. Deprovision on IBMCloud ROKS \u00a4 export CLUSTER_NAME=masinst1 export IBMCLOUD_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_roks_deprovision Deprovision on IBM DevIT Fyre \u00a4 export CLUSTER_NAME=masinst1 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_fyre_deprovision","title":"OCP"},{"location":"playbooks/ocp/#ocp-playbooks","text":"","title":"OCP Playbooks"},{"location":"playbooks/ocp/#provision","text":"Refer to the ocp_provision role documentation for more information.","title":"Provision"},{"location":"playbooks/ocp/#provision-on-aws-rosa","text":"This playbook uses your ROSA API Token to provision a brand new OCP cluster, provision an instance of EFS and set up the cluster with a ReadWriteMany storage class named efs utilizing that instance. To obtain your API token login to the OpenShift cluster manager . export AWS_ACCESS_KEY_ID=xxx export AWS_SECRET_ACCESS_KEY=xxx export ROSA_TOKEN=xxx export CLUSTER_NAME=masonrosa export OCP_VERSION=4.15 export ROSA_COMPUTE_NODES=5 export ROSA_CLUSTER_ADMIN_PASSWORD=xxx ansible-playbook ibm.mas_devops.ocp_rosa_provision","title":"Provision on AWS ROSA"},{"location":"playbooks/ocp/#provision-on-ibmcloud-roks","text":"This playbook uses your IBMCloud API key to provision a brand new OCP cluster. The playbook supports installing an IBM entitlement key as a cluster-wide image pull secret and reboot all worker nodes, which is required for IBM Cloud Pak for Data v4; this can be enabled by setting REBOOT_WORKER_NODES to true and providing the entitlement key with CPD_ENTITLEMENT_KEY . This also supports upgrading the storage volume used for the cluster's internal image registry from 100Gb to 400Gb, this must be enabled by setting UPGRADE_IMAGE_REGISTRY_STORAGE to true . This option is stringly recommended if you intend to install the Watson services from Cloud Pak for Data as the default volume size is too small. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.15_openshift export IBMCLOUD_APIKEY=xxx export REBOOT_WORKER_NODES=true export CPD_ENTITLEMENT_KEY=xxx export UPGRADE_IMAGE_REGISTRY_STORAGE=true ansible-playbook ibm.mas_devops.ocp_roks_provision","title":"Provision on IBMCloud ROKS"},{"location":"playbooks/ocp/#provision-on-ibm-devit-fyre","text":"This playbook will provision a QuickBurn OCP cluster in IBM DevIT Fyre service, QuickBurn clusters will be automatically deprovisioned after 36 hours and are only suitable for small scale deployments for local development and demostration systems. export CLUSTER_NAME=masinst1 export OCP_VERSION=4.15 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx export FYRE_PRODUCT_ID=xxx ansible-playbook ibm.mas_devops.ocp_fyre_provision","title":"Provision on IBM DevIT Fyre"},{"location":"playbooks/ocp/#deprovision","text":"Refer to the ocp_deprovision role documentation for more information.","title":"Deprovision"},{"location":"playbooks/ocp/#deprovision-on-ibmcloud-roks","text":"export CLUSTER_NAME=masinst1 export IBMCLOUD_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_roks_deprovision","title":"Deprovision on IBMCloud ROKS"},{"location":"playbooks/ocp/#deprovision-on-ibm-devit-fyre","text":"export CLUSTER_NAME=masinst1 export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx ansible-playbook ibm.mas_devops.ocp_fyre_deprovision","title":"Deprovision on IBM DevIT Fyre"},{"location":"playbooks/oneclick-aibroker/","text":"Install AI Broker Application \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift v4.14 or above with IBM Maximo Application Suite Core v9.x already be installed, the oneclick-core playbook can be used to set this up. Dependencies: \u00a4 Object Storage Minio ( if customer does not want to use AWS S3 bucket) AWS S3 ( if customer use AWS S3 bucket bucket) MariaDB database (installed in cluster where aibroker instance) IBM Maximo Application Suite Core v9.x Overview \u00a4 This playbook will add AI Broker v1.0.x to an existing IBM Maximo Application Suite Core installation. This playbook can be ran against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install IBM Maximo Application Suite Core v9.x (~30 Minutes) Install MariaDB (~5 minutes) Install Minio (~5 minutes) Install AI broker application (using playbook): Install application (~10 Minutes) Configure AI Broker (kmodels, tenant, etc) (~5 Minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing application, just customize the application install and configure stages at the end of the playbook. Storage providers \u00a4 Notice !!! AI Broker supports AWS and Minio storage providers. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the AI Broker install MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry MAS_ENTITLEMENT_USERNAME Your IBM Entitlement user to access the IBM Container Registry MAS_AIBROKER_STORAGE_ACCESSKEY Your strage provider access key MAS_AIBROKER_STORAGE_SECRETKEY Your storage provider secret key MAS_AIBROKER_STORAGE_HOST Your storage provider host MAS_AIBROKER_STORAGE_REGION Your storage provider region - only when use AWS S3 instance MAS_AIBROKER_STORAGE_PROVIDER Your storage provider name MAS_AIBROKER_STORAGE_SSL Your storage ssl (true/false) MAS_AIBROKER_STORAGE_PIPELINES_BUCKET Your piplines bucket MAS_AIBROKER_STORAGE_TENANTS_BUCKET Your tenants bucket MAS_AIBROKER_STORAGE_TEMPLATES_BUCKET Your templates bucket MAS_AIBROKER_WATSONXAI_APIKEY You WatsonX AI api key MAS_AIBROKER_WATSONXAI_URL You WatsonX AI url MAS_AIBROKER_WATSONXAI_PROJECT_ID You WatsonX projedt Id MAS_AIBROKER_DB_HOST Your database instance host MAS_AIBROKER_DB_PORT Your database instance port MAS_AIBROKER_DB_USER Your database instance user MAS_AIBROKER_DB_DATABASE Your database instance datbase name MAS_AIBROKER_DB_SECRET_NAME Your database instance secret name MAS_AIBROKER_DB_SECRET_VALUE Your database instance password Optional environment variables \u00a4 MAS_AIBROKER_CHANNEL Your custom AI broker application channel MAS_ICR_CP Provide custom registry for AI Broker applications MAS_ICR_CPOPEN Provide custom registry for AI Broker operator MAS_CATALOG_VERSION Your custom AI broker catalog version ARTIFACTORY_USERNAME Your artifactory user name to access - this is needed if user deploy from custom registry for example docker-na-public.artifactory.swg-devops.com ARTIFACTORY_TOKEN Your artifactory token for user to access - this is needed if user deploy from custom registry for example docker-na-public.artifactory.swg-devops.com MAS_AIBROKER_TENANT_ACTION Whether to install or remove tenant (default value is: install) MAS_AIBROKER_APIKEY_ACTION Whether to install or remove or update apikey (default value is: install) MAS_AIBROKER_WATSONX_ACTION Whether to install or remove watsonx secret (default value is: install) MAS_AIBROKER_S3_ACTION Whether to install or remove s3 (default value is: install) Usage \u00a4 AI broker deployment steps \u00a4 Notice: For S3 manage please make sure you have deployed dependencies \u00a4 install boto3 python module (use python environment) \u00a4 python3 -m venv /tmp/venv source /tmp/venv/bin/activate python3 -m pip install boto3 Run playbooks for deploy AI Broker from internal registry ex. docker-na-public.artifactory.swg-devops.com \u00a4 export ARTIFACTORY_USERNAME=\"<artifactory user>\" export ARTIFACTORY_TOKEN=\"<artifactory token>\" export MAS_ICR_CP=\"<internal redistry for aibroker applications>\" export MAS_ICR_CPOPEN=\"<internal redistry for aibroker operator>\" export MAS_INSTANCE_ID=\"<instanceId>\" export MAS_AIBROKER_STORAGE_ACCESSKEY=\"<storage provider access key>\" export MAS_AIBROKER_STORAGE_SECRETKEY=\"<storage provider secret key>\" export MAS_AIBROKER_STORAGE_HOST=\"<storage provider host>\" export MAS_AIBROKER_STORAGE_SSL=\"true or false\" export MAS_AIBROKER_STORAGE_REGION=\"<storage provider region - only for aws>\" export MAS_AIBROKER_STORAGE_PROVIDER=\"<storage provider name>\" export MAS_AIBROKER_STORAGE_PORT=\"<storage provider port - only for minio>\" export MAS_AIBROKER_STORAGE_PIPELINES_BUCKET=\"<pipelines bucket name>\" export MAS_AIBROKER_STORAGE_TENANTS_BUCKET=\"<tenants bucket name>\" export MAS_AIBROKER_STORAGE_TEMPLATES_BUCKET=\"<templates bucket name>\" export MAS_AIBROKER_WATSONXAI_APIKEY=\"<watsonx AI api key>\" export MAS_AIBROKER_WATSONXAI_URL=\"<watsonx AI learning url>\" export MAS_AIBROKER_WATSONXAI_PROJECT_ID=\"<watsonx AI project ID>\" export MAS_AIBROKER_DB_HOST=\"<database instance host>\" export MAS_AIBROKER_DB_PORT=\"<database instance port>\" export MAS_AIBROKER_DB_USER=\"<database instance user>\" export MAS_AIBROKER_DB_DATABASE=\"<database instance datbase name>\" export MAS_AIBROKER_DB_SECRET_NAME=\"<database instance secret name>\" export MAS_AIBROKER_DB_SECRET_VALUE=\"<database instance password>\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/oneclick_add_aibroker.yml Run playbooks for deploy AI Broker from public registry ex. icr.io \u00a4 export MAS_ENTITLEMENT_USERNAME=\"<user>\" export MAS_ENTITLEMENT_KEY=\"<token>\" export MAS_INSTANCE_ID=\"<instanceId>\" export MAS_AIBROKER_STORAGE_ACCESSKEY=\"<storage provider access key>\" export MAS_AIBROKER_STORAGE_SECRETKEY=\"<storage provider secret key>\" export MAS_AIBROKER_STORAGE_HOST=\"<storage provider host>\" export MAS_AIBROKER_STORAGE_SSL=\"true or false\" export MAS_AIBROKER_STORAGE_REGION=\"<storage provider region - only for aws>\" export MAS_AIBROKER_STORAGE_PROVIDER=\"<storage provider name>\" export MAS_AIBROKER_STORAGE_PORT=\"<storage provider port - only for minio>\" export MAS_AIBROKER_STORAGE_PIPELINES_BUCKET=\"<pipelines bucket name>\" export MAS_AIBROKER_STORAGE_TENANTS_BUCKET=\"<tenants bucket name>\" export MAS_AIBROKER_STORAGE_TEMPLATES_BUCKET=\"<templates bucket name>\" export MAS_AIBROKER_WATSONXAI_APIKEY=\"<watsonx AI api key>\" export MAS_AIBROKER_WATSONXAI_URL=\"<watsonx AI url>\" export MAS_AIBROKER_WATSONXAI_PROJECT_ID=\"<watsonx AI project ID>\" export MAS_AIBROKER_DB_HOST=\"<database instance host>\" export MAS_AIBROKER_DB_PORT=\"<database instance port>\" export MAS_AIBROKER_DB_USER=\"<database instance user>\" export MAS_AIBROKER_DB_DATABASE=\"<database instance datbase name>\" export MAS_AIBROKER_DB_SECRET_NAME=\"<database instance secret name>\" export MAS_AIBROKER_DB_SECRET_VALUE=\"<database instance password>\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/oneclick_add_aibroker.yml \u00a4 NOTICE: playbook oneclick_add_aibroker.yml will run three roles: \u00a4 Role: odh \u00a4 Install Red Hat OpenShift Serverless Operator Install Red Hat OpenShift Service Mesh Operator Install Authorino Operator Install Open Data Hub Operator Create DSCInitialization instance Create Data Science Cluster Create Create Data Science Pipelines Application Role: kmodels \u00a4 Install Kmodel controller Install istio Install Kmodel store Install Kmodel watcher Role: aibroker \u00a4 Install AI Broker api application Create AI Broker tenant Create, delete AI Broker API Key Create, delete AWS S3 API Key Create, delete WatsonX AI API Key How to create S3 \u00a4 Prerequisites \u00a4 IBM AI Broker Application Run playbooks \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export MAS_AIBROKER_STORAGE_ACCESSKEY=\"<storage provider access key>\" export MAS_AIBROKER_STORAGE_SECRETKEY=\"<storage provider secret key>\" export MAS_AIBROKER_STORAGE_HOST=\"<storage provider host>\" export MAS_AIBROKER_STORAGE_REGION=\"<storage provider region>\" export MAS_AIBROKER_S3_ACTION=\"install\" export ROLE_NAME=\"aibroker\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml How to delete S3 \u00a4 Prerequisites \u00a4 S3 created in a cluster Run playbooks \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export MAS_AIBROKER_STORAGE_ACCESSKEY=\"<storage provider access key>\" export MAS_AIBROKER_STORAGE_SECRETKEY=\"<storage provider secret key>\" export MAS_AIBROKER_STORAGE_HOST=\"<storage provider host>\" export MAS_AIBROKER_STORAGE_REGION=\"<storage provider region>\" export MAS_AIBROKER_S3_ACTION=\"remove\" export ROLE_NAME=\"aibroker\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml How to create API Key \u00a4 Prerequisites \u00a4 IBM AI Broker Application Run playbooks \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export MAS_AIBROKER_APIKEY_ACTION=\"install\" export ROLE_NAME=\"aibroker\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml How to delete API Key \u00a4 Prerequisites \u00a4 API Key created in a cluster Run playbooks \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export MAS_AIBROKER_APIKEY_ACTION=\"remove\" export ROLE_NAME=\"aibroker\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml How to create WatsonX API Key \u00a4 Prerequisites \u00a4 IBM AI Broker Application Run playbooks \u00a4 export MAS_INSTANCE_ID=\"<instanceId>\" export MAS_AIBROKER_WATSONX_ACTION=\"install\" export ROLE_NAME=\"aibroker\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml How to WatsonX API Key \u00a4 Prerequisites \u00a4 WatsonX API Key created in a cluster Run playbooks \u00a4 ```bash export MAS_INSTANCE_ID=\" \" export MAS_AIBROKER_WATSONX_ACTION=\"remove\" export ROLE_NAME=\"aibroker\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Add AIBroker"},{"location":"playbooks/oneclick-aibroker/#install-ai-broker-application","text":"","title":"Install AI Broker Application"},{"location":"playbooks/oneclick-aibroker/#prerequisites","text":"You will need a RedHat OpenShift v4.14 or above with IBM Maximo Application Suite Core v9.x already be installed, the oneclick-core playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/oneclick-aibroker/#dependencies","text":"Object Storage Minio ( if customer does not want to use AWS S3 bucket) AWS S3 ( if customer use AWS S3 bucket bucket) MariaDB database (installed in cluster where aibroker instance) IBM Maximo Application Suite Core v9.x","title":"Dependencies:"},{"location":"playbooks/oneclick-aibroker/#overview","text":"This playbook will add AI Broker v1.0.x to an existing IBM Maximo Application Suite Core installation. This playbook can be ran against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install IBM Maximo Application Suite Core v9.x (~30 Minutes) Install MariaDB (~5 minutes) Install Minio (~5 minutes) Install AI broker application (using playbook): Install application (~10 Minutes) Configure AI Broker (kmodels, tenant, etc) (~5 Minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/oneclick-aibroker/#storage-providers","text":"Notice !!! AI Broker supports AWS and Minio storage providers.","title":"Storage providers"},{"location":"playbooks/oneclick-aibroker/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the AI Broker install MAS_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry MAS_ENTITLEMENT_USERNAME Your IBM Entitlement user to access the IBM Container Registry MAS_AIBROKER_STORAGE_ACCESSKEY Your strage provider access key MAS_AIBROKER_STORAGE_SECRETKEY Your storage provider secret key MAS_AIBROKER_STORAGE_HOST Your storage provider host MAS_AIBROKER_STORAGE_REGION Your storage provider region - only when use AWS S3 instance MAS_AIBROKER_STORAGE_PROVIDER Your storage provider name MAS_AIBROKER_STORAGE_SSL Your storage ssl (true/false) MAS_AIBROKER_STORAGE_PIPELINES_BUCKET Your piplines bucket MAS_AIBROKER_STORAGE_TENANTS_BUCKET Your tenants bucket MAS_AIBROKER_STORAGE_TEMPLATES_BUCKET Your templates bucket MAS_AIBROKER_WATSONXAI_APIKEY You WatsonX AI api key MAS_AIBROKER_WATSONXAI_URL You WatsonX AI url MAS_AIBROKER_WATSONXAI_PROJECT_ID You WatsonX projedt Id MAS_AIBROKER_DB_HOST Your database instance host MAS_AIBROKER_DB_PORT Your database instance port MAS_AIBROKER_DB_USER Your database instance user MAS_AIBROKER_DB_DATABASE Your database instance datbase name MAS_AIBROKER_DB_SECRET_NAME Your database instance secret name MAS_AIBROKER_DB_SECRET_VALUE Your database instance password","title":"Required environment variables"},{"location":"playbooks/oneclick-aibroker/#optional-environment-variables","text":"MAS_AIBROKER_CHANNEL Your custom AI broker application channel MAS_ICR_CP Provide custom registry for AI Broker applications MAS_ICR_CPOPEN Provide custom registry for AI Broker operator MAS_CATALOG_VERSION Your custom AI broker catalog version ARTIFACTORY_USERNAME Your artifactory user name to access - this is needed if user deploy from custom registry for example docker-na-public.artifactory.swg-devops.com ARTIFACTORY_TOKEN Your artifactory token for user to access - this is needed if user deploy from custom registry for example docker-na-public.artifactory.swg-devops.com MAS_AIBROKER_TENANT_ACTION Whether to install or remove tenant (default value is: install) MAS_AIBROKER_APIKEY_ACTION Whether to install or remove or update apikey (default value is: install) MAS_AIBROKER_WATSONX_ACTION Whether to install or remove watsonx secret (default value is: install) MAS_AIBROKER_S3_ACTION Whether to install or remove s3 (default value is: install)","title":"Optional environment variables"},{"location":"playbooks/oneclick-aibroker/#usage","text":"","title":"Usage"},{"location":"playbooks/oneclick-aibroker/#ai-broker-deployment-steps","text":"","title":"AI broker deployment steps"},{"location":"playbooks/oneclick-aibroker/#notice-for-s3-manage-please-make-sure-you-have-deployed-dependencies","text":"","title":"Notice: For S3 manage please make sure you have deployed dependencies"},{"location":"playbooks/oneclick-aibroker/#install-boto3-python-module-use-python-environment","text":"python3 -m venv /tmp/venv source /tmp/venv/bin/activate python3 -m pip install boto3","title":"install boto3 python module (use python environment)"},{"location":"playbooks/oneclick-aibroker/#run-playbooks-for-deploy-ai-broker-from-internal-registry-ex-docker-na-publicartifactoryswg-devopscom","text":"export ARTIFACTORY_USERNAME=\"<artifactory user>\" export ARTIFACTORY_TOKEN=\"<artifactory token>\" export MAS_ICR_CP=\"<internal redistry for aibroker applications>\" export MAS_ICR_CPOPEN=\"<internal redistry for aibroker operator>\" export MAS_INSTANCE_ID=\"<instanceId>\" export MAS_AIBROKER_STORAGE_ACCESSKEY=\"<storage provider access key>\" export MAS_AIBROKER_STORAGE_SECRETKEY=\"<storage provider secret key>\" export MAS_AIBROKER_STORAGE_HOST=\"<storage provider host>\" export MAS_AIBROKER_STORAGE_SSL=\"true or false\" export MAS_AIBROKER_STORAGE_REGION=\"<storage provider region - only for aws>\" export MAS_AIBROKER_STORAGE_PROVIDER=\"<storage provider name>\" export MAS_AIBROKER_STORAGE_PORT=\"<storage provider port - only for minio>\" export MAS_AIBROKER_STORAGE_PIPELINES_BUCKET=\"<pipelines bucket name>\" export MAS_AIBROKER_STORAGE_TENANTS_BUCKET=\"<tenants bucket name>\" export MAS_AIBROKER_STORAGE_TEMPLATES_BUCKET=\"<templates bucket name>\" export MAS_AIBROKER_WATSONXAI_APIKEY=\"<watsonx AI api key>\" export MAS_AIBROKER_WATSONXAI_URL=\"<watsonx AI learning url>\" export MAS_AIBROKER_WATSONXAI_PROJECT_ID=\"<watsonx AI project ID>\" export MAS_AIBROKER_DB_HOST=\"<database instance host>\" export MAS_AIBROKER_DB_PORT=\"<database instance port>\" export MAS_AIBROKER_DB_USER=\"<database instance user>\" export MAS_AIBROKER_DB_DATABASE=\"<database instance datbase name>\" export MAS_AIBROKER_DB_SECRET_NAME=\"<database instance secret name>\" export MAS_AIBROKER_DB_SECRET_VALUE=\"<database instance password>\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/oneclick_add_aibroker.yml","title":"Run playbooks for deploy AI Broker from internal registry ex. docker-na-public.artifactory.swg-devops.com"},{"location":"playbooks/oneclick-aibroker/#run-playbooks-for-deploy-ai-broker-from-public-registry-ex-icrio","text":"export MAS_ENTITLEMENT_USERNAME=\"<user>\" export MAS_ENTITLEMENT_KEY=\"<token>\" export MAS_INSTANCE_ID=\"<instanceId>\" export MAS_AIBROKER_STORAGE_ACCESSKEY=\"<storage provider access key>\" export MAS_AIBROKER_STORAGE_SECRETKEY=\"<storage provider secret key>\" export MAS_AIBROKER_STORAGE_HOST=\"<storage provider host>\" export MAS_AIBROKER_STORAGE_SSL=\"true or false\" export MAS_AIBROKER_STORAGE_REGION=\"<storage provider region - only for aws>\" export MAS_AIBROKER_STORAGE_PROVIDER=\"<storage provider name>\" export MAS_AIBROKER_STORAGE_PORT=\"<storage provider port - only for minio>\" export MAS_AIBROKER_STORAGE_PIPELINES_BUCKET=\"<pipelines bucket name>\" export MAS_AIBROKER_STORAGE_TENANTS_BUCKET=\"<tenants bucket name>\" export MAS_AIBROKER_STORAGE_TEMPLATES_BUCKET=\"<templates bucket name>\" export MAS_AIBROKER_WATSONXAI_APIKEY=\"<watsonx AI api key>\" export MAS_AIBROKER_WATSONXAI_URL=\"<watsonx AI url>\" export MAS_AIBROKER_WATSONXAI_PROJECT_ID=\"<watsonx AI project ID>\" export MAS_AIBROKER_DB_HOST=\"<database instance host>\" export MAS_AIBROKER_DB_PORT=\"<database instance port>\" export MAS_AIBROKER_DB_USER=\"<database instance user>\" export MAS_AIBROKER_DB_DATABASE=\"<database instance datbase name>\" export MAS_AIBROKER_DB_SECRET_NAME=\"<database instance secret name>\" export MAS_AIBROKER_DB_SECRET_VALUE=\"<database instance password>\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/oneclick_add_aibroker.yml","title":"Run playbooks for deploy AI Broker from public registry ex. icr.io"},{"location":"playbooks/oneclick-aibroker/#_1","text":"","title":""},{"location":"playbooks/oneclick-aibroker/#notice-playbook-oneclick_add_aibrokeryml-will-run-three-roles","text":"","title":"NOTICE: playbook oneclick_add_aibroker.yml will run three roles:"},{"location":"playbooks/oneclick-aibroker/#role-odh","text":"Install Red Hat OpenShift Serverless Operator Install Red Hat OpenShift Service Mesh Operator Install Authorino Operator Install Open Data Hub Operator Create DSCInitialization instance Create Data Science Cluster Create Create Data Science Pipelines Application","title":"Role: odh"},{"location":"playbooks/oneclick-aibroker/#role-kmodels","text":"Install Kmodel controller Install istio Install Kmodel store Install Kmodel watcher","title":"Role: kmodels"},{"location":"playbooks/oneclick-aibroker/#role-aibroker","text":"Install AI Broker api application Create AI Broker tenant Create, delete AI Broker API Key Create, delete AWS S3 API Key Create, delete WatsonX AI API Key","title":"Role: aibroker"},{"location":"playbooks/oneclick-aibroker/#how-to-create-s3","text":"","title":"How to create S3"},{"location":"playbooks/oneclick-aibroker/#prerequisites_1","text":"IBM AI Broker Application","title":"Prerequisites"},{"location":"playbooks/oneclick-aibroker/#run-playbooks","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export MAS_AIBROKER_STORAGE_ACCESSKEY=\"<storage provider access key>\" export MAS_AIBROKER_STORAGE_SECRETKEY=\"<storage provider secret key>\" export MAS_AIBROKER_STORAGE_HOST=\"<storage provider host>\" export MAS_AIBROKER_STORAGE_REGION=\"<storage provider region>\" export MAS_AIBROKER_S3_ACTION=\"install\" export ROLE_NAME=\"aibroker\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Run playbooks"},{"location":"playbooks/oneclick-aibroker/#how-to-delete-s3","text":"","title":"How to delete S3"},{"location":"playbooks/oneclick-aibroker/#prerequisites_2","text":"S3 created in a cluster","title":"Prerequisites"},{"location":"playbooks/oneclick-aibroker/#run-playbooks_1","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export MAS_AIBROKER_STORAGE_ACCESSKEY=\"<storage provider access key>\" export MAS_AIBROKER_STORAGE_SECRETKEY=\"<storage provider secret key>\" export MAS_AIBROKER_STORAGE_HOST=\"<storage provider host>\" export MAS_AIBROKER_STORAGE_REGION=\"<storage provider region>\" export MAS_AIBROKER_S3_ACTION=\"remove\" export ROLE_NAME=\"aibroker\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Run playbooks"},{"location":"playbooks/oneclick-aibroker/#how-to-create-api-key","text":"","title":"How to create API Key"},{"location":"playbooks/oneclick-aibroker/#prerequisites_3","text":"IBM AI Broker Application","title":"Prerequisites"},{"location":"playbooks/oneclick-aibroker/#run-playbooks_2","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export MAS_AIBROKER_APIKEY_ACTION=\"install\" export ROLE_NAME=\"aibroker\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Run playbooks"},{"location":"playbooks/oneclick-aibroker/#how-to-delete-api-key","text":"","title":"How to delete API Key"},{"location":"playbooks/oneclick-aibroker/#prerequisites_4","text":"API Key created in a cluster","title":"Prerequisites"},{"location":"playbooks/oneclick-aibroker/#run-playbooks_3","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export MAS_AIBROKER_APIKEY_ACTION=\"remove\" export ROLE_NAME=\"aibroker\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Run playbooks"},{"location":"playbooks/oneclick-aibroker/#how-to-create-watsonx-api-key","text":"","title":"How to create WatsonX API Key"},{"location":"playbooks/oneclick-aibroker/#prerequisites_5","text":"IBM AI Broker Application","title":"Prerequisites"},{"location":"playbooks/oneclick-aibroker/#run-playbooks_4","text":"export MAS_INSTANCE_ID=\"<instanceId>\" export MAS_AIBROKER_WATSONX_ACTION=\"install\" export ROLE_NAME=\"aibroker\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Run playbooks"},{"location":"playbooks/oneclick-aibroker/#how-to-watsonx-api-key","text":"","title":"How to WatsonX API Key"},{"location":"playbooks/oneclick-aibroker/#prerequisites_6","text":"WatsonX API Key created in a cluster","title":"Prerequisites"},{"location":"playbooks/oneclick-aibroker/#run-playbooks_5","text":"```bash export MAS_INSTANCE_ID=\" \" export MAS_AIBROKER_WATSONX_ACTION=\"remove\" export ROLE_NAME=\"aibroker\" oc login --token=xxxx --server=https://myocpserver ansible-playbook playbooks/run_role.yml","title":"Run playbooks"},{"location":"playbooks/oneclick-core/","text":"OneClick Install for MAS Core \u00a4 This playbook will install and configure IBM Maximo Application Suite Core along with all necessary dependencies. This can be ran against any OCP cluster regardless of its type, whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. It will take approximately 90 minutes to set up MAS core services and all of its dependencies, at the end of the process you will be able to login to the MAS admin dashboard to install any applications that you wish to use, or you can use our other playbooks to automate the installation of those applications (including any additional dependencies) Playbook Content \u00a4 Install IBM Operator Catalogs (1 minute) Install IBM Common Services (3 minutes) Install Certificate Manager Operator (3 minutes) Install Mongodb Operator and Create a Cluster (10 minutes) Install and bootstrap IBM Suite License Service (10 minutes) Install IBM User Data Services (30 minutes) Generate a MAS Workspace Configuration (1 minute) Configure Cloud Internet Services Integration for Maximo Application Suite (Optional, 1 minute) Install Maximo Application Suite Core Services (1 minute) Configure Maximo Application Suite (1 minute) Verify the Install and Configuration of Maximo Application Suite (25 minutes) All timings are estimates, see the individual pages for each of these roles for more information and full details of all configuration options available in this playbook. Preparation \u00a4 1. IBM Entitlement key \u00a4 Access Container Software Library using your IBMId to access your entitlement key 2. MAS License File \u00a4 Access IBM License Key Center , on the Get Keys menu select IBM AppPoint Suites . Select IBM MAXIMO APPLICATION SUITE AppPOINT LIC and on the next page fill in the information as below: Field Content Number of Keys How many AppPoints to assign to the license file Host ID Type Set to Ethernet Address Host ID Enter any 12 digit hexadecimal string Hostname Set to the hostname of your OCP instance Port Set to 27000 The other values can be left at their defaults. Finally, click Generate and download the license file to your home directory as entitlement.lic , set SLS_LICENSE_FILE to point to this location. Usage \u00a4 Required environment variables \u00a4 IBM_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in SLS_LICENSE_FILE SLS_LICENSE_FILE The path to the location of the license file. DRO_CONTACT_EMAIL Primary contact e-mail address DRO_CONTACT_FIRSTNAME Primary contact first name DRO_CONTACT_LASTNAME Primary contact last name Storage Class Configuraton \u00a4 Storage class configuration is built into the collection and the playbook will auto-select the appropriate storage classes when it detects the presence of certain storage classes in your cluster (IBM Cloud Storage or OpenShift Container Storage). If you are running the install on a cluster that does not have these storage classes then you will also must configure the following environment variables: ReadWriteMany Access Mode \u00a4 Usually fulfilled by file storage classes: PROMETHEUS_ALERTMGR_STORAGE_CLASS ReadWriteOnce Access Mode \u00a4 Usually fulfilled by block storage classes: PROMETHEUS_STORAGE_CLASS PROMETHEUS_USERWORKLOAD_STORAGE_CLASS GRAFANA_INSTANCE_STORAGE_CLASS MONGODB_STORAGE_CLASS UDS_STORAGE_CLASS Examples \u00a4 Release build \u00a4 The simplest configuration to deploy a release build of IBM Maximo Application Suite (core only) with dependencies is: export IBM_ENTITLEMENT_KEY=xxx export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli Pre-release build \u00a4 To deploy a pre-release build of IBM Maximo Application Suite (core only) with dependencies a number of additional parameters are required, note that pre-release builds are only available to IBM employees: export IBM_ENTITLEMENT_KEY=xxx export ARTIFACTORY_USERNAME=$W3_USERNAME_LOWERCASE export ARTIFACTORY_TOKEN=xxx export MAS_ICR_CP=docker-na-public.artifactory.swg-devops.com/wiotp-docker-local export MAS_ICR_CPOPEN=docker-na-public.artifactory.swg-devops.com/wiotp-docker-local export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_TOKEN export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export MAS_CATALOG_SOURCE=ibm-operator-catalog export MAS_CHANNEL=rp1dev88 export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli Tip Enable login for Maximo Application Suite self-signed certificates. If you are using self-signed certificates in a development or test environment, you must manually enable login by using either of the following methods: Download the certificates from the cluster and add them to your local certificate manager. In your browser, go to the Maximo Application Suite API URL: \"https://api.mas_domain/\" and then accept the certificate security risks. After you accept the risks, an AIUC01999E error is displayed. This message is expected. You can now continue with the setup process.","title":"Install Core"},{"location":"playbooks/oneclick-core/#oneclick-install-for-mas-core","text":"This playbook will install and configure IBM Maximo Application Suite Core along with all necessary dependencies. This can be ran against any OCP cluster regardless of its type, whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. It will take approximately 90 minutes to set up MAS core services and all of its dependencies, at the end of the process you will be able to login to the MAS admin dashboard to install any applications that you wish to use, or you can use our other playbooks to automate the installation of those applications (including any additional dependencies)","title":"OneClick Install for MAS Core"},{"location":"playbooks/oneclick-core/#playbook-content","text":"Install IBM Operator Catalogs (1 minute) Install IBM Common Services (3 minutes) Install Certificate Manager Operator (3 minutes) Install Mongodb Operator and Create a Cluster (10 minutes) Install and bootstrap IBM Suite License Service (10 minutes) Install IBM User Data Services (30 minutes) Generate a MAS Workspace Configuration (1 minute) Configure Cloud Internet Services Integration for Maximo Application Suite (Optional, 1 minute) Install Maximo Application Suite Core Services (1 minute) Configure Maximo Application Suite (1 minute) Verify the Install and Configuration of Maximo Application Suite (25 minutes) All timings are estimates, see the individual pages for each of these roles for more information and full details of all configuration options available in this playbook.","title":"Playbook Content"},{"location":"playbooks/oneclick-core/#preparation","text":"","title":"Preparation"},{"location":"playbooks/oneclick-core/#1-ibm-entitlement-key","text":"Access Container Software Library using your IBMId to access your entitlement key","title":"1. IBM Entitlement key"},{"location":"playbooks/oneclick-core/#2-mas-license-file","text":"Access IBM License Key Center , on the Get Keys menu select IBM AppPoint Suites . Select IBM MAXIMO APPLICATION SUITE AppPOINT LIC and on the next page fill in the information as below: Field Content Number of Keys How many AppPoints to assign to the license file Host ID Type Set to Ethernet Address Host ID Enter any 12 digit hexadecimal string Hostname Set to the hostname of your OCP instance Port Set to 27000 The other values can be left at their defaults. Finally, click Generate and download the license file to your home directory as entitlement.lic , set SLS_LICENSE_FILE to point to this location.","title":"2. MAS License File"},{"location":"playbooks/oneclick-core/#usage","text":"","title":"Usage"},{"location":"playbooks/oneclick-core/#required-environment-variables","text":"IBM_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in SLS_LICENSE_FILE SLS_LICENSE_FILE The path to the location of the license file. DRO_CONTACT_EMAIL Primary contact e-mail address DRO_CONTACT_FIRSTNAME Primary contact first name DRO_CONTACT_LASTNAME Primary contact last name","title":"Required environment variables"},{"location":"playbooks/oneclick-core/#storage-class-configuraton","text":"Storage class configuration is built into the collection and the playbook will auto-select the appropriate storage classes when it detects the presence of certain storage classes in your cluster (IBM Cloud Storage or OpenShift Container Storage). If you are running the install on a cluster that does not have these storage classes then you will also must configure the following environment variables:","title":"Storage Class Configuraton"},{"location":"playbooks/oneclick-core/#readwritemany-access-mode","text":"Usually fulfilled by file storage classes: PROMETHEUS_ALERTMGR_STORAGE_CLASS","title":"ReadWriteMany Access Mode"},{"location":"playbooks/oneclick-core/#readwriteonce-access-mode","text":"Usually fulfilled by block storage classes: PROMETHEUS_STORAGE_CLASS PROMETHEUS_USERWORKLOAD_STORAGE_CLASS GRAFANA_INSTANCE_STORAGE_CLASS MONGODB_STORAGE_CLASS UDS_STORAGE_CLASS","title":"ReadWriteOnce Access Mode"},{"location":"playbooks/oneclick-core/#examples","text":"","title":"Examples"},{"location":"playbooks/oneclick-core/#release-build","text":"The simplest configuration to deploy a release build of IBM Maximo Application Suite (core only) with dependencies is: export IBM_ENTITLEMENT_KEY=xxx export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Release build"},{"location":"playbooks/oneclick-core/#pre-release-build","text":"To deploy a pre-release build of IBM Maximo Application Suite (core only) with dependencies a number of additional parameters are required, note that pre-release builds are only available to IBM employees: export IBM_ENTITLEMENT_KEY=xxx export ARTIFACTORY_USERNAME=$W3_USERNAME_LOWERCASE export ARTIFACTORY_TOKEN=xxx export MAS_ICR_CP=docker-na-public.artifactory.swg-devops.com/wiotp-docker-local export MAS_ICR_CPOPEN=docker-na-public.artifactory.swg-devops.com/wiotp-docker-local export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_TOKEN export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export MAS_CATALOG_SOURCE=ibm-operator-catalog export MAS_CHANNEL=rp1dev88 export SLS_LICENSE_ID=xxx export SLS_LICENSE_FILE=/path/to/entitlement.lic export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli Tip Enable login for Maximo Application Suite self-signed certificates. If you are using self-signed certificates in a development or test environment, you must manually enable login by using either of the following methods: Download the certificates from the cluster and add them to your local certificate manager. In your browser, go to the Maximo Application Suite API URL: \"https://api.mas_domain/\" and then accept the certificate security risks. After you accept the risks, an AIUC01999E error is displayed. This message is expected. You can now continue with the setup process.","title":"Pre-release build"},{"location":"playbooks/oneclick-iot/","text":"Install IoT Application \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 already be installed, the oneclick-core playbook can be used to set this up. Overview \u00a4 This playbook will add Maximo IoT v8.4 to an existing IBM Maximo Application Suite Core installation. It will also creatie an in-cluster Db2 instance and Kafka cluster, both of which will be automatically set up as system-level configurations in MAS. IoT will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install IBM Db2 Universal Operator (2 minutes) Create Db2 Warehouse Instance (45 minutes) Install RedHat AMQ Streams Operator (2 minutes) Create Apache Kafka Cluster (15 minutes) Configure Maximo Application Suite: Set up Db2 instance as the system-level JDBC datasource Set up Kafka cluster as the system-level Kafka Install Maximo IoT application: Install application (90 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Storage Class Configuraton \u00a4 A persistent volume storage class is required for the FPL component. Storage class configuration is built into the collection and the playbook will auto-select the appropriate storage classes when it detects the presence of certain storage classes in your cluster (IBM Cloud Storage or OpenShift Container Storage). If you are running the install on a cluster that does not have these storage classes then you will also must configure the following environment variables: ReadWriteOnce Access Mode \u00a4 Usually fulfilled by block storage classes: MAS_APP_SETTINGS_IOT_FPL_PVC_STORAGE_CLASS Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Optional environment variables \u00a4 MAS_APP_SETTINGS_IOT_DEPLOYMENT_SIZE Define the IoT deployment size, one of dev , small or large . Defaults to small . Usage \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_iot Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Add IoT"},{"location":"playbooks/oneclick-iot/#install-iot-application","text":"","title":"Install IoT Application"},{"location":"playbooks/oneclick-iot/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 already be installed, the oneclick-core playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/oneclick-iot/#overview","text":"This playbook will add Maximo IoT v8.4 to an existing IBM Maximo Application Suite Core installation. It will also creatie an in-cluster Db2 instance and Kafka cluster, both of which will be automatically set up as system-level configurations in MAS. IoT will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install IBM Db2 Universal Operator (2 minutes) Create Db2 Warehouse Instance (45 minutes) Install RedHat AMQ Streams Operator (2 minutes) Create Apache Kafka Cluster (15 minutes) Configure Maximo Application Suite: Set up Db2 instance as the system-level JDBC datasource Set up Kafka cluster as the system-level Kafka Install Maximo IoT application: Install application (90 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/oneclick-iot/#storage-class-configuraton","text":"A persistent volume storage class is required for the FPL component. Storage class configuration is built into the collection and the playbook will auto-select the appropriate storage classes when it detects the presence of certain storage classes in your cluster (IBM Cloud Storage or OpenShift Container Storage). If you are running the install on a cluster that does not have these storage classes then you will also must configure the following environment variables:","title":"Storage Class Configuraton"},{"location":"playbooks/oneclick-iot/#readwriteonce-access-mode","text":"Usually fulfilled by block storage classes: MAS_APP_SETTINGS_IOT_FPL_PVC_STORAGE_CLASS","title":"ReadWriteOnce Access Mode"},{"location":"playbooks/oneclick-iot/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/oneclick-iot/#optional-environment-variables","text":"MAS_APP_SETTINGS_IOT_DEPLOYMENT_SIZE Define the IoT deployment size, one of dev , small or large . Defaults to small .","title":"Optional environment variables"},{"location":"playbooks/oneclick-iot/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_iot Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/oneclick-manage/","text":"Install Manage Application \u00a4 This playbook will add Maximo Manage to an existing IBM Maximo Application Suite Instance. Refer to the oneclick-core playbook to set up the MAS Core Platform before running this playbook. The playbook will also create an in-cluster Db2 instance using the IBM Db2 Universal Operator, which will be automatically set up as the system-level JDBC configuration in MAS. Playbook Content \u00a4 Install Cloud Pak for Data (optional, set CPD_INSTALL_PLATFORM ) Add Cognos to CP4D (optional, set CPD_INSTALL_COGNOS ) Add Watson Studio Local to CP4D (optional, set CPD_INSTALL_WSL ) Create Db2 Instance using IBM Db2 Universal Operator Initialize Db2 Instance for Maximo Manage Configure MAS to use the new Db2 Instance Configure MAS to use BYO database (optional, set CONFIGURE_EXTERNAL_DB ) Install Maximo Manage Application Configure Maximo Manage Workspace Configure Manage Attachments (optional, set CONFIGURE_MANAGE_ATTACHMENTS ) Configure Manage Building Information Models (optional, set CONFIGURE_MANAGE_BIM ) See the individual pages for each of these roles for more information and full details of all configuration options available in this playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Tip Manage requires the user to select one or more application components to enable in the workspace. By default the base component at the latest version will be installed if no MAS_APPWS_COMPONENTS is set. To customise the components that are enabled use the MAS_APPWS_COMPONENTS environment variable, for example to enable Manage(base) and Health set it to the following: export MAS_APPWS_COMPONENTS=\"base=latest,health=latest\" To enable Asset Investment Optimizer, optional feature of health. Set MANAGE_AIO_FLAG to true . By default this flag is set to false . This featue is only avalaible on Manage with health as a addon or on Health as a Standalone install. export MANAGE_AIO_FLAG=true Optional Cloud Pak for Data Installation \u00a4 Optional integration with Cloud Pak for Data is supported in Maximo Manage. This can be enabled in the playbook as below: export CPD_INSTALL_PLATFORM=true export CPD_INSTALL_COGNOS=true export CPD_INSTALL_WSL=true export CPD_PRODUCT_VERSION=x.y.z Usage \u00a4 Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli In-Cluster Db2 \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_manage Bring Your Own Database \u00a4 If you do not want to use the Db2 Universal Operator to provide the datbase for Maximo Manage then you can configure the playbook as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CONFIGURE_EXTERNAL_DB=true export DB_INSTANCE_ID=maxdbxx export MAS_JDBC_USER=maximo export MAS_JDBC_PASSWORD=xxx export MAS_JDBC_URL=xxx export MAS_APP_SETTINGS_DB_SCHEMA=maximo export MAS_APP_SETTINGS_TABLESPACE=maxdata export MAS_APP_SETTINGS_INDEXSPACE=maxindex oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_manage For full details of configuration options available refer to the gencfg_jdbc role documentation. Cloud Pak For Data Integration \u00a4 To install CP4D with Cognos and/or Watson Studio Local optional dependencies: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CPD_INSTALL_PLATFORM=\"true\" export CPD_INSTALL_COGNOS=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_PRODUCT_VERSION=\"4.6.6\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_manage For full details of configuration options available refer to the cp4d & cp4d_service role documentation. Health Standalone Install \u00a4 To install Health as a Standalone application, set MAS_APP_ID and MAS_APPWS_COMPONENTS as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_ID=health export MAS_APPWS_COMPONENTS=\"health=latest\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_manage Warning Note that installing Health standalone will prevent the use of all other Manage components. It is recommended to install Manage normally and just enable the Health component in the Manage application.","title":"Add Manage"},{"location":"playbooks/oneclick-manage/#install-manage-application","text":"This playbook will add Maximo Manage to an existing IBM Maximo Application Suite Instance. Refer to the oneclick-core playbook to set up the MAS Core Platform before running this playbook. The playbook will also create an in-cluster Db2 instance using the IBM Db2 Universal Operator, which will be automatically set up as the system-level JDBC configuration in MAS.","title":"Install Manage Application"},{"location":"playbooks/oneclick-manage/#playbook-content","text":"Install Cloud Pak for Data (optional, set CPD_INSTALL_PLATFORM ) Add Cognos to CP4D (optional, set CPD_INSTALL_COGNOS ) Add Watson Studio Local to CP4D (optional, set CPD_INSTALL_WSL ) Create Db2 Instance using IBM Db2 Universal Operator Initialize Db2 Instance for Maximo Manage Configure MAS to use the new Db2 Instance Configure MAS to use BYO database (optional, set CONFIGURE_EXTERNAL_DB ) Install Maximo Manage Application Configure Maximo Manage Workspace Configure Manage Attachments (optional, set CONFIGURE_MANAGE_ATTACHMENTS ) Configure Manage Building Information Models (optional, set CONFIGURE_MANAGE_BIM ) See the individual pages for each of these roles for more information and full details of all configuration options available in this playbook.","title":"Playbook Content"},{"location":"playbooks/oneclick-manage/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Tip Manage requires the user to select one or more application components to enable in the workspace. By default the base component at the latest version will be installed if no MAS_APPWS_COMPONENTS is set. To customise the components that are enabled use the MAS_APPWS_COMPONENTS environment variable, for example to enable Manage(base) and Health set it to the following: export MAS_APPWS_COMPONENTS=\"base=latest,health=latest\" To enable Asset Investment Optimizer, optional feature of health. Set MANAGE_AIO_FLAG to true . By default this flag is set to false . This featue is only avalaible on Manage with health as a addon or on Health as a Standalone install. export MANAGE_AIO_FLAG=true","title":"Required environment variables"},{"location":"playbooks/oneclick-manage/#optional-cloud-pak-for-data-installation","text":"Optional integration with Cloud Pak for Data is supported in Maximo Manage. This can be enabled in the playbook as below: export CPD_INSTALL_PLATFORM=true export CPD_INSTALL_COGNOS=true export CPD_INSTALL_WSL=true export CPD_PRODUCT_VERSION=x.y.z","title":"Optional Cloud Pak for Data Installation"},{"location":"playbooks/oneclick-manage/#usage","text":"Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/oneclick-manage/#in-cluster-db2","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_manage","title":"In-Cluster Db2"},{"location":"playbooks/oneclick-manage/#bring-your-own-database","text":"If you do not want to use the Db2 Universal Operator to provide the datbase for Maximo Manage then you can configure the playbook as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CONFIGURE_EXTERNAL_DB=true export DB_INSTANCE_ID=maxdbxx export MAS_JDBC_USER=maximo export MAS_JDBC_PASSWORD=xxx export MAS_JDBC_URL=xxx export MAS_APP_SETTINGS_DB_SCHEMA=maximo export MAS_APP_SETTINGS_TABLESPACE=maxdata export MAS_APP_SETTINGS_INDEXSPACE=maxindex oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_manage For full details of configuration options available refer to the gencfg_jdbc role documentation.","title":"Bring Your Own Database"},{"location":"playbooks/oneclick-manage/#cloud-pak-for-data-integration","text":"To install CP4D with Cognos and/or Watson Studio Local optional dependencies: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export CPD_INSTALL_PLATFORM=\"true\" export CPD_INSTALL_COGNOS=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_PRODUCT_VERSION=\"4.6.6\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_manage For full details of configuration options available refer to the cp4d & cp4d_service role documentation.","title":"Cloud Pak For Data Integration"},{"location":"playbooks/oneclick-manage/#health-standalone-install","text":"To install Health as a Standalone application, set MAS_APP_ID and MAS_APPWS_COMPONENTS as below: export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_ID=health export MAS_APPWS_COMPONENTS=\"health=latest\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_manage Warning Note that installing Health standalone will prevent the use of all other Manage components. It is recommended to install Manage normally and just enable the Health component in the Manage application.","title":"Health Standalone Install"},{"location":"playbooks/oneclick-monitor/","text":"Install Monitor Application \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 and Maximo IoT v8.4 already be installed, the oneclick-core and oneclick-iot playbooks can be used to set this up. Overview \u00a4 This playbook will add Maximo Monitor v8.7 to an existing IBM Maximo Application Suite Core installation. Monitor will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Monitor application: Install application (60 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Optional environment variables \u00a4 MAS_APP_SETTINGS_MONITOR_DEPLOYMENT_SIZE Define the Monitor deployment size, one of dev , small or large . Defaults to dev . Usage \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_monitor Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Add Monitor"},{"location":"playbooks/oneclick-monitor/#install-monitor-application","text":"","title":"Install Monitor Application"},{"location":"playbooks/oneclick-monitor/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.7 and Maximo IoT v8.4 already be installed, the oneclick-core and oneclick-iot playbooks can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/oneclick-monitor/#overview","text":"This playbook will add Maximo Monitor v8.7 to an existing IBM Maximo Application Suite Core installation. Monitor will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Monitor application: Install application (60 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/oneclick-monitor/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/oneclick-monitor/#optional-environment-variables","text":"MAS_APP_SETTINGS_MONITOR_DEPLOYMENT_SIZE Define the Monitor deployment size, one of dev , small or large . Defaults to dev .","title":"Optional environment variables"},{"location":"playbooks/oneclick-monitor/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_monitor Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/oneclick-optimizer/","text":"Install Optimizer Application \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.9. The [oneclick-core] (oneclick-core.md) playbook can be used to set this up. Overview \u00a4 This playbook will add Maximo Optimizer v8.3 to an existing IBM Maximo Application Suite Core installation. Optimizer will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Optimizer application: Install application (10 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Usage \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_optimizer Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Add Optimizer"},{"location":"playbooks/oneclick-optimizer/#install-optimizer-application","text":"","title":"Install Optimizer Application"},{"location":"playbooks/oneclick-optimizer/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.9. The [oneclick-core] (oneclick-core.md) playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/oneclick-optimizer/#overview","text":"This playbook will add Maximo Optimizer v8.3 to an existing IBM Maximo Application Suite Core installation. Optimizer will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install Maximo Optimizer application: Install application (10 minutes) Configure workspace (5 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/oneclick-optimizer/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/oneclick-optimizer/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_optimizer Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/oneclick-predict/","text":"Install Predict Application \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift v4.12 cluster with IBM Maximo Application Suite Core v8.11 already be installed, the oneclick-core playbook can be used to set this up. Overview \u00a4 This playbook will add Predict v8.9 to an existing IBM Maximo Application Suite Core installation. It will also install CloudPak for Data + CP4D services. This playbook can be ran against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install CP4D (~1 1/2 hours) Install Watson Studio (~3 hours) Install Watson Machine Learning (~2 1/2 hours) Install Spark (~30 minutes) Install Openscale (~1 hour) Install SPSS Install Predict application: Install application (~15 Minutes) Configure workspace (~30 Minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. As of MAS 8.10, predict 8.8.0 will start to support SPSS Modeler, to install SPSS as part of CP4D set CPD_INSTALL_SPSS=true in your environment variables before running the playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry CPD_WSL_PROJECT_ID - Ensure a Project ID Text box has a valid Watson Studio project ID. To obtain the project ID, Navigate to Cp4d/Watson Studio and create/Reuse a project. Open the project and look into the Browser URL, obtain the project ID from the URL and update Project ID settings. CPD_WML_INSTANCE_ID Set Default value to \"openshift\" CPD_WML_URL Set Default value to \"https://internal-nginx-svc.ibm-cpd.svc:12443\" . ibm-cpd in the URL corresponds to the project name (namespace) of cp4d installation CPD_PRODUCT_VERSION (Required if WML_VERSION is not informed) Cloud Pak for Data version installed in the cluster in 4.X format, it will be used to obtain the correct WML version to be installed WML_VERSION (Required if CPD_PRODUCT_VERSION is not informed) The wml_version for cp4d 4.0.x will be 4.0, if cp4d is 4.5.x , wml_version should change to 4.5, if cp4d is 4.6.x , wml_version should change to 4.6 These variables are required only if you set CP4D_INSTALL_WSL to false in optional varibles: CPD_ADMIN_USERNAME CP4D Username CPD_ADMIN_PASSWORD CP4D Password CPD_ADMIN_URL CP4D Base URL Warning When not using this playbook to install Cloud Pak for Data it is important to ensure that your existing instance already has all the required services enabled. Optional environment variables \u00a4 CPD_INSTALL_PLATFORM True/False - If you HAVE CP4D already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_WSL True/False - If you HAVE Watson Studio already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_WML True/False - If you HAVE Watson Machine Learning already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_SPARK True/False - If you HAVE Spark already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_OPENSCALE True/False - If you HAVE Openscale already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_SPSS True/False - If you HAVE SPSS Modeler already installed in your cluster you can skip this variable as False is set default Usage \u00a4 Tip If you do not want to set up all the dependencies on your local system, you can run the playbook from inside the CLI container image: docker run -ti --pull always quay.io/ibmmas/cli Cloud Pak for Data is already installed \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_CHANNEL=\"8.9.x\" export CPD_PRODUCT_VERSION=\"4.6.4\" export CPD_WSL_PROJECT_ID=\"xxxx\" export CPD_WML_INSTANCE_ID=\"openshift\" export CPD_WML_URL=\"https://internal-nginx-svc.ibm-cpd.svc:12443\" export WML_VERSION=\"4.6\" export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_ADMIN_URL=\"https://mycp4durl\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_predict Cloud Pak for Data is not installed \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_CHANNEL=\"8.9.x\" export CPD_PRODUCT_VERSION=\"4.6.4\" export CPD_INSTALL_PLATFORM=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_INSTALL_WML=\"true\" export CPD_INSTALL_SPARK=\"true\" export CPD_INSTALL_OPENSCALE=\"true\" export CPD_INSTALL_DISCOVERY=\"true\" export CPD_INSTALL_SPSS=\"true\" export CPD_WSL_PROJECT_ID=\"xxxx\" export CPD_WML_INSTANCE_ID=\"openshift\" export CPD_WML_URL=\"https://internal-nginx-svc.ibm-cpd.svc:12443\" export WML_VERSION=\"4.6\" export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_ADMIN_URL=\"https://mycp4durl\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_predict","title":"Add Predict"},{"location":"playbooks/oneclick-predict/#install-predict-application","text":"","title":"Install Predict Application"},{"location":"playbooks/oneclick-predict/#prerequisites","text":"You will need a RedHat OpenShift v4.12 cluster with IBM Maximo Application Suite Core v8.11 already be installed, the oneclick-core playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/oneclick-predict/#overview","text":"This playbook will add Predict v8.9 to an existing IBM Maximo Application Suite Core installation. It will also install CloudPak for Data + CP4D services. This playbook can be ran against any OCP cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install CP4D (~1 1/2 hours) Install Watson Studio (~3 hours) Install Watson Machine Learning (~2 1/2 hours) Install Spark (~30 minutes) Install Openscale (~1 hour) Install SPSS Install Predict application: Install application (~15 Minutes) Configure workspace (~30 Minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. As of MAS 8.10, predict 8.8.0 will start to support SPSS Modeler, to install SPSS as part of CP4D set CPD_INSTALL_SPSS=true in your environment variables before running the playbook.","title":"Overview"},{"location":"playbooks/oneclick-predict/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry CPD_WSL_PROJECT_ID - Ensure a Project ID Text box has a valid Watson Studio project ID. To obtain the project ID, Navigate to Cp4d/Watson Studio and create/Reuse a project. Open the project and look into the Browser URL, obtain the project ID from the URL and update Project ID settings. CPD_WML_INSTANCE_ID Set Default value to \"openshift\" CPD_WML_URL Set Default value to \"https://internal-nginx-svc.ibm-cpd.svc:12443\" . ibm-cpd in the URL corresponds to the project name (namespace) of cp4d installation CPD_PRODUCT_VERSION (Required if WML_VERSION is not informed) Cloud Pak for Data version installed in the cluster in 4.X format, it will be used to obtain the correct WML version to be installed WML_VERSION (Required if CPD_PRODUCT_VERSION is not informed) The wml_version for cp4d 4.0.x will be 4.0, if cp4d is 4.5.x , wml_version should change to 4.5, if cp4d is 4.6.x , wml_version should change to 4.6 These variables are required only if you set CP4D_INSTALL_WSL to false in optional varibles: CPD_ADMIN_USERNAME CP4D Username CPD_ADMIN_PASSWORD CP4D Password CPD_ADMIN_URL CP4D Base URL Warning When not using this playbook to install Cloud Pak for Data it is important to ensure that your existing instance already has all the required services enabled.","title":"Required environment variables"},{"location":"playbooks/oneclick-predict/#optional-environment-variables","text":"CPD_INSTALL_PLATFORM True/False - If you HAVE CP4D already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_WSL True/False - If you HAVE Watson Studio already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_WML True/False - If you HAVE Watson Machine Learning already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_SPARK True/False - If you HAVE Spark already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_OPENSCALE True/False - If you HAVE Openscale already installed in your cluster you can skip this variable as False is set default CPD_INSTALL_SPSS True/False - If you HAVE SPSS Modeler already installed in your cluster you can skip this variable as False is set default","title":"Optional environment variables"},{"location":"playbooks/oneclick-predict/#usage","text":"Tip If you do not want to set up all the dependencies on your local system, you can run the playbook from inside the CLI container image: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/oneclick-predict/#cloud-pak-for-data-is-already-installed","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_CHANNEL=\"8.9.x\" export CPD_PRODUCT_VERSION=\"4.6.4\" export CPD_WSL_PROJECT_ID=\"xxxx\" export CPD_WML_INSTANCE_ID=\"openshift\" export CPD_WML_URL=\"https://internal-nginx-svc.ibm-cpd.svc:12443\" export WML_VERSION=\"4.6\" export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_ADMIN_URL=\"https://mycp4durl\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_predict","title":"Cloud Pak for Data is already installed"},{"location":"playbooks/oneclick-predict/#cloud-pak-for-data-is-not-installed","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig export IBM_ENTITLEMENT_KEY=xxx export MAS_APP_CHANNEL=\"8.9.x\" export CPD_PRODUCT_VERSION=\"4.6.4\" export CPD_INSTALL_PLATFORM=\"true\" export CPD_INSTALL_WSL=\"true\" export CPD_INSTALL_WML=\"true\" export CPD_INSTALL_SPARK=\"true\" export CPD_INSTALL_OPENSCALE=\"true\" export CPD_INSTALL_DISCOVERY=\"true\" export CPD_INSTALL_SPSS=\"true\" export CPD_WSL_PROJECT_ID=\"xxxx\" export CPD_WML_INSTANCE_ID=\"openshift\" export CPD_WML_URL=\"https://internal-nginx-svc.ibm-cpd.svc:12443\" export WML_VERSION=\"4.6\" export CPD_ADMIN_USERNAME=\"admin\" export CPD_ADMIN_PASSWORD=\"xxx\" export CPD_ADMIN_URL=\"https://mycp4durl\" oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_predict","title":"Cloud Pak for Data is not installed"},{"location":"playbooks/oneclick-update/","text":"OneClick Update \u00a4 This playbook will update the IBM Maximo Operator Catalog on your OpenShift cluster. This will make available new operator updates, which will be automatically applied across the cluster. These updates will not change the functionality of the software in your cluster, they will only carry fixes for security vulnerabilities and bugs. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) this playbook can be ignored, as you will recieve catalog updates in your cluster as soon as they are released. This playbook is specifically for customers who choose to use static catalogs to control the consumption of updates in their cluster. This is distinct from an upgrade , which will modify the operator subscriptions on your cluster to deliver new features. Performing an updating may make new upgrades available in the cluster, but it will never initiate the upgrade, you must choose when to upgrade. Playbook Content \u00a4 Install IBM Operator Catalog (1 minute) Preparation \u00a4 You will need to determine the version of the IBM Maximo Operator Catalog that you wish to update to. Generally speaking, you should update the most recent catalog available. Important If you are using a private/mirror registry it is critical that you mirror the images from the updated catalog before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the catalog has been updated. You do not need to worry about translating the image tags to digests to make these catalogs compatible with image mirroring on OpenShift, the role will automatically usse the image digest when it installs any static operator catalog. Usage \u00a4 Required environment variables \u00a4 MAS_CATALOG_VERSION Example \u00a4 Only one parameter is required, the new tag of the IBM Maximo Operator Catalog that you wish to use: export MAS_CATALOG_VERSION=@@MAS_LATEST_CATALOG@@ oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_update Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm --pull always quay.io/ibmmas/cli","title":"Update"},{"location":"playbooks/oneclick-update/#oneclick-update","text":"This playbook will update the IBM Maximo Operator Catalog on your OpenShift cluster. This will make available new operator updates, which will be automatically applied across the cluster. These updates will not change the functionality of the software in your cluster, they will only carry fixes for security vulnerabilities and bugs. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) this playbook can be ignored, as you will recieve catalog updates in your cluster as soon as they are released. This playbook is specifically for customers who choose to use static catalogs to control the consumption of updates in their cluster. This is distinct from an upgrade , which will modify the operator subscriptions on your cluster to deliver new features. Performing an updating may make new upgrades available in the cluster, but it will never initiate the upgrade, you must choose when to upgrade.","title":"OneClick Update"},{"location":"playbooks/oneclick-update/#playbook-content","text":"Install IBM Operator Catalog (1 minute)","title":"Playbook Content"},{"location":"playbooks/oneclick-update/#preparation","text":"You will need to determine the version of the IBM Maximo Operator Catalog that you wish to update to. Generally speaking, you should update the most recent catalog available. Important If you are using a private/mirror registry it is critical that you mirror the images from the updated catalog before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the catalog has been updated. You do not need to worry about translating the image tags to digests to make these catalogs compatible with image mirroring on OpenShift, the role will automatically usse the image digest when it installs any static operator catalog.","title":"Preparation"},{"location":"playbooks/oneclick-update/#usage","text":"","title":"Usage"},{"location":"playbooks/oneclick-update/#required-environment-variables","text":"MAS_CATALOG_VERSION","title":"Required environment variables"},{"location":"playbooks/oneclick-update/#example","text":"Only one parameter is required, the new tag of the IBM Maximo Operator Catalog that you wish to use: export MAS_CATALOG_VERSION=@@MAS_LATEST_CATALOG@@ oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_update Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm --pull always quay.io/ibmmas/cli","title":"Example"},{"location":"playbooks/oneclick-upgrade/","text":"OneClick Upgrade \u00a4 This playbook will upgrade the channel subscriptions for IBM Maximo Application Suite on your OpenShift cluster. Upgrades can only be performed to releases available in the version of the IBM Maximo Pperator Catalog that is installed in your cluster. To update to a newer version of the operator catalog refer to the oneclick-update playbook documentation. The playbook will attempt to upgrade MAS Core and all installed applications. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) you will always have access to the latest MAS releases, as you will recieve catalog updates in your cluster as soon as they are released. Customers using the static catalogs to control the consumption of updates in their cluster will need to ensure that the version of the catalog they have installed supports the version of MAS that they wish to upgrade to. Playbook Content \u00a4 Upgrade MAS Core Verify MAS Core Upgrade MAS Application (Assist) Upgrade MAS Application (HP Utilities) Upgrade MAS Application (IoT) Upgrade MAS Application (Manage) Upgrade MAS Application (Monitor) Upgrade MAS Application (Optimizer) Upgrade MAS Application (Predict) Upgrade MAS Application (Safety) Upgrade MAS Application (Visual Inspection) Preparation \u00a4 If you are using a private/mirror registry it is critical that you mirror the images for the new release before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the subscription has been changed, if you have not mirrored the new images the subscription change itself may fail if the operator bundle is not on your private registry. Usage \u00a4 Required Parameters \u00a4 MAS_INSTANCE_ID Set the instance ID of the MAS installation to upgrade Optional Parameters \u00a4 If you provide no values for MAS Core or the individual applications, the roles will attempt to upgrade to the next level of MAS and upgrade applications to the latest version supported by the installed version of MAS Core (after upgrading MAS Core). MAS_CHANNEL Set the target subscription channel for MAS Core MAS_APP_CHANNEL_ASSIST Set the target subscription channel for Assist MAS_APP_CHANNEL_HPUTILITIES Set the target subscription channel for Health & Predict Utilities MAS_APP_CHANNEL_IOT Set the target subscription channel for IoT MAS_APP_CHANNEL_MONITOR Set the target subscription channel for Monitor MAS_APP_CHANNEL_OPTIMIZER Set the target subscription channel for Optimizer MAS_APP_CHANNEL_PREDICT Set the target subscription channel for Predict MAS_APP_CHANNEL_SAFETY Set the target subscription channel for Safety MAS_APP_CHANNEL_VISUALINSPECTION Set the target subscription channel for Visual Inspection Example \u00a4 The simplest way to upgrade MAS is to provide only the instance ID that you wish to upgrade, allowing the roles to determine the correct target version each application. export MAS_INSTANCE_ID=instance1 oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_upgrade You can also explicitly specify the target upgrade: export MAS_INSTANCE_ID=instance1 export MAS_CHANNEL=8.8.x export MAS_APP_CHANNEL_IOT=8.5.x oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_upgrade Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm --pull always quay.io/ibmmas/cli","title":"Upgrade"},{"location":"playbooks/oneclick-upgrade/#oneclick-upgrade","text":"This playbook will upgrade the channel subscriptions for IBM Maximo Application Suite on your OpenShift cluster. Upgrades can only be performed to releases available in the version of the IBM Maximo Pperator Catalog that is installed in your cluster. To update to a newer version of the operator catalog refer to the oneclick-update playbook documentation. The playbook will attempt to upgrade MAS Core and all installed applications. Note If you are using the dynamic catalog ( ibm-maximo-operator-catalog:v8 ) you will always have access to the latest MAS releases, as you will recieve catalog updates in your cluster as soon as they are released. Customers using the static catalogs to control the consumption of updates in their cluster will need to ensure that the version of the catalog they have installed supports the version of MAS that they wish to upgrade to.","title":"OneClick Upgrade"},{"location":"playbooks/oneclick-upgrade/#playbook-content","text":"Upgrade MAS Core Verify MAS Core Upgrade MAS Application (Assist) Upgrade MAS Application (HP Utilities) Upgrade MAS Application (IoT) Upgrade MAS Application (Manage) Upgrade MAS Application (Monitor) Upgrade MAS Application (Optimizer) Upgrade MAS Application (Predict) Upgrade MAS Application (Safety) Upgrade MAS Application (Visual Inspection)","title":"Playbook Content"},{"location":"playbooks/oneclick-upgrade/#preparation","text":"If you are using a private/mirror registry it is critical that you mirror the images for the new release before you run this playbook, otherwise you will see numerous containers in ImagePullBackoff as the updates are rolled out automatically after the subscription has been changed, if you have not mirrored the new images the subscription change itself may fail if the operator bundle is not on your private registry.","title":"Preparation"},{"location":"playbooks/oneclick-upgrade/#usage","text":"","title":"Usage"},{"location":"playbooks/oneclick-upgrade/#required-parameters","text":"MAS_INSTANCE_ID Set the instance ID of the MAS installation to upgrade","title":"Required Parameters"},{"location":"playbooks/oneclick-upgrade/#optional-parameters","text":"If you provide no values for MAS Core or the individual applications, the roles will attempt to upgrade to the next level of MAS and upgrade applications to the latest version supported by the installed version of MAS Core (after upgrading MAS Core). MAS_CHANNEL Set the target subscription channel for MAS Core MAS_APP_CHANNEL_ASSIST Set the target subscription channel for Assist MAS_APP_CHANNEL_HPUTILITIES Set the target subscription channel for Health & Predict Utilities MAS_APP_CHANNEL_IOT Set the target subscription channel for IoT MAS_APP_CHANNEL_MONITOR Set the target subscription channel for Monitor MAS_APP_CHANNEL_OPTIMIZER Set the target subscription channel for Optimizer MAS_APP_CHANNEL_PREDICT Set the target subscription channel for Predict MAS_APP_CHANNEL_SAFETY Set the target subscription channel for Safety MAS_APP_CHANNEL_VISUALINSPECTION Set the target subscription channel for Visual Inspection","title":"Optional Parameters"},{"location":"playbooks/oneclick-upgrade/#example","text":"The simplest way to upgrade MAS is to provide only the instance ID that you wish to upgrade, allowing the roles to determine the correct target version each application. export MAS_INSTANCE_ID=instance1 oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_upgrade You can also explicitly specify the target upgrade: export MAS_INSTANCE_ID=instance1 export MAS_CHANNEL=8.8.x export MAS_APP_CHANNEL_IOT=8.5.x oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_upgrade Tip If you do not want to set up all the dependencies on your local system, you can run the update from inside our container image: docker run -ti --rm --pull always quay.io/ibmmas/cli","title":"Example"},{"location":"playbooks/oneclick-visualinspection/","text":"Install Visual Inspection Application \u00a4 Prerequisites \u00a4 You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.9. The [oneclick-core] (oneclick-core.md) playbook can be used to set this up. Overview \u00a4 This playbook will add Maximo Visual Inspection v8.7 to an existing IBM Maximo Application Suite Core installation. MVI will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install NVIDIA Graphical Processing Unit (GPU) (10 minutes) Install Maximo Visual Inspection application: Install application (15 minutes) Configure workspace (10 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry Optional environment variables \u00a4 MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_CLASS Defines a custom file storage class for Visual Inspection application. If none provided, then a default storage class will be auto defined accordingly to your cluster's availability i.e ibmc-file-gold for IBM Cloud or azurefiles-premium for Azure clusters. MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_SIZE Defines persistent storage size for Visual Inspection application. If not provided, default is 100Gi . MAS_APP_SETTINGS_VISUALINSPECTION_OBJECT_STORAGE_ENABLED If set to true , enables Object Storage integration with Visual Inspection . MAS_APP_SETTINGS_VISUALINSPECTION_OBJECT_STORAGE_WORKSPACE Defines the Object Storage bucket name to be used for Visual Inspection integration. CONFIGURE_COS If set to true , an Object Storage instance will be configured as MAS system scope configuration which will be used for Visual Inspection integration. See cos role documentation for detailed information. CONFIGURE_COS_BUCKET If set to true , an Object Storage bucket will be configured to be used for Visual Inspection application. See cos_bucket role documentation for detailed information. Usage \u00a4 export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_visualinspection Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Add Visual Inspection"},{"location":"playbooks/oneclick-visualinspection/#install-visual-inspection-application","text":"","title":"Install Visual Inspection Application"},{"location":"playbooks/oneclick-visualinspection/#prerequisites","text":"You will need a RedHat OpenShift v4.8 cluster with IBM Maximo Application Suite Core v8.9. The [oneclick-core] (oneclick-core.md) playbook can be used to set this up.","title":"Prerequisites"},{"location":"playbooks/oneclick-visualinspection/#overview","text":"This playbook will add Maximo Visual Inspection v8.7 to an existing IBM Maximo Application Suite Core installation. MVI will be configured to accept automatic security updates and bug fixes, but not new feature releases. This playbook can be ran against any OpenShift cluster regardless of its type; whether it's running in IBM Cloud, Azure, AWS, or your local datacenter. Install dependencies: Install NVIDIA Graphical Processing Unit (GPU) (10 minutes) Install Maximo Visual Inspection application: Install application (15 minutes) Configure workspace (10 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"Overview"},{"location":"playbooks/oneclick-visualinspection/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) IBM_ENTITLEMENT_KEY Your IBM Entitlement key to access the IBM Container Registry","title":"Required environment variables"},{"location":"playbooks/oneclick-visualinspection/#optional-environment-variables","text":"MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_CLASS Defines a custom file storage class for Visual Inspection application. If none provided, then a default storage class will be auto defined accordingly to your cluster's availability i.e ibmc-file-gold for IBM Cloud or azurefiles-premium for Azure clusters. MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_SIZE Defines persistent storage size for Visual Inspection application. If not provided, default is 100Gi . MAS_APP_SETTINGS_VISUALINSPECTION_OBJECT_STORAGE_ENABLED If set to true , enables Object Storage integration with Visual Inspection . MAS_APP_SETTINGS_VISUALINSPECTION_OBJECT_STORAGE_WORKSPACE Defines the Object Storage bucket name to be used for Visual Inspection integration. CONFIGURE_COS If set to true , an Object Storage instance will be configured as MAS system scope configuration which will be used for Visual Inspection integration. See cos role documentation for detailed information. CONFIGURE_COS_BUCKET If set to true , an Object Storage bucket will be configured to be used for Visual Inspection application. See cos_bucket role documentation for detailed information.","title":"Optional environment variables"},{"location":"playbooks/oneclick-visualinspection/#usage","text":"export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=/home/david/masconfig export IBM_ENTITLEMENT_KEY=xxx oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.oneclick_add_visualinspection Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Usage"},{"location":"playbooks/uninstall-core/","text":"Uninstall for MAS Core \u00a4 This playbook will remove MAS Core Platform and its dependencies from your cluster. If you have installed any MAS applications you should uninstall them first. This playbook will effectively undo everything done in the oneclick-core playbook. The following will be removed from the cluster. - MAS Core Platform for the specified instance ID - IBM Suite Licensing Service - MongoDb - IBM User Data Services - IBM Certificate Manager - IBM Cloud Pak Foundational Services - IBM Maximo Operator Catalog - Cluster Monitoring (including Grafana) When using this playbook be sure that nothing else in your cluster is using any of the dependencies that will be removed. If you wish to skip the removal of one or more dependencies use the optional environment variables documented below to control exactly what is uninstalled. Warning This playbook will try to gracefully uninstall a target MAS instance by removing MAS related resources in the expected order. Therefore, make sure your MAS operator is up and running in a healthy state, and no errors are reported in your Suite custom resource. If there are any error reported in the logs of your MAS operator pod, or in your Suite custom resource, the uninstall process might get stuck and not complete successfully. In this case, you will need to debug and fix what is preventing your MAS operator to properly process the uninstall, or ultimately, forcibly uninstall MAS instance. For more information regarding MAS uninstall process, refer to Uninstalling Maximo Application Suite documentation. Usage \u00a4 Required environment variables \u00a4 MAS_INSTANCE_ID Declare the instance ID of MAS to remove Optional environment variables \u00a4 Any of these environment variables can be set to none to skip the uninstall of the associated dependency, this can be useful if you have multiple MAS instances installed on a single cluster for example. GRAFANA_ACTION SLS_ACTION MONGODB_ACTION UDS_ACTION CERT_MANAGER_ACTION COMMON_SERVICES_ACTION IBM_CATALOGS_ACTION Warning Although you could set the actions to install and run the playbook it is strongly recommended not to as it runs through the dependencies in the reverse order that they need to be installed in; use the oneclick-core playbook to repair a MAS Core installation. mas_wipe_mongo_data \u00a4 Defines whether Mongo databases should be deleted along with MAS uninstall Environment Variable: MAS_WIPE_MONGO_DATA Default: False Example \u00a4 export MAS_INSTANCE_ID=inst1 oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.uninstall_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Uninstall Core"},{"location":"playbooks/uninstall-core/#uninstall-for-mas-core","text":"This playbook will remove MAS Core Platform and its dependencies from your cluster. If you have installed any MAS applications you should uninstall them first. This playbook will effectively undo everything done in the oneclick-core playbook. The following will be removed from the cluster. - MAS Core Platform for the specified instance ID - IBM Suite Licensing Service - MongoDb - IBM User Data Services - IBM Certificate Manager - IBM Cloud Pak Foundational Services - IBM Maximo Operator Catalog - Cluster Monitoring (including Grafana) When using this playbook be sure that nothing else in your cluster is using any of the dependencies that will be removed. If you wish to skip the removal of one or more dependencies use the optional environment variables documented below to control exactly what is uninstalled. Warning This playbook will try to gracefully uninstall a target MAS instance by removing MAS related resources in the expected order. Therefore, make sure your MAS operator is up and running in a healthy state, and no errors are reported in your Suite custom resource. If there are any error reported in the logs of your MAS operator pod, or in your Suite custom resource, the uninstall process might get stuck and not complete successfully. In this case, you will need to debug and fix what is preventing your MAS operator to properly process the uninstall, or ultimately, forcibly uninstall MAS instance. For more information regarding MAS uninstall process, refer to Uninstalling Maximo Application Suite documentation.","title":"Uninstall for MAS Core"},{"location":"playbooks/uninstall-core/#usage","text":"","title":"Usage"},{"location":"playbooks/uninstall-core/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID of MAS to remove","title":"Required environment variables"},{"location":"playbooks/uninstall-core/#optional-environment-variables","text":"Any of these environment variables can be set to none to skip the uninstall of the associated dependency, this can be useful if you have multiple MAS instances installed on a single cluster for example. GRAFANA_ACTION SLS_ACTION MONGODB_ACTION UDS_ACTION CERT_MANAGER_ACTION COMMON_SERVICES_ACTION IBM_CATALOGS_ACTION Warning Although you could set the actions to install and run the playbook it is strongly recommended not to as it runs through the dependencies in the reverse order that they need to be installed in; use the oneclick-core playbook to repair a MAS Core installation.","title":"Optional environment variables"},{"location":"playbooks/uninstall-core/#mas_wipe_mongo_data","text":"Defines whether Mongo databases should be deleted along with MAS uninstall Environment Variable: MAS_WIPE_MONGO_DATA Default: False","title":"mas_wipe_mongo_data"},{"location":"playbooks/uninstall-core/#example","text":"export MAS_INSTANCE_ID=inst1 oc login --token=xxxx --server=https://myocpserver ansible-playbook ibm.mas_devops.uninstall_core Tip If you do not want to set up all the dependencies on your local system, you can run the install inside our docker image as well: docker run -ti --pull always quay.io/ibmmas/cli","title":"Example"},{"location":"roles/aibroker/","text":"AI Broker \u00a4 ===== This role provides support to install and configure AI Broker: Install AI Broker api application Create, delete AI Broker tenant Create, delete AI Broker API Key Create, delete AWS S3 API Key Create, delete WatsonX AI API Key Role Variables \u00a4 tenant_action \u00a4 Action to be performed by AI Broker role. Valid values are install or remove . Environment Variable: TENANT_ACTION Default Value: install tenantName \u00a4 The tenant name for AI Broker role. Environment Variable: MAS_AIBROKER_TENANT_NAME Default Value: user app_domain \u00a4 The application domain for AI Broker role. Valid values is domain string apps.domain Environment Variable: APP_DOMAIN Default Value: `` mas_aibroker_s3_action \u00a4 Action to be performed by AI Broker role. Valid values are install or remove Environment Variable: MAS_AIBROKER_S3_ACTION Default Value: install mas_aibroker_storage_host \u00a4 The storge host for AI Broker role. Environment Variable: MAS_AIBROKER_STORAGE_HOST Default Value: `` mas_aibroker_storage_accesskey \u00a4 The storage accesskey for AI Broker role. Environment Variable: MAS_AIBROKER_STORAGE_ACCESSKEY Default Value: `` mas_aibroker_storage_secretkey \u00a4 The storage secretkey for AI Broker role. Environment Variable: MAS_AIBROKER_STORAGE_SECRETKEY Default Value: `` mas_aibroker_storage_region \u00a4 The storage region for AI Broker role. Environment Variable: MAS_AIBROKER_STORAGE_REGION Default Value: `` mas_aibroker_watsonx_action \u00a4 Action to be performed by AI Broker role. Valid values are install or remove Environment Variable: MAS_AIBROKER_WATSONX_ACTION Default Value: install mas_aibroker_watsonxai_apikey \u00a4 The watsonxai apikey for AI Broker role. Environment Variable: MAS_AIBROKER_WATSONXAI_APIKEY Default Value: `` mas_aibroker_watsonxai_url \u00a4 The watsonxai url for AI Broker role. Environment Variable: MAS_AIBROKER_WATSONXAI_URL Default Value: `` mas_aibroker_watsonxai_project_id \u00a4 The watsonxai project id for AI Broker role. Environment Variable: MAS_AIBROKER_WATSONXAI_PROJECT_ID Default Value: `` License \u00a4 EPL-2.0","title":"AI Broker"},{"location":"roles/aibroker/#ai-broker","text":"===== This role provides support to install and configure AI Broker: Install AI Broker api application Create, delete AI Broker tenant Create, delete AI Broker API Key Create, delete AWS S3 API Key Create, delete WatsonX AI API Key","title":"AI Broker"},{"location":"roles/aibroker/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aibroker/#tenant_action","text":"Action to be performed by AI Broker role. Valid values are install or remove . Environment Variable: TENANT_ACTION Default Value: install","title":"tenant_action"},{"location":"roles/aibroker/#tenantname","text":"The tenant name for AI Broker role. Environment Variable: MAS_AIBROKER_TENANT_NAME Default Value: user","title":"tenantName"},{"location":"roles/aibroker/#app_domain","text":"The application domain for AI Broker role. Valid values is domain string apps.domain Environment Variable: APP_DOMAIN Default Value: ``","title":"app_domain"},{"location":"roles/aibroker/#mas_aibroker_s3_action","text":"Action to be performed by AI Broker role. Valid values are install or remove Environment Variable: MAS_AIBROKER_S3_ACTION Default Value: install","title":"mas_aibroker_s3_action"},{"location":"roles/aibroker/#mas_aibroker_storage_host","text":"The storge host for AI Broker role. Environment Variable: MAS_AIBROKER_STORAGE_HOST Default Value: ``","title":"mas_aibroker_storage_host"},{"location":"roles/aibroker/#mas_aibroker_storage_accesskey","text":"The storage accesskey for AI Broker role. Environment Variable: MAS_AIBROKER_STORAGE_ACCESSKEY Default Value: ``","title":"mas_aibroker_storage_accesskey"},{"location":"roles/aibroker/#mas_aibroker_storage_secretkey","text":"The storage secretkey for AI Broker role. Environment Variable: MAS_AIBROKER_STORAGE_SECRETKEY Default Value: ``","title":"mas_aibroker_storage_secretkey"},{"location":"roles/aibroker/#mas_aibroker_storage_region","text":"The storage region for AI Broker role. Environment Variable: MAS_AIBROKER_STORAGE_REGION Default Value: ``","title":"mas_aibroker_storage_region"},{"location":"roles/aibroker/#mas_aibroker_watsonx_action","text":"Action to be performed by AI Broker role. Valid values are install or remove Environment Variable: MAS_AIBROKER_WATSONX_ACTION Default Value: install","title":"mas_aibroker_watsonx_action"},{"location":"roles/aibroker/#mas_aibroker_watsonxai_apikey","text":"The watsonxai apikey for AI Broker role. Environment Variable: MAS_AIBROKER_WATSONXAI_APIKEY Default Value: ``","title":"mas_aibroker_watsonxai_apikey"},{"location":"roles/aibroker/#mas_aibroker_watsonxai_url","text":"The watsonxai url for AI Broker role. Environment Variable: MAS_AIBROKER_WATSONXAI_URL Default Value: ``","title":"mas_aibroker_watsonxai_url"},{"location":"roles/aibroker/#mas_aibroker_watsonxai_project_id","text":"The watsonxai project id for AI Broker role. Environment Variable: MAS_AIBROKER_WATSONXAI_PROJECT_ID Default Value: ``","title":"mas_aibroker_watsonxai_project_id"},{"location":"roles/aibroker/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ansible_version_check/","text":"ansible_version_check \u00a4 Internal-use role that all other roles in the collection declare a dependency upon to ensure that the minimum supported level of Ansible is used. License \u00a4 EPL-2.0","title":"ansible_version_check"},{"location":"roles/ansible_version_check/#ansible_version_check","text":"Internal-use role that all other roles in the collection declare a dependency upon to ensure that the minimum supported level of Ansible is used.","title":"ansible_version_check"},{"location":"roles/ansible_version_check/#license","text":"EPL-2.0","title":"License"},{"location":"roles/appconnect/","text":"appconnect \u00a4 Installs IBM AppConnect and generates configuration that can be directly applied to IBM Maximo Application Suite. This dependency is required by the Health and Predict Utilities application: HP Utilities v8.4 requires support for 12.0.4.0-r2 AppConnect dashboards (License ID L-APEH-C9NCK6 ) HP Utilities v8.3 requires support for 12.0.2.0-r2 AppConnect dashboards (License ID L-KSBM-C87FU2 ) HP Utilities v8.2 requires support for 12.0.1.0-r2 AppConnect dashboards (license ID L-KSBM-C37J2R ) HP Utilities AppConnect License Dashboard Versions v8.4 v4.1 - v5.2 L-APEH-C9NCK6 12.0.4.0-r1, 12.0.4.0-r2 v8.3 v3.0 - v4.2 L-KSBM-C87FU2 12.0.2.0-r2 v8.2 v1.5 - v3.1 L-KSBM-C37J2R 12.0.1.0-r1, 12.0.1.0-r2 For more information review the licensing reference for IBM App Connect Operator . Important All defaults in this role are currently set for compatability with HP Utilities version 8.4. If you are installing App Connect for use with older release of HP Utilities then you must set the appconnect_channel and appconnect_license_id variables (and it would be sensible to customize appconnect_dashboard_name as well). Role Variables - Installation \u00a4 ibm_entitlement_key \u00a4 Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None appconnect_entitlement_username \u00a4 An IBM entitlement key specific for AppConnect installation, primarily used to override ibm_entitlement_key in development. Optional Environment Variable: APPCONNECT_ENTITLEMENT_USERNAME Default: None appconnect_namespace \u00a4 Defines the targetted cluster namespace/project where AppConnect will be installed. If not provided, default AppConnect namespace will be ibm-app-connect . Optional Environment Variable: APPCONNECT_NAMESPACE Default Value: ibm-app-connect appconnect_channel \u00a4 Subscription channel, this must align with the version of HP Utilities (see table above). Optional Environment Variable: APPCONNECT_CHANNEL Default Value: v6.2 Role Variables - Configuration \u00a4 appconnect_storage_class \u00a4 Storage class where AppConnect will be installed - for IBM Cloud clusters, ibmc-file-gold-gid must be used as per documentation . Required Environment Variable: APPCONNECT_STORAGE_CLASS Default Value: None Note : The App Connect Dashboard requires a file-based storage class with ReadWriteMany (RWX) capability. appconnect_dashboard_name \u00a4 AppConnect dashboard instance name. Defaults to dashboard-12040r2 as a reference to AppConnect Dashboard version 12.0.4.0-r2 that is compatible with the default subscription channel and license ID. Optional Environment Variable: APPCONNECT_DASHBOARD_NAME Default Value: dashboard-12060r1 appconnect_dashboard_version \u00a4 AppConnect dashboard version, this must align with the License ID used. Optional Environment Variable: APPCONNECT_DASHBOARD_VERSION Default Value: 12.0.6.0-r1 appconnect_license_id \u00a4 AppConnect license ID. Optional Environment Variable: APPCONNECT_LICENSE_ID Default Value: L-APEH-CFZE47 Role Variables - MAS Configuration \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the AppConnect configuration will target. If this or mas_config_dir are not set then the role will not generate an AppConnect template. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated AppConnect resource definition. This can be used to manually configure a MAS instance to connect to AppConnect instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate an AppConnect template. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Example Playbooks \u00a4 Install IBM App Connect for the latest release of HP Utilties (v8.4) \u00a4 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxx roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.appconnect Install IBM App Connect for HP Utilties v8.3 \u00a4 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxx appconnect_channel: v4.2 appconnect_license_id: L-KSBM-C87FU2 appconnect_dashboard_name: dashboard-12020r2 roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.appconnect License \u00a4 EPL-2.0","title":"appconnect"},{"location":"roles/appconnect/#appconnect","text":"Installs IBM AppConnect and generates configuration that can be directly applied to IBM Maximo Application Suite. This dependency is required by the Health and Predict Utilities application: HP Utilities v8.4 requires support for 12.0.4.0-r2 AppConnect dashboards (License ID L-APEH-C9NCK6 ) HP Utilities v8.3 requires support for 12.0.2.0-r2 AppConnect dashboards (License ID L-KSBM-C87FU2 ) HP Utilities v8.2 requires support for 12.0.1.0-r2 AppConnect dashboards (license ID L-KSBM-C37J2R ) HP Utilities AppConnect License Dashboard Versions v8.4 v4.1 - v5.2 L-APEH-C9NCK6 12.0.4.0-r1, 12.0.4.0-r2 v8.3 v3.0 - v4.2 L-KSBM-C87FU2 12.0.2.0-r2 v8.2 v1.5 - v3.1 L-KSBM-C37J2R 12.0.1.0-r1, 12.0.1.0-r2 For more information review the licensing reference for IBM App Connect Operator . Important All defaults in this role are currently set for compatability with HP Utilities version 8.4. If you are installing App Connect for use with older release of HP Utilities then you must set the appconnect_channel and appconnect_license_id variables (and it would be sensible to customize appconnect_dashboard_name as well).","title":"appconnect"},{"location":"roles/appconnect/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/appconnect/#ibm_entitlement_key","text":"Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None","title":"ibm_entitlement_key"},{"location":"roles/appconnect/#appconnect_entitlement_username","text":"An IBM entitlement key specific for AppConnect installation, primarily used to override ibm_entitlement_key in development. Optional Environment Variable: APPCONNECT_ENTITLEMENT_USERNAME Default: None","title":"appconnect_entitlement_username"},{"location":"roles/appconnect/#appconnect_namespace","text":"Defines the targetted cluster namespace/project where AppConnect will be installed. If not provided, default AppConnect namespace will be ibm-app-connect . Optional Environment Variable: APPCONNECT_NAMESPACE Default Value: ibm-app-connect","title":"appconnect_namespace"},{"location":"roles/appconnect/#appconnect_channel","text":"Subscription channel, this must align with the version of HP Utilities (see table above). Optional Environment Variable: APPCONNECT_CHANNEL Default Value: v6.2","title":"appconnect_channel"},{"location":"roles/appconnect/#role-variables-configuration","text":"","title":"Role Variables - Configuration"},{"location":"roles/appconnect/#appconnect_storage_class","text":"Storage class where AppConnect will be installed - for IBM Cloud clusters, ibmc-file-gold-gid must be used as per documentation . Required Environment Variable: APPCONNECT_STORAGE_CLASS Default Value: None Note : The App Connect Dashboard requires a file-based storage class with ReadWriteMany (RWX) capability.","title":"appconnect_storage_class"},{"location":"roles/appconnect/#appconnect_dashboard_name","text":"AppConnect dashboard instance name. Defaults to dashboard-12040r2 as a reference to AppConnect Dashboard version 12.0.4.0-r2 that is compatible with the default subscription channel and license ID. Optional Environment Variable: APPCONNECT_DASHBOARD_NAME Default Value: dashboard-12060r1","title":"appconnect_dashboard_name"},{"location":"roles/appconnect/#appconnect_dashboard_version","text":"AppConnect dashboard version, this must align with the License ID used. Optional Environment Variable: APPCONNECT_DASHBOARD_VERSION Default Value: 12.0.6.0-r1","title":"appconnect_dashboard_version"},{"location":"roles/appconnect/#appconnect_license_id","text":"AppConnect license ID. Optional Environment Variable: APPCONNECT_LICENSE_ID Default Value: L-APEH-CFZE47","title":"appconnect_license_id"},{"location":"roles/appconnect/#role-variables-mas-configuration","text":"","title":"Role Variables - MAS Configuration"},{"location":"roles/appconnect/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the AppConnect configuration will target. If this or mas_config_dir are not set then the role will not generate an AppConnect template. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/appconnect/#mas_config_dir","text":"Local directory to save the generated AppConnect resource definition. This can be used to manually configure a MAS instance to connect to AppConnect instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate an AppConnect template. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/appconnect/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/appconnect/#example-playbooks","text":"","title":"Example Playbooks"},{"location":"roles/appconnect/#install-ibm-app-connect-for-the-latest-release-of-hp-utilties-v84","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxx roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.appconnect","title":"Install IBM App Connect for the latest release of HP Utilties (v8.4)"},{"location":"roles/appconnect/#install-ibm-app-connect-for-hp-utilties-v83","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxx appconnect_channel: v4.2 appconnect_license_id: L-KSBM-C87FU2 appconnect_dashboard_name: dashboard-12020r2 roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.appconnect","title":"Install IBM App Connect for HP Utilties v8.3"},{"location":"roles/appconnect/#license","text":"EPL-2.0","title":"License"},{"location":"roles/arcgis/","text":"arcgis \u00a4 Installs IBM Maximo Location Services for Esri . This dependency is an alternative solution if you are planning to leverage geospatial and map features with Maximo Spatial. The biggest benefit of using it is that you could have both IBM Maximo Location Services for Esri and Maximo Spatial deployed and running into the same cluster, which improves significantly your overall networking performance. Note: IBM Maximo Location Services for Esri will make use of MAS cluster issuers while managing internal and public certificates thus, you while using suite_dns to setup cluster issuer and public certificates for your MAS instances, these are automatically reused for your instance of IBM Maximo Location Services for Esri . Deployment details \u00a4 Here are the full deployment details for a default installation, considering number of running pods, cpu/memory and storage utilization: oc get deployments -n mas-$MAS_INSTANCE_ID-arcgis NAME READY UP-TO-DATE AVAILABLE AGE arcgis-enterprise-apps 1/1 1 1 101m arcgis-enterprise-manager 1/1 1 1 114m arcgis-enterprise-portal 1/1 1 1 101m arcgis-enterprise-web-style-app 1/1 1 1 101m arcgis-featureserver-webhook-processor 1/1 1 1 74m arcgis-gpserver-webhook-processor 1/1 1 1 74m arcgis-help 1/1 1 1 114m arcgis-ingress-controller 1/1 1 1 115m arcgis-javascript-api 1/1 1 1 101m arcgis-ki7vnxghb8526ejnxiqcf-mapserver 1/1 1 1 82m arcgis-kphc76hvhto7lzv74xxts-featureserver 1/1 1 1 73m arcgis-kvrl4t01w78hbbyl1fsof-mapserver 1/1 1 1 77m arcgis-private-ingress-controller 2/2 2 2 106m arcgis-rest-administrator-api 1/1 1 1 114m arcgis-rest-services-api 1/1 1 1 97m arcgis-service-lifecycle-manager 1/1 1 1 97m arcgis-system-cachingcontrollers-gpserver 1/1 1 1 89m arcgis-system-cachingcontrollers-gpsyncserver 1/1 1 1 89m arcgis-system-cachingtools-gpserver 1/1 1 1 89m arcgis-system-cachingtools-gpsyncserver 1/1 1 1 89m arcgis-system-featureservicetools-gpserver 1/1 1 1 84m arcgis-system-featureservicetools-gpsyncserver 1/1 1 1 84m arcgis-system-publishingtools-gpserver 1/1 1 1 89m arcgis-system-publishingtools-gpsyncserver 3/3 3 3 89m arcgis-system-reportingtools-gpserver 1/1 1 1 89m arcgis-system-spatialanalysistools-gpserver 1/1 1 1 82m arcgis-system-spatialanalysistools-gpsyncserver 1/1 1 1 82m arcgis-system-synctools-gpserver 1/1 1 1 84m arcgis-system-synctools-gpsyncserver 1/1 1 1 84m arcgis-utilities-geocodingtools-gpserver 1/1 1 1 80m arcgis-utilities-geocodingtools-gpsyncserver 1/1 1 1 80m arcgis-utilities-geometry-geometryserver 1/1 1 1 81m arcgis-utilities-offlinepackaging-gpserver 1/1 1 1 79m arcgis-utilities-offlinepackaging-gpsyncserver 1/1 1 1 79m arcgis-utilities-printingtools-gpserver 1/1 1 1 80m arcgis-utilities-symbols-symbolserver- 1/1 1 1 79m ibm-mas-arcgis-entitymgr-ws 1/1 1 1 118m ibm-mas-arcgis-operator 1/1 1 1 121m Total of 49 running pods. oc adm top pods -n mas-$MAS_INSTANCE_ID-arcgis NAME CPU(cores) MEMORY(bytes) arcgis-enterprise-apps 1m 105Mi arcgis-enterprise-manager 0m 132Mi arcgis-enterprise-portal 1m 144Mi arcgis-enterprise-web-style-app 1m 80Mi arcgis-featureserver-webhook-processor 2m 426Mi arcgis-gpserver-webhook-processor 5m 438Mi arcgis-help 1m 96Mi arcgis-in-memory-store 5m 378Mi arcgis-ingress-controller 2m 122Mi arcgis-javascript-api 0m 12Mi arcgis-ki7vnxghb8526ejnxiqcf-mapserver 8m 1102Mi arcgis-kphc76hvhto7lzv74xxts-featureserver 4m 2250Mi arcgis-kvrl4t01w78hbbyl1fsof-mapserver 8m 934Mi arcgis-object-store 29m 3355Mi arcgis-private-ingress-controller 4m 114Mi arcgis-private-ingress-controller 2m 113Mi arcgis-queue-store-cgatl-0 16m 186Mi arcgis-relational-store-pfxpx-mcap-0 14m 894Mi arcgis-relational-store-pfxpx-yjnr-0 4m 687Mi arcgis-rest-administrator-api 26m 706Mi arcgis-rest-metrics-api-nmbtw-0 4m 62Mi arcgis-rest-portal-api-rpcnv-0 4m 678Mi arcgis-rest-services-api 24m 876Mi arcgis-service-lifecycle-manager 6m 744Mi arcgis-spatiotemporal-index-store-dejcm-coordinator-0 4m 3612Mi arcgis-system-cachingcontrollers-gpserver 12m 919Mi arcgis-system-cachingcontrollers-gpsyncserver 2m 1179Mi arcgis-system-cachingtools-gpserver 7m 906Mi arcgis-system-cachingtools-gpsyncserver 1m 1273Mi arcgis-system-featureservicetools-gpserver 7m 985Mi arcgis-system-featureservicetools-gpsyncserver 6m 977Mi arcgis-system-publishingtools-gpserver 13m 1043Mi arcgis-system-publishingtools-gpsyncserver 8m 939Mi arcgis-system-publishingtools-gpsyncserver 10m 1189Mi arcgis-system-publishingtools-gpsyncserver 16m 891Mi arcgis-system-reportingtools-gpserver 20m 636Mi arcgis-system-spatialanalysistools-gpserver 8m 982Mi arcgis-system-spatialanalysistools-gpsyncserver 14m 927Mi arcgis-system-synctools-gpserver 11m 1093Mi arcgis-system-synctools-gpsyncserver 23m 960Mi arcgis-utilities-geocodingtools-gpserver 24m 959Mi arcgis-utilities-geocodingtools-gpsyncserver 10m 1189Mi arcgis-utilities-geometry-geometryserver 6m 1053Mi arcgis-utilities-offlinepackaging-gpserver 19m 983Mi arcgis-utilities-offlinepackaging-gpsyncserver 6m 914Mi arcgis-utilities-printingtools-gpserver 21m 1323Mi arcgis-utilities-symbols-symbolserver 9m 580Mi ibm-mas-arcgis-entitymgr-ws 1222m 201Mi ibm-mas-arcgis-operator 0m 48Mi Average of 1650 milicores (1.65 vCPUs) and 40 gigabytes of memory RAM. oc get pvc -n mas-$MAS_INSTANCE_ID-arcgis NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE arcgis-in-memory-store-feiz3-0-data-volume Bound pvc-432e190c-dbe4-44d5-828b-bf8126a326fe 20Gi RWO ibmc-block-gold 108m arcgis-rest-portal-api-rpcnv-0-portal-sharing-volume Bound pvc-a1201747-6b7e-42b8-beed-c842c21cfa01 20Gi RWO ibmc-block-gold 62m data-volume-arcgis-object-store-o0vq5-awsrx-0 Bound pvc-6c97716f-20d4-46b2-a110-706546dda95d 32Gi RWO ibmc-block-gold 108m data-volume-arcgis-relational-store-pfxpx-mcap-0 Bound pvc-28635771-0e4a-475d-b6aa-2012c48dd470 20Gi RWO ibmc-block-gold 111m data-volume-arcgis-relational-store-pfxpx-yjnr-0 Bound pvc-0e2dbc22-11af-4c5c-b5e7-b00d21a060fe 20Gi RWO ibmc-block-gold 100m data-volume-arcgis-spatiotemporal-index-store-dejcm-coordinator-0 Bound pvc-2e0ca0f0-c830-4353-abf0-196ce0e75b87 20Gi RWO ibmc-block-gold 108m prometheus-volume-arcgis-rest-metrics-api-nmbtw-0 Bound pvc-fbc5c0ce-26eb-441f-8161-2191fd113a80 30Gi RWO ibmc-block-gold 108m queue-data-volume-arcgis-queue-store-cgatl-0 Bound pvc-93451297-0e3d-4a56-bf4e-cff9bda43fb7 20Gi RWO ibmc-block-gold 108m Average of 182 gigabyes of required capacity. Role Variables - Installation \u00a4 ibm_entitlement_key \u00a4 Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None mas_catalog_source \u00a4 Defines the catalog to be used to install MAS. You can set it to ibm-operator-catalog for both release as well as for development install Optional Environment Variable: MAS_CATALOG_SOURCE Default Value: ibm-operator-catalog mas_arcgis_channel \u00a4 Subscription channel for IBM Maximo Location Services for Esri. Optional Environment Variable: MAS_ARCGIS_CHANNEL Default Value: 9.0.x Role Variables - MAS Configuration \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the AppConnect configuration will target. If this or mas_config_dir are not set then the role will not generate an AppConnect template. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Example Playbooks \u00a4 Install IBM Maximo Location Services for Esri \u00a4 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxx roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.arcgis License \u00a4 EPL-2.0","title":"arcgis"},{"location":"roles/arcgis/#arcgis","text":"Installs IBM Maximo Location Services for Esri . This dependency is an alternative solution if you are planning to leverage geospatial and map features with Maximo Spatial. The biggest benefit of using it is that you could have both IBM Maximo Location Services for Esri and Maximo Spatial deployed and running into the same cluster, which improves significantly your overall networking performance. Note: IBM Maximo Location Services for Esri will make use of MAS cluster issuers while managing internal and public certificates thus, you while using suite_dns to setup cluster issuer and public certificates for your MAS instances, these are automatically reused for your instance of IBM Maximo Location Services for Esri .","title":"arcgis"},{"location":"roles/arcgis/#deployment-details","text":"Here are the full deployment details for a default installation, considering number of running pods, cpu/memory and storage utilization: oc get deployments -n mas-$MAS_INSTANCE_ID-arcgis NAME READY UP-TO-DATE AVAILABLE AGE arcgis-enterprise-apps 1/1 1 1 101m arcgis-enterprise-manager 1/1 1 1 114m arcgis-enterprise-portal 1/1 1 1 101m arcgis-enterprise-web-style-app 1/1 1 1 101m arcgis-featureserver-webhook-processor 1/1 1 1 74m arcgis-gpserver-webhook-processor 1/1 1 1 74m arcgis-help 1/1 1 1 114m arcgis-ingress-controller 1/1 1 1 115m arcgis-javascript-api 1/1 1 1 101m arcgis-ki7vnxghb8526ejnxiqcf-mapserver 1/1 1 1 82m arcgis-kphc76hvhto7lzv74xxts-featureserver 1/1 1 1 73m arcgis-kvrl4t01w78hbbyl1fsof-mapserver 1/1 1 1 77m arcgis-private-ingress-controller 2/2 2 2 106m arcgis-rest-administrator-api 1/1 1 1 114m arcgis-rest-services-api 1/1 1 1 97m arcgis-service-lifecycle-manager 1/1 1 1 97m arcgis-system-cachingcontrollers-gpserver 1/1 1 1 89m arcgis-system-cachingcontrollers-gpsyncserver 1/1 1 1 89m arcgis-system-cachingtools-gpserver 1/1 1 1 89m arcgis-system-cachingtools-gpsyncserver 1/1 1 1 89m arcgis-system-featureservicetools-gpserver 1/1 1 1 84m arcgis-system-featureservicetools-gpsyncserver 1/1 1 1 84m arcgis-system-publishingtools-gpserver 1/1 1 1 89m arcgis-system-publishingtools-gpsyncserver 3/3 3 3 89m arcgis-system-reportingtools-gpserver 1/1 1 1 89m arcgis-system-spatialanalysistools-gpserver 1/1 1 1 82m arcgis-system-spatialanalysistools-gpsyncserver 1/1 1 1 82m arcgis-system-synctools-gpserver 1/1 1 1 84m arcgis-system-synctools-gpsyncserver 1/1 1 1 84m arcgis-utilities-geocodingtools-gpserver 1/1 1 1 80m arcgis-utilities-geocodingtools-gpsyncserver 1/1 1 1 80m arcgis-utilities-geometry-geometryserver 1/1 1 1 81m arcgis-utilities-offlinepackaging-gpserver 1/1 1 1 79m arcgis-utilities-offlinepackaging-gpsyncserver 1/1 1 1 79m arcgis-utilities-printingtools-gpserver 1/1 1 1 80m arcgis-utilities-symbols-symbolserver- 1/1 1 1 79m ibm-mas-arcgis-entitymgr-ws 1/1 1 1 118m ibm-mas-arcgis-operator 1/1 1 1 121m Total of 49 running pods. oc adm top pods -n mas-$MAS_INSTANCE_ID-arcgis NAME CPU(cores) MEMORY(bytes) arcgis-enterprise-apps 1m 105Mi arcgis-enterprise-manager 0m 132Mi arcgis-enterprise-portal 1m 144Mi arcgis-enterprise-web-style-app 1m 80Mi arcgis-featureserver-webhook-processor 2m 426Mi arcgis-gpserver-webhook-processor 5m 438Mi arcgis-help 1m 96Mi arcgis-in-memory-store 5m 378Mi arcgis-ingress-controller 2m 122Mi arcgis-javascript-api 0m 12Mi arcgis-ki7vnxghb8526ejnxiqcf-mapserver 8m 1102Mi arcgis-kphc76hvhto7lzv74xxts-featureserver 4m 2250Mi arcgis-kvrl4t01w78hbbyl1fsof-mapserver 8m 934Mi arcgis-object-store 29m 3355Mi arcgis-private-ingress-controller 4m 114Mi arcgis-private-ingress-controller 2m 113Mi arcgis-queue-store-cgatl-0 16m 186Mi arcgis-relational-store-pfxpx-mcap-0 14m 894Mi arcgis-relational-store-pfxpx-yjnr-0 4m 687Mi arcgis-rest-administrator-api 26m 706Mi arcgis-rest-metrics-api-nmbtw-0 4m 62Mi arcgis-rest-portal-api-rpcnv-0 4m 678Mi arcgis-rest-services-api 24m 876Mi arcgis-service-lifecycle-manager 6m 744Mi arcgis-spatiotemporal-index-store-dejcm-coordinator-0 4m 3612Mi arcgis-system-cachingcontrollers-gpserver 12m 919Mi arcgis-system-cachingcontrollers-gpsyncserver 2m 1179Mi arcgis-system-cachingtools-gpserver 7m 906Mi arcgis-system-cachingtools-gpsyncserver 1m 1273Mi arcgis-system-featureservicetools-gpserver 7m 985Mi arcgis-system-featureservicetools-gpsyncserver 6m 977Mi arcgis-system-publishingtools-gpserver 13m 1043Mi arcgis-system-publishingtools-gpsyncserver 8m 939Mi arcgis-system-publishingtools-gpsyncserver 10m 1189Mi arcgis-system-publishingtools-gpsyncserver 16m 891Mi arcgis-system-reportingtools-gpserver 20m 636Mi arcgis-system-spatialanalysistools-gpserver 8m 982Mi arcgis-system-spatialanalysistools-gpsyncserver 14m 927Mi arcgis-system-synctools-gpserver 11m 1093Mi arcgis-system-synctools-gpsyncserver 23m 960Mi arcgis-utilities-geocodingtools-gpserver 24m 959Mi arcgis-utilities-geocodingtools-gpsyncserver 10m 1189Mi arcgis-utilities-geometry-geometryserver 6m 1053Mi arcgis-utilities-offlinepackaging-gpserver 19m 983Mi arcgis-utilities-offlinepackaging-gpsyncserver 6m 914Mi arcgis-utilities-printingtools-gpserver 21m 1323Mi arcgis-utilities-symbols-symbolserver 9m 580Mi ibm-mas-arcgis-entitymgr-ws 1222m 201Mi ibm-mas-arcgis-operator 0m 48Mi Average of 1650 milicores (1.65 vCPUs) and 40 gigabytes of memory RAM. oc get pvc -n mas-$MAS_INSTANCE_ID-arcgis NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE arcgis-in-memory-store-feiz3-0-data-volume Bound pvc-432e190c-dbe4-44d5-828b-bf8126a326fe 20Gi RWO ibmc-block-gold 108m arcgis-rest-portal-api-rpcnv-0-portal-sharing-volume Bound pvc-a1201747-6b7e-42b8-beed-c842c21cfa01 20Gi RWO ibmc-block-gold 62m data-volume-arcgis-object-store-o0vq5-awsrx-0 Bound pvc-6c97716f-20d4-46b2-a110-706546dda95d 32Gi RWO ibmc-block-gold 108m data-volume-arcgis-relational-store-pfxpx-mcap-0 Bound pvc-28635771-0e4a-475d-b6aa-2012c48dd470 20Gi RWO ibmc-block-gold 111m data-volume-arcgis-relational-store-pfxpx-yjnr-0 Bound pvc-0e2dbc22-11af-4c5c-b5e7-b00d21a060fe 20Gi RWO ibmc-block-gold 100m data-volume-arcgis-spatiotemporal-index-store-dejcm-coordinator-0 Bound pvc-2e0ca0f0-c830-4353-abf0-196ce0e75b87 20Gi RWO ibmc-block-gold 108m prometheus-volume-arcgis-rest-metrics-api-nmbtw-0 Bound pvc-fbc5c0ce-26eb-441f-8161-2191fd113a80 30Gi RWO ibmc-block-gold 108m queue-data-volume-arcgis-queue-store-cgatl-0 Bound pvc-93451297-0e3d-4a56-bf4e-cff9bda43fb7 20Gi RWO ibmc-block-gold 108m Average of 182 gigabyes of required capacity.","title":"Deployment details"},{"location":"roles/arcgis/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/arcgis/#ibm_entitlement_key","text":"Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None","title":"ibm_entitlement_key"},{"location":"roles/arcgis/#mas_catalog_source","text":"Defines the catalog to be used to install MAS. You can set it to ibm-operator-catalog for both release as well as for development install Optional Environment Variable: MAS_CATALOG_SOURCE Default Value: ibm-operator-catalog","title":"mas_catalog_source"},{"location":"roles/arcgis/#mas_arcgis_channel","text":"Subscription channel for IBM Maximo Location Services for Esri. Optional Environment Variable: MAS_ARCGIS_CHANNEL Default Value: 9.0.x","title":"mas_arcgis_channel"},{"location":"roles/arcgis/#role-variables-mas-configuration","text":"","title":"Role Variables - MAS Configuration"},{"location":"roles/arcgis/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the AppConnect configuration will target. If this or mas_config_dir are not set then the role will not generate an AppConnect template. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/arcgis/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/arcgis/#example-playbooks","text":"","title":"Example Playbooks"},{"location":"roles/arcgis/#install-ibm-maximo-location-services-for-esri","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxx roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.arcgis","title":"Install IBM Maximo Location Services for Esri"},{"location":"roles/arcgis/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_bucket_access_point/","text":"aws_bucket_access_point \u00a4 This role will create an access point and associates it with the specified s3/aws bucket in the targeted AWS account. Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Role Variables \u00a4 aws_access_point_name \u00a4 The name you want to assign to this access point. Required. Environment Variable: AWS_ACCESS_POINT_NAME Default Value: access-point-c1 aws_access_point_bucket_name \u00a4 The name of the bucket that you want to associate this access point with. Required. Environment Variable: COS_BUCKET_NAME Default Value: None aws_access_point_region \u00a4 The region where the bucket is located. Required. Environment Variable: AWS_REGION Default Value: us-east-2 aws_access_point_username \u00a4 The AWS account or username who is allowed access to the actions defined in by the access point policy. By default, the defined aws_access_point_username will have read-only permissions to the bucket objects through the created access point alias. Required. Environment Variable: AWS_ACCESS_POINT_USERNAME Default Value: None Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_access_point_name: \"{{ lookup('env', 'AWS_ACCESS_POINT_NAME') | default('access-point-c1', True) }}\" aws_access_point_bucket_name: \"{{ lookup('env', 'COS_BUCKET_NAME') }}\" aws_access_point_region: \"{{ lookup('env', 'AWS_REGION') | default('us-east-2', True) }}\" aws_access_point_username: \"{{ lookup('env', 'AWS_ACCESS_POINT_USERNAME') }}\" roles: - ibm.mas_devops.aws_bucket_access_point Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_ACCESS_POINT_NAME=my-aws-access-point export COS_BUCKET_NAME=my-aws-bucket export AWS_ACCESS_POINT_USERNAME=my-aws-username ROLE_NAME=aws_bucket_access_point ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aws_bucket_access_point"},{"location":"roles/aws_bucket_access_point/#aws_bucket_access_point","text":"This role will create an access point and associates it with the specified s3/aws bucket in the targeted AWS account.","title":"aws_bucket_access_point"},{"location":"roles/aws_bucket_access_point/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/aws_bucket_access_point/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_bucket_access_point/#aws_access_point_name","text":"The name you want to assign to this access point. Required. Environment Variable: AWS_ACCESS_POINT_NAME Default Value: access-point-c1","title":"aws_access_point_name"},{"location":"roles/aws_bucket_access_point/#aws_access_point_bucket_name","text":"The name of the bucket that you want to associate this access point with. Required. Environment Variable: COS_BUCKET_NAME Default Value: None","title":"aws_access_point_bucket_name"},{"location":"roles/aws_bucket_access_point/#aws_access_point_region","text":"The region where the bucket is located. Required. Environment Variable: AWS_REGION Default Value: us-east-2","title":"aws_access_point_region"},{"location":"roles/aws_bucket_access_point/#aws_access_point_username","text":"The AWS account or username who is allowed access to the actions defined in by the access point policy. By default, the defined aws_access_point_username will have read-only permissions to the bucket objects through the created access point alias. Required. Environment Variable: AWS_ACCESS_POINT_USERNAME Default Value: None","title":"aws_access_point_username"},{"location":"roles/aws_bucket_access_point/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_access_point_name: \"{{ lookup('env', 'AWS_ACCESS_POINT_NAME') | default('access-point-c1', True) }}\" aws_access_point_bucket_name: \"{{ lookup('env', 'COS_BUCKET_NAME') }}\" aws_access_point_region: \"{{ lookup('env', 'AWS_REGION') | default('us-east-2', True) }}\" aws_access_point_username: \"{{ lookup('env', 'AWS_ACCESS_POINT_USERNAME') }}\" roles: - ibm.mas_devops.aws_bucket_access_point","title":"Example Playbook"},{"location":"roles/aws_bucket_access_point/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_ACCESS_POINT_NAME=my-aws-access-point export COS_BUCKET_NAME=my-aws-bucket export AWS_ACCESS_POINT_USERNAME=my-aws-username ROLE_NAME=aws_bucket_access_point ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aws_bucket_access_point/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_documentdb_user/","text":"aws_documentdb_user \u00a4 This role creates a docdb user for MAS instance and saves username and password as k8 Secret in specified config directory Prerequisites \u00a4 To run this role with providers you must have already installed the Mongo Shell . Role variables \u00a4 mas_instance_id \u00a4 Required.The instance ID of Maximo Application Suite required for creating docdb user credentials secret Environment Variable: MAS_INSTANCE_ID docdb_host \u00a4 AWS DocumentDB Instance Host Address, Required if docdb_hosts is not set Environment Variable: DOCDB_HOST docdb_port \u00a4 AWS DocumentDB Port Address, Required if docdb_hosts is not set Environment Variable: DOCDB_PORT docdb_hosts \u00a4 AWS DocumentDB Instance Host Address & Port Address, Required if both docdb_host & docdb_port are not set. docdb_hosts takes precedence if both docdb_hosts and (docdb_host & docdb_port) are set Environment Variable: DOCDB_HOSTS docdb_master_username \u00a4 Required. AWS DocumentDB Master Username Environment Variable: DOCDB_MASTER_USERNAME docdb_master_password \u00a4 Required. AWS DocumentDB Master Password Environment Variable: DOCDB_MASTER_PASSWORD - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 docdb_master_username: test-user docdb_master_password: test-pass-*** docdb_host: test1.aws-01.... docdb_port: 27017 roles: - ibm.mas_devops.aws_documentdb_user - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 docdb_master_username: test-user docdb_master_password: test-pass-*** docdb_hosts: test1.aws-01:27017,test1.aws-02:27017,test1.aws-03:27017 roles: - ibm.mas_devops.aws_documentdb_user","title":"aws_documentdb_user"},{"location":"roles/aws_documentdb_user/#aws_documentdb_user","text":"This role creates a docdb user for MAS instance and saves username and password as k8 Secret in specified config directory","title":"aws_documentdb_user"},{"location":"roles/aws_documentdb_user/#prerequisites","text":"To run this role with providers you must have already installed the Mongo Shell .","title":"Prerequisites"},{"location":"roles/aws_documentdb_user/#role-variables","text":"","title":"Role variables"},{"location":"roles/aws_documentdb_user/#mas_instance_id","text":"Required.The instance ID of Maximo Application Suite required for creating docdb user credentials secret Environment Variable: MAS_INSTANCE_ID","title":"mas_instance_id"},{"location":"roles/aws_documentdb_user/#docdb_host","text":"AWS DocumentDB Instance Host Address, Required if docdb_hosts is not set Environment Variable: DOCDB_HOST","title":"docdb_host"},{"location":"roles/aws_documentdb_user/#docdb_port","text":"AWS DocumentDB Port Address, Required if docdb_hosts is not set Environment Variable: DOCDB_PORT","title":"docdb_port"},{"location":"roles/aws_documentdb_user/#docdb_hosts","text":"AWS DocumentDB Instance Host Address & Port Address, Required if both docdb_host & docdb_port are not set. docdb_hosts takes precedence if both docdb_hosts and (docdb_host & docdb_port) are set Environment Variable: DOCDB_HOSTS","title":"docdb_hosts"},{"location":"roles/aws_documentdb_user/#docdb_master_username","text":"Required. AWS DocumentDB Master Username Environment Variable: DOCDB_MASTER_USERNAME","title":"docdb_master_username"},{"location":"roles/aws_documentdb_user/#docdb_master_password","text":"Required. AWS DocumentDB Master Password Environment Variable: DOCDB_MASTER_PASSWORD - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 docdb_master_username: test-user docdb_master_password: test-pass-*** docdb_host: test1.aws-01.... docdb_port: 27017 roles: - ibm.mas_devops.aws_documentdb_user - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 docdb_master_username: test-user docdb_master_password: test-pass-*** docdb_hosts: test1.aws-01:27017,test1.aws-02:27017,test1.aws-03:27017 roles: - ibm.mas_devops.aws_documentdb_user","title":"docdb_master_password"},{"location":"roles/aws_policy/","text":"aws_policy \u00a4 This role will create an AWS IAM Policy from a JSON file in the targeted AWS account. Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Role Variables \u00a4 aws_policy_name \u00a4 AWS Policy name. Required. Environment Variable: AWS_POLICY_NAME Default Value: None aws_policy_json_file_path_local \u00a4 Local path for the AWS Policy json file. The AWS Policy json file should be structured as the sample found in /files/policy-template-sample.json Required. Environment Variable: AWS_POLICY_JSON_FILE_PATH_LOCAL Default Value: None Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_policy: \"{{ lookup('env', 'AWS_POLICY_NAME') }}\" aws_policy_json_file_path_local: \"{{ lookup('env', 'AWS_POLICY_JSON_FILE_PATH_LOCAL') }}\" roles: - ibm.mas_devops.aws_policy Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_POLICY_NAME=my-aws-policy export AWS_POLICY_JSON_FILE_PATH_LOCAL=/tmp/local/my-aws-policy.json ROLE_NAME=aws_policy ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aws_policy"},{"location":"roles/aws_policy/#aws_policy","text":"This role will create an AWS IAM Policy from a JSON file in the targeted AWS account.","title":"aws_policy"},{"location":"roles/aws_policy/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/aws_policy/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_policy/#aws_policy_name","text":"AWS Policy name. Required. Environment Variable: AWS_POLICY_NAME Default Value: None","title":"aws_policy_name"},{"location":"roles/aws_policy/#aws_policy_json_file_path_local","text":"Local path for the AWS Policy json file. The AWS Policy json file should be structured as the sample found in /files/policy-template-sample.json Required. Environment Variable: AWS_POLICY_JSON_FILE_PATH_LOCAL Default Value: None","title":"aws_policy_json_file_path_local"},{"location":"roles/aws_policy/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_policy: \"{{ lookup('env', 'AWS_POLICY_NAME') }}\" aws_policy_json_file_path_local: \"{{ lookup('env', 'AWS_POLICY_JSON_FILE_PATH_LOCAL') }}\" roles: - ibm.mas_devops.aws_policy","title":"Example Playbook"},{"location":"roles/aws_policy/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_POLICY_NAME=my-aws-policy export AWS_POLICY_JSON_FILE_PATH_LOCAL=/tmp/local/my-aws-policy.json ROLE_NAME=aws_policy ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aws_policy/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_route53/","text":"aws_route53 \u00a4 This role will create an AWS Route53 public hosted zone in the targeted AWS Account. For further details on how to create and configure an AWS Route53 instance, refer to AWS Route53 documentation . Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Role Variables \u00a4 route53_hosted_zone_name \u00a4 AWS Route53 Hosted Zone name. Required. Environment Variable: ROUTE53_HOSTED_ZONE_NAME Default Value: None route53_hosted_zone_region \u00a4 AWS Route53 Hosted Zone region. Required. Environment Variable: ROUTE53_HOSTED_ZONE_REGION Default Value: Same value as defined in AWS_REGION , or if none defined, then us-east-2 is the defaulted region. Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: route53_hosted_zone_name: \"{{ lookup('env', 'ROUTE53_HOSTED_ZONE_NAME') }}\" # mycompany.com route53_hosted_zone_region: \"{{ lookup('env', 'ROUTE53_HOSTED_ZONE_REGION') }}\" # us-east-2 roles: - ibm.mas_devops.aws_route53 Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export ROUTE53_HOSTED_ZONE_NAME=mycompany.com export ROUTE53_HOSTED_ZONE_REGION=us-east-2 ROLE_NAME=aws_route53 ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aws_route53"},{"location":"roles/aws_route53/#aws_route53","text":"This role will create an AWS Route53 public hosted zone in the targeted AWS Account. For further details on how to create and configure an AWS Route53 instance, refer to AWS Route53 documentation .","title":"aws_route53"},{"location":"roles/aws_route53/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/aws_route53/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_route53/#route53_hosted_zone_name","text":"AWS Route53 Hosted Zone name. Required. Environment Variable: ROUTE53_HOSTED_ZONE_NAME Default Value: None","title":"route53_hosted_zone_name"},{"location":"roles/aws_route53/#route53_hosted_zone_region","text":"AWS Route53 Hosted Zone region. Required. Environment Variable: ROUTE53_HOSTED_ZONE_REGION Default Value: Same value as defined in AWS_REGION , or if none defined, then us-east-2 is the defaulted region.","title":"route53_hosted_zone_region"},{"location":"roles/aws_route53/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: route53_hosted_zone_name: \"{{ lookup('env', 'ROUTE53_HOSTED_ZONE_NAME') }}\" # mycompany.com route53_hosted_zone_region: \"{{ lookup('env', 'ROUTE53_HOSTED_ZONE_REGION') }}\" # us-east-2 roles: - ibm.mas_devops.aws_route53","title":"Example Playbook"},{"location":"roles/aws_route53/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export ROUTE53_HOSTED_ZONE_NAME=mycompany.com export ROUTE53_HOSTED_ZONE_REGION=us-east-2 ROLE_NAME=aws_route53 ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aws_route53/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_user_creation/","text":"aws_user_creation \u00a4 This role will create an AWS IAM Username and corresponding IAM Access Key ID and Secret Access Key in the targeted AWS account. Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Role Variables \u00a4 aws_username \u00a4 AWS Username. Required. Environment Variable: AWS_USERNAME Default Value: None aws_username_create_access_key_flag \u00a4 Flag that defines if IAM Access Key ID and Secret Access Key should be created for the AWS Username. If set to False , then only the AWS Username will be created but no IAM Access Key ID and Secret Access Key. Optional Environment Variable: AWS_USERNAME_CREATE_ACCESS_KEY_FLAG Default Value: True . aws_username_access_key_id \u00a4 Defines an existing IAM Access Key ID for your AWS username. If both aws_username_access_key_id and aws_username_secret_access_key are defined, then aws_username_create_access_key_flag will be automatically forced to False , therefore if you want to create new pair of credentials for the username, do not set this property. Optional Environment Variable: AWS_USERNAME_ACCESS_KEY_ID Default Value: None. aws_username_secret_access_key \u00a4 Defines and existing IAM Secret Access Key for your AWS username. If both aws_username_access_key_id and aws_username_secret_access_key are defined, then aws_username_create_access_key_flag will be automatically forced to False , therefore if you want to create new pair of credentials for the username, do not set this property. Optional Environment Variable: AWS_USERNAME_SECRET_ACCESS_KEY Default Value: None. aws_policy_arn \u00a4 If set, then it will attach the corresponding policy to the AWS Username's permissions. Optional Environment Variable: AWS_POLICY_ARN Default Value: None. Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_username: \"{{ lookup('env', 'AWS_USERNAME') }}\" aws_username_create_access_key_flag: \"{{ lookup('env', 'AWS_USERNAME_CREATE_ACCESS_KEY_FLAG') }}\" aws_policy_arn: \"{{ lookup('env', 'AWS_POLICY_ARN') }}\" roles: - ibm.mas_devops.aws_policy Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_USERNAME=my-aws-username export AWS_USERNAME_CREATE_ACCESS_KEY_FLAG=True export AWS_POLICY_ARN=arn:aws:iam::my-id:policy/my-policy-name ROLE_NAME=aws_user_creation ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aws_user_creation"},{"location":"roles/aws_user_creation/#aws_user_creation","text":"This role will create an AWS IAM Username and corresponding IAM Access Key ID and Secret Access Key in the targeted AWS account.","title":"aws_user_creation"},{"location":"roles/aws_user_creation/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/aws_user_creation/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_user_creation/#aws_username","text":"AWS Username. Required. Environment Variable: AWS_USERNAME Default Value: None","title":"aws_username"},{"location":"roles/aws_user_creation/#aws_username_create_access_key_flag","text":"Flag that defines if IAM Access Key ID and Secret Access Key should be created for the AWS Username. If set to False , then only the AWS Username will be created but no IAM Access Key ID and Secret Access Key. Optional Environment Variable: AWS_USERNAME_CREATE_ACCESS_KEY_FLAG Default Value: True .","title":"aws_username_create_access_key_flag"},{"location":"roles/aws_user_creation/#aws_username_access_key_id","text":"Defines an existing IAM Access Key ID for your AWS username. If both aws_username_access_key_id and aws_username_secret_access_key are defined, then aws_username_create_access_key_flag will be automatically forced to False , therefore if you want to create new pair of credentials for the username, do not set this property. Optional Environment Variable: AWS_USERNAME_ACCESS_KEY_ID Default Value: None.","title":"aws_username_access_key_id"},{"location":"roles/aws_user_creation/#aws_username_secret_access_key","text":"Defines and existing IAM Secret Access Key for your AWS username. If both aws_username_access_key_id and aws_username_secret_access_key are defined, then aws_username_create_access_key_flag will be automatically forced to False , therefore if you want to create new pair of credentials for the username, do not set this property. Optional Environment Variable: AWS_USERNAME_SECRET_ACCESS_KEY Default Value: None.","title":"aws_username_secret_access_key"},{"location":"roles/aws_user_creation/#aws_policy_arn","text":"If set, then it will attach the corresponding policy to the AWS Username's permissions. Optional Environment Variable: AWS_POLICY_ARN Default Value: None.","title":"aws_policy_arn"},{"location":"roles/aws_user_creation/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost vars: aws_username: \"{{ lookup('env', 'AWS_USERNAME') }}\" aws_username_create_access_key_flag: \"{{ lookup('env', 'AWS_USERNAME_CREATE_ACCESS_KEY_FLAG') }}\" aws_policy_arn: \"{{ lookup('env', 'AWS_POLICY_ARN') }}\" roles: - ibm.mas_devops.aws_policy","title":"Example Playbook"},{"location":"roles/aws_user_creation/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export AWS_USERNAME=my-aws-username export AWS_USERNAME_CREATE_ACCESS_KEY_FLAG=True export AWS_POLICY_ARN=arn:aws:iam::my-id:policy/my-policy-name ROLE_NAME=aws_user_creation ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aws_user_creation/#license","text":"EPL-2.0","title":"License"},{"location":"roles/aws_vpc/","text":"aws_vpc \u00a4 This role will create VPC with specified CIDR IP Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Role Variables \u00a4 mas_config_dir \u00a4 Generate k8 resources like username and password Secret for created user will be saved in this directory Required Environment Variable: MAS_CONFIG_DIR Default Value: None. aws_region \u00a4 Specify AWS Region where vpc will be created Optional Environment Variable: AWS_REGION Default Value: us-east-1 vpc_action \u00a4 Specify action(provision/deprovision) to performed by the role Optional Environment Variable: VPC_ACTION Default Value: provision vpc_cidr \u00a4 Specify IP Address CIDR range for VPC Required Environment Variable: VPC_CIDR Default Value: None vpc_name \u00a4 Specify Name for VPC Required Environment Variable: VPC_NAME Default Value: None Example Playbook \u00a4 - hosts: localhost vars: mas_config_dir: ~/masconfig vpc_name: test-vpc vpc_cidr: 10.0.0.0/16 vpc_action: provision roles: - ibm.mas_devops.aws_vpc Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export VPC_NAME=test-vpc export VPC_CIDR='10.0.0.0/16' export VPC_ACTION=provision export MAS_CONFIG_DIR=/pathtoconfig ROLE_NAME=aws_vpc ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"aws_vpc"},{"location":"roles/aws_vpc/#aws_vpc","text":"This role will create VPC with specified CIDR IP","title":"aws_vpc"},{"location":"roles/aws_vpc/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/aws_vpc/#role-variables","text":"","title":"Role Variables"},{"location":"roles/aws_vpc/#mas_config_dir","text":"Generate k8 resources like username and password Secret for created user will be saved in this directory Required Environment Variable: MAS_CONFIG_DIR Default Value: None.","title":"mas_config_dir"},{"location":"roles/aws_vpc/#aws_region","text":"Specify AWS Region where vpc will be created Optional Environment Variable: AWS_REGION Default Value: us-east-1","title":"aws_region"},{"location":"roles/aws_vpc/#vpc_action","text":"Specify action(provision/deprovision) to performed by the role Optional Environment Variable: VPC_ACTION Default Value: provision","title":"vpc_action"},{"location":"roles/aws_vpc/#vpc_cidr","text":"Specify IP Address CIDR range for VPC Required Environment Variable: VPC_CIDR Default Value: None","title":"vpc_cidr"},{"location":"roles/aws_vpc/#vpc_name","text":"Specify Name for VPC Required Environment Variable: VPC_NAME Default Value: None","title":"vpc_name"},{"location":"roles/aws_vpc/#example-playbook","text":"- hosts: localhost vars: mas_config_dir: ~/masconfig vpc_name: test-vpc vpc_cidr: 10.0.0.0/16 vpc_action: provision roles: - ibm.mas_devops.aws_vpc","title":"Example Playbook"},{"location":"roles/aws_vpc/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. export VPC_NAME=test-vpc export VPC_CIDR='10.0.0.0/16' export VPC_ACTION=provision export MAS_CONFIG_DIR=/pathtoconfig ROLE_NAME=aws_vpc ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/aws_vpc/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cert_manager/","text":"cert_manager \u00a4 Deploy IBM Certificate Manager Operator or **Red Hat Certificate Manager Operator into the target OCP cluster. IBM Certificate Manager Operator and Operand will be installed into the ibm-common-services namespace Red Hat Certificate Manager Operatos will be installed into the cert-manager-operator namespace and the Operand will be created in the cert-manager namespace. The role supports migrtation from an existing IBM Certificate Manager install to the Red Hat Certificate Manager, and will configure the cluster resources namespace to ibm-common-services in this case to ensure compatibility with all existing ClusterIssuers . Prerequisites \u00a4 IBM Certificate Manager \u00a4 You must have already installed a CatalogSource that contains IBM Certificate Manager and installed the IBM Cloud Pak Foundational Services Operator . These tasks can be achieved using the ibm_catalogs and common_services roles in this collection. Red Hat Certificate Manager \u00a4 You must have already installed the Red Hat Operators CatalogSource. Role Variables \u00a4 cert_manager_action \u00a4 Inform the role whether to perform an install or an uninstall the Certificate Manager service, action can also be set to none to instruct the role to take no action. Optional Environment Variable: CERT_MANAGER_ACTION Default: install cert_manager_provider \u00a4 Choose which flavour of Certificate Manager to install; IBM ( ibm ), or Red Hat ( redhat ) Optional Environment Variable: CERT_MANAGER_PROVIDER Default: redhat Note: Certificate Manager is a cluster-wide dependency, therefore be really careful when uninstalling it as this might be used by several applications and dependencies installed in the cluster. Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. IBM Certificate Manager \u00a4 - hosts: localhost vars: - cert_manager_action: install - cert_manager_provider: ibm roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.common_services - ibm.mas_devops.cert_manager Red Hat Certificate Manager \u00a4 - hosts: localhost vars: - cert_manager_action: install - cert_manager_provider: redhat roles: - ibm.mas_devops.cert_manager Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"cert_manager"},{"location":"roles/cert_manager/#cert_manager","text":"Deploy IBM Certificate Manager Operator or **Red Hat Certificate Manager Operator into the target OCP cluster. IBM Certificate Manager Operator and Operand will be installed into the ibm-common-services namespace Red Hat Certificate Manager Operatos will be installed into the cert-manager-operator namespace and the Operand will be created in the cert-manager namespace. The role supports migrtation from an existing IBM Certificate Manager install to the Red Hat Certificate Manager, and will configure the cluster resources namespace to ibm-common-services in this case to ensure compatibility with all existing ClusterIssuers .","title":"cert_manager"},{"location":"roles/cert_manager/#prerequisites","text":"","title":"Prerequisites"},{"location":"roles/cert_manager/#ibm-certificate-manager","text":"You must have already installed a CatalogSource that contains IBM Certificate Manager and installed the IBM Cloud Pak Foundational Services Operator . These tasks can be achieved using the ibm_catalogs and common_services roles in this collection.","title":"IBM Certificate Manager"},{"location":"roles/cert_manager/#red-hat-certificate-manager","text":"You must have already installed the Red Hat Operators CatalogSource.","title":"Red Hat Certificate Manager"},{"location":"roles/cert_manager/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cert_manager/#cert_manager_action","text":"Inform the role whether to perform an install or an uninstall the Certificate Manager service, action can also be set to none to instruct the role to take no action. Optional Environment Variable: CERT_MANAGER_ACTION Default: install","title":"cert_manager_action"},{"location":"roles/cert_manager/#cert_manager_provider","text":"Choose which flavour of Certificate Manager to install; IBM ( ibm ), or Red Hat ( redhat ) Optional Environment Variable: CERT_MANAGER_PROVIDER Default: redhat Note: Certificate Manager is a cluster-wide dependency, therefore be really careful when uninstalling it as this might be used by several applications and dependencies installed in the cluster.","title":"cert_manager_provider"},{"location":"roles/cert_manager/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks.","title":"Example Playbook"},{"location":"roles/cert_manager/#ibm-certificate-manager_1","text":"- hosts: localhost vars: - cert_manager_action: install - cert_manager_provider: ibm roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.common_services - ibm.mas_devops.cert_manager","title":"IBM Certificate Manager"},{"location":"roles/cert_manager/#red-hat-certificate-manager_1","text":"- hosts: localhost vars: - cert_manager_action: install - cert_manager_provider: redhat roles: - ibm.mas_devops.cert_manager","title":"Red Hat Certificate Manager"},{"location":"roles/cert_manager/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=cert_manager ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/cert_manager/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cis/","text":"cis \u00a4 This role provides support for Configuring IBM Cloud Internet Services.During CIS provisioning it performs four tasks during provisioning in given order: 1. Provision CIS Instance in customer account 2. Add customer domain to customer's CIS Instance 3. Configure Domain settings in customer CIS Instance 4. Add DNS Records of type `NS` for customer's Domain nameservers to Master CIS Account During CIS Instance deprovisioing role will perform following tasks: 1. Delete DNS Record from Master Account 2. Delete Domain from Customer Account 3. Delete Customer CIS Instance Role Variables \u00a4 cis_action \u00a4 Required. Action to be performed by CIS role. Valid values are provision or deprovision Environment Variable: CIS_ACTION Default Value: provision cis_plan \u00a4 Required. The plan type of the service Environment Variable: CIS_PLAN Default Value: standard ibmcloud_apikey \u00a4 Required. Provide your IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY Default Value: None ibmcloud_resourcegroup \u00a4 Provide the name of the resource group which will own the CIS instance. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default master_ibmcloud_api_key \u00a4 Required. Provide IBM Cloud API Key for Account where Master CIS Instance is running. Environment Variable: MASTER_IBMCLOUD_APIKEY Default Value: None master_cis_resource_group \u00a4 Required. Provide the name of the resource group which owns the Master CIS instance. Environment Variable: MASTER_CIS_RESOURCE_GROUP Default Value: manager master_cis_resource_name \u00a4 Required. Master CIS Instance name Environment Variable: MASTER_CIS_RESOURCE_NAME Default Value: {{mas_instance_id}}-cis master_cis_base_domain \u00a4 Required. Domain from Master CIS Instance - Environment Variable: MASTER_CIS_BASE_DOMAIN mas_instance_id \u00a4 Used as suffix string to define CIS Service name. Environment Variable: MAS_INSTANCE_ID Default Value: None cluster_name \u00a4 Used as prefix string to define CIS Service name. Environment Variable: CLUSTER_NAME Default Value: None mas_config_dir \u00a4 Local directory to save the generated CIS Instance details as ConfigMap. Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbook \u00a4 Create CIS Instance alongwith save Instance details in MAS_CONFIG_DIR path as ConfigMap - hosts: localhost any_errors_fatal: true vars: cis_action: provision mas_instance_id: masinst1 mas_config_dir: ~/masconfig ibmcloud_apikey: \"****\" master_ibmcloud_api_key: \"******\" cluster_name: \"test\" roles: - ibm.mas_devops.cis","title":"cis"},{"location":"roles/cis/#cis","text":"This role provides support for Configuring IBM Cloud Internet Services.During CIS provisioning it performs four tasks during provisioning in given order: 1. Provision CIS Instance in customer account 2. Add customer domain to customer's CIS Instance 3. Configure Domain settings in customer CIS Instance 4. Add DNS Records of type `NS` for customer's Domain nameservers to Master CIS Account During CIS Instance deprovisioing role will perform following tasks: 1. Delete DNS Record from Master Account 2. Delete Domain from Customer Account 3. Delete Customer CIS Instance","title":"cis"},{"location":"roles/cis/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cis/#cis_action","text":"Required. Action to be performed by CIS role. Valid values are provision or deprovision Environment Variable: CIS_ACTION Default Value: provision","title":"cis_action"},{"location":"roles/cis/#cis_plan","text":"Required. The plan type of the service Environment Variable: CIS_PLAN Default Value: standard","title":"cis_plan"},{"location":"roles/cis/#ibmcloud_apikey","text":"Required. Provide your IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/cis/#ibmcloud_resourcegroup","text":"Provide the name of the resource group which will own the CIS instance. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/cis/#master_ibmcloud_api_key","text":"Required. Provide IBM Cloud API Key for Account where Master CIS Instance is running. Environment Variable: MASTER_IBMCLOUD_APIKEY Default Value: None","title":"master_ibmcloud_api_key"},{"location":"roles/cis/#master_cis_resource_group","text":"Required. Provide the name of the resource group which owns the Master CIS instance. Environment Variable: MASTER_CIS_RESOURCE_GROUP Default Value: manager","title":"master_cis_resource_group"},{"location":"roles/cis/#master_cis_resource_name","text":"Required. Master CIS Instance name Environment Variable: MASTER_CIS_RESOURCE_NAME Default Value: {{mas_instance_id}}-cis","title":"master_cis_resource_name"},{"location":"roles/cis/#master_cis_base_domain","text":"Required. Domain from Master CIS Instance - Environment Variable: MASTER_CIS_BASE_DOMAIN","title":"master_cis_base_domain"},{"location":"roles/cis/#mas_instance_id","text":"Used as suffix string to define CIS Service name. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cis/#cluster_name","text":"Used as prefix string to define CIS Service name. Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/cis/#mas_config_dir","text":"Local directory to save the generated CIS Instance details as ConfigMap. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/cis/#example-playbook","text":"Create CIS Instance alongwith save Instance details in MAS_CONFIG_DIR path as ConfigMap - hosts: localhost any_errors_fatal: true vars: cis_action: provision mas_instance_id: masinst1 mas_config_dir: ~/masconfig ibmcloud_apikey: \"****\" master_ibmcloud_api_key: \"******\" cluster_name: \"test\" roles: - ibm.mas_devops.cis","title":"Example Playbook"},{"location":"roles/common_services/","text":"common_services \u00a4 This role will install the following operators into the ibm-common-services namespace of the target cluster: IBM Cloud Pak Foundational Services IBM NamespaceScope Operator Operand Deployment Lifecycle Manager Also, an operator group will be created in the namespace if one does not already exist. Prerequisites \u00a4 To run this role successfully you must have already installed a CatalogSource that contains IBM Cloud Pak Foundational Services, this can be achieved using the ibm_catalogs role in this collection. By default a catalog source of ibm-operator-catalog will be expected, but this can be customized using the common_services_catalog_source variable. Role Variables \u00a4 common_services_action \u00a4 Inform the role whether to perform an install, upgrade or uninstall of IBM Cloud Pak Foundational Services. Optional Environment Variable: COMMON_SERVICES_ACTION Default: install common_services_catalog_source \u00a4 Used to override the operator catalog source used when creating the ibm-common-service-operator subscription. Optional Environment Variable: COMMON_SERVICES_CATALOG_SOURCE Default Value: ibm-operator-catalog common_services_channel \u00a4 Used to override the operator catalog source used when creating the ibm-common-service-operator subscription. Optional Environment Variable: COMMON_SERVICES_CHANNEL Default Value: Role will lookup the default channel from the operator's package manifest. Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.common_services Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=common_services ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"common-services"},{"location":"roles/common_services/#common_services","text":"This role will install the following operators into the ibm-common-services namespace of the target cluster: IBM Cloud Pak Foundational Services IBM NamespaceScope Operator Operand Deployment Lifecycle Manager Also, an operator group will be created in the namespace if one does not already exist.","title":"common_services"},{"location":"roles/common_services/#prerequisites","text":"To run this role successfully you must have already installed a CatalogSource that contains IBM Cloud Pak Foundational Services, this can be achieved using the ibm_catalogs role in this collection. By default a catalog source of ibm-operator-catalog will be expected, but this can be customized using the common_services_catalog_source variable.","title":"Prerequisites"},{"location":"roles/common_services/#role-variables","text":"","title":"Role Variables"},{"location":"roles/common_services/#common_services_action","text":"Inform the role whether to perform an install, upgrade or uninstall of IBM Cloud Pak Foundational Services. Optional Environment Variable: COMMON_SERVICES_ACTION Default: install","title":"common_services_action"},{"location":"roles/common_services/#common_services_catalog_source","text":"Used to override the operator catalog source used when creating the ibm-common-service-operator subscription. Optional Environment Variable: COMMON_SERVICES_CATALOG_SOURCE Default Value: ibm-operator-catalog","title":"common_services_catalog_source"},{"location":"roles/common_services/#common_services_channel","text":"Used to override the operator catalog source used when creating the ibm-common-service-operator subscription. Optional Environment Variable: COMMON_SERVICES_CHANNEL Default Value: Role will lookup the default channel from the operator's package manifest.","title":"common_services_channel"},{"location":"roles/common_services/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost roles: - ibm.mas_devops.ibm_catalogs - ibm.mas_devops.common_services","title":"Example Playbook"},{"location":"roles/common_services/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=common_services ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/common_services/#license","text":"EPL-2.0","title":"License"},{"location":"roles/configure_manage_eventstreams/","text":"configure_manage_eventstreams \u00a4 This role configures manage to use IBM Cloud Eventstreams. NOTE This role inserts dummy kafka password during configuration (not the actual one),so user has to follow below guide to configure kafka password manually via Manage Dashboard. Role Variables \u00a4 mas_instance_id \u00a4 Required Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \u00a4 Required Environment Variable: MAS_WORKSPACE_ID Default Value: None ibmcloud_apikey \u00a4 Required Environment Variable: IBMCLOUD_APIKEY Default Value: None ibmcloud_region \u00a4 Optional Environment Variable: IBMCLOUD_REGION Default Value: us-east ibmcloud_resourcegroup \u00a4 IBM Cloud Resource Group Name where the IBM Cloud Eventstreams is provisioned. - Optional - Environment Variable: IBMCLOUD_RESOURCEGROUP - Default Value: Default eventstreams_name \u00a4 IBM Cloud Eventstreams Service Name - Required - Environment Variable: EVENTSTREAMS_NAME - Default Value: None eventstreams_location \u00a4 IBM Cloud Eventstreams Service Location - Optional - Environment Variable: EVENTSTREAMS_LOCATION - Default Value: us-east db2wh_dbname \u00a4 DB2 Database name where configurations will be done for Manage to use IBM Cloud Eventstreams - Optional - Environment Variable: DB2WH_DBNAME - Default Value: BLUDB cpd_meta_namespace \u00a4 Required Environment Variable: CPD_NAMESPACE Default Value: None db2_instance_name \u00a4 Required to build up pod name running db2 Required Environment Variable: DB2_INSTANCE_NAME Default Value: None mas_app_id \u00a4 Optional Environment Variable: MAS_APP_ID Default Value: manage mas_app_ws_fqn \u00a4 Fully Qualified Name for MAS Application - Optional - Environment Variable: MAS_APP_WS_FQN - Default Value: manageworkspaces.apps.mas.ibm.com Example Playbook \u00a4 Configures IBM Cloud Evenstreams service with Manage - hosts: localhost any_errors_fatal: true vars: mas_instance_id: 'test-instance' mas_workspace_id: 'main' ibmcloud_apikey: 'test-api-key' eventstreams_name: 'test-es' cpd_meta_namespace: 'db2u' db2_instance_name: 'test-db2' roles: - ibm.mas_devops.configure_manage_cfg","title":"configure_manage_eventstreams"},{"location":"roles/configure_manage_eventstreams/#configure_manage_eventstreams","text":"This role configures manage to use IBM Cloud Eventstreams. NOTE This role inserts dummy kafka password during configuration (not the actual one),so user has to follow below guide to configure kafka password manually via Manage Dashboard.","title":"configure_manage_eventstreams"},{"location":"roles/configure_manage_eventstreams/#role-variables","text":"","title":"Role Variables"},{"location":"roles/configure_manage_eventstreams/#mas_instance_id","text":"Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/configure_manage_eventstreams/#mas_workspace_id","text":"Required Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/configure_manage_eventstreams/#ibmcloud_apikey","text":"Required Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/configure_manage_eventstreams/#ibmcloud_region","text":"Optional Environment Variable: IBMCLOUD_REGION Default Value: us-east","title":"ibmcloud_region"},{"location":"roles/configure_manage_eventstreams/#ibmcloud_resourcegroup","text":"IBM Cloud Resource Group Name where the IBM Cloud Eventstreams is provisioned. - Optional - Environment Variable: IBMCLOUD_RESOURCEGROUP - Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/configure_manage_eventstreams/#eventstreams_name","text":"IBM Cloud Eventstreams Service Name - Required - Environment Variable: EVENTSTREAMS_NAME - Default Value: None","title":"eventstreams_name"},{"location":"roles/configure_manage_eventstreams/#eventstreams_location","text":"IBM Cloud Eventstreams Service Location - Optional - Environment Variable: EVENTSTREAMS_LOCATION - Default Value: us-east","title":"eventstreams_location"},{"location":"roles/configure_manage_eventstreams/#db2wh_dbname","text":"DB2 Database name where configurations will be done for Manage to use IBM Cloud Eventstreams - Optional - Environment Variable: DB2WH_DBNAME - Default Value: BLUDB","title":"db2wh_dbname"},{"location":"roles/configure_manage_eventstreams/#cpd_meta_namespace","text":"Required Environment Variable: CPD_NAMESPACE Default Value: None","title":"cpd_meta_namespace"},{"location":"roles/configure_manage_eventstreams/#db2_instance_name","text":"Required to build up pod name running db2 Required Environment Variable: DB2_INSTANCE_NAME Default Value: None","title":"db2_instance_name"},{"location":"roles/configure_manage_eventstreams/#mas_app_id","text":"Optional Environment Variable: MAS_APP_ID Default Value: manage","title":"mas_app_id"},{"location":"roles/configure_manage_eventstreams/#mas_app_ws_fqn","text":"Fully Qualified Name for MAS Application - Optional - Environment Variable: MAS_APP_WS_FQN - Default Value: manageworkspaces.apps.mas.ibm.com","title":"mas_app_ws_fqn"},{"location":"roles/configure_manage_eventstreams/#example-playbook","text":"Configures IBM Cloud Evenstreams service with Manage - hosts: localhost any_errors_fatal: true vars: mas_instance_id: 'test-instance' mas_workspace_id: 'main' ibmcloud_apikey: 'test-api-key' eventstreams_name: 'test-es' cpd_meta_namespace: 'db2u' db2_instance_name: 'test-db2' roles: - ibm.mas_devops.configure_manage_cfg","title":"Example Playbook"},{"location":"roles/cos/","text":"cos \u00a4 This role provides support for: Provisioning and Configuring Cloud Object Storage in MAS. It currently supports two providers: In-cluster Ceph Object Storage leveraging OpenShift Container Storage IBM Cloud Object Storage Deprovision Cloud Object Store. It currently supports one provider: IBM Cloud Object Storage Currently this role only supports generating a system-scoped ObjectStorageCfg resource, but the generated file can be modified if you wish to use other scopes. Role Variables - General \u00a4 cos_type \u00a4 Required. Which COS provider to use; can be set to either ibm for IBM Cloud Object Storage or ocs for OpenShift Container Storage Environment Variable: COS_TYPE Default Value: None cos_action \u00a4 Required. Which action you want to run for the COS instance. You can either provision or deprovision a COS instance in your IBM Cloud account. Environment Variable: COS_ACTION Default Value: provision cos_use_hmac \u00a4 Supported values are true and false, this is used when ibm cloud-cos to be setup with hmac encrypted credentials. Environment Variable: COS_USE_HMAC Default: false cluster ingres tls secret name \u00a4 Specify the name of the cluster's ingres tls secret which contains the default router certificate. Optional Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Role Variables - IBM COS \u00a4 cos_instance_name \u00a4 Provide an optional name for the Object Storage instance. This is only used when cos_type is set to ibm for IBM Cloud Object Storage. Environment Variable: COS_INSTANCE_NAME Default Value: Object Storage for MAS , if mas_instance_id is set the MAS instance ID will be appended to this name. cos_location_info \u00a4 Required. The location where the instance available - Environment Variable: COS_LOCATION - Default Value: global cos_plan_type \u00a4 Required (For Provisioning). The plan type of the service - Environment Variable: COS_PLAN - Default Value: standard cos_url \u00a4 Required (For Provisioning). The COS region location url endpoint. Needed to generage a system-scoped ObjectStorageCfg resource configuration file for MAS. - Environment Variable: COS_REGION_LOCATION_URL - Default Value: https://s3.us.cloud-object-storage.appdomain.cloud cos_resource_key_iam_role \u00a4 Provide an optional role when cos service credential is getting created during COS provisioning. - Environment Variable: COS_RESOURCE_KEY_IAM_ROLE - Default Value: Manager cos_apikey \u00a4 Required if cos_type is set to ibm . Provide your less priviledged IBM Cloud API Key for COS only Environment Variable: COS_APIKEY Default Value: ibmcloud_apikey ibmcloud_apikey \u00a4 Required if cos_type is set to ibm . Provide your IBM Cloud API Key that will be used as the default API Key across multiple roles in this collection. Environment Variable: IBMCLOUD_APIKEY Default Value: None cos_resourcegroup \u00a4 Only used when cos_type is set to ibm . Provide the name of the resource group which will own the COS instance. Environment Variable: COS_RESOURCEGROUP Default Value: ibmcloud_resourcegroup ibmcloud_resourcegroup \u00a4 Only used when cos_type is set to ibm . Provide the name of the default resource group used across multiple roles in this collection. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default Role Variables - MAS Configuration \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the ObjectStorageCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated ObjectStorageCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None include_cluster_ingress_cert_chain \u00a4 Optional. When set to True , includes the complete certificates chain in the generated MAS configuration, when a trusted certificate authority is found in your cluster's ingress. Optional Environment Variable: INCLUDE_CLUSTER_INGRESS_CERT_CHAIN Default: False Example Playbook \u00a4 Create the Ceph Object store on the existing OCS cluster and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ocs mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos Create the IBM Cloud Object storage Instance and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm ibmcloud_apikey: <Your IBM Cloud API Key> mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos License \u00a4 EPL-2.0","title":"cos"},{"location":"roles/cos/#cos","text":"This role provides support for: Provisioning and Configuring Cloud Object Storage in MAS. It currently supports two providers: In-cluster Ceph Object Storage leveraging OpenShift Container Storage IBM Cloud Object Storage Deprovision Cloud Object Store. It currently supports one provider: IBM Cloud Object Storage Currently this role only supports generating a system-scoped ObjectStorageCfg resource, but the generated file can be modified if you wish to use other scopes.","title":"cos"},{"location":"roles/cos/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/cos/#cos_type","text":"Required. Which COS provider to use; can be set to either ibm for IBM Cloud Object Storage or ocs for OpenShift Container Storage Environment Variable: COS_TYPE Default Value: None","title":"cos_type"},{"location":"roles/cos/#cos_action","text":"Required. Which action you want to run for the COS instance. You can either provision or deprovision a COS instance in your IBM Cloud account. Environment Variable: COS_ACTION Default Value: provision","title":"cos_action"},{"location":"roles/cos/#cos_use_hmac","text":"Supported values are true and false, this is used when ibm cloud-cos to be setup with hmac encrypted credentials. Environment Variable: COS_USE_HMAC Default: false","title":"cos_use_hmac"},{"location":"roles/cos/#cluster-ingres-tls-secret-name","text":"Specify the name of the cluster's ingres tls secret which contains the default router certificate. Optional Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default","title":"cluster ingres tls secret name"},{"location":"roles/cos/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/cos/#role-variables-ibm-cos","text":"","title":"Role Variables - IBM COS"},{"location":"roles/cos/#cos_instance_name","text":"Provide an optional name for the Object Storage instance. This is only used when cos_type is set to ibm for IBM Cloud Object Storage. Environment Variable: COS_INSTANCE_NAME Default Value: Object Storage for MAS , if mas_instance_id is set the MAS instance ID will be appended to this name.","title":"cos_instance_name"},{"location":"roles/cos/#cos_location_info","text":"Required. The location where the instance available - Environment Variable: COS_LOCATION - Default Value: global","title":"cos_location_info"},{"location":"roles/cos/#cos_plan_type","text":"Required (For Provisioning). The plan type of the service - Environment Variable: COS_PLAN - Default Value: standard","title":"cos_plan_type"},{"location":"roles/cos/#cos_url","text":"Required (For Provisioning). The COS region location url endpoint. Needed to generage a system-scoped ObjectStorageCfg resource configuration file for MAS. - Environment Variable: COS_REGION_LOCATION_URL - Default Value: https://s3.us.cloud-object-storage.appdomain.cloud","title":"cos_url"},{"location":"roles/cos/#cos_resource_key_iam_role","text":"Provide an optional role when cos service credential is getting created during COS provisioning. - Environment Variable: COS_RESOURCE_KEY_IAM_ROLE - Default Value: Manager","title":"cos_resource_key_iam_role"},{"location":"roles/cos/#cos_apikey","text":"Required if cos_type is set to ibm . Provide your less priviledged IBM Cloud API Key for COS only Environment Variable: COS_APIKEY Default Value: ibmcloud_apikey","title":"cos_apikey"},{"location":"roles/cos/#ibmcloud_apikey","text":"Required if cos_type is set to ibm . Provide your IBM Cloud API Key that will be used as the default API Key across multiple roles in this collection. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/cos/#cos_resourcegroup","text":"Only used when cos_type is set to ibm . Provide the name of the resource group which will own the COS instance. Environment Variable: COS_RESOURCEGROUP Default Value: ibmcloud_resourcegroup","title":"cos_resourcegroup"},{"location":"roles/cos/#ibmcloud_resourcegroup","text":"Only used when cos_type is set to ibm . Provide the name of the default resource group used across multiple roles in this collection. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/cos/#role-variables-mas-configuration","text":"","title":"Role Variables - MAS Configuration"},{"location":"roles/cos/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the ObjectStorageCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cos/#mas_config_dir","text":"Local directory to save the generated ObjectStorageCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/cos/#include_cluster_ingress_cert_chain","text":"Optional. When set to True , includes the complete certificates chain in the generated MAS configuration, when a trusted certificate authority is found in your cluster's ingress. Optional Environment Variable: INCLUDE_CLUSTER_INGRESS_CERT_CHAIN Default: False","title":"include_cluster_ingress_cert_chain"},{"location":"roles/cos/#example-playbook","text":"Create the Ceph Object store on the existing OCS cluster and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ocs mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos Create the IBM Cloud Object storage Instance and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm ibmcloud_apikey: <Your IBM Cloud API Key> mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos","title":"Example Playbook"},{"location":"roles/cos/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cos_bucket/","text":"cos_bucket \u00a4 This role extends support to create or deprovision Cloud Object Storage buckets. Role Variables \u00a4 cos_type \u00a4 Required. Which COS provider to use; can be set to either ibm for IBM Cloud Object Storage or aws for S3 bucket types (aws support under development). Environment Variable: COS_TYPE Default Value: None cos_bucket_action \u00a4 Required. Which action you want to run for the COS bucket. You can either create or delete a COS bucket. Environment Variable: COS_BUCKET_ACTION Default Value: create Role Variables - IBM Cloud Object Storage buckets \u00a4 cos_bucket_name \u00a4 Optional name for your IBM Cloud Object Storage bucket. Environment Variable: COS_BUCKET_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID-bucket cos_bucket_storage_class \u00a4 Optional. IBM Cloud Object Storage bucket storage class. Supported options are smart , vault , cold and flex . For more details, see IBM Cloud Object Storage documentation Environment Variable: COS_BUCKET_STORAGE_CLASS Default Value: smart cos_instance_name \u00a4 Provide the Object Storage instance name, will be used to find the targeted COS instance to create/deprovision the buckets. This is only used when cos_type is set to ibm for IBM Cloud Object Storage. Environment Variable: COS_INSTANCE_NAME Default Value: None cos_location_info \u00a4 Required. The location where the COS instance is available Environment Variable: COS_LOCATION Default Value: global cos_bucket_region_location_type \u00a4 Required. This defines the resiliency of your COS bucket. Supported options are cross_region_location (Highest availability) or region_location (Best performance). For more details, see IBM Cloud Object Storage documentation Environment Variable: COS_BUCKET_REGION_LOCATION_TYPE Default Value: cross_region_location cos_bucket_region_location: \"{{ lookup('env', 'COS_BUCKET_REGION_LOCATION') | default(bucket_cross_reg_loc, true) }}\" cos_bucket_region_location \u00a4 Required. This defines the specific region of your COS bucket. For cross_region_location type, the supported regions are us , ap and eu . For region_location type, the supported regions are au-syd , eu-de , eu-gb , jp-tok , us-east , us-south , ca-tor , jp-osa and br-sao . For more details, see IBM Cloud Object Storage documentation ibmcloud_region \u00a4 Optional. For cross region location type buckets, the IBM Cloud region can be used as alternative to determine which cross region location to be used while creating the buckets. - Environment Variable: IBMCLOUD_REGION - Default Value: us-east cos_url \u00a4 Required (For bucket creation). The COS region location url endpoint. Needed to specify the COS bucket region location. - Environment Variable: COS_REGION_LOCATION_URL - Default Value: https://s3.us.cloud-object-storage.appdomain.cloud cos_plan_type \u00a4 Required (For Provisioning). The plan type of the service - Environment Variable: COS_PLAN - Default Value: standard resource_key_iam_role \u00a4 Provide an optional role when cos service credential is getting created during COS bucket creation. - Environment Variable: RESOURCE_KEY_IAM_ROLE - Default Value: Manager ibmcloud_apikey \u00a4 Required if cos_type is set to ibm . Provide your IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY Default Value: None ibmcloud_resourcegroup \u00a4 Only used when cos_type is set to ibm . Provide the name of the resource group which will own the COS instance for the targeted buckets. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default Role Variables - AWS S3 Buckets \u00a4 To run this role successfully for AWS s3 buckets, you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. aws_bucket_name \u00a4 Optional name for your AWS/S3 bucket. Environment Variable: COS_BUCKET_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID-bucket aws_region \u00a4 The region where the bucket is located. Required. Environment Variable: AWS_REGION Default Value: us-east-2 aws_bucket_versioning_flag \u00a4 Flag to define if versioning should be enabled for the bucket Optional. Environment Variable: COS_BUCKET_VERSIONING_FLAG Default Value: True aws_bucket_encryption \u00a4 JSON formatted string to define default encryption configuration for AWS S3 bucket. Optional. Environment Variable: COS_BUCKET_ENCRYPTION Default Value: None aws_bucket_force_deletion_flag \u00a4 Deletes S3 AWS bucket objects prior deleting the S3 bucket. This option only works if versioning is not enabled in the bucket. Note: To delete AWS bucket, cos_bucket_action must be set to delete . Optional. Environment Variable: COS_BUCKET_FORCE_DELETION_FLAG Default Value: True Example Playbook \u00a4 Create the IBM Cloud Object storage bucket. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm cos_bucket_action: create cos_bucket_name: my-ibm-bucket cos_instance_name: my-ibmcos-instance-name ibmcloud_apikey: my-ibm-cloud-apikey roles: - ibm.mas_devops.cos_bucket Create the AWS S3 storage bucket. - hosts: localhost any_errors_fatal: true vars: cos_type: aws cos_bucket_action: create aws_bucket_name: my-aws-bucket aws_region: us-east-2 aws_bucket_versioning_flag: True aws_bucket_encryption: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}' roles: - ibm.mas_devops.cos_bucket License \u00a4 EPL-2.0","title":"cos_bucket"},{"location":"roles/cos_bucket/#cos_bucket","text":"This role extends support to create or deprovision Cloud Object Storage buckets.","title":"cos_bucket"},{"location":"roles/cos_bucket/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cos_bucket/#cos_type","text":"Required. Which COS provider to use; can be set to either ibm for IBM Cloud Object Storage or aws for S3 bucket types (aws support under development). Environment Variable: COS_TYPE Default Value: None","title":"cos_type"},{"location":"roles/cos_bucket/#cos_bucket_action","text":"Required. Which action you want to run for the COS bucket. You can either create or delete a COS bucket. Environment Variable: COS_BUCKET_ACTION Default Value: create","title":"cos_bucket_action"},{"location":"roles/cos_bucket/#role-variables-ibm-cloud-object-storage-buckets","text":"","title":"Role Variables - IBM Cloud Object Storage buckets"},{"location":"roles/cos_bucket/#cos_bucket_name","text":"Optional name for your IBM Cloud Object Storage bucket. Environment Variable: COS_BUCKET_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID-bucket","title":"cos_bucket_name"},{"location":"roles/cos_bucket/#cos_bucket_storage_class","text":"Optional. IBM Cloud Object Storage bucket storage class. Supported options are smart , vault , cold and flex . For more details, see IBM Cloud Object Storage documentation Environment Variable: COS_BUCKET_STORAGE_CLASS Default Value: smart","title":"cos_bucket_storage_class"},{"location":"roles/cos_bucket/#cos_instance_name","text":"Provide the Object Storage instance name, will be used to find the targeted COS instance to create/deprovision the buckets. This is only used when cos_type is set to ibm for IBM Cloud Object Storage. Environment Variable: COS_INSTANCE_NAME Default Value: None","title":"cos_instance_name"},{"location":"roles/cos_bucket/#cos_location_info","text":"Required. The location where the COS instance is available Environment Variable: COS_LOCATION Default Value: global","title":"cos_location_info"},{"location":"roles/cos_bucket/#cos_bucket_region_location_type","text":"Required. This defines the resiliency of your COS bucket. Supported options are cross_region_location (Highest availability) or region_location (Best performance). For more details, see IBM Cloud Object Storage documentation Environment Variable: COS_BUCKET_REGION_LOCATION_TYPE Default Value: cross_region_location cos_bucket_region_location: \"{{ lookup('env', 'COS_BUCKET_REGION_LOCATION') | default(bucket_cross_reg_loc, true) }}\"","title":"cos_bucket_region_location_type"},{"location":"roles/cos_bucket/#cos_bucket_region_location","text":"Required. This defines the specific region of your COS bucket. For cross_region_location type, the supported regions are us , ap and eu . For region_location type, the supported regions are au-syd , eu-de , eu-gb , jp-tok , us-east , us-south , ca-tor , jp-osa and br-sao . For more details, see IBM Cloud Object Storage documentation","title":"cos_bucket_region_location"},{"location":"roles/cos_bucket/#ibmcloud_region","text":"Optional. For cross region location type buckets, the IBM Cloud region can be used as alternative to determine which cross region location to be used while creating the buckets. - Environment Variable: IBMCLOUD_REGION - Default Value: us-east","title":"ibmcloud_region"},{"location":"roles/cos_bucket/#cos_url","text":"Required (For bucket creation). The COS region location url endpoint. Needed to specify the COS bucket region location. - Environment Variable: COS_REGION_LOCATION_URL - Default Value: https://s3.us.cloud-object-storage.appdomain.cloud","title":"cos_url"},{"location":"roles/cos_bucket/#cos_plan_type","text":"Required (For Provisioning). The plan type of the service - Environment Variable: COS_PLAN - Default Value: standard","title":"cos_plan_type"},{"location":"roles/cos_bucket/#resource_key_iam_role","text":"Provide an optional role when cos service credential is getting created during COS bucket creation. - Environment Variable: RESOURCE_KEY_IAM_ROLE - Default Value: Manager","title":"resource_key_iam_role"},{"location":"roles/cos_bucket/#ibmcloud_apikey","text":"Required if cos_type is set to ibm . Provide your IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/cos_bucket/#ibmcloud_resourcegroup","text":"Only used when cos_type is set to ibm . Provide the name of the resource group which will own the COS instance for the targeted buckets. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/cos_bucket/#role-variables-aws-s3-buckets","text":"To run this role successfully for AWS s3 buckets, you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Role Variables - AWS S3 Buckets"},{"location":"roles/cos_bucket/#aws_bucket_name","text":"Optional name for your AWS/S3 bucket. Environment Variable: COS_BUCKET_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID-bucket","title":"aws_bucket_name"},{"location":"roles/cos_bucket/#aws_region","text":"The region where the bucket is located. Required. Environment Variable: AWS_REGION Default Value: us-east-2","title":"aws_region"},{"location":"roles/cos_bucket/#aws_bucket_versioning_flag","text":"Flag to define if versioning should be enabled for the bucket Optional. Environment Variable: COS_BUCKET_VERSIONING_FLAG Default Value: True","title":"aws_bucket_versioning_flag"},{"location":"roles/cos_bucket/#aws_bucket_encryption","text":"JSON formatted string to define default encryption configuration for AWS S3 bucket. Optional. Environment Variable: COS_BUCKET_ENCRYPTION Default Value: None","title":"aws_bucket_encryption"},{"location":"roles/cos_bucket/#aws_bucket_force_deletion_flag","text":"Deletes S3 AWS bucket objects prior deleting the S3 bucket. This option only works if versioning is not enabled in the bucket. Note: To delete AWS bucket, cos_bucket_action must be set to delete . Optional. Environment Variable: COS_BUCKET_FORCE_DELETION_FLAG Default Value: True","title":"aws_bucket_force_deletion_flag"},{"location":"roles/cos_bucket/#example-playbook","text":"Create the IBM Cloud Object storage bucket. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm cos_bucket_action: create cos_bucket_name: my-ibm-bucket cos_instance_name: my-ibmcos-instance-name ibmcloud_apikey: my-ibm-cloud-apikey roles: - ibm.mas_devops.cos_bucket Create the AWS S3 storage bucket. - hosts: localhost any_errors_fatal: true vars: cos_type: aws cos_bucket_action: create aws_bucket_name: my-aws-bucket aws_region: us-east-2 aws_bucket_versioning_flag: True aws_bucket_encryption: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}' roles: - ibm.mas_devops.cos_bucket","title":"Example Playbook"},{"location":"roles/cos_bucket/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d/","text":"cp4d \u00a4 This role installs or upgrades IBM Cloud Pak for Data Operator in the target cluster. Currently supported Cloud Pak for Data release versions are: 4.6.0 4.6.3 4.6.4 4.6.6 4.8.0 5.0.0 The role will automatically install or upgrade (if targeted to an existing CPD deployment) the corresponding Zen version associated to the chosen Cloud Pak for Data release, for example: Cloud Pak for Data release version 4.6.0 installs Zen/Control Plane version 4.8.0 . Cloud Pak for Data release version 4.6.3 installs Zen/Control Plane version 4.8.1 . Cloud Pak for Data release version 4.6.4 installs Zen/Control Plane version 4.8.2 . Cloud Pak for Data release version 4.6.6 installs Zen/Control Plane version 4.8.2 . Cloud Pak for Data release version 4.8.0 installs Zen/Control Plane version 5.1.0 Cloud Pak for Data release version 5.0.0 installs Zen/Control Plane version 6.0.1 For more information about CPD versioning, see IBM Cloud Pak for Data Operator and operand versions 4.9.x or IBM Cloud Pak for Data Operator and operand versions 5.0.x Cloud Pak for Data version mapping to MAS Catalog \u00a4 Introduced with 4.8.x support, users can still choose to install an specific version of Cloud Pak for Data by setting CPD_PRODUCT_VERSION variable. However, by default, now it will possible to install an specific version of Cloud Pak for Data that is compatible with an specific version of MAS catalog (ibm-operator-catalog). If CPD_PRODUCT_VERSION variable is not defined, then the automation will try to find the installed MAS catalog (ibm-operator-catalog) in the target cluster, and lookup the corresponding default Cloud Pak for Data version that is mapped with the retrieved MAS catalog version. If still not able to find the MAS catalog, then by default, the Cloud Pak for Data version will be defined by the version supported by the latest released MAS catalog. Upgrade \u00a4 This role also supports seamlessly CPD control plane (or also called Zen service) minor version upgrades (CPD 4.6.x > CPD 4.8.0 or CPD 4.8.0 > CPD 5.0.0), and patch version upgrades (i.e CPD 4.6.0 -> CPD 4.6.6). All you need to do is to define cpd_product_version variable to the version you target to upgrade and run this role against an existing CPD instance. For more information about IBM Cloud Pak for Data upgrade process, refer to the Cloud Pak for Data official documentation . The role assumes that you have already installed the IBM Operator Catalog and configured IBM Cloud Pak Foundational services (only a must have if installing CPD 4.6.x) in the target cluster. These actions are performed by the ibm_catalogs common_services roles in this collection. Cloud Pak for Data will be configured as a specialized installation Info A specialized installation allows a user with project administrator permissions to install the software after a cluster administrator completes the initial cluster setup. A specialized installation also facilitates strict division between Red Hat OpenShift Container Platform projects (Kubernetes namespaces). In a specialized installation, the IBM Cloud Pak foundational services operators are installed in the ibm-common-services project and the Cloud Pak for Data operators are installed in a separate project (typically cpd-operators). Each project has a dedicated: Operator group, which specifies the OwnNamespace installation mode NamespaceScope Operator, which allows the operators in the project to manage operators and service workloads in specific projects In this way, you can specify different settings for the IBM Cloud Pak foundational services and for the Cloud Pak for Data operators. Cloud Pak for Data deployment details \u00a4 5.0.x version: \u00a4 Cloud Pak for Data 5.0.x leverages Cloud Pak Foundational Services v4, which runs its deployments in isolated/dedicated scope model, that means that its dependencies will be grouped and installed within the Cloud Pak for Data related projects/namespaces. There are only two namespaces that will be used: CPD instance namespace (e.g ibm-cpd ) and CPD operators namespace (e.g ibm-cpd-operators ). In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 17h ibm-common-service-operator 1/1 1 1 17h ibm-namespace-scope-operator 1/1 1 1 17h ibm-zen-operator 1/1 1 1 17h meta-api-deploy 1/1 1 1 17h operand-deployment-lifecycle-manager 1/1 1 1 17h postgresql-operator-controller-manager-1-18-7 1/1 1 1 17h In the ibm-cpd namespace: oc -n ibm-cpd get zenservice,ibmcpd,deployments,sts,pvc NAME VERSION STATUS AGE zenservice.zen.cpd.ibm.com/lite-cr 6.0.1 Completed 17h NAME AGE ibmcpd.cpd.ibm.com/ibmcpd-cr 17h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ibm-mcs-hubwork 1/1 1 1 17h deployment.apps/ibm-mcs-placement 1/1 1 1 17h deployment.apps/ibm-mcs-storage 1/1 1 1 17h deployment.apps/ibm-nginx 3/3 3 3 16h deployment.apps/ibm-nginx-tester 1/1 1 1 16h deployment.apps/usermgmt 3/3 3 3 16h deployment.apps/zen-audit 2/2 2 2 16h deployment.apps/zen-core 3/3 3 3 16h deployment.apps/zen-core-api 3/3 3 3 16h deployment.apps/zen-watchdog 2/2 2 2 16h deployment.apps/zen-watcher 1/1 1 1 16h NAME READY AGE statefulset.apps/zen-minio 3/3 17h NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/export-zen-minio-0 Bound pvc-b2a2a729-13c1-4e7f-b672-0b5efc6aa40a 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/export-zen-minio-1 Bound pvc-7e772a3a-8849-4291-8e14-501f49e79182 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/export-zen-minio-2 Bound pvc-e0dd31dc-916d-4b15-9d9c-351db0a2b47f 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/ibm-cs-postgres-backup Bound pvc-ef788b99-784f-4531-a1b3-12611f112551 20Gi RWO ibmc-block-gold 16h persistentvolumeclaim/ibm-zen-objectstore-backup-pvc Bound pvc-d5e61dcf-65a3-4930-9cbf-ab80d04dda00 20Gi RWO ibmc-block-gold 16h persistentvolumeclaim/zen-metastore-edb-1 Bound pvc-19d44f17-05ab-4dc0-bb5d-1b5f15ffd201 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/zen-metastore-edb-2 Bound pvc-741ea444-b6f0-44ff-a123-bb4615d97381 20Gi RWO ibmc-block-gold 17h Tip You can retrieve the Initial Cloud Pak for Data password from the admin-user-details secret: oc -n ibm-cpd get secret admin-user-details -o jsonpath=\"{.data.initial_admin_password}\" | base64 -d 4.8.x version: \u00a4 Cloud Pak for Data 4.8.x leverages Cloud Pak Foundational Services v4, which runs its deployments in isolated/dedicated scope model, that means that its dependencies will be grouped and installed within the Cloud Pak for Data related projects/namespaces. Differently from CPD 4.6.x, there are only two namespaces that will be used: CPD instance namespace (e.g ibm-cpd ) and CPD operators namespace (e.g ibm-cpd-operators ). In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 39d ibm-common-service-operator 1/1 1 1 39d ibm-commonui-operator 1/1 1 1 39d ibm-iam-operator 1/1 1 1 39d ibm-mongodb-operator 1/1 1 1 39d ibm-namespace-scope-operator 1/1 1 1 39d ibm-zen-operator 1/1 1 1 39d meta-api-deploy 1/1 1 1 39d operand-deployment-lifecycle-manager 1/1 1 1 39d postgresql-operator-controller-manager-1-18-7 1/1 1 1 39d In the ibm-cpd namespace: oc -n ibm-cpd get zenservice,ibmcpd,deployments,sts,pvc NAME STATUS AGE zenservice.zen.cpd.ibm.com/lite-cr Completed 39d NAME AGE ibmcpd.cpd.ibm.com/ibmcpd-cr 39d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/common-web-ui 1/1 1 1 39d deployment.apps/ibm-nginx 2/2 2 2 39d deployment.apps/ibm-nginx-tester 1/1 1 1 39d deployment.apps/platform-auth-service 1/1 1 1 39d deployment.apps/platform-identity-management 1/1 1 1 39d deployment.apps/platform-identity-provider 1/1 1 1 39d deployment.apps/usermgmt 2/2 2 2 39d deployment.apps/zen-audit 1/1 1 1 39d deployment.apps/zen-core 2/2 2 2 39d deployment.apps/zen-core-api 2/2 2 2 39d deployment.apps/zen-watchdog 1/1 1 1 39d deployment.apps/zen-watcher 1/1 1 1 39d NAME READY AGE statefulset.apps/icp-mongodb 3/3 39d statefulset.apps/zen-minio 3/3 39d NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/ibm-zen-cs-mongo-backup Bound pvc-bdba4bb2-dff5-43cb-a4b6-3540955ccb92 20Gi RWO ocs-storagecluster-ceph-rbd 39d persistentvolumeclaim/ibm-zen-objectstore-backup-pvc Bound pvc-46595a1b-2629-4c62-9e16-6b9553635738 20Gi RWO ocs-storagecluster-ceph-rbd 39d persistentvolumeclaim/mongodbdir-icp-mongodb-0 Bound pvc-1d3f7ee5-b2ef-4ca0-8b95-8db79bc88b19 20Gi RWO ocs-storagecluster-ceph-rbd 39d persistentvolumeclaim/mongodbdir-icp-mongodb-1 Bound pvc-9dada920-c6be-40f2-b4e2-56c989935a16 20Gi RWO ocs-storagecluster-ceph-rbd 39d persistentvolumeclaim/mongodbdir-icp-mongodb-2 Bound pvc-27713ee7-4d57-49e2-94ce-6955bbcd74f4 20Gi RWO ocs-storagecluster-ceph-rbd 39d persistentvolumeclaim/zen-metastore-edb-1 Bound pvc-34319e6e-ef9b-40cf-adf4-d70a1ab94321 10Gi RWO ocs-storagecluster-ceph-rbd 39d persistentvolumeclaim/zen-metastore-edb-2 Bound pvc-d9a5c1f3-a423-44b0-a7af-601359cbc5cd 10Gi RWO ocs-storagecluster-ceph-rbd 39d Tip You can retrieve the Cloud Pak for Data password from the ibm-iam-bindinfo-platform-auth-idp-credentials secret: oc -n ibm-cpd get secret ibm-iam-bindinfo-platform-auth-idp-credentials -o jsonpath=\"{.data.admin_password}\" | base64 -d 4.6.x version: \u00a4 Cloud Pak for Data 4.6.x is made up of many moving parts across multiple namespaces. In the ibm-common-services namespace: oc -n ibm-common-services get deployments NAME READY UP-TO-DATE AVAILABLE AGE cert-manager-cainjector 1/1 1 1 85m cert-manager-controller 1/1 1 1 85m cert-manager-webhook 1/1 1 1 85m configmap-watcher 1/1 1 1 85m ibm-cert-manager-operator 1/1 1 1 87m ibm-common-service-operator 1/1 1 1 92m ibm-common-service-webhook 1/1 1 1 91m ibm-namespace-scope-operator 1/1 1 1 91m ibm-zen-operator 1/1 1 1 87m meta-api-deploy 1/1 1 1 86m operand-deployment-lifecycle-manager 1/1 1 1 90m secretshare 1/1 1 1 91m In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 87m ibm-common-service-operator 1/1 1 1 87m ibm-namespace-scope-operator 1/1 1 1 87m In the ibm-cpd namespace: oc -n ibm-cpd get zenservice,ibmcpd,deployments,sts NAME AGE zenservice.zen.cpd.ibm.com/lite-cr 81m NAME AGE ibmcpd.cpd.ibm.com/ibmcpd 85m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ibm-nginx 3/3 3 3 62m deployment.apps/usermgmt 3/3 3 3 64m deployment.apps/zen-audit 1/1 1 1 56m deployment.apps/zen-core 3/3 3 3 55m deployment.apps/zen-core-api 3/3 3 3 55m deployment.apps/zen-data-sorcerer 2/2 2 2 48m deployment.apps/zen-watchdog 1/1 1 1 48m deployment.apps/zen-watcher 1/1 1 1 55m NAME READY AGE statefulset.apps/dsx-influxdb 1/1 51m statefulset.apps/zen-metastoredb 3/3 68m Tip You can retrieve the Cloud Pak for Data password from the admin-user-details secret: oc -n ibm-cpd get secret admin-user-details -o jsonpath=\"{.data.initial_admin_password}\" | base64 -d Role Variables \u00a4 cpd_product_version \u00a4 Defines the IBM Cloud Pak for Data release version to be installed. Required Environment Variable: CPD_PRODUCT_VERSION Default Value: Defined by the installed MAS catalog version ibm_entitlement_key \u00a4 Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None cpd_entitlement_key \u00a4 An IBM entitlement key specific for Cloud Pak for Data installation, primarily used to override ibm_entitlement_key in development. Optional Environment Variable: CPD_ENTITLEMENT_KEY Default: None cpd_primary_storage_class \u00a4 Primary storage class for Cloud Pak for Data. For more details please read the Storage Considerations for IBM Cloud Pak for Data . According to the mentioned documentation, Cloud Pak for Data uses the following access modes for storage classes: - RWX file storage: ocs-storagecluster-cephfs - RWX file storage: ibmc-file-gold-gid Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: CPD_PRIMARY_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available) cpd_metadata_storage_class \u00a4 Storage class for the Cloud Pak for Data Zen meta database. This must support ReadWriteOnce (RWO access) access mode. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: CPD_METADATA_STORAGE_CLASS Default Value: ibmc-block-gold , ocs-storagecluster-ceph-rbd , or managed-premium (if available) cpd_operators_namespace \u00a4 Namespace where Cloud Pak for Data operators will be installed. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default Value: ibm-cpd-operators cpd_instance_namespace \u00a4 Namespace that the Cloud Pak for Data operators will be configured to watch. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default Value: ibm-cpd cpd_scale_config \u00a4 Adjust and scale the resources for your Cloud Pak for Data instance to increase processing capacity. For more information, refer to Managing resources in IBM Cloud Pak for Data documentation. Optional Environment Variable: CPD_SCALE_CONFIG Default Value: medium cpd_admin_username \u00a4 The CP4D Admin username to authenticate with CP4D APIs. If you didn't change the initial admin username after installing CP4D then you don't need to provide this. Optional Environment Variable: CPD_ADMIN_USERNAME Default Value: admin (CPD 4.6) cpadmin (CPD 4.8) cpd_admin_password \u00a4 The CP4D Admin User password to call CP4D API to provision Discovery Instance. If you didn't change the initial admin password after CP4D install, you don't need to provide it. The initial admin user password for admin or cpdamin will be used. Optional Environment Variable: CPD_ADMIN_PASSWORD Default Value: CPD 4.6: Looked up from the admin-user-details secret in the cpd_instance_namespace namespace CPD 4.8: Looked up from the ibm-iam-bindinfo-platform-auth-idp-credentials secret in the cpd_instance_namespace namespace Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.0.0 cpd_primary_storage_class: ibmc-file-gold-gid cpd_metadata_storage_class: ibmc-block-gold roles: - ibm.mas_devops.cp4d License \u00a4 EPL-2.0","title":"cp4d"},{"location":"roles/cp4d/#cp4d","text":"This role installs or upgrades IBM Cloud Pak for Data Operator in the target cluster. Currently supported Cloud Pak for Data release versions are: 4.6.0 4.6.3 4.6.4 4.6.6 4.8.0 5.0.0 The role will automatically install or upgrade (if targeted to an existing CPD deployment) the corresponding Zen version associated to the chosen Cloud Pak for Data release, for example: Cloud Pak for Data release version 4.6.0 installs Zen/Control Plane version 4.8.0 . Cloud Pak for Data release version 4.6.3 installs Zen/Control Plane version 4.8.1 . Cloud Pak for Data release version 4.6.4 installs Zen/Control Plane version 4.8.2 . Cloud Pak for Data release version 4.6.6 installs Zen/Control Plane version 4.8.2 . Cloud Pak for Data release version 4.8.0 installs Zen/Control Plane version 5.1.0 Cloud Pak for Data release version 5.0.0 installs Zen/Control Plane version 6.0.1 For more information about CPD versioning, see IBM Cloud Pak for Data Operator and operand versions 4.9.x or IBM Cloud Pak for Data Operator and operand versions 5.0.x","title":"cp4d"},{"location":"roles/cp4d/#cloud-pak-for-data-version-mapping-to-mas-catalog","text":"Introduced with 4.8.x support, users can still choose to install an specific version of Cloud Pak for Data by setting CPD_PRODUCT_VERSION variable. However, by default, now it will possible to install an specific version of Cloud Pak for Data that is compatible with an specific version of MAS catalog (ibm-operator-catalog). If CPD_PRODUCT_VERSION variable is not defined, then the automation will try to find the installed MAS catalog (ibm-operator-catalog) in the target cluster, and lookup the corresponding default Cloud Pak for Data version that is mapped with the retrieved MAS catalog version. If still not able to find the MAS catalog, then by default, the Cloud Pak for Data version will be defined by the version supported by the latest released MAS catalog.","title":"Cloud Pak for Data version mapping to MAS Catalog"},{"location":"roles/cp4d/#upgrade","text":"This role also supports seamlessly CPD control plane (or also called Zen service) minor version upgrades (CPD 4.6.x > CPD 4.8.0 or CPD 4.8.0 > CPD 5.0.0), and patch version upgrades (i.e CPD 4.6.0 -> CPD 4.6.6). All you need to do is to define cpd_product_version variable to the version you target to upgrade and run this role against an existing CPD instance. For more information about IBM Cloud Pak for Data upgrade process, refer to the Cloud Pak for Data official documentation . The role assumes that you have already installed the IBM Operator Catalog and configured IBM Cloud Pak Foundational services (only a must have if installing CPD 4.6.x) in the target cluster. These actions are performed by the ibm_catalogs common_services roles in this collection. Cloud Pak for Data will be configured as a specialized installation Info A specialized installation allows a user with project administrator permissions to install the software after a cluster administrator completes the initial cluster setup. A specialized installation also facilitates strict division between Red Hat OpenShift Container Platform projects (Kubernetes namespaces). In a specialized installation, the IBM Cloud Pak foundational services operators are installed in the ibm-common-services project and the Cloud Pak for Data operators are installed in a separate project (typically cpd-operators). Each project has a dedicated: Operator group, which specifies the OwnNamespace installation mode NamespaceScope Operator, which allows the operators in the project to manage operators and service workloads in specific projects In this way, you can specify different settings for the IBM Cloud Pak foundational services and for the Cloud Pak for Data operators.","title":"Upgrade"},{"location":"roles/cp4d/#cloud-pak-for-data-deployment-details","text":"","title":"Cloud Pak for Data deployment details"},{"location":"roles/cp4d/#50x-version","text":"Cloud Pak for Data 5.0.x leverages Cloud Pak Foundational Services v4, which runs its deployments in isolated/dedicated scope model, that means that its dependencies will be grouped and installed within the Cloud Pak for Data related projects/namespaces. There are only two namespaces that will be used: CPD instance namespace (e.g ibm-cpd ) and CPD operators namespace (e.g ibm-cpd-operators ). In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 17h ibm-common-service-operator 1/1 1 1 17h ibm-namespace-scope-operator 1/1 1 1 17h ibm-zen-operator 1/1 1 1 17h meta-api-deploy 1/1 1 1 17h operand-deployment-lifecycle-manager 1/1 1 1 17h postgresql-operator-controller-manager-1-18-7 1/1 1 1 17h In the ibm-cpd namespace: oc -n ibm-cpd get zenservice,ibmcpd,deployments,sts,pvc NAME VERSION STATUS AGE zenservice.zen.cpd.ibm.com/lite-cr 6.0.1 Completed 17h NAME AGE ibmcpd.cpd.ibm.com/ibmcpd-cr 17h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ibm-mcs-hubwork 1/1 1 1 17h deployment.apps/ibm-mcs-placement 1/1 1 1 17h deployment.apps/ibm-mcs-storage 1/1 1 1 17h deployment.apps/ibm-nginx 3/3 3 3 16h deployment.apps/ibm-nginx-tester 1/1 1 1 16h deployment.apps/usermgmt 3/3 3 3 16h deployment.apps/zen-audit 2/2 2 2 16h deployment.apps/zen-core 3/3 3 3 16h deployment.apps/zen-core-api 3/3 3 3 16h deployment.apps/zen-watchdog 2/2 2 2 16h deployment.apps/zen-watcher 1/1 1 1 16h NAME READY AGE statefulset.apps/zen-minio 3/3 17h NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/export-zen-minio-0 Bound pvc-b2a2a729-13c1-4e7f-b672-0b5efc6aa40a 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/export-zen-minio-1 Bound pvc-7e772a3a-8849-4291-8e14-501f49e79182 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/export-zen-minio-2 Bound pvc-e0dd31dc-916d-4b15-9d9c-351db0a2b47f 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/ibm-cs-postgres-backup Bound pvc-ef788b99-784f-4531-a1b3-12611f112551 20Gi RWO ibmc-block-gold 16h persistentvolumeclaim/ibm-zen-objectstore-backup-pvc Bound pvc-d5e61dcf-65a3-4930-9cbf-ab80d04dda00 20Gi RWO ibmc-block-gold 16h persistentvolumeclaim/zen-metastore-edb-1 Bound pvc-19d44f17-05ab-4dc0-bb5d-1b5f15ffd201 20Gi RWO ibmc-block-gold 17h persistentvolumeclaim/zen-metastore-edb-2 Bound pvc-741ea444-b6f0-44ff-a123-bb4615d97381 20Gi RWO ibmc-block-gold 17h Tip You can retrieve the Initial Cloud Pak for Data password from the admin-user-details secret: oc -n ibm-cpd get secret admin-user-details -o jsonpath=\"{.data.initial_admin_password}\" | base64 -d","title":"5.0.x version:"},{"location":"roles/cp4d/#48x-version","text":"Cloud Pak for Data 4.8.x leverages Cloud Pak Foundational Services v4, which runs its deployments in isolated/dedicated scope model, that means that its dependencies will be grouped and installed within the Cloud Pak for Data related projects/namespaces. Differently from CPD 4.6.x, there are only two namespaces that will be used: CPD instance namespace (e.g ibm-cpd ) and CPD operators namespace (e.g ibm-cpd-operators ). In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 39d ibm-common-service-operator 1/1 1 1 39d ibm-commonui-operator 1/1 1 1 39d ibm-iam-operator 1/1 1 1 39d ibm-mongodb-operator 1/1 1 1 39d ibm-namespace-scope-operator 1/1 1 1 39d ibm-zen-operator 1/1 1 1 39d meta-api-deploy 1/1 1 1 39d operand-deployment-lifecycle-manager 1/1 1 1 39d postgresql-operator-controller-manager-1-18-7 1/1 1 1 39d In the ibm-cpd namespace: oc -n ibm-cpd get zenservice,ibmcpd,deployments,sts,pvc NAME STATUS AGE zenservice.zen.cpd.ibm.com/lite-cr Completed 39d NAME AGE ibmcpd.cpd.ibm.com/ibmcpd-cr 39d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/common-web-ui 1/1 1 1 39d deployment.apps/ibm-nginx 2/2 2 2 39d deployment.apps/ibm-nginx-tester 1/1 1 1 39d deployment.apps/platform-auth-service 1/1 1 1 39d deployment.apps/platform-identity-management 1/1 1 1 39d deployment.apps/platform-identity-provider 1/1 1 1 39d deployment.apps/usermgmt 2/2 2 2 39d deployment.apps/zen-audit 1/1 1 1 39d deployment.apps/zen-core 2/2 2 2 39d deployment.apps/zen-core-api 2/2 2 2 39d deployment.apps/zen-watchdog 1/1 1 1 39d deployment.apps/zen-watcher 1/1 1 1 39d NAME READY AGE statefulset.apps/icp-mongodb 3/3 39d statefulset.apps/zen-minio 3/3 39d NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/ibm-zen-cs-mongo-backup Bound pvc-bdba4bb2-dff5-43cb-a4b6-3540955ccb92 20Gi RWO ocs-storagecluster-ceph-rbd 39d persistentvolumeclaim/ibm-zen-objectstore-backup-pvc Bound pvc-46595a1b-2629-4c62-9e16-6b9553635738 20Gi RWO ocs-storagecluster-ceph-rbd 39d persistentvolumeclaim/mongodbdir-icp-mongodb-0 Bound pvc-1d3f7ee5-b2ef-4ca0-8b95-8db79bc88b19 20Gi RWO ocs-storagecluster-ceph-rbd 39d persistentvolumeclaim/mongodbdir-icp-mongodb-1 Bound pvc-9dada920-c6be-40f2-b4e2-56c989935a16 20Gi RWO ocs-storagecluster-ceph-rbd 39d persistentvolumeclaim/mongodbdir-icp-mongodb-2 Bound pvc-27713ee7-4d57-49e2-94ce-6955bbcd74f4 20Gi RWO ocs-storagecluster-ceph-rbd 39d persistentvolumeclaim/zen-metastore-edb-1 Bound pvc-34319e6e-ef9b-40cf-adf4-d70a1ab94321 10Gi RWO ocs-storagecluster-ceph-rbd 39d persistentvolumeclaim/zen-metastore-edb-2 Bound pvc-d9a5c1f3-a423-44b0-a7af-601359cbc5cd 10Gi RWO ocs-storagecluster-ceph-rbd 39d Tip You can retrieve the Cloud Pak for Data password from the ibm-iam-bindinfo-platform-auth-idp-credentials secret: oc -n ibm-cpd get secret ibm-iam-bindinfo-platform-auth-idp-credentials -o jsonpath=\"{.data.admin_password}\" | base64 -d","title":"4.8.x version:"},{"location":"roles/cp4d/#46x-version","text":"Cloud Pak for Data 4.6.x is made up of many moving parts across multiple namespaces. In the ibm-common-services namespace: oc -n ibm-common-services get deployments NAME READY UP-TO-DATE AVAILABLE AGE cert-manager-cainjector 1/1 1 1 85m cert-manager-controller 1/1 1 1 85m cert-manager-webhook 1/1 1 1 85m configmap-watcher 1/1 1 1 85m ibm-cert-manager-operator 1/1 1 1 87m ibm-common-service-operator 1/1 1 1 92m ibm-common-service-webhook 1/1 1 1 91m ibm-namespace-scope-operator 1/1 1 1 91m ibm-zen-operator 1/1 1 1 87m meta-api-deploy 1/1 1 1 86m operand-deployment-lifecycle-manager 1/1 1 1 90m secretshare 1/1 1 1 91m In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE cpd-platform-operator-manager 1/1 1 1 87m ibm-common-service-operator 1/1 1 1 87m ibm-namespace-scope-operator 1/1 1 1 87m In the ibm-cpd namespace: oc -n ibm-cpd get zenservice,ibmcpd,deployments,sts NAME AGE zenservice.zen.cpd.ibm.com/lite-cr 81m NAME AGE ibmcpd.cpd.ibm.com/ibmcpd 85m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ibm-nginx 3/3 3 3 62m deployment.apps/usermgmt 3/3 3 3 64m deployment.apps/zen-audit 1/1 1 1 56m deployment.apps/zen-core 3/3 3 3 55m deployment.apps/zen-core-api 3/3 3 3 55m deployment.apps/zen-data-sorcerer 2/2 2 2 48m deployment.apps/zen-watchdog 1/1 1 1 48m deployment.apps/zen-watcher 1/1 1 1 55m NAME READY AGE statefulset.apps/dsx-influxdb 1/1 51m statefulset.apps/zen-metastoredb 3/3 68m Tip You can retrieve the Cloud Pak for Data password from the admin-user-details secret: oc -n ibm-cpd get secret admin-user-details -o jsonpath=\"{.data.initial_admin_password}\" | base64 -d","title":"4.6.x version:"},{"location":"roles/cp4d/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d/#cpd_product_version","text":"Defines the IBM Cloud Pak for Data release version to be installed. Required Environment Variable: CPD_PRODUCT_VERSION Default Value: Defined by the installed MAS catalog version","title":"cpd_product_version"},{"location":"roles/cp4d/#ibm_entitlement_key","text":"Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None","title":"ibm_entitlement_key"},{"location":"roles/cp4d/#cpd_entitlement_key","text":"An IBM entitlement key specific for Cloud Pak for Data installation, primarily used to override ibm_entitlement_key in development. Optional Environment Variable: CPD_ENTITLEMENT_KEY Default: None","title":"cpd_entitlement_key"},{"location":"roles/cp4d/#cpd_primary_storage_class","text":"Primary storage class for Cloud Pak for Data. For more details please read the Storage Considerations for IBM Cloud Pak for Data . According to the mentioned documentation, Cloud Pak for Data uses the following access modes for storage classes: - RWX file storage: ocs-storagecluster-cephfs - RWX file storage: ibmc-file-gold-gid Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: CPD_PRIMARY_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available)","title":"cpd_primary_storage_class"},{"location":"roles/cp4d/#cpd_metadata_storage_class","text":"Storage class for the Cloud Pak for Data Zen meta database. This must support ReadWriteOnce (RWO access) access mode. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: CPD_METADATA_STORAGE_CLASS Default Value: ibmc-block-gold , ocs-storagecluster-ceph-rbd , or managed-premium (if available)","title":"cpd_metadata_storage_class"},{"location":"roles/cp4d/#cpd_operators_namespace","text":"Namespace where Cloud Pak for Data operators will be installed. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default Value: ibm-cpd-operators","title":"cpd_operators_namespace"},{"location":"roles/cp4d/#cpd_instance_namespace","text":"Namespace that the Cloud Pak for Data operators will be configured to watch. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default Value: ibm-cpd","title":"cpd_instance_namespace"},{"location":"roles/cp4d/#cpd_scale_config","text":"Adjust and scale the resources for your Cloud Pak for Data instance to increase processing capacity. For more information, refer to Managing resources in IBM Cloud Pak for Data documentation. Optional Environment Variable: CPD_SCALE_CONFIG Default Value: medium","title":"cpd_scale_config"},{"location":"roles/cp4d/#cpd_admin_username","text":"The CP4D Admin username to authenticate with CP4D APIs. If you didn't change the initial admin username after installing CP4D then you don't need to provide this. Optional Environment Variable: CPD_ADMIN_USERNAME Default Value: admin (CPD 4.6) cpadmin (CPD 4.8)","title":"cpd_admin_username"},{"location":"roles/cp4d/#cpd_admin_password","text":"The CP4D Admin User password to call CP4D API to provision Discovery Instance. If you didn't change the initial admin password after CP4D install, you don't need to provide it. The initial admin user password for admin or cpdamin will be used. Optional Environment Variable: CPD_ADMIN_PASSWORD Default Value: CPD 4.6: Looked up from the admin-user-details secret in the cpd_instance_namespace namespace CPD 4.8: Looked up from the ibm-iam-bindinfo-platform-auth-idp-credentials secret in the cpd_instance_namespace namespace","title":"cpd_admin_password"},{"location":"roles/cp4d/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.0.0 cpd_primary_storage_class: ibmc-file-gold-gid cpd_metadata_storage_class: ibmc-block-gold roles: - ibm.mas_devops.cp4d","title":"Example Playbook"},{"location":"roles/cp4d/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d_admin_pwd_update/","text":"cp4d_admin_pwd_update \u00a4 This role will update the password on an existing cp4d instance. By default it will update the password to a randomly generated new password only when the instance is still using the 'initial_admin_password' although using the 'cp4d_admin_password_force_update' variable referenced below will override this to update the password regardless of the current one being used. The new password will be added to the same yaml file that the 'initial_admin_password' was generated into - 'admin-user-details' by default. Role Variables \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the cp4d password updater will target. Environment Variable: MAS_INSTANCE_ID Default Value: None cp4d_namespace \u00a4 The instance of cp4d in your cluster that the cp4d password updater will target - defaults to 'ibm-cpd'. Environment Variable: CP4D_NAMESPACE Default Value: 'ibm-cpd' cp4d_admin_credentials_secret_name \u00a4 The secret inside your cp4d instance that stores the intial admin password - defaults to 'admin-user-details'. Environment Variable: CP4D_ADMIN_CREDENTIALS_SECRET_NAME Default Value: 'admin-user-details' cp4d_admin_username \u00a4 The username for your cp4d instance - defaults to 'admin'. Environment Variable: CP4D_ADMIN_USERNAME Default Value: 'admin' cp4d_admin_password \u00a4 The password for your cp4d insrance - an optional addition as the password updater will attempt to collect this value from the 'cp4d_admin_credentials_secret_name' secret. Optional Environment Variable: CP4D_ADMIN_PASSWORD Default Value: None cp4d_admin_password_force_update \u00a4 Typically the password updater will only update the password if the cp4d instance is using the initial password provided in the secret - setting this value to 'True' will ensure that is resets the password regardless. Environment Variable: CP4D_ADMIN_PASSWORD_FORCE_UPDATE Default Value: 'False' Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" cp4d_namespace: ibm-cpd cp4d_admin_credentials_secret_name: admin-user-details cp4d_admin_username: admin cp4d_admin_password: password123 cp4d_admin_password_force_update: True roles: - ibm.mas_devops.cp4d_admin_pwd_update","title":"cp4d_admin_pwd_update"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_admin_pwd_update","text":"This role will update the password on an existing cp4d instance. By default it will update the password to a randomly generated new password only when the instance is still using the 'initial_admin_password' although using the 'cp4d_admin_password_force_update' variable referenced below will override this to update the password regardless of the current one being used. The new password will be added to the same yaml file that the 'initial_admin_password' was generated into - 'admin-user-details' by default.","title":"cp4d_admin_pwd_update"},{"location":"roles/cp4d_admin_pwd_update/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d_admin_pwd_update/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the cp4d password updater will target. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_namespace","text":"The instance of cp4d in your cluster that the cp4d password updater will target - defaults to 'ibm-cpd'. Environment Variable: CP4D_NAMESPACE Default Value: 'ibm-cpd'","title":"cp4d_namespace"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_admin_credentials_secret_name","text":"The secret inside your cp4d instance that stores the intial admin password - defaults to 'admin-user-details'. Environment Variable: CP4D_ADMIN_CREDENTIALS_SECRET_NAME Default Value: 'admin-user-details'","title":"cp4d_admin_credentials_secret_name"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_admin_username","text":"The username for your cp4d instance - defaults to 'admin'. Environment Variable: CP4D_ADMIN_USERNAME Default Value: 'admin'","title":"cp4d_admin_username"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_admin_password","text":"The password for your cp4d insrance - an optional addition as the password updater will attempt to collect this value from the 'cp4d_admin_credentials_secret_name' secret. Optional Environment Variable: CP4D_ADMIN_PASSWORD Default Value: None","title":"cp4d_admin_password"},{"location":"roles/cp4d_admin_pwd_update/#cp4d_admin_password_force_update","text":"Typically the password updater will only update the password if the cp4d instance is using the initial password provided in the secret - setting this value to 'True' will ensure that is resets the password regardless. Environment Variable: CP4D_ADMIN_PASSWORD_FORCE_UPDATE Default Value: 'False'","title":"cp4d_admin_password_force_update"},{"location":"roles/cp4d_admin_pwd_update/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" cp4d_namespace: ibm-cpd cp4d_admin_credentials_secret_name: admin-user-details cp4d_admin_username: admin cp4d_admin_password: password123 cp4d_admin_password_force_update: True roles: - ibm.mas_devops.cp4d_admin_pwd_update","title":"Example Playbook"},{"location":"roles/cp4d_service/","text":"cp4d_service \u00a4 Install or upgrade a chosen CloudPak for Data service. Currently supported Cloud Pak for Data release versions supported are: 4.6.0 4.6.3 4.6.4 4.6.6 4.8.0 5.0.0 The role will automatically install the corresponding CPD service operator channel and custom resource version associated to the chosen Cloud Pak for Data release version. For more information about the specific CPD services channels and versions associated to a particular Cloud Pak for Data release can be found here . Services Supported \u00a4 These services can be deployed and configured using this role: Watson Studio required by Predict Watson Machine Learning required by Predict Analytics Services (Apache Spark) required by Predict Watson OpenScale an optional dependency for Predict SPSS Modeler optional dependency for Predict Watson Discovery required by Assist - Not supported with CPD 5.0 Cognos Analytics optional dependency for Manage application Upgrade \u00a4 This role also supports seamlessly CPD services minor version upgrades (CPD 4.6.x > CPD 4.8.0 or CPD 4.8.0 > CPD 5.0.0), as well as patch version upgrades (e.g. CPD 4.6.0 -> CPD 4.6.6), with the exception of Watson Discovery . All you need to do is to define cpd_product_version variable to the version you target to upgrade and run this role for a particular CPD service. It's important that before you upgrade CPD services, the CPD Control Plane/Zen is also upgraded to the same release version. For more information about IBM Cloud Pak for Data upgrade process, refer to the CPD official documentation . Application Support For more information on how Predict and HP Utilities make use of Watson Studio, refer to Predict/HP Utilities documentation Warning The reconcile of many CP4D resources will be marked as Failed multiple times during initial installation, these are misleading status updates , the install is just really slow and the operators can not properly handle this. For example, if you are watching the install of CCS you will see that each rabbitmq-ha pod takes 10-15 minutes to start up and it looks like there is a problem because the pod log will just stop at a certain point. If you see something like this as the last message in the pod log WAL: ra_log_wal init, open tbls: ra_log_open_mem_tables, closed tbls: ra_log_closed_mem_tables be assured that there's nothing wrong, it's just there's a long delay between that message and the next ( starting system coordination ) being logged. Note : Watson Discovery 4.8.x introduces breaking changes that blocks seamless upgrade from 4.6.x. It requires backing up your data, uninstall 4.6.x, install 4.8.x and restoring Watson Discovery data. Thus, this action requires additional steps that are not covered by this automation. For detailed steps about upgrading WD 4.6 -> 4.8, read Upgrading Watson Discovery from version 4.6 to 4.8 . Watson Studio \u00a4 Subscriptions related to Watson Studio: cpd-platform-operator ibm-cpd-wsl ibm-cpd-ccs ibm-cpd-datarefinery ibm-cpd-ws-runtimes Watson Studio is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 83m ibm-cpd-datarefinery-operator 1/1 1 1 83m ibm-cpd-ws-operator 1/1 1 1 83m ibm-cpd-ws-runtimes-operator 1/1 1 1 83m ibm-elasticsearch-operator-ibm-es-controller-manager 1/1 1 1 83m In the ibm-cpd namespace: oc -n ibm-cpd get ccs,ws,datarefinery,notebookruntimes,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 9.0.0 9.0.0 Completed 82m NAME VERSION RECONCILED STATUS AGE ws.ws.cpd.ibm.com/ws-cr 9.0.0 9.0.0 Completed 83m NAME VERSION RECONCILED STATUS AGE datarefinery.datarefinery.cpd.ibm.com/datarefinery-cr 9.0.0 9.0.0 Completed 36m NAME NLP MODELS VERSION RECONCILED STATUS AGE notebookruntime.ws.cpd.ibm.com/ibm-cpd-ws-runtime-241-py 9.0.0 9.0.0 Completed 22m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/asset-files-api 1/1 1 1 60m deployment.apps/ax-cdsx-jupyter-notebooks-converter-deploy 1/1 1 1 15m deployment.apps/ax-cdsx-notebooks-job-manager-deploy 1/1 1 1 15m deployment.apps/ax-environments-api-deploy 1/1 1 1 53m deployment.apps/ax-environments-ui-deploy 1/1 1 1 53m deployment.apps/ax-wdp-notebooks-api-deploy 1/1 1 1 15m deployment.apps/ax-ws-notebooks-ui-deploy 1/1 1 1 15m deployment.apps/catalog-api 2/2 2 2 69m deployment.apps/dataview-api-service 1/1 1 1 48m deployment.apps/dc-main 1/1 1 1 65m deployment.apps/event-logger-api 1/1 1 1 59m deployment.apps/ibm-0100-model-viewer-prod 1/1 1 1 14m deployment.apps/jobs-api 1/1 1 1 49m deployment.apps/jobs-ui 1/1 1 1 49m deployment.apps/ngp-projects-api 1/1 1 1 60m deployment.apps/portal-catalog 1/1 1 1 65m deployment.apps/portal-common-api 1/1 1 1 60m deployment.apps/portal-job-manager 1/1 1 1 60m deployment.apps/portal-main 1/1 1 1 60m deployment.apps/portal-ml-dl 1/1 1 1 14m deployment.apps/portal-notifications 1/1 1 1 60m deployment.apps/portal-projects 1/1 1 1 60m deployment.apps/redis-ha-haproxy 1/1 1 1 78m deployment.apps/runtime-assemblies-operator 1/1 1 1 59m deployment.apps/runtime-manager-api 1/1 1 1 59m deployment.apps/spaces 1/1 1 1 48m deployment.apps/task-credentials 1/1 1 1 48m deployment.apps/wdp-connect-connection 1/1 1 1 66m deployment.apps/wdp-connect-connector 1/1 1 1 66m deployment.apps/wdp-connect-flight 1/1 1 1 66m deployment.apps/wdp-dataprep 1/1 1 1 29m deployment.apps/wdp-dataview 1/1 1 1 48m deployment.apps/wdp-shaper 1/1 1 1 29m deployment.apps/wkc-search 1/1 1 1 66m deployment.apps/wml-main 1/1 1 1 48m NAME READY AGE statefulset.apps/elasticsea-0ac3-ib-6fb9-es-server-esnodes 3/3 74m statefulset.apps/rabbitmq-ha 3/3 79m statefulset.apps/redis-ha-server 3/3 79m statefulset.apps/wdp-couchdb 3/3 79m Watson Machine Learning \u00a4 Subscriptions related to Watson Machine Learning: cpd-platform-operator ibm-cpd-wml ibm-cpd-ccs Watson Machine Learning is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 134m ibm-cpd-datarefinery-operator 1/1 1 1 134m ibm-cpd-wml-operator 1/1 1 1 49m ibm-elasticsearch-operator-ibm-es-controller-manager 1/1 1 1 134m In the ibm-cpd namespace: oc -n ibm-cpd get ccs,wmlbase,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 9.0.0 9.0.0 Completed 133m NAME VERSION BUILD STATUS RECONCILED AGE wmlbase.wml.cpd.ibm.com/wml-cr 5.0.0 5.0.0-918 Completed 5.0.0 50m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/wml-deployment-envoy 1/1 1 1 23m deployment.apps/wml-deployment-manager 1/1 1 1 19m deployment.apps/wml-main 1/1 1 1 99m deployment.apps/wml-repositoryv4 1/1 1 1 16m deployment.apps/wmltraining 1/1 1 1 15m deployment.apps/wmltrainingorchestrator 1/1 1 1 14m NAME READY AGE statefulset.apps/wml-cpd-etcd 3/3 26m statefulset.apps/wml-deployment-agent 1/1 21m Analytics Engine \u00a4 Subscriptions related to Analytics Engine: cpd-platform-operator analyticsengine-operator Analytics Engine is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ae-operator 1/1 1 1 31m In the ibm-cpd namespace: oc -n ibm-cpd get analyticsengine,deployments NAME VERSION RECONCILED STATUS AGE analyticsengine.ae.cpd.ibm.com/analyticsengine-sample 5.0.0 5.0.0 Completed 31m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/spark-hb-br-recovery 1/1 1 1 11m deployment.apps/spark-hb-control-plane 1/1 1 1 19m deployment.apps/spark-hb-create-trust-store 1/1 1 1 25m deployment.apps/spark-hb-deployer-agent 1/1 1 1 19m deployment.apps/spark-hb-nginx 1/1 1 1 19m deployment.apps/spark-hb-register-hb-dataplane 1/1 1 1 10m deployment.apps/spark-hb-ui 1/1 1 1 19m Watson OpenScale \u00a4 Subscriptions related to Watson OpenScale (in the ibm-cpd-operators namespace): cpd-platform-operator ibm-cpd-wos-operator Analytics Engine is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-wos-operator 1/1 1 1 30m In the ibm-cpd namespace: oc -n ibm-cpd get woservice,deployments,sts NAME TYPE STORAGE SCALECONFIG PHASE RECONCILED STATUS woservice.wos.cpd.ibm.com/openscale-defaultinstance service small Ready 5.0.0 Completed NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/openscale-defaultinstance-ibm-aios-bias 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-bkpicombined 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-common-api 1/1 1 1 22m deployment.apps/openscale-defaultinstance-ibm-aios-configuration 1/1 1 1 22m deployment.apps/openscale-defaultinstance-ibm-aios-dashboard 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-datamart 1/1 1 1 22m deployment.apps/openscale-defaultinstance-ibm-aios-drift 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-explainability 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-fast-path 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-feedback 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-metrics-compute-manager 0/0 0 0 21m deployment.apps/openscale-defaultinstance-ibm-aios-ml-gateway-discovery 1/1 1 1 22m deployment.apps/openscale-defaultinstance-ibm-aios-ml-gateway-service 1/1 1 1 22m deployment.apps/openscale-defaultinstance-ibm-aios-mrm 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-nginx 1/1 1 1 25m deployment.apps/openscale-defaultinstance-ibm-aios-notification 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-payload-logging 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-payload-logging-api 1/1 1 1 22m deployment.apps/openscale-defaultinstance-ibm-aios-redis 1/1 1 1 28m deployment.apps/openscale-defaultinstance-ibm-aios-scheduling 1/1 1 1 21m NAME READY AGE statefulset.apps/openscale-defaultinstance-ibm-aios-etcd 3/3 27m statefulset.apps/openscale-defaultinstance-ibm-aios-kafka 3/3 27m Cognos Analytics \u00a4 Subscriptions related to Cognos Analytics (in the ibm-cpd-operators namespace): cpd-platform-operator ibm-ca-operator-controller-manager Cognos Analytics is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-ca-operator-controller-manager 1/1 1 1 19m In the ibm-cpd namespace: oc -n ibm-cpd get caservice,deployments NAME AGE caservice.ca.cpd.ibm.com/ca-addon-cr 19m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cognos-analytics-cognos-analytics-addon 1/1 1 1 9m17s SPSS \u00a4 Subscriptions related to SPSS (in the ibm-cpd-operators namespace): cpd-platform-operator ibm-cpd-spss-operator SPSS is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-canvasbase-operator 1/1 1 1 38m ibm-cpd-spss-operator 1/1 1 1 38m In the ibm-cpd namespace: oc -n ibm-cpd get spss,deployments NAME VERSION STATUS AGE spss.spssmodeler.cpd.ibm.com/spssmodeler 9.0.0 Completed 38m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/canvasbase-flow-api 1/1 1 1 35m deployment.apps/canvasbase-flow-ui 1/1 1 1 30m deployment.apps/spss-modeler-modeler-flow-api 1/1 1 1 22m Role Variables - Installation \u00a4 cpd_service_name \u00a4 Name of the service to install, supported values are: wsl , wml , wd , aiopenscale , spark , ca and spss Required Environment Variable: CPD_SERVICE_NAME Default Value: None cpd_product_version \u00a4 The product version (also known as operand version) of this service to install. Required Environment Variable: CPD_PRODUCT_VERSION Default Value: Defined by the installed MAS catalog version cpd_service_storage_class \u00a4 This is used to set spec.storageClass in all CPD services that uses file storage class (read-write-many RWX). Required , unless IBMCloud storage classes are available. Environment Variable: CPD_SERVICE_STORAGE_CLASS Default Value: Auto determined if default storage classes are provided and available by your cloud provider. i.e ibmc-file for IBM Cloud, efs for AWS. cpd_service_block_storage_class \u00a4 This is used to set spec.blockStorageClass in all CPD services that uses block storage class (read-write-only RWO). Required , unless IBMCloud storage classes are available. Environment Variable: CPD_SERVICE_BLOCK_STORAGE_CLASS Default Value: Auto determined if default storage classes are provided and available by your cloud provider. i.e ibmc-block for IBM Cloud, gp2 for AWS. cpd_instance_namespace \u00a4 Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default Value: ibm-cpd cpd_operator_namespace \u00a4 Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default Value: ibm-cpd-operators cpd_admin_username \u00a4 The CP4D Admin username to authenticate with CP4D APIs. If you didn't change the initial admin username after installing CP4D then you don't need to provide this. Optional Environment Variable: CPD_ADMIN_USERNAME Default Value: admin (CPD 4.6) cpadmin (CPD 4.8 and 5.0) cpd_admin_password \u00a4 The CP4D Admin User password to call CP4D API to provision Discovery Instance. If you didn't change the initial admin password after CP4D install, you don't need to provide it. The initial admin user password for admin or cpdamin will be used. Optional Environment Variable: CPD_ADMIN_PASSWORD Default Value: CPD 4.6: Looked up from the admin-user-details secret in the cpd_instance_namespace namespace CPD 4.8 and 5.0: Looked up from the ibm-iam-bindinfo-platform-auth-idp-credentials secret in the cpd_instance_namespace namespace cpd_service_scale_config \u00a4 Adjust and scale the resources for your Cloud Pak for Data services to increase processing capacity. For more information, refer to Managing resources in IBM Cloud Pak for Data documentation. Optional Environment Variable: CPD_SERVICE_SCALE_CONFIG Default Value: small Role Variables - Watson Studio \u00a4 cpd_wsl_project_name \u00a4 Stores the CP4D Watson Studio Project name that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_NAME Default Value: wsl-mas-${mas_instance_id}-hputilities cpd_wsl_project_description \u00a4 Optional - Stores the CP4D Watson Studio Project description that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_DESCRIPTION Default Value: Watson Studio Project for Maximo Application Suite Role Variables - Watson Discovery \u00a4 cpd_wd_instance_name \u00a4 Stores the name of the CP4D Watson Discovery Instance that can be used to configure Assist application in MAS. Optional, only supported when cpd_service_name = wd Environment Variable: CPD_WD_INSTANCE_NAME Default Value: wd-mas-${mas_instance_id}-assist cpd_wd_deployment_type \u00a4 Defines the CP4D Watson Discovery deployment type: Starter : One replica pod for each wd service/component, uses fewer resources in your cluster. Production : Multiple replica pods for each Watson Discovery service/component, recommended for production deployments to increase workload capacity however consumes more cluster resources. Optional Environment Variable: CPD_WD_DEPLOYMENT_TYPE Default Value: Starter Note: Deployment type cannot be changed in the future neither while upgrading the service. If you need to change the deployment type, you must uninstall Watson Discovery and reinstall with the desired deployment type. More information, see Upgrading Watson Discovery . Role Variables - MAS Configuration Generation \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that a generated configuration will target. If this or mas_config_dir are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated resource definition. This can be used to manually configure a MAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbook \u00a4 --- - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.0.0 cpd_service_storage_class: ibmc-file-gold-gid cpd_service_name: wsl roles: - ibm.mas_devops.cp4d_service License \u00a4 EPL-2.0","title":"cp4d_service"},{"location":"roles/cp4d_service/#cp4d_service","text":"Install or upgrade a chosen CloudPak for Data service. Currently supported Cloud Pak for Data release versions supported are: 4.6.0 4.6.3 4.6.4 4.6.6 4.8.0 5.0.0 The role will automatically install the corresponding CPD service operator channel and custom resource version associated to the chosen Cloud Pak for Data release version. For more information about the specific CPD services channels and versions associated to a particular Cloud Pak for Data release can be found here .","title":"cp4d_service"},{"location":"roles/cp4d_service/#services-supported","text":"These services can be deployed and configured using this role: Watson Studio required by Predict Watson Machine Learning required by Predict Analytics Services (Apache Spark) required by Predict Watson OpenScale an optional dependency for Predict SPSS Modeler optional dependency for Predict Watson Discovery required by Assist - Not supported with CPD 5.0 Cognos Analytics optional dependency for Manage application","title":"Services Supported"},{"location":"roles/cp4d_service/#upgrade","text":"This role also supports seamlessly CPD services minor version upgrades (CPD 4.6.x > CPD 4.8.0 or CPD 4.8.0 > CPD 5.0.0), as well as patch version upgrades (e.g. CPD 4.6.0 -> CPD 4.6.6), with the exception of Watson Discovery . All you need to do is to define cpd_product_version variable to the version you target to upgrade and run this role for a particular CPD service. It's important that before you upgrade CPD services, the CPD Control Plane/Zen is also upgraded to the same release version. For more information about IBM Cloud Pak for Data upgrade process, refer to the CPD official documentation . Application Support For more information on how Predict and HP Utilities make use of Watson Studio, refer to Predict/HP Utilities documentation Warning The reconcile of many CP4D resources will be marked as Failed multiple times during initial installation, these are misleading status updates , the install is just really slow and the operators can not properly handle this. For example, if you are watching the install of CCS you will see that each rabbitmq-ha pod takes 10-15 minutes to start up and it looks like there is a problem because the pod log will just stop at a certain point. If you see something like this as the last message in the pod log WAL: ra_log_wal init, open tbls: ra_log_open_mem_tables, closed tbls: ra_log_closed_mem_tables be assured that there's nothing wrong, it's just there's a long delay between that message and the next ( starting system coordination ) being logged. Note : Watson Discovery 4.8.x introduces breaking changes that blocks seamless upgrade from 4.6.x. It requires backing up your data, uninstall 4.6.x, install 4.8.x and restoring Watson Discovery data. Thus, this action requires additional steps that are not covered by this automation. For detailed steps about upgrading WD 4.6 -> 4.8, read Upgrading Watson Discovery from version 4.6 to 4.8 .","title":"Upgrade"},{"location":"roles/cp4d_service/#watson-studio","text":"Subscriptions related to Watson Studio: cpd-platform-operator ibm-cpd-wsl ibm-cpd-ccs ibm-cpd-datarefinery ibm-cpd-ws-runtimes Watson Studio is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 83m ibm-cpd-datarefinery-operator 1/1 1 1 83m ibm-cpd-ws-operator 1/1 1 1 83m ibm-cpd-ws-runtimes-operator 1/1 1 1 83m ibm-elasticsearch-operator-ibm-es-controller-manager 1/1 1 1 83m In the ibm-cpd namespace: oc -n ibm-cpd get ccs,ws,datarefinery,notebookruntimes,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 9.0.0 9.0.0 Completed 82m NAME VERSION RECONCILED STATUS AGE ws.ws.cpd.ibm.com/ws-cr 9.0.0 9.0.0 Completed 83m NAME VERSION RECONCILED STATUS AGE datarefinery.datarefinery.cpd.ibm.com/datarefinery-cr 9.0.0 9.0.0 Completed 36m NAME NLP MODELS VERSION RECONCILED STATUS AGE notebookruntime.ws.cpd.ibm.com/ibm-cpd-ws-runtime-241-py 9.0.0 9.0.0 Completed 22m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/asset-files-api 1/1 1 1 60m deployment.apps/ax-cdsx-jupyter-notebooks-converter-deploy 1/1 1 1 15m deployment.apps/ax-cdsx-notebooks-job-manager-deploy 1/1 1 1 15m deployment.apps/ax-environments-api-deploy 1/1 1 1 53m deployment.apps/ax-environments-ui-deploy 1/1 1 1 53m deployment.apps/ax-wdp-notebooks-api-deploy 1/1 1 1 15m deployment.apps/ax-ws-notebooks-ui-deploy 1/1 1 1 15m deployment.apps/catalog-api 2/2 2 2 69m deployment.apps/dataview-api-service 1/1 1 1 48m deployment.apps/dc-main 1/1 1 1 65m deployment.apps/event-logger-api 1/1 1 1 59m deployment.apps/ibm-0100-model-viewer-prod 1/1 1 1 14m deployment.apps/jobs-api 1/1 1 1 49m deployment.apps/jobs-ui 1/1 1 1 49m deployment.apps/ngp-projects-api 1/1 1 1 60m deployment.apps/portal-catalog 1/1 1 1 65m deployment.apps/portal-common-api 1/1 1 1 60m deployment.apps/portal-job-manager 1/1 1 1 60m deployment.apps/portal-main 1/1 1 1 60m deployment.apps/portal-ml-dl 1/1 1 1 14m deployment.apps/portal-notifications 1/1 1 1 60m deployment.apps/portal-projects 1/1 1 1 60m deployment.apps/redis-ha-haproxy 1/1 1 1 78m deployment.apps/runtime-assemblies-operator 1/1 1 1 59m deployment.apps/runtime-manager-api 1/1 1 1 59m deployment.apps/spaces 1/1 1 1 48m deployment.apps/task-credentials 1/1 1 1 48m deployment.apps/wdp-connect-connection 1/1 1 1 66m deployment.apps/wdp-connect-connector 1/1 1 1 66m deployment.apps/wdp-connect-flight 1/1 1 1 66m deployment.apps/wdp-dataprep 1/1 1 1 29m deployment.apps/wdp-dataview 1/1 1 1 48m deployment.apps/wdp-shaper 1/1 1 1 29m deployment.apps/wkc-search 1/1 1 1 66m deployment.apps/wml-main 1/1 1 1 48m NAME READY AGE statefulset.apps/elasticsea-0ac3-ib-6fb9-es-server-esnodes 3/3 74m statefulset.apps/rabbitmq-ha 3/3 79m statefulset.apps/redis-ha-server 3/3 79m statefulset.apps/wdp-couchdb 3/3 79m","title":"Watson Studio"},{"location":"roles/cp4d_service/#watson-machine-learning","text":"Subscriptions related to Watson Machine Learning: cpd-platform-operator ibm-cpd-wml ibm-cpd-ccs Watson Machine Learning is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ccs-operator 1/1 1 1 134m ibm-cpd-datarefinery-operator 1/1 1 1 134m ibm-cpd-wml-operator 1/1 1 1 49m ibm-elasticsearch-operator-ibm-es-controller-manager 1/1 1 1 134m In the ibm-cpd namespace: oc -n ibm-cpd get ccs,wmlbase,deployments,sts NAME VERSION RECONCILED STATUS AGE ccs.ccs.cpd.ibm.com/ccs-cr 9.0.0 9.0.0 Completed 133m NAME VERSION BUILD STATUS RECONCILED AGE wmlbase.wml.cpd.ibm.com/wml-cr 5.0.0 5.0.0-918 Completed 5.0.0 50m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/wml-deployment-envoy 1/1 1 1 23m deployment.apps/wml-deployment-manager 1/1 1 1 19m deployment.apps/wml-main 1/1 1 1 99m deployment.apps/wml-repositoryv4 1/1 1 1 16m deployment.apps/wmltraining 1/1 1 1 15m deployment.apps/wmltrainingorchestrator 1/1 1 1 14m NAME READY AGE statefulset.apps/wml-cpd-etcd 3/3 26m statefulset.apps/wml-deployment-agent 1/1 21m","title":"Watson Machine Learning"},{"location":"roles/cp4d_service/#analytics-engine","text":"Subscriptions related to Analytics Engine: cpd-platform-operator analyticsengine-operator Analytics Engine is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-ae-operator 1/1 1 1 31m In the ibm-cpd namespace: oc -n ibm-cpd get analyticsengine,deployments NAME VERSION RECONCILED STATUS AGE analyticsengine.ae.cpd.ibm.com/analyticsengine-sample 5.0.0 5.0.0 Completed 31m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/spark-hb-br-recovery 1/1 1 1 11m deployment.apps/spark-hb-control-plane 1/1 1 1 19m deployment.apps/spark-hb-create-trust-store 1/1 1 1 25m deployment.apps/spark-hb-deployer-agent 1/1 1 1 19m deployment.apps/spark-hb-nginx 1/1 1 1 19m deployment.apps/spark-hb-register-hb-dataplane 1/1 1 1 10m deployment.apps/spark-hb-ui 1/1 1 1 19m","title":"Analytics Engine"},{"location":"roles/cp4d_service/#watson-openscale","text":"Subscriptions related to Watson OpenScale (in the ibm-cpd-operators namespace): cpd-platform-operator ibm-cpd-wos-operator Analytics Engine is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-wos-operator 1/1 1 1 30m In the ibm-cpd namespace: oc -n ibm-cpd get woservice,deployments,sts NAME TYPE STORAGE SCALECONFIG PHASE RECONCILED STATUS woservice.wos.cpd.ibm.com/openscale-defaultinstance service small Ready 5.0.0 Completed NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/openscale-defaultinstance-ibm-aios-bias 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-bkpicombined 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-common-api 1/1 1 1 22m deployment.apps/openscale-defaultinstance-ibm-aios-configuration 1/1 1 1 22m deployment.apps/openscale-defaultinstance-ibm-aios-dashboard 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-datamart 1/1 1 1 22m deployment.apps/openscale-defaultinstance-ibm-aios-drift 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-explainability 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-fast-path 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-feedback 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-metrics-compute-manager 0/0 0 0 21m deployment.apps/openscale-defaultinstance-ibm-aios-ml-gateway-discovery 1/1 1 1 22m deployment.apps/openscale-defaultinstance-ibm-aios-ml-gateway-service 1/1 1 1 22m deployment.apps/openscale-defaultinstance-ibm-aios-mrm 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-nginx 1/1 1 1 25m deployment.apps/openscale-defaultinstance-ibm-aios-notification 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-payload-logging 1/1 1 1 21m deployment.apps/openscale-defaultinstance-ibm-aios-payload-logging-api 1/1 1 1 22m deployment.apps/openscale-defaultinstance-ibm-aios-redis 1/1 1 1 28m deployment.apps/openscale-defaultinstance-ibm-aios-scheduling 1/1 1 1 21m NAME READY AGE statefulset.apps/openscale-defaultinstance-ibm-aios-etcd 3/3 27m statefulset.apps/openscale-defaultinstance-ibm-aios-kafka 3/3 27m","title":"Watson OpenScale"},{"location":"roles/cp4d_service/#cognos-analytics","text":"Subscriptions related to Cognos Analytics (in the ibm-cpd-operators namespace): cpd-platform-operator ibm-ca-operator-controller-manager Cognos Analytics is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: oc -n ibm-cpd-operators get deployments NAME READY UP-TO-DATE AVAILABLE AGE ibm-ca-operator-controller-manager 1/1 1 1 19m In the ibm-cpd namespace: oc -n ibm-cpd get caservice,deployments NAME AGE caservice.ca.cpd.ibm.com/ca-addon-cr 19m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cognos-analytics-cognos-analytics-addon 1/1 1 1 9m17s","title":"Cognos Analytics"},{"location":"roles/cp4d_service/#spss","text":"Subscriptions related to SPSS (in the ibm-cpd-operators namespace): cpd-platform-operator ibm-cpd-spss-operator SPSS is made up of many moving parts across multiple namespaces. In the ibm-cpd-operators namespace: NAME READY UP-TO-DATE AVAILABLE AGE ibm-cpd-canvasbase-operator 1/1 1 1 38m ibm-cpd-spss-operator 1/1 1 1 38m In the ibm-cpd namespace: oc -n ibm-cpd get spss,deployments NAME VERSION STATUS AGE spss.spssmodeler.cpd.ibm.com/spssmodeler 9.0.0 Completed 38m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/canvasbase-flow-api 1/1 1 1 35m deployment.apps/canvasbase-flow-ui 1/1 1 1 30m deployment.apps/spss-modeler-modeler-flow-api 1/1 1 1 22m","title":"SPSS"},{"location":"roles/cp4d_service/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/cp4d_service/#cpd_service_name","text":"Name of the service to install, supported values are: wsl , wml , wd , aiopenscale , spark , ca and spss Required Environment Variable: CPD_SERVICE_NAME Default Value: None","title":"cpd_service_name"},{"location":"roles/cp4d_service/#cpd_product_version","text":"The product version (also known as operand version) of this service to install. Required Environment Variable: CPD_PRODUCT_VERSION Default Value: Defined by the installed MAS catalog version","title":"cpd_product_version"},{"location":"roles/cp4d_service/#cpd_service_storage_class","text":"This is used to set spec.storageClass in all CPD services that uses file storage class (read-write-many RWX). Required , unless IBMCloud storage classes are available. Environment Variable: CPD_SERVICE_STORAGE_CLASS Default Value: Auto determined if default storage classes are provided and available by your cloud provider. i.e ibmc-file for IBM Cloud, efs for AWS.","title":"cpd_service_storage_class"},{"location":"roles/cp4d_service/#cpd_service_block_storage_class","text":"This is used to set spec.blockStorageClass in all CPD services that uses block storage class (read-write-only RWO). Required , unless IBMCloud storage classes are available. Environment Variable: CPD_SERVICE_BLOCK_STORAGE_CLASS Default Value: Auto determined if default storage classes are provided and available by your cloud provider. i.e ibmc-block for IBM Cloud, gp2 for AWS.","title":"cpd_service_block_storage_class"},{"location":"roles/cp4d_service/#cpd_instance_namespace","text":"Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default Value: ibm-cpd","title":"cpd_instance_namespace"},{"location":"roles/cp4d_service/#cpd_operator_namespace","text":"Namespace where the CP4D instance is deployed. Optional Environment Variable: CPD_OPERATORS_NAMESPACE Default Value: ibm-cpd-operators","title":"cpd_operator_namespace"},{"location":"roles/cp4d_service/#cpd_admin_username","text":"The CP4D Admin username to authenticate with CP4D APIs. If you didn't change the initial admin username after installing CP4D then you don't need to provide this. Optional Environment Variable: CPD_ADMIN_USERNAME Default Value: admin (CPD 4.6) cpadmin (CPD 4.8 and 5.0)","title":"cpd_admin_username"},{"location":"roles/cp4d_service/#cpd_admin_password","text":"The CP4D Admin User password to call CP4D API to provision Discovery Instance. If you didn't change the initial admin password after CP4D install, you don't need to provide it. The initial admin user password for admin or cpdamin will be used. Optional Environment Variable: CPD_ADMIN_PASSWORD Default Value: CPD 4.6: Looked up from the admin-user-details secret in the cpd_instance_namespace namespace CPD 4.8 and 5.0: Looked up from the ibm-iam-bindinfo-platform-auth-idp-credentials secret in the cpd_instance_namespace namespace","title":"cpd_admin_password"},{"location":"roles/cp4d_service/#cpd_service_scale_config","text":"Adjust and scale the resources for your Cloud Pak for Data services to increase processing capacity. For more information, refer to Managing resources in IBM Cloud Pak for Data documentation. Optional Environment Variable: CPD_SERVICE_SCALE_CONFIG Default Value: small","title":"cpd_service_scale_config"},{"location":"roles/cp4d_service/#role-variables-watson-studio","text":"","title":"Role Variables - Watson Studio"},{"location":"roles/cp4d_service/#cpd_wsl_project_name","text":"Stores the CP4D Watson Studio Project name that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_NAME Default Value: wsl-mas-${mas_instance_id}-hputilities","title":"cpd_wsl_project_name"},{"location":"roles/cp4d_service/#cpd_wsl_project_description","text":"Optional - Stores the CP4D Watson Studio Project description that can be used to configure HP Utilities application in MAS. Optional, only supported when cpd_service_name = wsl Environment Variable: CPD_WSL_PROJECT_DESCRIPTION Default Value: Watson Studio Project for Maximo Application Suite","title":"cpd_wsl_project_description"},{"location":"roles/cp4d_service/#role-variables-watson-discovery","text":"","title":"Role Variables - Watson Discovery"},{"location":"roles/cp4d_service/#cpd_wd_instance_name","text":"Stores the name of the CP4D Watson Discovery Instance that can be used to configure Assist application in MAS. Optional, only supported when cpd_service_name = wd Environment Variable: CPD_WD_INSTANCE_NAME Default Value: wd-mas-${mas_instance_id}-assist","title":"cpd_wd_instance_name"},{"location":"roles/cp4d_service/#cpd_wd_deployment_type","text":"Defines the CP4D Watson Discovery deployment type: Starter : One replica pod for each wd service/component, uses fewer resources in your cluster. Production : Multiple replica pods for each Watson Discovery service/component, recommended for production deployments to increase workload capacity however consumes more cluster resources. Optional Environment Variable: CPD_WD_DEPLOYMENT_TYPE Default Value: Starter Note: Deployment type cannot be changed in the future neither while upgrading the service. If you need to change the deployment type, you must uninstall Watson Discovery and reinstall with the desired deployment type. More information, see Upgrading Watson Discovery .","title":"cpd_wd_deployment_type"},{"location":"roles/cp4d_service/#role-variables-mas-configuration-generation","text":"","title":"Role Variables - MAS Configuration Generation"},{"location":"roles/cp4d_service/#mas_instance_id","text":"The instance ID of Maximo Application Suite that a generated configuration will target. If this or mas_config_dir are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cp4d_service/#mas_config_dir","text":"Local directory to save the generated resource definition. This can be used to manually configure a MAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a resource template. Optional, only supported when cpd_service_name = wsl Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/cp4d_service/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true vars: cpd_product_version: 5.0.0 cpd_service_storage_class: ibmc-file-gold-gid cpd_service_name: wsl roles: - ibm.mas_devops.cp4d_service","title":"Example Playbook"},{"location":"roles/cp4d_service/#license","text":"EPL-2.0","title":"License"},{"location":"roles/db2/","text":"db2 \u00a4 This role creates or upgrades a Db2 instance using the Db2u Operator. When installing db2, the db2u operator will now be installed into the same namespace as the db2 instance ( db2ucluster ). If you already have db2 operator and db2 instances running in separate namespaces, this role will take care of migrating (by deleting & reinstalling) the db2 operators from ibm-common-services to the namespace defined by db2_namespace property (in case of a new role execution for a db2 install or db2 upgrade). A private root CA certificate is created and is used to secure the TLS connections to the database. A Db2 Warehouse cluster will be created along with a public TLS encrypted route to allow external access to the cluster (access is via the ssl-server nodeport port on the -db2u-engn-svc service). Internal access is via the -db2u-engn-svc service and port 50001. Both the external route and the internal service use the same server certificate. The private root CA certificate and the server certificate are available from the db2u-ca and db2u-certificate secrets in the db2 namespace. The default user is db2inst1 and the password is available in the instancepassword secret in the same namespace. You can examine the deployed resources in the db2 namespace. This example assumes the default namespace db2u : oc -n db2u get db2ucluster NAME STATE MAINTENANCESTATE AGE db2u-db01 Ready None 29m It typically takes 20-30 minutes from the db2ucluster being created till it is ready. If the db2ucluster is not ready after that period then check that all the PersistentVolumeClaims in the db2 namespace are ready and that the pods in the namespace are not stuck in init state. If the c-<db2_instance_name>-db2u-0 pod is running then you can exec into the pod and check the /var/log/db2u.log for any issue. If the mas_instance_id and mas_config_dir are provided then the role will generate the JdbcCfg yaml that can be used to configure MAS to connect to this database. It does not apply the yaml to the cluster but does provide you with the yaml files to apply if needed. When upgrading db2, specify the existing namespace where the db2uCluster instances exist. All the instances under that namespace will be upgraded to the db2 version specified. The version of db2 must match the channel of db2 being used for the upgrade. Role Variables - Installation \u00a4 common_services_namespace \u00a4 Namespace where IBM Common Services is installed. Optional Environment Variable: COMMON_SERVICES_NAMESPACE Default Value: ibm-common-services db2_action \u00a4 Inform the role whether to perform an install, upgrade, backup or restore of DB2 Database. This can be set to install , upgrade , backup or restore . When DB2_ACTION is set to upgrade, then all instances in the DB2_NAMESPACE will be upgraded to the DB2_VERSION . Optional Environment Variable: DB2_ACTION Default: install db2_namespace \u00a4 Name of the namespace where Db2 operators and Db2 instances (DB2UCluster custom resources) will be created Optional Environment Variable: DB2_NAMESPACE Default: db2u db2_channel \u00a4 The subscription channel for the DB2 Universal Operator. Optional Environment Variable: DB2_CHANNEL Default: The default channel, as defined in the operator package, will be used if this is not set. db2_instance_name \u00a4 Name of the database instance, note that this is the instance name . Required Environment Variable: DB2_INSTANCE_NAME Default: None ibm_entitlement_key \u00a4 Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None db2_dbname \u00a4 Name of the database within the instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB db2_version \u00a4 Version of the DB2 engine to be used while creating/upgrading the DB2 instances. Optional Environment Variable: DB2_VERSION Default: The default db2 engine version will be automatically defined to the latest version supported by the installed DB2 operator if this is not set. The DB2 engine versions supported by the installed DB2 operator are stored in db2u-release configmap under ibm-common-services namespace. db2_type \u00a4 Type of the DB2 instance. Available options are db2wh and db2oltp . Optional Environment Variable: DB2_TYPE Default: db2wh db2_timezone \u00a4 Server timezone code of the DB2 instance. If you want to align the same timezone with Manage's DB2 database, you also need to must also set MAS_APP_SETTINGS_SERVER_TIMEZONE variable to the same value. Optional Environment Variable: DB2_TIMEZONE Default: GMT db2_4k_device_support \u00a4 Whether 4K device support is turned on or not. Optional Environment Variable: DB2_4K_DEVICE_SUPPORT Default: ON db2_workload \u00a4 The workload profile of the db2 instance, possible values are PUREDATA_OLAP or ANALYTICS . Optional Environment Variable: DB2_WORKLOAD Default: ANALYTICS db2_table_org \u00a4 The way database tables will be organized. It can be set to either ROW or COLUMN . Optional Environment Variable: DB2_TABLE_ORG Default: ROW db2_ldap_username \u00a4 Define the username of db2 in the local LDAP registry. If this is defined, the LDAP user will be the user identity passed into the MAS JDBC configuration. Optional Environment Variable: DB2_LDAP_USERNAME Default: None db2_ldap_password \u00a4 Define the password of above db2 user in the local LDAP registry. Must define when db2_ldap_username is used. Optional Environment Variable: DB2_LDAP_PASSWORD Default: None db2_rotate_password \u00a4 Determines if the role should rotate the LDAP password for current LDAP user configured within Db2 for MAS. When using this capability, LDAP user password will auto generated by this role and configured with MAS. Optional Environment Variable: DB2_LDAP_ROTATE_PASSWORD Default: False Role Variables - Storage \u00a4 We recommend reviewing the Db2 documentation about the certified storage options for Db2 on Red Hat OpenShift. Please ensure your storage class meets the specified deployment requirements for Db2. https://www.ibm.com/docs/en/db2/11.5?topic=storage-certified-options db2_meta_storage_class \u00a4 Storage class used for metadata. This must support ReadWriteMany(RWX) access mode. Required Environment Variable: DB2_META_STORAGE_CLASS Default: Defaults to ibmc-file-gold if the storage class is available in the cluster. db2_meta_storage_size \u00a4 Size of the metadata persistent volume, in gigabytes Optional Environment Variable: DB2_META_STORAGE_SIZE Default: 20Gi db2_meta_storage_accessmode \u00a4 The access mode for the storage. Optional Environment Variable: DB2_META_STORAGE_ACCESSMODE Default: ReadWriteMany db2_data_storage_class \u00a4 Storage class used for user data. This must support ReadWriteMany(RWX) access mode. Required Environment Variable: DB2_DATA_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. db2_data_storage_size \u00a4 Size of data persistent volume. Optional Environment Variable: DB2_DATA_STORAGE_SIZE Default: 100Gi db2_data_storage_accessmode \u00a4 The access mode for the storage. Optional Environment Variable: DB2_DATA_STORAGE_ACCESSMODE Default: ReadWriteOnce db2_backup_storage_class \u00a4 Storage class used for backup. This must support ReadWriteMany(RWX) access mode. Optional Environment Variable: DB2_BACKUP_STORAGE_CLASS Default: Defaults to ibmc-file-gold if the storage class is available in the cluster. Set to None will drop the backup storage on DB2ucluster CR. db2_backup_storage_size \u00a4 Size of backup persistent volume. Optional Environment Variable: DB2_BACKUP_STORAGE_SIZE Default: 100Gi db2_backup_storage_accessmode \u00a4 The access mode for the storage. Optional Environment Variable: DB2_BACKUP_STORAGE_ACCESSMODE Default: ReadWriteMany db2_logs_storage_class \u00a4 Storage class used for transaction logs. This must support ReadWriteMany(RWX) access mode. Optional Environment Variable: DB2_LOGS_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. Set to None will drop the logs storage on DB2ucluster CR. db2_logs_storage_size \u00a4 Size of transaction logs persistent volume. Optional Environment Variable: DB2_LOGS_STORAGE_SIZE Default: 100Gi db2_logs_storage_accessmode \u00a4 The access mode for the storage. Optional Environment Variable: DB2_LOGS_STORAGE_ACCESSMODE Default: ReadWriteOnce db2_temp_storage_class \u00a4 Storage class used for temporary data. This must support ReadWriteMany(RWX) access mode. Optional Environment Variable: DB2_TEMP_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. Set to None will drop the tempts storage on DB2ucluster CR. db2_temp_storage_size \u00a4 Size of temporary persistent volume. Optional Environment Variable: DB2_TEMP_STORAGE_SIZE Default: 100Gi db2_temp_storage_accessmode \u00a4 The access mode for the storage. This must support ReadWriteOnce(RWO) access mode. Optional Environment Variable: DB2_TEMP_STORAGE_ACCESSMODE Default: ReadWriteOnce Role Variables - Resource Requests \u00a4 These variables allow you to customize the resources available to the Db2 pod in your cluster. In most circumstances you will want to set these properties because it's impossible for us to provide a default value that will be appropriate for all users. We have set defaults that are suitable for deploying Db2 onto a dedicated worker node with 4cpu and 16gb memory. Tip Note that you must take into account the system overhead on any given node when setting these parameters, if you set the requests equal to the number of CPU or amount of memory on your node then the scheduler will not be able to schedule the Db2 pod because not 100% of the worker nodes' resource will be available to pod on that node, even if there's only a single pod on it. Db2 is sensitive to both CPU and memory issues, particularly memory, we recommend setting requests and limits to the same values, ensuring the scheduler always reserves the resources that Db2 expects to be available to it. db2_cpu_requests \u00a4 Define the Kubernetes CPU request for the Db2 pod. Optional Environment Variable: DB2_CPU_REQUESTS Default: 4000m db2_cpu_limits \u00a4 Define the Kubernetes CPU limit for the Db2 pod. Optional Environment Variable: DB2_CPU_LIMITS Default: 6000m db2_memory_requests \u00a4 Define the Kubernetes memory request for the Db2 pod. Optional Environment Variable: DB2_MEMORY_REQUESTS Default: 8Gi db2_memory_limits \u00a4 Define the Kubernetes memory limit for the Db2 pod. Optional Environment Variable: DB2_MEMORY_LIMITS Default: 16Gi Role Variables - Node Label Affinity \u00a4 Specify both db2_affinity_key and db2_affinity_value to configure requiredDuringSchedulingIgnoredDuringExecution affinity with appropriately labelled nodes. db2_affinity_key \u00a4 Specify the key of a node label to declare affinity with. Optional Environment Variable: DB2_AFFINITY_KEY Default: None db2_affinity_value \u00a4 Specify the value of a node label to declare affinity with. Optional Environment Variable: DB2_AFFINITY_VALUE Default: None Role Variables - Node Taint Toleration \u00a4 Specify db2_tolerate_key , db2_tolerate_value , and db2_tolerate_effect to configure a toleration policy to allow the db2 instance to be scheduled on nodes with the specified taint. db2_tolerate_key \u00a4 Specify the key of the taint that is to be tolerated. Optional Environment Variable: DB2_TOLERATE_KEY Default: None db2_tolerate_value \u00a4 Specify the value of the taint that is to be tolerated. Optional Environment Variable: DB2_TOLERATE_VALUE Default: None db2_tolerate_effect \u00a4 Specify the type of taint effect that will be tolerated ( NoSchedule , PreferNoSchedule , or NoExecute ). Optional Environment Variable: DB2_TOLERATE_EFFECT Default: None Role Variables - DB2UCluster Database Configuration Settings \u00a4 The following variables will overwrite DB2UCluster default properties for the DB2 configuration sections: spec.environment.database.dbConfig spec.environment.instance.dbmConfig spec.environment.instance.registry db2_database_db_config \u00a4 Overwrites the db2ucluster database configuration settings under spec.environment.database.dbConfig section. - Optional - Environment Variable: DB2_DATABASE_DB_CONFIG - Default: None db2_instance_dbm_config \u00a4 Overwrites the db2ucluster instance database configuration settings under spec.environment.instance.dbmConfig section. Important Do not set instance_memory . The Db2 engine does not know Db2 is running inside a container, setting dbmConfig.INSTANCE_MEMORY: automatic will cause it to read the cgroups of the node and potentially go beyond the pod memory limit. Db2U has logic built in to use a normalized percentage that takes into account the memory limit and free memory of the node. Optional Environment Variable: DB2_INSTANCE_DBM_CONFIG Default: None db2_instance_registry \u00a4 Overwrites the db2ucluster instance database configuration settings under spec.environment.instance.registry section. You can define parameters to be included in this section using semicolon separated values. Optional Environment Variable: DB2_INSTANCE_REGISTRY Default: None Role Variables - MPP System \u00a4 Warning Do not use these variables if you intend to use the Db2 instance with IBM Maximo Application Suite; no MAS application supports Db2 MPP db2_mln_count \u00a4 The number of logical nodes (i.e. database partitions to create). Note: ensure that the application using this Db2 can support Db2 MPP (which is created when DB2_MLN_COUNT is greater than 1). Optional Environment Variable: 'DB2_MLN_COUNT Default: 1 db2_num_pods \u00a4 The number of Db2 pods to create in the instance. Note that db2_num_pods must be less than or equal to db2_mln_count . A single db2u pod can contain multiple logical nodes. So be sure to avoid specifying a large number for db2_mln_count while specifying a small number for db2_num_pods . If in doubt, make db2_mln_count = db2_num_pods . For more information refer to the Db2 documentation . Optional Environment Variable: 'DB2_NUM_PODS Default: 1 Role Variables - MAS Configuration \u00a4 mas_instance_id \u00a4 Providing this and mas_config_dir will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_INSTANCE_ID Default: None mas_config_dir \u00a4 Providing this and mas_instance_id will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_CONFIG_DIR Default: None mas_config_scope \u00a4 Supported values are system , ws , app , or wsapp , this is only used when both mas_config_dir and mas_instance_id are set. Optional Environment Variable: MAS_CONFIG_SCOPE Default: system mas_workspace_id \u00a4 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Optional Environment Variable: MAS_WORKSPACE_ID Default: None mas_application_id \u00a4 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either app or wsapp Optional Environment Variable: 'MAS_APP_ID Default: None Role Variables - Backup and Restore \u00a4 masbr_confirm_cluster \u00a4 Set true or false to indicate the role whether to confirm the currently connected cluster before running the backup or restore job. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false masbr_copy_timeout_sec \u00a4 Set the transfer files timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) masbr_job_timezone \u00a4 Set the time zone for creating scheduled backup job. If not set a value for this variable, this role will use UTC time zone when creating a CronJob for running scheduled backup job. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None masbr_storage_type \u00a4 Set local or cloud to indicate this role to save the backup files to local file system or cloud object storage. Required Environment Variable: MASBR_STORAGE_TYPE Default: None masbr_storage_local_folder \u00a4 Set local path to save the backup files. Required only when MASBR_STORAGE_TYPE=local Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None masbr_storage_cloud_rclone_file \u00a4 Set the path of rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_FILE Default: None masbr_storage_cloud_rclone_name \u00a4 Set the configuration name defined in rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_NAME Default: None masbr_storage_cloud_bucket \u00a4 Set the object storage bucket name for saving the backup files Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_BUCKET Default: None masbr_slack_enabled \u00a4 Set true or false to indicate whether this role will send Slack notification messages of the backup and restore progress. Optional Environment Variable: MASBR_SLACK_ENABLED Default: false masbr_slack_level \u00a4 Set failure , info or verbose to indicate this role to send Slack notification messages in which backup and resore phases: Slack level Backup/Restore phases failure Failed , PartiallyFailed info Completed , Failed , PartiallyFailed verbose InProgress , Completed , Failed , PartiallyFailed Optional Environment Variable: MASBR_SLACK_LEVEL Default: info masbr_slack_token \u00a4 The Slack integration token. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_TOKEN Default: None masbr_slack_channel \u00a4 The Slack channel to send the notification messages to. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_CHANNEL Default: None masbr_slack_user \u00a4 The sender of the Slack notification message. Optional Environment Variable: MASBR_SLACK_USER Default: MASBR masbr_backup_type \u00a4 Set full or incr to indicate the role to create a full backup or incremental backup. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full masbr_backup_from_version \u00a4 Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). This variable is only valid when MASBR_BACKUP_TYPE=incr . If not set a value for this variable, this role will try to find the latest full backup version from the specified storage location. Optional Environment Variable: MASBR_BACKUP_FROM_VERSION Default: None masbr_backup_schedule \u00a4 Set Cron expression to create a scheduled backup. If not set a value for this varialbe, this role will create an on-demand backup. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None masbr_restore_from_version \u00a4 Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) Required only when DB2_ACTION=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Example Playbook \u00a4 Install Db2 \u00a4 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxxx # Configuration for the Db2 cluster db2_instance_name: db2u-db01 db2_meta_storage_class: \"ibmc-file-gold\" db2_data_storage_class: \"ibmc-block-gold\" db2_backup_storage_class: \"ibmc-file-gold\" db2_logs_storage_class: \"ibmc-block-gold\" db2_temp_storage_class: \"ibmc-block-gold\" # Create the MAS JdbcCfg & Secret resource definitions mas_instance_id: inst1 mas_config_dir: /home/david/masconfig roles: - ibm.mas_devops.db2 Backup Db2 \u00a4 - hosts: localhost any_errors_fatal: true vars: db2_action: backup db2_instance_name: db2u-db01 masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.db2 Restore Db2 \u00a4 - hosts: localhost any_errors_fatal: true vars: db2_action: restore db2_instance_name: db2u-db01 masbr_restore_from_version: 20240621021316 masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.db2 License \u00a4 EPL-2.0","title":"db2"},{"location":"roles/db2/#db2","text":"This role creates or upgrades a Db2 instance using the Db2u Operator. When installing db2, the db2u operator will now be installed into the same namespace as the db2 instance ( db2ucluster ). If you already have db2 operator and db2 instances running in separate namespaces, this role will take care of migrating (by deleting & reinstalling) the db2 operators from ibm-common-services to the namespace defined by db2_namespace property (in case of a new role execution for a db2 install or db2 upgrade). A private root CA certificate is created and is used to secure the TLS connections to the database. A Db2 Warehouse cluster will be created along with a public TLS encrypted route to allow external access to the cluster (access is via the ssl-server nodeport port on the -db2u-engn-svc service). Internal access is via the -db2u-engn-svc service and port 50001. Both the external route and the internal service use the same server certificate. The private root CA certificate and the server certificate are available from the db2u-ca and db2u-certificate secrets in the db2 namespace. The default user is db2inst1 and the password is available in the instancepassword secret in the same namespace. You can examine the deployed resources in the db2 namespace. This example assumes the default namespace db2u : oc -n db2u get db2ucluster NAME STATE MAINTENANCESTATE AGE db2u-db01 Ready None 29m It typically takes 20-30 minutes from the db2ucluster being created till it is ready. If the db2ucluster is not ready after that period then check that all the PersistentVolumeClaims in the db2 namespace are ready and that the pods in the namespace are not stuck in init state. If the c-<db2_instance_name>-db2u-0 pod is running then you can exec into the pod and check the /var/log/db2u.log for any issue. If the mas_instance_id and mas_config_dir are provided then the role will generate the JdbcCfg yaml that can be used to configure MAS to connect to this database. It does not apply the yaml to the cluster but does provide you with the yaml files to apply if needed. When upgrading db2, specify the existing namespace where the db2uCluster instances exist. All the instances under that namespace will be upgraded to the db2 version specified. The version of db2 must match the channel of db2 being used for the upgrade.","title":"db2"},{"location":"roles/db2/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/db2/#common_services_namespace","text":"Namespace where IBM Common Services is installed. Optional Environment Variable: COMMON_SERVICES_NAMESPACE Default Value: ibm-common-services","title":"common_services_namespace"},{"location":"roles/db2/#db2_action","text":"Inform the role whether to perform an install, upgrade, backup or restore of DB2 Database. This can be set to install , upgrade , backup or restore . When DB2_ACTION is set to upgrade, then all instances in the DB2_NAMESPACE will be upgraded to the DB2_VERSION . Optional Environment Variable: DB2_ACTION Default: install","title":"db2_action"},{"location":"roles/db2/#db2_namespace","text":"Name of the namespace where Db2 operators and Db2 instances (DB2UCluster custom resources) will be created Optional Environment Variable: DB2_NAMESPACE Default: db2u","title":"db2_namespace"},{"location":"roles/db2/#db2_channel","text":"The subscription channel for the DB2 Universal Operator. Optional Environment Variable: DB2_CHANNEL Default: The default channel, as defined in the operator package, will be used if this is not set.","title":"db2_channel"},{"location":"roles/db2/#db2_instance_name","text":"Name of the database instance, note that this is the instance name . Required Environment Variable: DB2_INSTANCE_NAME Default: None","title":"db2_instance_name"},{"location":"roles/db2/#ibm_entitlement_key","text":"Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None","title":"ibm_entitlement_key"},{"location":"roles/db2/#db2_dbname","text":"Name of the database within the instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB","title":"db2_dbname"},{"location":"roles/db2/#db2_version","text":"Version of the DB2 engine to be used while creating/upgrading the DB2 instances. Optional Environment Variable: DB2_VERSION Default: The default db2 engine version will be automatically defined to the latest version supported by the installed DB2 operator if this is not set. The DB2 engine versions supported by the installed DB2 operator are stored in db2u-release configmap under ibm-common-services namespace.","title":"db2_version"},{"location":"roles/db2/#db2_type","text":"Type of the DB2 instance. Available options are db2wh and db2oltp . Optional Environment Variable: DB2_TYPE Default: db2wh","title":"db2_type"},{"location":"roles/db2/#db2_timezone","text":"Server timezone code of the DB2 instance. If you want to align the same timezone with Manage's DB2 database, you also need to must also set MAS_APP_SETTINGS_SERVER_TIMEZONE variable to the same value. Optional Environment Variable: DB2_TIMEZONE Default: GMT","title":"db2_timezone"},{"location":"roles/db2/#db2_4k_device_support","text":"Whether 4K device support is turned on or not. Optional Environment Variable: DB2_4K_DEVICE_SUPPORT Default: ON","title":"db2_4k_device_support"},{"location":"roles/db2/#db2_workload","text":"The workload profile of the db2 instance, possible values are PUREDATA_OLAP or ANALYTICS . Optional Environment Variable: DB2_WORKLOAD Default: ANALYTICS","title":"db2_workload"},{"location":"roles/db2/#db2_table_org","text":"The way database tables will be organized. It can be set to either ROW or COLUMN . Optional Environment Variable: DB2_TABLE_ORG Default: ROW","title":"db2_table_org"},{"location":"roles/db2/#db2_ldap_username","text":"Define the username of db2 in the local LDAP registry. If this is defined, the LDAP user will be the user identity passed into the MAS JDBC configuration. Optional Environment Variable: DB2_LDAP_USERNAME Default: None","title":"db2_ldap_username"},{"location":"roles/db2/#db2_ldap_password","text":"Define the password of above db2 user in the local LDAP registry. Must define when db2_ldap_username is used. Optional Environment Variable: DB2_LDAP_PASSWORD Default: None","title":"db2_ldap_password"},{"location":"roles/db2/#db2_rotate_password","text":"Determines if the role should rotate the LDAP password for current LDAP user configured within Db2 for MAS. When using this capability, LDAP user password will auto generated by this role and configured with MAS. Optional Environment Variable: DB2_LDAP_ROTATE_PASSWORD Default: False","title":"db2_rotate_password"},{"location":"roles/db2/#role-variables-storage","text":"We recommend reviewing the Db2 documentation about the certified storage options for Db2 on Red Hat OpenShift. Please ensure your storage class meets the specified deployment requirements for Db2. https://www.ibm.com/docs/en/db2/11.5?topic=storage-certified-options","title":"Role Variables - Storage"},{"location":"roles/db2/#db2_meta_storage_class","text":"Storage class used for metadata. This must support ReadWriteMany(RWX) access mode. Required Environment Variable: DB2_META_STORAGE_CLASS Default: Defaults to ibmc-file-gold if the storage class is available in the cluster.","title":"db2_meta_storage_class"},{"location":"roles/db2/#db2_meta_storage_size","text":"Size of the metadata persistent volume, in gigabytes Optional Environment Variable: DB2_META_STORAGE_SIZE Default: 20Gi","title":"db2_meta_storage_size"},{"location":"roles/db2/#db2_meta_storage_accessmode","text":"The access mode for the storage. Optional Environment Variable: DB2_META_STORAGE_ACCESSMODE Default: ReadWriteMany","title":"db2_meta_storage_accessmode"},{"location":"roles/db2/#db2_data_storage_class","text":"Storage class used for user data. This must support ReadWriteMany(RWX) access mode. Required Environment Variable: DB2_DATA_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster.","title":"db2_data_storage_class"},{"location":"roles/db2/#db2_data_storage_size","text":"Size of data persistent volume. Optional Environment Variable: DB2_DATA_STORAGE_SIZE Default: 100Gi","title":"db2_data_storage_size"},{"location":"roles/db2/#db2_data_storage_accessmode","text":"The access mode for the storage. Optional Environment Variable: DB2_DATA_STORAGE_ACCESSMODE Default: ReadWriteOnce","title":"db2_data_storage_accessmode"},{"location":"roles/db2/#db2_backup_storage_class","text":"Storage class used for backup. This must support ReadWriteMany(RWX) access mode. Optional Environment Variable: DB2_BACKUP_STORAGE_CLASS Default: Defaults to ibmc-file-gold if the storage class is available in the cluster. Set to None will drop the backup storage on DB2ucluster CR.","title":"db2_backup_storage_class"},{"location":"roles/db2/#db2_backup_storage_size","text":"Size of backup persistent volume. Optional Environment Variable: DB2_BACKUP_STORAGE_SIZE Default: 100Gi","title":"db2_backup_storage_size"},{"location":"roles/db2/#db2_backup_storage_accessmode","text":"The access mode for the storage. Optional Environment Variable: DB2_BACKUP_STORAGE_ACCESSMODE Default: ReadWriteMany","title":"db2_backup_storage_accessmode"},{"location":"roles/db2/#db2_logs_storage_class","text":"Storage class used for transaction logs. This must support ReadWriteMany(RWX) access mode. Optional Environment Variable: DB2_LOGS_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. Set to None will drop the logs storage on DB2ucluster CR.","title":"db2_logs_storage_class"},{"location":"roles/db2/#db2_logs_storage_size","text":"Size of transaction logs persistent volume. Optional Environment Variable: DB2_LOGS_STORAGE_SIZE Default: 100Gi","title":"db2_logs_storage_size"},{"location":"roles/db2/#db2_logs_storage_accessmode","text":"The access mode for the storage. Optional Environment Variable: DB2_LOGS_STORAGE_ACCESSMODE Default: ReadWriteOnce","title":"db2_logs_storage_accessmode"},{"location":"roles/db2/#db2_temp_storage_class","text":"Storage class used for temporary data. This must support ReadWriteMany(RWX) access mode. Optional Environment Variable: DB2_TEMP_STORAGE_CLASS Default: Defaults to ibmc-block-gold if the storage class is available in the cluster. Set to None will drop the tempts storage on DB2ucluster CR.","title":"db2_temp_storage_class"},{"location":"roles/db2/#db2_temp_storage_size","text":"Size of temporary persistent volume. Optional Environment Variable: DB2_TEMP_STORAGE_SIZE Default: 100Gi","title":"db2_temp_storage_size"},{"location":"roles/db2/#db2_temp_storage_accessmode","text":"The access mode for the storage. This must support ReadWriteOnce(RWO) access mode. Optional Environment Variable: DB2_TEMP_STORAGE_ACCESSMODE Default: ReadWriteOnce","title":"db2_temp_storage_accessmode"},{"location":"roles/db2/#role-variables-resource-requests","text":"These variables allow you to customize the resources available to the Db2 pod in your cluster. In most circumstances you will want to set these properties because it's impossible for us to provide a default value that will be appropriate for all users. We have set defaults that are suitable for deploying Db2 onto a dedicated worker node with 4cpu and 16gb memory. Tip Note that you must take into account the system overhead on any given node when setting these parameters, if you set the requests equal to the number of CPU or amount of memory on your node then the scheduler will not be able to schedule the Db2 pod because not 100% of the worker nodes' resource will be available to pod on that node, even if there's only a single pod on it. Db2 is sensitive to both CPU and memory issues, particularly memory, we recommend setting requests and limits to the same values, ensuring the scheduler always reserves the resources that Db2 expects to be available to it.","title":"Role Variables - Resource Requests"},{"location":"roles/db2/#db2_cpu_requests","text":"Define the Kubernetes CPU request for the Db2 pod. Optional Environment Variable: DB2_CPU_REQUESTS Default: 4000m","title":"db2_cpu_requests"},{"location":"roles/db2/#db2_cpu_limits","text":"Define the Kubernetes CPU limit for the Db2 pod. Optional Environment Variable: DB2_CPU_LIMITS Default: 6000m","title":"db2_cpu_limits"},{"location":"roles/db2/#db2_memory_requests","text":"Define the Kubernetes memory request for the Db2 pod. Optional Environment Variable: DB2_MEMORY_REQUESTS Default: 8Gi","title":"db2_memory_requests"},{"location":"roles/db2/#db2_memory_limits","text":"Define the Kubernetes memory limit for the Db2 pod. Optional Environment Variable: DB2_MEMORY_LIMITS Default: 16Gi","title":"db2_memory_limits"},{"location":"roles/db2/#role-variables-node-label-affinity","text":"Specify both db2_affinity_key and db2_affinity_value to configure requiredDuringSchedulingIgnoredDuringExecution affinity with appropriately labelled nodes.","title":"Role Variables - Node Label Affinity"},{"location":"roles/db2/#db2_affinity_key","text":"Specify the key of a node label to declare affinity with. Optional Environment Variable: DB2_AFFINITY_KEY Default: None","title":"db2_affinity_key"},{"location":"roles/db2/#db2_affinity_value","text":"Specify the value of a node label to declare affinity with. Optional Environment Variable: DB2_AFFINITY_VALUE Default: None","title":"db2_affinity_value"},{"location":"roles/db2/#role-variables-node-taint-toleration","text":"Specify db2_tolerate_key , db2_tolerate_value , and db2_tolerate_effect to configure a toleration policy to allow the db2 instance to be scheduled on nodes with the specified taint.","title":"Role Variables - Node Taint Toleration"},{"location":"roles/db2/#db2_tolerate_key","text":"Specify the key of the taint that is to be tolerated. Optional Environment Variable: DB2_TOLERATE_KEY Default: None","title":"db2_tolerate_key"},{"location":"roles/db2/#db2_tolerate_value","text":"Specify the value of the taint that is to be tolerated. Optional Environment Variable: DB2_TOLERATE_VALUE Default: None","title":"db2_tolerate_value"},{"location":"roles/db2/#db2_tolerate_effect","text":"Specify the type of taint effect that will be tolerated ( NoSchedule , PreferNoSchedule , or NoExecute ). Optional Environment Variable: DB2_TOLERATE_EFFECT Default: None","title":"db2_tolerate_effect"},{"location":"roles/db2/#role-variables-db2ucluster-database-configuration-settings","text":"The following variables will overwrite DB2UCluster default properties for the DB2 configuration sections: spec.environment.database.dbConfig spec.environment.instance.dbmConfig spec.environment.instance.registry","title":"Role Variables - DB2UCluster Database Configuration Settings"},{"location":"roles/db2/#db2_database_db_config","text":"Overwrites the db2ucluster database configuration settings under spec.environment.database.dbConfig section. - Optional - Environment Variable: DB2_DATABASE_DB_CONFIG - Default: None","title":"db2_database_db_config"},{"location":"roles/db2/#db2_instance_dbm_config","text":"Overwrites the db2ucluster instance database configuration settings under spec.environment.instance.dbmConfig section. Important Do not set instance_memory . The Db2 engine does not know Db2 is running inside a container, setting dbmConfig.INSTANCE_MEMORY: automatic will cause it to read the cgroups of the node and potentially go beyond the pod memory limit. Db2U has logic built in to use a normalized percentage that takes into account the memory limit and free memory of the node. Optional Environment Variable: DB2_INSTANCE_DBM_CONFIG Default: None","title":"db2_instance_dbm_config"},{"location":"roles/db2/#db2_instance_registry","text":"Overwrites the db2ucluster instance database configuration settings under spec.environment.instance.registry section. You can define parameters to be included in this section using semicolon separated values. Optional Environment Variable: DB2_INSTANCE_REGISTRY Default: None","title":"db2_instance_registry"},{"location":"roles/db2/#role-variables-mpp-system","text":"Warning Do not use these variables if you intend to use the Db2 instance with IBM Maximo Application Suite; no MAS application supports Db2 MPP","title":"Role Variables - MPP System"},{"location":"roles/db2/#db2_mln_count","text":"The number of logical nodes (i.e. database partitions to create). Note: ensure that the application using this Db2 can support Db2 MPP (which is created when DB2_MLN_COUNT is greater than 1). Optional Environment Variable: 'DB2_MLN_COUNT Default: 1","title":"db2_mln_count"},{"location":"roles/db2/#db2_num_pods","text":"The number of Db2 pods to create in the instance. Note that db2_num_pods must be less than or equal to db2_mln_count . A single db2u pod can contain multiple logical nodes. So be sure to avoid specifying a large number for db2_mln_count while specifying a small number for db2_num_pods . If in doubt, make db2_mln_count = db2_num_pods . For more information refer to the Db2 documentation . Optional Environment Variable: 'DB2_NUM_PODS Default: 1","title":"db2_num_pods"},{"location":"roles/db2/#role-variables-mas-configuration","text":"","title":"Role Variables - MAS Configuration"},{"location":"roles/db2/#mas_instance_id","text":"Providing this and mas_config_dir will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/db2/#mas_config_dir","text":"Providing this and mas_instance_id will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Optional Environment Variable: MAS_CONFIG_DIR Default: None","title":"mas_config_dir"},{"location":"roles/db2/#mas_config_scope","text":"Supported values are system , ws , app , or wsapp , this is only used when both mas_config_dir and mas_instance_id are set. Optional Environment Variable: MAS_CONFIG_SCOPE Default: system","title":"mas_config_scope"},{"location":"roles/db2/#mas_workspace_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Optional Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/db2/#mas_application_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either app or wsapp Optional Environment Variable: 'MAS_APP_ID Default: None","title":"mas_application_id"},{"location":"roles/db2/#role-variables-backup-and-restore","text":"","title":"Role Variables - Backup and Restore"},{"location":"roles/db2/#masbr_confirm_cluster","text":"Set true or false to indicate the role whether to confirm the currently connected cluster before running the backup or restore job. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false","title":"masbr_confirm_cluster"},{"location":"roles/db2/#masbr_copy_timeout_sec","text":"Set the transfer files timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours)","title":"masbr_copy_timeout_sec"},{"location":"roles/db2/#masbr_job_timezone","text":"Set the time zone for creating scheduled backup job. If not set a value for this variable, this role will use UTC time zone when creating a CronJob for running scheduled backup job. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None","title":"masbr_job_timezone"},{"location":"roles/db2/#masbr_storage_type","text":"Set local or cloud to indicate this role to save the backup files to local file system or cloud object storage. Required Environment Variable: MASBR_STORAGE_TYPE Default: None","title":"masbr_storage_type"},{"location":"roles/db2/#masbr_storage_local_folder","text":"Set local path to save the backup files. Required only when MASBR_STORAGE_TYPE=local Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None","title":"masbr_storage_local_folder"},{"location":"roles/db2/#masbr_storage_cloud_rclone_file","text":"Set the path of rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_FILE Default: None","title":"masbr_storage_cloud_rclone_file"},{"location":"roles/db2/#masbr_storage_cloud_rclone_name","text":"Set the configuration name defined in rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_NAME Default: None","title":"masbr_storage_cloud_rclone_name"},{"location":"roles/db2/#masbr_storage_cloud_bucket","text":"Set the object storage bucket name for saving the backup files Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_BUCKET Default: None","title":"masbr_storage_cloud_bucket"},{"location":"roles/db2/#masbr_slack_enabled","text":"Set true or false to indicate whether this role will send Slack notification messages of the backup and restore progress. Optional Environment Variable: MASBR_SLACK_ENABLED Default: false","title":"masbr_slack_enabled"},{"location":"roles/db2/#masbr_slack_level","text":"Set failure , info or verbose to indicate this role to send Slack notification messages in which backup and resore phases: Slack level Backup/Restore phases failure Failed , PartiallyFailed info Completed , Failed , PartiallyFailed verbose InProgress , Completed , Failed , PartiallyFailed Optional Environment Variable: MASBR_SLACK_LEVEL Default: info","title":"masbr_slack_level"},{"location":"roles/db2/#masbr_slack_token","text":"The Slack integration token. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_TOKEN Default: None","title":"masbr_slack_token"},{"location":"roles/db2/#masbr_slack_channel","text":"The Slack channel to send the notification messages to. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_CHANNEL Default: None","title":"masbr_slack_channel"},{"location":"roles/db2/#masbr_slack_user","text":"The sender of the Slack notification message. Optional Environment Variable: MASBR_SLACK_USER Default: MASBR","title":"masbr_slack_user"},{"location":"roles/db2/#masbr_backup_type","text":"Set full or incr to indicate the role to create a full backup or incremental backup. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full","title":"masbr_backup_type"},{"location":"roles/db2/#masbr_backup_from_version","text":"Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). This variable is only valid when MASBR_BACKUP_TYPE=incr . If not set a value for this variable, this role will try to find the latest full backup version from the specified storage location. Optional Environment Variable: MASBR_BACKUP_FROM_VERSION Default: None","title":"masbr_backup_from_version"},{"location":"roles/db2/#masbr_backup_schedule","text":"Set Cron expression to create a scheduled backup. If not set a value for this varialbe, this role will create an on-demand backup. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None","title":"masbr_backup_schedule"},{"location":"roles/db2/#masbr_restore_from_version","text":"Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) Required only when DB2_ACTION=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None","title":"masbr_restore_from_version"},{"location":"roles/db2/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/db2/#install-db2","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxxx # Configuration for the Db2 cluster db2_instance_name: db2u-db01 db2_meta_storage_class: \"ibmc-file-gold\" db2_data_storage_class: \"ibmc-block-gold\" db2_backup_storage_class: \"ibmc-file-gold\" db2_logs_storage_class: \"ibmc-block-gold\" db2_temp_storage_class: \"ibmc-block-gold\" # Create the MAS JdbcCfg & Secret resource definitions mas_instance_id: inst1 mas_config_dir: /home/david/masconfig roles: - ibm.mas_devops.db2","title":"Install Db2"},{"location":"roles/db2/#backup-db2","text":"- hosts: localhost any_errors_fatal: true vars: db2_action: backup db2_instance_name: db2u-db01 masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.db2","title":"Backup Db2"},{"location":"roles/db2/#restore-db2","text":"- hosts: localhost any_errors_fatal: true vars: db2_action: restore db2_instance_name: db2u-db01 masbr_restore_from_version: 20240621021316 masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.db2","title":"Restore Db2"},{"location":"roles/db2/#license","text":"EPL-2.0","title":"License"},{"location":"roles/dro/","text":"dro [Data Reporter Operator] \u00a4 DRO will be supported on the following MAS versions - MAS 8.10.6 + - MAS 8.11.2 + - MAS 9.0 + Installs Data Reporter Operator in the redhat-marketplace namespace. If mas_instance_id and the others associated parameters are provided then the role will also generate a configuration file that can be directly applied to IBM Maximo Application Suite. Role Variables - Installation \u00a4 dro_action \u00a4 Inform the role whether to perform an install or uninstall of Data Reporter Operator. Supported values are install-dro and uninstall . Optional Environment Variable: DRO_ACTION Default: install-dro Note The install verb for dro_action is chosen to avoid conflict with the existing uds_action variable from the uds role ( install ) to ease migration from UDS to DRO, this allows the value of uds_action and dro_action to be set once and provide clarity around which dependency should be installed. The uninstall action works across both uds and dro roles. dro_namespace \u00a4 DRO can be installed on a different namespace, on certain type of OCP clusters where redhat* namespaces have restricted access, User can configure and install DRO on a custom namespace of their choosing by supplying a name using DRO_NAMESPACE Environment Variable: DRO_NAMESPACE Default Value: redhat-marketplace dro_migration \u00a4 To migrate from IBM User Data Services to ibm-data-reporter , set DRO_MIGRATION variable to True . Environment Variable: DRO_MIGRATION Default Value: False ibm_entitlement_key \u00a4 Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None dro_storage_class \u00a4 Required. Storage class where DRO will be installed. MAS ansible playbooks will automatically try to determine a RWO (Read Write Once) storage class from a cluster if DRO_STORAGE_CLASS is not supplied. If a cluster is setup with a customized storage solution, please provide a valid RWO storage class name using DRO_STORAGE_CLASS. Optional Environment Variable: DRO_STORAGE_CLASS Default Value: None Note : The storage class must support the RWO(Read Write Once) access Mode Role Variables - BASCfg Generation \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the BasCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated BasCfg resource definition. This can be used to manually configure a MAS instance to connect to BAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None dro_endpoint_url \u00a4 DRO url from ibm-data-reporter route found in redhat-marketplace namespace, this variable is needed if you wish to connect to an existing DRO instance. Optional Environment Variable: DRO_ENDPOINT_URL Default Value: None dro_api_key \u00a4 DRO api_key is a token obtained from ibm-data-reporter-operator-api-token secret found in redhat-marketplace namespace, this variable is needed if you wish to connect to an existing DRO instance. Optional Environment Variable: DRO_APIKEY Default Value: None dro_crt_path \u00a4 DRO uses default OCP cluster ingress certificates. these can be obtained from either router-certs-default secret found in openshift-ingress namespace or trustedCA config map found in openshift-config namespace, copy the contents of tls.crt into a .pem file and provide the filepath of the .pem file to DRO_CERTIFICATE_PATH , this variable is needed if you wish to connect to an existing DRO instance. Optional Environment Variable: DRO_CERTIFICATE_PATH Default Value: None dro_contact.email \u00a4 Sets the Contact e-mail address used by the MAS instance's DRO configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: DRO_CONTACT_EMAIL Default Value: None dro_contact.first_name \u00a4 Sets the Contact first name used by the MAS instance's DRO configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: DRO_CONTACT_FIRSTNAME Default Value: None dro_contact.last_name \u00a4 Sets the Contact last name used by the MAS instance's DRO configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: DRO_CONTACT_LASTNAME Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None mas_pod_templates_dir \u00a4 Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-bascfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the BasCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None include_cluster_ingress_cert_chain \u00a4 Optional. When set to True , includes the complete certificates chain in the generated MAS configuration, when a trusted certificate authority is found in your cluster's ingress. Optional Environment Variable: INCLUDE_CLUSTER_INGRESS_CERT_CHAIN Default: False Example Playbook \u00a4 Install in-cluster and generate MAS configuration \u00a4 To install DRO export IBM_ENTITLEMENT_KEY=<valid ibm entitlement key> export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export DRO_ACTION=install-dro export MAS_CONFIG_DIR=<valid local path to the config folder> export MAS_INSTANCE_ID=<valid mas instance id> export DRO_STORAGE_CLASS=<valid storage class name> export ROLE_NAME='dro' export DRO_NAMESPACE=ibm-dro ansible-playbook playbooks/run_role.yml To connect to an existing DRO export DRO_ENDPOINT_URL=<valid DRO url> export DRO_APIKEY=<valid DRO apikey> export DRO_CERTIFICATE_PATH=/temp/cert.pem export IBM_ENTITLEMENT_KEY=<valid ibm entitlement key> export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export MAS_CONFIG_DIR=<valid local path to the config folder> export MAS_INSTANCE_ID=<valid mas instance id> export DRO_ACTION=install-dro export ROLE_NAME='dro' ansible-playbook playbooks/run_role.yml To uninstall DRO export DRO_ACTION=uninstall export ROLE_NAME='dro' export DRO_NAMESPACE=ibm-dro ansible-playbook playbooks/run_role.yml - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig dro_contact: email: 'john@email.com' first_name: 'john' last_name: 'winter' roles: - ibm.mas_devops.dro License \u00a4 EPL-2.0","title":"dro"},{"location":"roles/dro/#dro-data-reporter-operator","text":"DRO will be supported on the following MAS versions - MAS 8.10.6 + - MAS 8.11.2 + - MAS 9.0 + Installs Data Reporter Operator in the redhat-marketplace namespace. If mas_instance_id and the others associated parameters are provided then the role will also generate a configuration file that can be directly applied to IBM Maximo Application Suite.","title":"dro [Data Reporter Operator]"},{"location":"roles/dro/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/dro/#dro_action","text":"Inform the role whether to perform an install or uninstall of Data Reporter Operator. Supported values are install-dro and uninstall . Optional Environment Variable: DRO_ACTION Default: install-dro Note The install verb for dro_action is chosen to avoid conflict with the existing uds_action variable from the uds role ( install ) to ease migration from UDS to DRO, this allows the value of uds_action and dro_action to be set once and provide clarity around which dependency should be installed. The uninstall action works across both uds and dro roles.","title":"dro_action"},{"location":"roles/dro/#dro_namespace","text":"DRO can be installed on a different namespace, on certain type of OCP clusters where redhat* namespaces have restricted access, User can configure and install DRO on a custom namespace of their choosing by supplying a name using DRO_NAMESPACE Environment Variable: DRO_NAMESPACE Default Value: redhat-marketplace","title":"dro_namespace"},{"location":"roles/dro/#dro_migration","text":"To migrate from IBM User Data Services to ibm-data-reporter , set DRO_MIGRATION variable to True . Environment Variable: DRO_MIGRATION Default Value: False","title":"dro_migration"},{"location":"roles/dro/#ibm_entitlement_key","text":"Provide your IBM entitlement key . Required Environment Variable: IBM_ENTITLEMENT_KEY Default: None","title":"ibm_entitlement_key"},{"location":"roles/dro/#dro_storage_class","text":"Required. Storage class where DRO will be installed. MAS ansible playbooks will automatically try to determine a RWO (Read Write Once) storage class from a cluster if DRO_STORAGE_CLASS is not supplied. If a cluster is setup with a customized storage solution, please provide a valid RWO storage class name using DRO_STORAGE_CLASS. Optional Environment Variable: DRO_STORAGE_CLASS Default Value: None Note : The storage class must support the RWO(Read Write Once) access Mode","title":"dro_storage_class"},{"location":"roles/dro/#role-variables-bascfg-generation","text":"","title":"Role Variables - BASCfg Generation"},{"location":"roles/dro/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the BasCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/dro/#mas_config_dir","text":"Local directory to save the generated BasCfg resource definition. This can be used to manually configure a MAS instance to connect to BAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/dro/#dro_endpoint_url","text":"DRO url from ibm-data-reporter route found in redhat-marketplace namespace, this variable is needed if you wish to connect to an existing DRO instance. Optional Environment Variable: DRO_ENDPOINT_URL Default Value: None","title":"dro_endpoint_url"},{"location":"roles/dro/#dro_api_key","text":"DRO api_key is a token obtained from ibm-data-reporter-operator-api-token secret found in redhat-marketplace namespace, this variable is needed if you wish to connect to an existing DRO instance. Optional Environment Variable: DRO_APIKEY Default Value: None","title":"dro_api_key"},{"location":"roles/dro/#dro_crt_path","text":"DRO uses default OCP cluster ingress certificates. these can be obtained from either router-certs-default secret found in openshift-ingress namespace or trustedCA config map found in openshift-config namespace, copy the contents of tls.crt into a .pem file and provide the filepath of the .pem file to DRO_CERTIFICATE_PATH , this variable is needed if you wish to connect to an existing DRO instance. Optional Environment Variable: DRO_CERTIFICATE_PATH Default Value: None","title":"dro_crt_path"},{"location":"roles/dro/#dro_contactemail","text":"Sets the Contact e-mail address used by the MAS instance's DRO configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: DRO_CONTACT_EMAIL Default Value: None","title":"dro_contact.email"},{"location":"roles/dro/#dro_contactfirst_name","text":"Sets the Contact first name used by the MAS instance's DRO configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: DRO_CONTACT_FIRSTNAME Default Value: None","title":"dro_contact.first_name"},{"location":"roles/dro/#dro_contactlast_name","text":"Sets the Contact last name used by the MAS instance's DRO configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: DRO_CONTACT_LASTNAME Default Value: None","title":"dro_contact.last_name"},{"location":"roles/dro/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/dro/#mas_pod_templates_dir","text":"Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-bascfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the BasCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None","title":"mas_pod_templates_dir"},{"location":"roles/dro/#include_cluster_ingress_cert_chain","text":"Optional. When set to True , includes the complete certificates chain in the generated MAS configuration, when a trusted certificate authority is found in your cluster's ingress. Optional Environment Variable: INCLUDE_CLUSTER_INGRESS_CERT_CHAIN Default: False","title":"include_cluster_ingress_cert_chain"},{"location":"roles/dro/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/dro/#install-in-cluster-and-generate-mas-configuration","text":"To install DRO export IBM_ENTITLEMENT_KEY=<valid ibm entitlement key> export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export DRO_ACTION=install-dro export MAS_CONFIG_DIR=<valid local path to the config folder> export MAS_INSTANCE_ID=<valid mas instance id> export DRO_STORAGE_CLASS=<valid storage class name> export ROLE_NAME='dro' export DRO_NAMESPACE=ibm-dro ansible-playbook playbooks/run_role.yml To connect to an existing DRO export DRO_ENDPOINT_URL=<valid DRO url> export DRO_APIKEY=<valid DRO apikey> export DRO_CERTIFICATE_PATH=/temp/cert.pem export IBM_ENTITLEMENT_KEY=<valid ibm entitlement key> export DRO_CONTACT_EMAIL=xxx@xxx.com export DRO_CONTACT_FIRSTNAME=xxx export DRO_CONTACT_LASTNAME=xxx export MAS_CONFIG_DIR=<valid local path to the config folder> export MAS_INSTANCE_ID=<valid mas instance id> export DRO_ACTION=install-dro export ROLE_NAME='dro' ansible-playbook playbooks/run_role.yml To uninstall DRO export DRO_ACTION=uninstall export ROLE_NAME='dro' export DRO_NAMESPACE=ibm-dro ansible-playbook playbooks/run_role.yml - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig dro_contact: email: 'john@email.com' first_name: 'john' last_name: 'winter' roles: - ibm.mas_devops.dro","title":"Install in-cluster and generate MAS configuration"},{"location":"roles/dro/#license","text":"EPL-2.0","title":"License"},{"location":"roles/eck/","text":"eck \u00a4 This role provides support to install Elastic Cloud on Kubernetes (ECK). Elasticsearch is configured with a default user named elastic , you can obtain the password for this user by running the following command: oc -n eck get secret mas-es-elastic-user -o go-template='{{.data.elastic | base64decode}}'; echo Role Variables \u00a4 eck_action \u00a4 Action to be performed by the role. The only valid value currently is install . Environment Variable: ECK_ACTION Default Value: install eck_enable_elasticsearch \u00a4 Whether to include Elasticsearch when performing the desired action. Environment Variable: ECK_ENABLE_ELASTICSEARCH Default Value: false eck_enable_kibana \u00a4 Whether to include Kibana when performing the desired action. Environment Variable: ECK_ENABLE_KIBANA Default Value: false eck_enable_logstash \u00a4 Whether to include Logstash when performing the desired action. Environment Variable: ECK_ENABLE_LOGSTASH Default Value: false eck_enable_filebeat \u00a4 Whether to include Filebeat when performing the desired action. Environment Variable: ECK_ENABLE_FILEBEAT Default Value: false Role Variables - Remote Elasticsearch \u00a4 When eck_remote_es_hosts , eck_remote_es_username , and eck_remote_es_password are all set, and eck_enable_logstash is true , the Logstash server will be configured to send log messages to the remote Elasticsearch instance defined. eck_remote_es_hosts \u00a4 A list of one or more hosts for the remote Elasticsearch instance. When using an environment varible, the value should be in the format of a comma-seperated list. Environment Variable: ECK_ENABLE_FILEBEAT Default Value: false eck_remote_es_username \u00a4 The username that will be used to authenticate with the remote Elasticsearch instance. Environment Variable: ECK_ENABLE_FILEBEAT Default Value: false eck_remote_es_password \u00a4 The password that will be used to authenticate with the remote Elasticsearch instance. Environment Variable: ECK_ENABLE_FILEBEAT Default Value: false Role Variables - Domains and Certificates \u00a4 Elasticsearch and Kibana can be configured with a custom domain and a certificate signed by LetsEncrypt . es_domain \u00a4 The domain that Elasticsearch will be accessed from, must be routable to the target OCP cluster. Environment Variable: ECK_ELASTICSEARCH_DOMAIN Default Value: None kibana_domain \u00a4 The domain that Kibana will be accessed from, must be routable to the target OCP cluster. Environment Variable: ECK_KIBANA_DOMAIN Default Value: None letsencrypt_email \u00a4 Provide the email address which will be used to register the certificates with LetsEncrypt. When this is provided and one or both domains are set, an Issuer will be configured for LetsEncrypt production using a HTTP solver that will installed automatically by Cert-Manager in the ECK namespace. Environment Variable: LETSENCRYPT_EMAIL Default Value: None Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: eck_action: install eck_enable_elasticsearch: true eck_enable_kibana: true eck_enable_logstash: true roles: - ibm.mas_devops.eck License \u00a4 EPL-2.0","title":"eck"},{"location":"roles/eck/#eck","text":"This role provides support to install Elastic Cloud on Kubernetes (ECK). Elasticsearch is configured with a default user named elastic , you can obtain the password for this user by running the following command: oc -n eck get secret mas-es-elastic-user -o go-template='{{.data.elastic | base64decode}}'; echo","title":"eck"},{"location":"roles/eck/#role-variables","text":"","title":"Role Variables"},{"location":"roles/eck/#eck_action","text":"Action to be performed by the role. The only valid value currently is install . Environment Variable: ECK_ACTION Default Value: install","title":"eck_action"},{"location":"roles/eck/#eck_enable_elasticsearch","text":"Whether to include Elasticsearch when performing the desired action. Environment Variable: ECK_ENABLE_ELASTICSEARCH Default Value: false","title":"eck_enable_elasticsearch"},{"location":"roles/eck/#eck_enable_kibana","text":"Whether to include Kibana when performing the desired action. Environment Variable: ECK_ENABLE_KIBANA Default Value: false","title":"eck_enable_kibana"},{"location":"roles/eck/#eck_enable_logstash","text":"Whether to include Logstash when performing the desired action. Environment Variable: ECK_ENABLE_LOGSTASH Default Value: false","title":"eck_enable_logstash"},{"location":"roles/eck/#eck_enable_filebeat","text":"Whether to include Filebeat when performing the desired action. Environment Variable: ECK_ENABLE_FILEBEAT Default Value: false","title":"eck_enable_filebeat"},{"location":"roles/eck/#role-variables-remote-elasticsearch","text":"When eck_remote_es_hosts , eck_remote_es_username , and eck_remote_es_password are all set, and eck_enable_logstash is true , the Logstash server will be configured to send log messages to the remote Elasticsearch instance defined.","title":"Role Variables - Remote Elasticsearch"},{"location":"roles/eck/#eck_remote_es_hosts","text":"A list of one or more hosts for the remote Elasticsearch instance. When using an environment varible, the value should be in the format of a comma-seperated list. Environment Variable: ECK_ENABLE_FILEBEAT Default Value: false","title":"eck_remote_es_hosts"},{"location":"roles/eck/#eck_remote_es_username","text":"The username that will be used to authenticate with the remote Elasticsearch instance. Environment Variable: ECK_ENABLE_FILEBEAT Default Value: false","title":"eck_remote_es_username"},{"location":"roles/eck/#eck_remote_es_password","text":"The password that will be used to authenticate with the remote Elasticsearch instance. Environment Variable: ECK_ENABLE_FILEBEAT Default Value: false","title":"eck_remote_es_password"},{"location":"roles/eck/#role-variables-domains-and-certificates","text":"Elasticsearch and Kibana can be configured with a custom domain and a certificate signed by LetsEncrypt .","title":"Role Variables - Domains and Certificates"},{"location":"roles/eck/#es_domain","text":"The domain that Elasticsearch will be accessed from, must be routable to the target OCP cluster. Environment Variable: ECK_ELASTICSEARCH_DOMAIN Default Value: None","title":"es_domain"},{"location":"roles/eck/#kibana_domain","text":"The domain that Kibana will be accessed from, must be routable to the target OCP cluster. Environment Variable: ECK_KIBANA_DOMAIN Default Value: None","title":"kibana_domain"},{"location":"roles/eck/#letsencrypt_email","text":"Provide the email address which will be used to register the certificates with LetsEncrypt. When this is provided and one or both domains are set, an Issuer will be configured for LetsEncrypt production using a HTTP solver that will installed automatically by Cert-Manager in the ECK namespace. Environment Variable: LETSENCRYPT_EMAIL Default Value: None","title":"letsencrypt_email"},{"location":"roles/eck/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: eck_action: install eck_enable_elasticsearch: true eck_enable_kibana: true eck_enable_logstash: true roles: - ibm.mas_devops.eck","title":"Example Playbook"},{"location":"roles/eck/#license","text":"EPL-2.0","title":"License"},{"location":"roles/entitlement_key_rotation/","text":"entitlement_key_rotation \u00a4 This role creates/updates the entitlement username and password that are stored in the secrets used to pull images throughout all MAS related namespaces for one or multiple clusters. The main secret that is updated by this role is the ibm-entitlement which holds the credentials needed to pull the MAS images used by MAS Core or the MAS applications. By default, this role will search for all MAS related namespaces that might contain the secret that holds the entitlement key to be updated. The list of namespaces to be updated with new username/password credentials are: All namespaces starting with mas- , which means by default it will update the ibm-entitlement secret with the new username/password credentials for all MAS namespaces/instances in the cluster. SLS namespace - holds ibm-entitlement which pulls Suite License Services related images. openshift-marketplace - holds wiot-docker-local which pulls the pre-release/development catalog source image for ibm-operator-catalog . Requires the artifactory_username and artifactory_token to be set. Note This role uses ocp_login to login into the target clusters, therefore make sure you export the corresponding environment variables accordingly to the cluster type you want to target. Role Variables \u00a4 artifactory_username \u00a4 Required to rotate the ibm-entitlement and wiotp-docker-local secret credentials which is used to pull images across MAS namespaces for development installs and pre-release catalog sources. Environment Variable: ARTIFACTORY_USERNAME Default Value: None artifactory_token \u00a4 Required to rotate the ibm-entitlement and wiotp-docker-local secret credentials which is used to pull images across MAS namespaces for development installs and pre-release catalog sources. Environment Variable: ARTIFACTORY_TOKEN Default Value: None mas_entitlement_username \u00a4 Required to rotate the ibm-entitlement secret credentials only, which is used to pull images across MAS namespaces for MAS installs using release catalog sources. Environment Variable: MAS_ENTITLEMENT_USERNAME Default Value: None mas_entitlement_key \u00a4 Required to rotate the ibm-entitlement secret credentials only, which is used to pull images across MAS namespaces for MAS installs using release catalog sources. Environment Variable: MAS_ENTITLEMENT_KEY Default Value: None cluster_name \u00a4 Required. The target cluster to rotate the credentials/entitlement key secrets. Environment Variable: CLUSTER_NAME Default Value: None sls_namespace \u00a4 Optional. Defines the SLS namespace that holds ibm-entitlement secret which pulls Suite License Services related images. Environment Variable: SLS_NAMESPACE Default Value: ibm-sls Role Variables - Advanced mode \u00a4 Use the following variables to change the default behavior of this role to only rotate the entitlement key for specific clusters or namespaces, instead of running it for all MAS related namespaces. mas_clusters_entitlement_key_rotation_list \u00a4 Optionally define a list of clusters to loop through the entitlement key rotation. Environment Variable: MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST Default Value: If not set, the cluster_name property will be used to target the cluster while executing this role. Example: export MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST='cluster1,cluster2' mas_namespaces_entitlement_key_rotation_list \u00a4 Optionally define a specific list of namespaces to loop through the entitlement key rotation. Environment Variable: MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST Default Value: If not set, all MAS related namespaces for all MAS instances will be target for entitlement key rotation. Example: export MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST='ibm-sls,openshift-marketplace' Example Playbook \u00a4 Rotate entitlement credentials across all MAS instances for a given target cluster: - hosts: localhost any_errors_fatal: true vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME') }}\" cluster_type: \"{{ lookup('env', 'CLUSTER_TYPE') }}\" ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" artifactory_username: \"{{ lookup('env', 'ARTIFACTORY_USERNAME') }}\" artifactory_token: \"{{ lookup('env', 'ARTIFACTORY_TOKEN') }}\" mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.entitlement_key_rotation Rotate entitlement credentials across a specific list of namespaces, targeting multiple clusters: - hosts: localhost any_errors_fatal: true vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME') }}\" # this is the original cluster that will keep the login session context at the end of the rotation loop. cluster_type: \"{{ lookup('env', 'CLUSTER_TYPE') }}\" ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" artifactory_username: \"{{ lookup('env', 'ARTIFACTORY_USERNAME') }}\" artifactory_token: \"{{ lookup('env', 'ARTIFACTORY_TOKEN') }}\" mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" mas_clusters_entitlement_key_rotation_list: \"{{ lookup('env', 'MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST') }}\" mas_namespaces_entitlement_key_rotation_list: \"{{ lookup('env', 'MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST') }}\" roles: - ibm.mas_devops.entitlement_key_rotation License \u00a4 EPL-2.0","title":"entitlement_key_rotation"},{"location":"roles/entitlement_key_rotation/#entitlement_key_rotation","text":"This role creates/updates the entitlement username and password that are stored in the secrets used to pull images throughout all MAS related namespaces for one or multiple clusters. The main secret that is updated by this role is the ibm-entitlement which holds the credentials needed to pull the MAS images used by MAS Core or the MAS applications. By default, this role will search for all MAS related namespaces that might contain the secret that holds the entitlement key to be updated. The list of namespaces to be updated with new username/password credentials are: All namespaces starting with mas- , which means by default it will update the ibm-entitlement secret with the new username/password credentials for all MAS namespaces/instances in the cluster. SLS namespace - holds ibm-entitlement which pulls Suite License Services related images. openshift-marketplace - holds wiot-docker-local which pulls the pre-release/development catalog source image for ibm-operator-catalog . Requires the artifactory_username and artifactory_token to be set. Note This role uses ocp_login to login into the target clusters, therefore make sure you export the corresponding environment variables accordingly to the cluster type you want to target.","title":"entitlement_key_rotation"},{"location":"roles/entitlement_key_rotation/#role-variables","text":"","title":"Role Variables"},{"location":"roles/entitlement_key_rotation/#artifactory_username","text":"Required to rotate the ibm-entitlement and wiotp-docker-local secret credentials which is used to pull images across MAS namespaces for development installs and pre-release catalog sources. Environment Variable: ARTIFACTORY_USERNAME Default Value: None","title":"artifactory_username"},{"location":"roles/entitlement_key_rotation/#artifactory_token","text":"Required to rotate the ibm-entitlement and wiotp-docker-local secret credentials which is used to pull images across MAS namespaces for development installs and pre-release catalog sources. Environment Variable: ARTIFACTORY_TOKEN Default Value: None","title":"artifactory_token"},{"location":"roles/entitlement_key_rotation/#mas_entitlement_username","text":"Required to rotate the ibm-entitlement secret credentials only, which is used to pull images across MAS namespaces for MAS installs using release catalog sources. Environment Variable: MAS_ENTITLEMENT_USERNAME Default Value: None","title":"mas_entitlement_username"},{"location":"roles/entitlement_key_rotation/#mas_entitlement_key","text":"Required to rotate the ibm-entitlement secret credentials only, which is used to pull images across MAS namespaces for MAS installs using release catalog sources. Environment Variable: MAS_ENTITLEMENT_KEY Default Value: None","title":"mas_entitlement_key"},{"location":"roles/entitlement_key_rotation/#cluster_name","text":"Required. The target cluster to rotate the credentials/entitlement key secrets. Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/entitlement_key_rotation/#sls_namespace","text":"Optional. Defines the SLS namespace that holds ibm-entitlement secret which pulls Suite License Services related images. Environment Variable: SLS_NAMESPACE Default Value: ibm-sls","title":"sls_namespace"},{"location":"roles/entitlement_key_rotation/#role-variables-advanced-mode","text":"Use the following variables to change the default behavior of this role to only rotate the entitlement key for specific clusters or namespaces, instead of running it for all MAS related namespaces.","title":"Role Variables - Advanced mode"},{"location":"roles/entitlement_key_rotation/#mas_clusters_entitlement_key_rotation_list","text":"Optionally define a list of clusters to loop through the entitlement key rotation. Environment Variable: MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST Default Value: If not set, the cluster_name property will be used to target the cluster while executing this role. Example: export MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST='cluster1,cluster2'","title":"mas_clusters_entitlement_key_rotation_list"},{"location":"roles/entitlement_key_rotation/#mas_namespaces_entitlement_key_rotation_list","text":"Optionally define a specific list of namespaces to loop through the entitlement key rotation. Environment Variable: MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST Default Value: If not set, all MAS related namespaces for all MAS instances will be target for entitlement key rotation. Example: export MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST='ibm-sls,openshift-marketplace'","title":"mas_namespaces_entitlement_key_rotation_list"},{"location":"roles/entitlement_key_rotation/#example-playbook","text":"Rotate entitlement credentials across all MAS instances for a given target cluster: - hosts: localhost any_errors_fatal: true vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME') }}\" cluster_type: \"{{ lookup('env', 'CLUSTER_TYPE') }}\" ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" artifactory_username: \"{{ lookup('env', 'ARTIFACTORY_USERNAME') }}\" artifactory_token: \"{{ lookup('env', 'ARTIFACTORY_TOKEN') }}\" mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.entitlement_key_rotation Rotate entitlement credentials across a specific list of namespaces, targeting multiple clusters: - hosts: localhost any_errors_fatal: true vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME') }}\" # this is the original cluster that will keep the login session context at the end of the rotation loop. cluster_type: \"{{ lookup('env', 'CLUSTER_TYPE') }}\" ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" artifactory_username: \"{{ lookup('env', 'ARTIFACTORY_USERNAME') }}\" artifactory_token: \"{{ lookup('env', 'ARTIFACTORY_TOKEN') }}\" mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" mas_clusters_entitlement_key_rotation_list: \"{{ lookup('env', 'MAS_CLUSTERS_ENTITLEMENT_KEY_ROTATION_LIST') }}\" mas_namespaces_entitlement_key_rotation_list: \"{{ lookup('env', 'MAS_NAMESPACES_ENTITLEMENT_KEY_ROTATION_LIST') }}\" roles: - ibm.mas_devops.entitlement_key_rotation","title":"Example Playbook"},{"location":"roles/entitlement_key_rotation/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_jdbc/","text":"gencfg_jdbc \u00a4 This role is used to configure database in Maximo Application Suite. It will use the database SSL certificate if ssl_enabled flag is true. The db_pem-file defines the location of the pem file used for JDBC connection in MAS installation. Role Variables - Data Source \u00a4 db_instance_id \u00a4 Defines the instance id that is used for the db configure in MAS installation Required Environment Variable: DB_INSTANCE_ID Default: dbinst db_username \u00a4 Defines the username that is used for the db configure in MAS installation Required Environment Variable: MAS_JDBC_USER Default: None jdbc_instance_password \u00a4 Defines the password that is used to connect to db in MAS installation Required Environment Variable: MAS_JDBC_PASSWORD Default: None jdbc_url \u00a4 Defines the jdbc URL that is used to connect to db in MAS installation: Required Environment Variable: MAS_JDBC_URL Default: None Tip Example URL strings: IBM Db2 (insecure): jdbc:db2://dbserverxx:50000/maxdbxx IBM Db2 (secure): jdbc:db2://dbserverxx:50000/maxdbxx:sslConnection=true Oracle Database: jdbc:oracle:thin:@dbserverxx:1521:maximo Microsoft SQL Server (insecure): jdbc:sqlserver://;serverName=dbserverxx;portNumber=1433;databaseName=msdbxx;integratedSecurity=false;sendStringParametersAsUnicode=false;selectMethod=cursor;encrypt=false;trustServerCertificate=false; Microsoft SQL Server (secure): jdbc:sqlserver://;serverName=dbserverxx;portNumber=1433;databaseName=msdbxx;integratedSecurity=false;sendStringParametersAsUnicode=false;selectMethod=cursor;encrypt=true;trustServerCertificate=true; db_pem_file \u00a4 Defines the location of the pem file used for JDBC connection in MAS installation Optional Environment Variable: MAS_JDBC_CERT_LOCAL_FILE Default: None Role Variables - MAS Configuration \u00a4 mas_config_scope \u00a4 Configure whether to generate a binding suitable for System, Workspace, Application, or Workspace-Application use within MAS ( system , ws , app , or wsapp ). Required Environment Variable: MAS_CONFIG_SCOPE Default: None mas_config_dir \u00a4 Configure the destination directory for the generated yaml file. Required Environment Variable: MAS_CONFIG_DIR Default: None mas_instance_id \u00a4 MAS Instance ID we are generating a configuration for. Required Environment Variable: MAS_INSTANCE_ID Default: None mas_workspace_id \u00a4 Set the workspace ID when generating a configuration for workspace or workspace-application scope. Required if mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None mas_application_id \u00a4 Set the application ID when generating a configuration for application or workspace-application scope. Required if mas_config_scope is set to either app or wsapp Environment Variable: MAS_APP_ID Default: None ssl_enabled \u00a4 Some applications in MAS are unable to determine whether SSL is enabled or disable via the JDBC string, and require this additional setting. Make sure to set this to match the setting in jdbc_url . Required Environment Variable: SSL_ENABLED Default: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default: None Example Playbook \u00a4 --- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_jdbc License \u00a4 EPL-2.0","title":"gencfg_jdbc"},{"location":"roles/gencfg_jdbc/#gencfg_jdbc","text":"This role is used to configure database in Maximo Application Suite. It will use the database SSL certificate if ssl_enabled flag is true. The db_pem-file defines the location of the pem file used for JDBC connection in MAS installation.","title":"gencfg_jdbc"},{"location":"roles/gencfg_jdbc/#role-variables-data-source","text":"","title":"Role Variables - Data Source"},{"location":"roles/gencfg_jdbc/#db_instance_id","text":"Defines the instance id that is used for the db configure in MAS installation Required Environment Variable: DB_INSTANCE_ID Default: dbinst","title":"db_instance_id"},{"location":"roles/gencfg_jdbc/#db_username","text":"Defines the username that is used for the db configure in MAS installation Required Environment Variable: MAS_JDBC_USER Default: None","title":"db_username"},{"location":"roles/gencfg_jdbc/#jdbc_instance_password","text":"Defines the password that is used to connect to db in MAS installation Required Environment Variable: MAS_JDBC_PASSWORD Default: None","title":"jdbc_instance_password"},{"location":"roles/gencfg_jdbc/#jdbc_url","text":"Defines the jdbc URL that is used to connect to db in MAS installation: Required Environment Variable: MAS_JDBC_URL Default: None Tip Example URL strings: IBM Db2 (insecure): jdbc:db2://dbserverxx:50000/maxdbxx IBM Db2 (secure): jdbc:db2://dbserverxx:50000/maxdbxx:sslConnection=true Oracle Database: jdbc:oracle:thin:@dbserverxx:1521:maximo Microsoft SQL Server (insecure): jdbc:sqlserver://;serverName=dbserverxx;portNumber=1433;databaseName=msdbxx;integratedSecurity=false;sendStringParametersAsUnicode=false;selectMethod=cursor;encrypt=false;trustServerCertificate=false; Microsoft SQL Server (secure): jdbc:sqlserver://;serverName=dbserverxx;portNumber=1433;databaseName=msdbxx;integratedSecurity=false;sendStringParametersAsUnicode=false;selectMethod=cursor;encrypt=true;trustServerCertificate=true;","title":"jdbc_url"},{"location":"roles/gencfg_jdbc/#db_pem_file","text":"Defines the location of the pem file used for JDBC connection in MAS installation Optional Environment Variable: MAS_JDBC_CERT_LOCAL_FILE Default: None","title":"db_pem_file"},{"location":"roles/gencfg_jdbc/#role-variables-mas-configuration","text":"","title":"Role Variables - MAS Configuration"},{"location":"roles/gencfg_jdbc/#mas_config_scope","text":"Configure whether to generate a binding suitable for System, Workspace, Application, or Workspace-Application use within MAS ( system , ws , app , or wsapp ). Required Environment Variable: MAS_CONFIG_SCOPE Default: None","title":"mas_config_scope"},{"location":"roles/gencfg_jdbc/#mas_config_dir","text":"Configure the destination directory for the generated yaml file. Required Environment Variable: MAS_CONFIG_DIR Default: None","title":"mas_config_dir"},{"location":"roles/gencfg_jdbc/#mas_instance_id","text":"MAS Instance ID we are generating a configuration for. Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/gencfg_jdbc/#mas_workspace_id","text":"Set the workspace ID when generating a configuration for workspace or workspace-application scope. Required if mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/gencfg_jdbc/#mas_application_id","text":"Set the application ID when generating a configuration for application or workspace-application scope. Required if mas_config_scope is set to either app or wsapp Environment Variable: MAS_APP_ID Default: None","title":"mas_application_id"},{"location":"roles/gencfg_jdbc/#ssl_enabled","text":"Some applications in MAS are unable to determine whether SSL is enabled or disable via the JDBC string, and require this additional setting. Make sure to set this to match the setting in jdbc_url . Required Environment Variable: SSL_ENABLED Default: None","title":"ssl_enabled"},{"location":"roles/gencfg_jdbc/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default: None","title":"custom_labels"},{"location":"roles/gencfg_jdbc/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_jdbc","title":"Example Playbook"},{"location":"roles/gencfg_jdbc/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_mongo/","text":"gencfg_mongo \u00a4 This role is used to generate mongo configuration in Maximo Application Suite This generated mongo configuration can be used as an input to the suite_config role, to configure a MAS instance to connect with an existing Mongo cluster Role Variables \u00a4 mongodb_namespace \u00a4 The generated Mongo Config file name will be suffixed with this namespace value eg, mongo-< >.yml Environment Variable: MONGODB_NAMESPACE Default: mongoce mongodb_admin_username \u00a4 Required. MongoDB admin username Environment Variable: MONGODB_ADMIN_USERNAME Default: None mongodb_admin_password \u00a4 Required. MongoDB admin password Environment Variable: MONGODB_ADMIN_PASSWORD Default: None mongodb_authentication_mechanism \u00a4 Required. MongoDB authentication mechanism. Specify DEFAULT for SCRAM-SHA-256 or SCRAM-SHA-1. For LDAP authentication use PLAIN Environment Variable: MONGODB_AUTHENTICATION_MECHANISM Default: DEFAULT mongodb_authentication_database \u00a4 Required. MongoDB authentication database. This value must be $external if PLAIN has been specified for mongodb_authentication_mechanism Environment Variable: MONGODB_AUTHENTICATION_DATABASE Default: admin mongodb_hosts \u00a4 Required. In case if there are multiple instances, the host address should be seperated by a ,. Example: docdb-1.abc.ca-central-1.docdb.amazonaws.com:27017,docdb-2.def.ca-central-1.docdb.amazonaws.com:27017 Environment Variable: MONGODB_HOSTS Default: None mongodb_retry_writes \u00a4 Set to true if MongoDB support retryable writes. In case if retryable writes is not supported (like in case of Amazon DocumentDB), set to false Optional Environment Variable: MONGODB_RETRY_WRITES Default: true mongodb_ca_pem_local_file \u00a4 Required. defines the CA pem file's local file path Environment Variable: MONGODB_CA_PEM_LOCAL_FILE Default: None mas_instance_id \u00a4 Required. The instance ID of Maximo Application Suite for which the MongoCfg configuration will generate Environment Variable: MAS_INSTANCE_ID Default: None mas_config_dir \u00a4 Required. Local directory to save the generated MongoCfg resource definition. This can be used as an input to the suite_config role, to configure a MAS instance to connect with an existing Mongo cluster Environment Variable: MAS_CONFIG_DIR Default: None Example Playbook \u00a4 --- - hosts: localhost any_errors_fatal: true vars: mongodb_namespace: mongoce mongodb_admin_username: mongoadmin mongodb_admin_password: mongo-strong-password mongodb_hosts: docdb-1.abc.ca-central-1.docdb.amazonaws.com:27017,docdb-2.def.ca-central-1.docdb.amazonaws.com:27017 mongodb_retry_writes: false mongodb_ca_pem_local_file: /tmp/mongo-ca.pem mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.gencfg_mongo License \u00a4 EPL-2.0","title":"gencfg_mongo"},{"location":"roles/gencfg_mongo/#gencfg_mongo","text":"This role is used to generate mongo configuration in Maximo Application Suite This generated mongo configuration can be used as an input to the suite_config role, to configure a MAS instance to connect with an existing Mongo cluster","title":"gencfg_mongo"},{"location":"roles/gencfg_mongo/#role-variables","text":"","title":"Role Variables"},{"location":"roles/gencfg_mongo/#mongodb_namespace","text":"The generated Mongo Config file name will be suffixed with this namespace value eg, mongo-< >.yml Environment Variable: MONGODB_NAMESPACE Default: mongoce","title":"mongodb_namespace"},{"location":"roles/gencfg_mongo/#mongodb_admin_username","text":"Required. MongoDB admin username Environment Variable: MONGODB_ADMIN_USERNAME Default: None","title":"mongodb_admin_username"},{"location":"roles/gencfg_mongo/#mongodb_admin_password","text":"Required. MongoDB admin password Environment Variable: MONGODB_ADMIN_PASSWORD Default: None","title":"mongodb_admin_password"},{"location":"roles/gencfg_mongo/#mongodb_authentication_mechanism","text":"Required. MongoDB authentication mechanism. Specify DEFAULT for SCRAM-SHA-256 or SCRAM-SHA-1. For LDAP authentication use PLAIN Environment Variable: MONGODB_AUTHENTICATION_MECHANISM Default: DEFAULT","title":"mongodb_authentication_mechanism"},{"location":"roles/gencfg_mongo/#mongodb_authentication_database","text":"Required. MongoDB authentication database. This value must be $external if PLAIN has been specified for mongodb_authentication_mechanism Environment Variable: MONGODB_AUTHENTICATION_DATABASE Default: admin","title":"mongodb_authentication_database"},{"location":"roles/gencfg_mongo/#mongodb_hosts","text":"Required. In case if there are multiple instances, the host address should be seperated by a ,. Example: docdb-1.abc.ca-central-1.docdb.amazonaws.com:27017,docdb-2.def.ca-central-1.docdb.amazonaws.com:27017 Environment Variable: MONGODB_HOSTS Default: None","title":"mongodb_hosts"},{"location":"roles/gencfg_mongo/#mongodb_retry_writes","text":"Set to true if MongoDB support retryable writes. In case if retryable writes is not supported (like in case of Amazon DocumentDB), set to false Optional Environment Variable: MONGODB_RETRY_WRITES Default: true","title":"mongodb_retry_writes"},{"location":"roles/gencfg_mongo/#mongodb_ca_pem_local_file","text":"Required. defines the CA pem file's local file path Environment Variable: MONGODB_CA_PEM_LOCAL_FILE Default: None","title":"mongodb_ca_pem_local_file"},{"location":"roles/gencfg_mongo/#mas_instance_id","text":"Required. The instance ID of Maximo Application Suite for which the MongoCfg configuration will generate Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/gencfg_mongo/#mas_config_dir","text":"Required. Local directory to save the generated MongoCfg resource definition. This can be used as an input to the suite_config role, to configure a MAS instance to connect with an existing Mongo cluster Environment Variable: MAS_CONFIG_DIR Default: None","title":"mas_config_dir"},{"location":"roles/gencfg_mongo/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true vars: mongodb_namespace: mongoce mongodb_admin_username: mongoadmin mongodb_admin_password: mongo-strong-password mongodb_hosts: docdb-1.abc.ca-central-1.docdb.amazonaws.com:27017,docdb-2.def.ca-central-1.docdb.amazonaws.com:27017 mongodb_retry_writes: false mongodb_ca_pem_local_file: /tmp/mongo-ca.pem mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.gencfg_mongo","title":"Example Playbook"},{"location":"roles/gencfg_mongo/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_watsonstudio/","text":"gencfg_watsonstudio \u00a4 This role is used to configure WatsonStudio in Maximo Application Suite. Role Variables \u00a4 mas_instance_id \u00a4 Providing this and mas_config_dir will instruct the role to generate a WatsonStudioCfg template that can be used to configure MAS to connect to WatsonStudio. Environment Variable: MAS_INSTANCE_ID Default: None mas_workspace_id \u00a4 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None CPD_ADMIN_USERNAME \u00a4 Defines the username that is used for the WatsonStudio configure in MAS installation Environment Variable: CPD_ADMIN_USERNAME Default: None CPD_ADMIN_PASSWORD \u00a4 Defines the password that is used to connect to WatsonStudio in MAS installation Environment Variable: CPD_ADMIN_PASSWORD Default: None CPD_ADMIN_URL \u00a4 Defines the url that is used to connect to WatsonStudio in MAS installation Environment Variable: CPD_ADMIN_URL Default: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default: None Example Playbook \u00a4 --- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_watsonstudio License \u00a4 EPL-2.0","title":"gencfg_watsonstudio"},{"location":"roles/gencfg_watsonstudio/#gencfg_watsonstudio","text":"This role is used to configure WatsonStudio in Maximo Application Suite.","title":"gencfg_watsonstudio"},{"location":"roles/gencfg_watsonstudio/#role-variables","text":"","title":"Role Variables"},{"location":"roles/gencfg_watsonstudio/#mas_instance_id","text":"Providing this and mas_config_dir will instruct the role to generate a WatsonStudioCfg template that can be used to configure MAS to connect to WatsonStudio. Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/gencfg_watsonstudio/#mas_workspace_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/gencfg_watsonstudio/#cpd_admin_username","text":"Defines the username that is used for the WatsonStudio configure in MAS installation Environment Variable: CPD_ADMIN_USERNAME Default: None","title":"CPD_ADMIN_USERNAME"},{"location":"roles/gencfg_watsonstudio/#cpd_admin_password","text":"Defines the password that is used to connect to WatsonStudio in MAS installation Environment Variable: CPD_ADMIN_PASSWORD Default: None","title":"CPD_ADMIN_PASSWORD"},{"location":"roles/gencfg_watsonstudio/#cpd_admin_url","text":"Defines the url that is used to connect to WatsonStudio in MAS installation Environment Variable: CPD_ADMIN_URL Default: None","title":"CPD_ADMIN_URL"},{"location":"roles/gencfg_watsonstudio/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default: None","title":"custom_labels"},{"location":"roles/gencfg_watsonstudio/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gencfg_watsonstudio","title":"Example Playbook"},{"location":"roles/gencfg_watsonstudio/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_workspace/","text":"gencfg_workspace \u00a4 This role is used to generate a Workspace custom resource that can be applied to Maximo Application Suite manually, or using the suite_config role. The configuration will be saved to local disk in the directory specified by the mas_config_dir variable. Role Variables \u00a4 mas_instance_id \u00a4 Required. The MAS instance ID that the workspace will be used in Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \u00a4 Required. The ID of the workspace Environment Variable: MAS_WORKSPACE_ID Default Value: None mas_workspace_name \u00a4 Required. The display name for the workspace Environment Variable: MAS_WORKSPACE_NAME Default Value: None mas_config_dir \u00a4 Required. The directory to save the configuration to. Environment Variable: MAS_CONFIG_DIR Default Value: None custom_labels \u00a4 Optional. List of comma separated key=value pairs for setting custom labels on instance specific resources. Environment Variable: CUSTOM_LABELS Default Value: None Example Playbook \u00a4 --- - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_workspace_id: \"masdev\" mas_workspace_name: \"MAS Development\" mas_config_dir: \"/home/david/masconfig/inst1\" roles: - ibm.mas_devops.gencfg_workspace License \u00a4 EPL-2.0","title":"gencfg_workspace"},{"location":"roles/gencfg_workspace/#gencfg_workspace","text":"This role is used to generate a Workspace custom resource that can be applied to Maximo Application Suite manually, or using the suite_config role. The configuration will be saved to local disk in the directory specified by the mas_config_dir variable.","title":"gencfg_workspace"},{"location":"roles/gencfg_workspace/#role-variables","text":"","title":"Role Variables"},{"location":"roles/gencfg_workspace/#mas_instance_id","text":"Required. The MAS instance ID that the workspace will be used in Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/gencfg_workspace/#mas_workspace_id","text":"Required. The ID of the workspace Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/gencfg_workspace/#mas_workspace_name","text":"Required. The display name for the workspace Environment Variable: MAS_WORKSPACE_NAME Default Value: None","title":"mas_workspace_name"},{"location":"roles/gencfg_workspace/#mas_config_dir","text":"Required. The directory to save the configuration to. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/gencfg_workspace/#custom_labels","text":"Optional. List of comma separated key=value pairs for setting custom labels on instance specific resources. Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/gencfg_workspace/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_workspace_id: \"masdev\" mas_workspace_name: \"MAS Development\" mas_config_dir: \"/home/david/masconfig/inst1\" roles: - ibm.mas_devops.gencfg_workspace","title":"Example Playbook"},{"location":"roles/gencfg_workspace/#license","text":"EPL-2.0","title":"License"},{"location":"roles/grafana/","text":"grafana \u00a4 Installs and configures an instance of Grafana for use with IBM Maximo Application Suite, using the community grafana operator Note The credentials for the grafana admin user are stored in grafana-admin-credentials secret in the grafana namespace. A route is created in the grafana namespace to allow access to the grafana UI. Role Variables \u00a4 grafana_action \u00a4 Inform the role whether to perform an install , uninstall , or update of Grafana. Note When using this role to upgrade from Grafana 4 to 5, the Grafana 5 instance will have a new URL and will not inherit the user database from the old v4 installation, the admin password will be new, and user accounts set up in the v4 instance will need to be recreated in the v5 instance. Optional Environment Variable: GRAFANA_ACTION Default: install grafana_major_version \u00a4 Sets the major version of the grafana operator to install. 4 or 5 Optional Environment Variable: GRAFANA_MAJOR_VERSION Default Value: 5 grafana_v4_namespace \u00a4 Sets the namespace to install the grafana operator V4 and grafana instance Optional Environment Variable: GRAFANA_NAMESPACE Default Value: grafana grafana_v5_namespace \u00a4 Sets the namespace to install the grafana operator V5 and grafana instance Optional Environment Variable: GRAFANA_V5_NAMESPACE Default Value: grafana5 grafana_instance_storage_class \u00a4 Declare the storage class for Grafana Instance user data persistent volume. Required if one of the known supported storage classes is not installed in the cluster. Storage classes must support ReadWriteOnce (RWO) access mode. Environment Variable: GRAFANA_INSTANCE_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available) grafana_instance_storage_size \u00a4 Adjust the size of the volume used to store Grafana user data. Optional Environment Variable: GRAFANA_INSTANCE_STORAGE_SIZE Default Value: 10Gi Example Playbook \u00a4 - hosts: localhost vars: grafana_instance_storage_class: \"ibmc-file-gold-gid\" grafana_instance_storage_class: \"15Gi\" roles: - ibm.mas_devops.grafana To Upgrade from Grafana Operator from V4 to V5 - hosts: localhost vars: grafana_action: \"update\" roles: - ibm.mas_devops.grafana Note note that the upgraded v5 grafana inherits the storage class and size from the v4 configuration unless they are defined as environment variables. License \u00a4 EPL-2.0","title":"grafana"},{"location":"roles/grafana/#grafana","text":"Installs and configures an instance of Grafana for use with IBM Maximo Application Suite, using the community grafana operator Note The credentials for the grafana admin user are stored in grafana-admin-credentials secret in the grafana namespace. A route is created in the grafana namespace to allow access to the grafana UI.","title":"grafana"},{"location":"roles/grafana/#role-variables","text":"","title":"Role Variables"},{"location":"roles/grafana/#grafana_action","text":"Inform the role whether to perform an install , uninstall , or update of Grafana. Note When using this role to upgrade from Grafana 4 to 5, the Grafana 5 instance will have a new URL and will not inherit the user database from the old v4 installation, the admin password will be new, and user accounts set up in the v4 instance will need to be recreated in the v5 instance. Optional Environment Variable: GRAFANA_ACTION Default: install","title":"grafana_action"},{"location":"roles/grafana/#grafana_major_version","text":"Sets the major version of the grafana operator to install. 4 or 5 Optional Environment Variable: GRAFANA_MAJOR_VERSION Default Value: 5","title":"grafana_major_version"},{"location":"roles/grafana/#grafana_v4_namespace","text":"Sets the namespace to install the grafana operator V4 and grafana instance Optional Environment Variable: GRAFANA_NAMESPACE Default Value: grafana","title":"grafana_v4_namespace"},{"location":"roles/grafana/#grafana_v5_namespace","text":"Sets the namespace to install the grafana operator V5 and grafana instance Optional Environment Variable: GRAFANA_V5_NAMESPACE Default Value: grafana5","title":"grafana_v5_namespace"},{"location":"roles/grafana/#grafana_instance_storage_class","text":"Declare the storage class for Grafana Instance user data persistent volume. Required if one of the known supported storage classes is not installed in the cluster. Storage classes must support ReadWriteOnce (RWO) access mode. Environment Variable: GRAFANA_INSTANCE_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available)","title":"grafana_instance_storage_class"},{"location":"roles/grafana/#grafana_instance_storage_size","text":"Adjust the size of the volume used to store Grafana user data. Optional Environment Variable: GRAFANA_INSTANCE_STORAGE_SIZE Default Value: 10Gi","title":"grafana_instance_storage_size"},{"location":"roles/grafana/#example-playbook","text":"- hosts: localhost vars: grafana_instance_storage_class: \"ibmc-file-gold-gid\" grafana_instance_storage_class: \"15Gi\" roles: - ibm.mas_devops.grafana To Upgrade from Grafana Operator from V4 to V5 - hosts: localhost vars: grafana_action: \"update\" roles: - ibm.mas_devops.grafana Note note that the upgraded v5 grafana inherits the storage class and size from the v4 configuration unless they are defined as environment variables.","title":"Example Playbook"},{"location":"roles/grafana/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ibm_catalogs/","text":"ibm_catalogs \u00a4 This role installs the IBM Maximo Operator Catalog , which is a curated Operator Catalog derived from the IBM Operator Catalog , with all content certified compatible with IBM Maximo Application Suite: Additional, for IBM employees only, the pre-release development operator catalog can be installed, this is achieved by setting both the artifactory_username and artifactory_token variables. Role Variables \u00a4 mas_catalog_version \u00a4 Version of the IBM Maximo Operator Catalog to install. Optional Environment Variable: MAS_CATALOG_VERSION Default Value: @@MAS_LATEST_CATALOG@@ artifactory_username \u00a4 Use to enable the install of development catalog sources for pre-release installation. Optional Environment Variable: ARTIFACTORY_USERNAME Default Value: None artifactory_token \u00a4 Use to enable the install of development catalog sources for pre-release installation. Optional Environment Variable: ARTIFACTORY_TOKEN Default Value: None Example Playbook \u00a4 After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost roles: - ibm.mas_devops.ibm_catalogs Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=ibm_catalogs ansible-playbook ibm.mas_devops.run_role License \u00a4 EPL-2.0","title":"ibm_catalogs"},{"location":"roles/ibm_catalogs/#ibm_catalogs","text":"This role installs the IBM Maximo Operator Catalog , which is a curated Operator Catalog derived from the IBM Operator Catalog , with all content certified compatible with IBM Maximo Application Suite: Additional, for IBM employees only, the pre-release development operator catalog can be installed, this is achieved by setting both the artifactory_username and artifactory_token variables.","title":"ibm_catalogs"},{"location":"roles/ibm_catalogs/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ibm_catalogs/#mas_catalog_version","text":"Version of the IBM Maximo Operator Catalog to install. Optional Environment Variable: MAS_CATALOG_VERSION Default Value: @@MAS_LATEST_CATALOG@@","title":"mas_catalog_version"},{"location":"roles/ibm_catalogs/#artifactory_username","text":"Use to enable the install of development catalog sources for pre-release installation. Optional Environment Variable: ARTIFACTORY_USERNAME Default Value: None","title":"artifactory_username"},{"location":"roles/ibm_catalogs/#artifactory_token","text":"Use to enable the install of development catalog sources for pre-release installation. Optional Environment Variable: ARTIFACTORY_TOKEN Default Value: None","title":"artifactory_token"},{"location":"roles/ibm_catalogs/#example-playbook","text":"After installing the Ansible Collection you can include this role in your own custom playbooks. - hosts: localhost roles: - ibm.mas_devops.ibm_catalogs","title":"Example Playbook"},{"location":"roles/ibm_catalogs/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ROLE_NAME=ibm_catalogs ansible-playbook ibm.mas_devops.run_role","title":"Run Role Playbook"},{"location":"roles/ibm_catalogs/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ibmcloud_resource_key/","text":"ibmcloud_resource_key \u00a4 Create IBM Cloud resource keys (apikeys for specific services associated to the account) Role Variables \u00a4 service_instance \u00a4 Name of the service that the key will be referencing. (Eg: DB2, Mongo, cp4d, etc) Required Environment Variable: SERVICE_INSTANCE service_resource_key_name \u00a4 Name of the key to either create or delete. If unset will be dervied from the service_instance as \" service_instance _resource-key\" Optional Environment Variable: SERVICE_RESOURCE_KEY_NAME delete_service_key \u00a4 Set this to true to force deletion of the service_resource_key_name provided Optional Environment Variable: DELETE_SERVICE_KEY Default: false output_service_key_details_to_file \u00a4 If set will output a json file with the full service key details and credentials as \"service-key_ service_resource_key_name .json\" Optional Environment Variable: OUTPUT_SERVICE_KEY_DETAILS_TO_FILE Default: false output_dir \u00a4 Location to output the service key details json file if output_service_key_details_to_file is set Optional Environment Variable: OUTPUT_DIR Default: . (which will set the directory file in ibm/mas_devops) Example Playbook \u00a4 --- - hosts: localhost any_errors_fatal: true vars: ibmcloud_apikey: xxx service_instance: xxx service_resource_key_name: xxx output_service_key_details_to_file: True OR False delete_service_key: True OR False roles: - ibmcloud_resource_key License \u00a4 EPL-2.0","title":"ibmcloud_resource_key"},{"location":"roles/ibmcloud_resource_key/#ibmcloud_resource_key","text":"Create IBM Cloud resource keys (apikeys for specific services associated to the account)","title":"ibmcloud_resource_key"},{"location":"roles/ibmcloud_resource_key/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ibmcloud_resource_key/#service_instance","text":"Name of the service that the key will be referencing. (Eg: DB2, Mongo, cp4d, etc) Required Environment Variable: SERVICE_INSTANCE","title":"service_instance"},{"location":"roles/ibmcloud_resource_key/#service_resource_key_name","text":"Name of the key to either create or delete. If unset will be dervied from the service_instance as \" service_instance _resource-key\" Optional Environment Variable: SERVICE_RESOURCE_KEY_NAME","title":"service_resource_key_name"},{"location":"roles/ibmcloud_resource_key/#delete_service_key","text":"Set this to true to force deletion of the service_resource_key_name provided Optional Environment Variable: DELETE_SERVICE_KEY Default: false","title":"delete_service_key"},{"location":"roles/ibmcloud_resource_key/#output_service_key_details_to_file","text":"If set will output a json file with the full service key details and credentials as \"service-key_ service_resource_key_name .json\" Optional Environment Variable: OUTPUT_SERVICE_KEY_DETAILS_TO_FILE Default: false","title":"output_service_key_details_to_file"},{"location":"roles/ibmcloud_resource_key/#output_dir","text":"Location to output the service key details json file if output_service_key_details_to_file is set Optional Environment Variable: OUTPUT_DIR Default: . (which will set the directory file in ibm/mas_devops)","title":"output_dir"},{"location":"roles/ibmcloud_resource_key/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true vars: ibmcloud_apikey: xxx service_instance: xxx service_resource_key_name: xxx output_service_key_details_to_file: True OR False delete_service_key: True OR False roles: - ibmcloud_resource_key","title":"Example Playbook"},{"location":"roles/ibmcloud_resource_key/#license","text":"EPL-2.0","title":"License"},{"location":"roles/install_operator/","text":"install_operator \u00a4 TODO: Summarize role Role Variables \u00a4 custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None TODO: Finish documentation Example Playbook \u00a4 TODO: Add example License \u00a4 EPL-2.0","title":"install_operator"},{"location":"roles/install_operator/#install_operator","text":"TODO: Summarize role","title":"install_operator"},{"location":"roles/install_operator/#role-variables","text":"","title":"Role Variables"},{"location":"roles/install_operator/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None TODO: Finish documentation","title":"custom_labels"},{"location":"roles/install_operator/#example-playbook","text":"TODO: Add example","title":"Example Playbook"},{"location":"roles/install_operator/#license","text":"EPL-2.0","title":"License"},{"location":"roles/kafka/","text":"kafka \u00a4 This role provides support to install a Kafka Cluster using Strimzi , Red Hat AMQ Streams , IBM Event Streams or AWS MSK and generate configuration that can be directly applied to Maximo Application Suite. Both Strimzi and Red Hat AMQ streams component are massively scalable, distributed, and high-performance data streaming platform based on the Apache Kafka project. Both offer a distributed backbone that allows microservices and other applications to share data with high throughput and low latency. As more applications move to Kubernetes and Red Hat OpenShift, it is increasingly important to be able to run the communication infrastructure on the same platform. Red Hat OpenShift, as a highly scalable platform, is a natural fit for messaging technologies such as Kafka. The AMQ streams component makes running and managing Apache Kafka OpenShift native through the use of powerful operators that simplify the deployment, configuration, management, and use of Apache Kafka on Red Hat OpenShift. The AMQ streams component is part of the Red Hat AMQ family, which also includes the AMQ broker, a longtime innovation leader in Java\u2122 Message Service (JMS) and polyglot messaging, as well as the AMQ interconnect router, a wide-area, peer-to-peer messaging solution. Under the covers, AMQ streams leverages Strimzi's architecture, resources and configurations. Note: The MAS license does not include entitlement for AMQ streams. The MAS Devops Collection supports this Kafka deployment as an example only. Therefore, we recommend the use of Strimzi for an opensource Kafka provider. Tip The role will generate a yaml file containing the definition of a Secret and KafkaCfg resource that can be used to configure the deployed cluster as the MAS system Kafka. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/kafkacfg-amqstreams-system.yaml or used in conjunction with the suite_config role. Role Variables \u00a4 kafka_action \u00a4 Action to be performed by Kafka role. Valid values are install , upgrade or uninstall . The upgrade action applies only to the strimzi and redhat providers. Environment Variable: KAFKA_ACTION Default Value: install kafka_provider \u00a4 Valid kafka providers are strimzi (opensource), redhat (installs AMQ Streams which requires a license that is not included with MAS entitlement), ibm (provisions a paid Event Streams instance in the target IBM Cloud account) and aws (provisions a paid MSK instance in the target AWS account). Environment Variable: KAFKA_PROVIDER Default Value: strimzi Red Hat AMQ Streams & Strimzi Role Variables \u00a4 kafka_version \u00a4 The version of Kafka to deploy by the operator. Before changing the kafka_version make the version is supported by the amq-streams operator version or strimzi operator version . Environment Variable: KAFKA_VERSION Default Value: 3.5.0 for AMQ Streams and 3.7.0 for Strimzi. kafka_namespace \u00a4 The namespace where the operator and Kafka cluster will be deployed. Environment Variable: KAFKA_NAMESPACE Default Value: amq-streams for AMQ Streams and strimzi for Strimzi. kafka_cluster_name \u00a4 The name of the Kafka cluster that will be created Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka kafka_cluster_size \u00a4 The configuration to apply, there are two configurations available: small and large. Environment Variable: KAFKA_CLUSTER_SIZE Default Value: small kafka_storage_class \u00a4 The name of the storage class to configure the AMQStreams operator to use for persistent storage in the Kafka cluster. Storage class must support ReadWriteOnce(RWO) access mode. Environment Variable: KAFKA_STORAGE_CLASS Default Value: lookup supported storage classes in the cluster kafka_storage_size \u00a4 The size of the storage to configure the AMQStreams operator to use for persistent storage in the Kafka cluster. Environment Variable: KAFKA_STORAGE_SIZE Default Value: 100Gi zookeeper_storage_class \u00a4 The name of the storage class to configure the AMQStreams operator to use for persistent storage in the Zookeeper cluster. Storage class must support ReadWriteOnce(RWO) access mode. Environment Variable: ZOOKEEPER_STORAGE_CLASS Default Value: lookup supported storage classes in the cluster zookeeper_storage_size \u00a4 The size of the storage to configure the AMQStreams operator to use for persistent storage in the Zookeeper cluster. Environment Variable: ZOOKEEPER_STORAGE_SIZE Default Value: 10Gi kafka_user_name \u00a4 The name of the user to setup in the cluster for MAS. Environment Variable: KAFKA_USER_NAME Default Value: masuser kafka_user_password (supported in Strimzi operator verion 0.25.0 - amq streams operator version 2.x) \u00a4 The password of the user to setup in the cluster for MAS. Environment Variable: KAFKA_USER_PASSWORD Default Value: a randomly generated password is used if one is not specified mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None IBM Cloud Evenstreams Role Variables \u00a4 ibmcloud_apikey \u00a4 Defines IBM Cloud API Key. This API Key needs to have access to manage (provision/deprovision) IBM Cloud Event Streams. Required Environment Variable: IBMCLOUD_APIKEY Default Value: None eventstreams_resourcegroup \u00a4 Defines the IBM Cloud Resource Group to target the Event Streams instance. Optional Environment Variable: EVENTSTREAMS_RESOURCEGROUP Default Value: Default or value defined by IBMCLOUD_RESOURCEGROUP eventstreams_name \u00a4 Event Streams instance name. Required Environment Variable: EVENTSTREAMS_NAME Default Value: None eventstreams_plan \u00a4 Event Streams instance plan. Optional Environment Variable: EVENTSTREAMS_PLAN Default Value: standard eventstreams_location \u00a4 Optional Environment Variable: EVENTSTREAMS_LOCATION Default Value: us-east or value defined by IBMCLOUD_REGION eventstreams_retention \u00a4 Event Streams topic retention period (in miliseconds). Optional Environment Variable: EVENTSTREAMS_RETENTION Default Value: 1209600000 eventstreams_create_manage_jms_topic \u00a4 Defines whether to create specific Manage application JMS topics by default. Optional Environment Variable: EVENTSTREAMS_CREATE_MANAGE_JMS_TOPICS Default Value: True mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # Set storage class suitable for use on IBM Cloud ROKS kafka_storage_class: ibmc-block-gold # Generate a KafkaCfg template mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.kafka AWS MSK Role Variables \u00a4 Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. kafka_version \u00a4 The version of Kafka to deploy for AWS MSK. Environment Variable: KAFKA_VERSION Default Value: 3.3.1 kafka_cluster_name \u00a4 The name of the Kafka cluster that will be created Required Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka aws_region \u00a4 Required Environment Variable: AWS_REGION Default Value: None vpc_id \u00a4 The AWS Virtual Private Cloud identifier (VPC ID) where the MSK instance will be hosted. Required Environment Variable: VPC_ID Default Value: None aws_msk_cidr_az1 \u00a4 The CIDR address for the first Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ1 Default Value: None aws_msk_cidr_az2 \u00a4 The CIDR address for the second Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ2 Default Value: None aws_msk_cidr_az3 \u00a4 The CIDR address for the third Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ3 Default Value: None aws_msk_ingress_cidr \u00a4 The IPv4 CIDR address for ingress connection. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_INGRESS_CIDR Default Value: None aws_msk_egress_cidr \u00a4 The IPv4 CIDR address for egress connection. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_EGRESS_CIDR Default Value: None aws_kafka_user_name \u00a4 The name of the user to setup in the cluster for MAS. Required Environment Variable: AWS_KAFKA_USER_NAME Default Value: None aws_kafka_user_password \u00a4 The password of the user to setup in the cluster for MAS. Optional Environment Variable: AWS_KAFKA_USER_PASSWORD Default Value: None aws_msk_instance_type \u00a4 The type/flavor of your MSK instance. Optional Environment Variable: AWS_MSK_INSTANCE_TYPE Default Value: kafka.m5.large aws_msk_volume_size \u00a4 The storage/volume size of your MSK instance. Optional Environment Variable: AWS_MSK_VOLUME_SIZE Default Value: 100 aws_msk_instance_number \u00a4 The number of broker/instances of your MSK instance. Optional Environment Variable: AWS_MSK_INSTANCE_NUMBER Default Value: 3 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None aws_msk_secret \u00a4 AWS MSK Secret name. The secret name must begin with the prefix AmazonMSK_. If this is not set, then default secret name will be AmazonMSK_SECRET_{{kafka_cluster_name}} Optional Environment Variable: AWS_MSK_SECRET Default Value: AmazonMSK_SECRET_{{kafka_cluster_name}}' Example Playbook to install AWS MSK \u00a4 - hosts: localhost any_errors_fatal: true vars: aws_region: ca-central-1 aws_access_key_id: ***** aws_secret_access_key: ***** kafka_version: 3.3.1 kafka_provider: aws kafka_action: install kafka_cluster_name: msk-abcd0zyxw kafka_namespace: msk-abcd0zyxw vpc_id: vpc-07088da510b3c35c5 aws_kafka_user_name: mskuser-abcd0zyxw aws_msk_instance_type: kafka.t3.small aws_msk_volume_size: 100 aws_msk_instance_number: 3 aws_msk_cidr_az1: \"10.0.128.0/20\" aws_msk_cidr_az2: \"10.0.144.0/20\" aws_msk_cidr_az3: \"10.0.160.0/20\" aws_msk_ingress_cidr: \"10.0.0.0/16\" aws_msk_egress_cidr: \"10.0.0.0/16\" # Generate a KafkaCfg template mas_config_dir: /var/tmp/masconfigdir mas_instance_id: abcd0zyxw roles: - ibm.mas_devops.kafka Example Playbook to uninstall AWS MSK \u00a4 - hosts: localhost any_errors_fatal: true vars: aws_region: ca-central-1 aws_access_key_id: ***** aws_secret_access_key: ***** vpc_id: vpc-07088da510b3c35c5 kafka_provider: aws kafka_action: uninstall kafka_cluster_name: msk-abcd0zyxw aws_msk_cidr_az1: \"10.0.128.0/20\" aws_msk_cidr_az2: \"10.0.144.0/20\" aws_msk_cidr_az3: \"10.0.160.0/20\" roles: - ibm.mas_devops.kafka License \u00a4 EPL-2.0","title":"kafka"},{"location":"roles/kafka/#kafka","text":"This role provides support to install a Kafka Cluster using Strimzi , Red Hat AMQ Streams , IBM Event Streams or AWS MSK and generate configuration that can be directly applied to Maximo Application Suite. Both Strimzi and Red Hat AMQ streams component are massively scalable, distributed, and high-performance data streaming platform based on the Apache Kafka project. Both offer a distributed backbone that allows microservices and other applications to share data with high throughput and low latency. As more applications move to Kubernetes and Red Hat OpenShift, it is increasingly important to be able to run the communication infrastructure on the same platform. Red Hat OpenShift, as a highly scalable platform, is a natural fit for messaging technologies such as Kafka. The AMQ streams component makes running and managing Apache Kafka OpenShift native through the use of powerful operators that simplify the deployment, configuration, management, and use of Apache Kafka on Red Hat OpenShift. The AMQ streams component is part of the Red Hat AMQ family, which also includes the AMQ broker, a longtime innovation leader in Java\u2122 Message Service (JMS) and polyglot messaging, as well as the AMQ interconnect router, a wide-area, peer-to-peer messaging solution. Under the covers, AMQ streams leverages Strimzi's architecture, resources and configurations. Note: The MAS license does not include entitlement for AMQ streams. The MAS Devops Collection supports this Kafka deployment as an example only. Therefore, we recommend the use of Strimzi for an opensource Kafka provider. Tip The role will generate a yaml file containing the definition of a Secret and KafkaCfg resource that can be used to configure the deployed cluster as the MAS system Kafka. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/kafkacfg-amqstreams-system.yaml or used in conjunction with the suite_config role.","title":"kafka"},{"location":"roles/kafka/#role-variables","text":"","title":"Role Variables"},{"location":"roles/kafka/#kafka_action","text":"Action to be performed by Kafka role. Valid values are install , upgrade or uninstall . The upgrade action applies only to the strimzi and redhat providers. Environment Variable: KAFKA_ACTION Default Value: install","title":"kafka_action"},{"location":"roles/kafka/#kafka_provider","text":"Valid kafka providers are strimzi (opensource), redhat (installs AMQ Streams which requires a license that is not included with MAS entitlement), ibm (provisions a paid Event Streams instance in the target IBM Cloud account) and aws (provisions a paid MSK instance in the target AWS account). Environment Variable: KAFKA_PROVIDER Default Value: strimzi","title":"kafka_provider"},{"location":"roles/kafka/#red-hat-amq-streams-strimzi-role-variables","text":"","title":"Red Hat AMQ Streams &amp; Strimzi Role Variables"},{"location":"roles/kafka/#kafka_version","text":"The version of Kafka to deploy by the operator. Before changing the kafka_version make the version is supported by the amq-streams operator version or strimzi operator version . Environment Variable: KAFKA_VERSION Default Value: 3.5.0 for AMQ Streams and 3.7.0 for Strimzi.","title":"kafka_version"},{"location":"roles/kafka/#kafka_namespace","text":"The namespace where the operator and Kafka cluster will be deployed. Environment Variable: KAFKA_NAMESPACE Default Value: amq-streams for AMQ Streams and strimzi for Strimzi.","title":"kafka_namespace"},{"location":"roles/kafka/#kafka_cluster_name","text":"The name of the Kafka cluster that will be created Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka","title":"kafka_cluster_name"},{"location":"roles/kafka/#kafka_cluster_size","text":"The configuration to apply, there are two configurations available: small and large. Environment Variable: KAFKA_CLUSTER_SIZE Default Value: small","title":"kafka_cluster_size"},{"location":"roles/kafka/#kafka_storage_class","text":"The name of the storage class to configure the AMQStreams operator to use for persistent storage in the Kafka cluster. Storage class must support ReadWriteOnce(RWO) access mode. Environment Variable: KAFKA_STORAGE_CLASS Default Value: lookup supported storage classes in the cluster","title":"kafka_storage_class"},{"location":"roles/kafka/#kafka_storage_size","text":"The size of the storage to configure the AMQStreams operator to use for persistent storage in the Kafka cluster. Environment Variable: KAFKA_STORAGE_SIZE Default Value: 100Gi","title":"kafka_storage_size"},{"location":"roles/kafka/#zookeeper_storage_class","text":"The name of the storage class to configure the AMQStreams operator to use for persistent storage in the Zookeeper cluster. Storage class must support ReadWriteOnce(RWO) access mode. Environment Variable: ZOOKEEPER_STORAGE_CLASS Default Value: lookup supported storage classes in the cluster","title":"zookeeper_storage_class"},{"location":"roles/kafka/#zookeeper_storage_size","text":"The size of the storage to configure the AMQStreams operator to use for persistent storage in the Zookeeper cluster. Environment Variable: ZOOKEEPER_STORAGE_SIZE Default Value: 10Gi","title":"zookeeper_storage_size"},{"location":"roles/kafka/#kafka_user_name","text":"The name of the user to setup in the cluster for MAS. Environment Variable: KAFKA_USER_NAME Default Value: masuser","title":"kafka_user_name"},{"location":"roles/kafka/#kafka_user_password-supported-in-strimzi-operator-verion-0250-amq-streams-operator-version-2x","text":"The password of the user to setup in the cluster for MAS. Environment Variable: KAFKA_USER_PASSWORD Default Value: a randomly generated password is used if one is not specified","title":"kafka_user_password (supported in Strimzi operator verion 0.25.0 - amq streams operator version 2.x)"},{"location":"roles/kafka/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/kafka/#mas_config_dir","text":"Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/kafka/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/kafka/#ibm-cloud-evenstreams-role-variables","text":"","title":"IBM Cloud Evenstreams Role Variables"},{"location":"roles/kafka/#ibmcloud_apikey","text":"Defines IBM Cloud API Key. This API Key needs to have access to manage (provision/deprovision) IBM Cloud Event Streams. Required Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/kafka/#eventstreams_resourcegroup","text":"Defines the IBM Cloud Resource Group to target the Event Streams instance. Optional Environment Variable: EVENTSTREAMS_RESOURCEGROUP Default Value: Default or value defined by IBMCLOUD_RESOURCEGROUP","title":"eventstreams_resourcegroup"},{"location":"roles/kafka/#eventstreams_name","text":"Event Streams instance name. Required Environment Variable: EVENTSTREAMS_NAME Default Value: None","title":"eventstreams_name"},{"location":"roles/kafka/#eventstreams_plan","text":"Event Streams instance plan. Optional Environment Variable: EVENTSTREAMS_PLAN Default Value: standard","title":"eventstreams_plan"},{"location":"roles/kafka/#eventstreams_location","text":"Optional Environment Variable: EVENTSTREAMS_LOCATION Default Value: us-east or value defined by IBMCLOUD_REGION","title":"eventstreams_location"},{"location":"roles/kafka/#eventstreams_retention","text":"Event Streams topic retention period (in miliseconds). Optional Environment Variable: EVENTSTREAMS_RETENTION Default Value: 1209600000","title":"eventstreams_retention"},{"location":"roles/kafka/#eventstreams_create_manage_jms_topic","text":"Defines whether to create specific Manage application JMS topics by default. Optional Environment Variable: EVENTSTREAMS_CREATE_MANAGE_JMS_TOPICS Default Value: True","title":"eventstreams_create_manage_jms_topic"},{"location":"roles/kafka/#mas_instance_id_1","text":"The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/kafka/#mas_config_dir_1","text":"Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/kafka/#custom_labels_1","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/kafka/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Set storage class suitable for use on IBM Cloud ROKS kafka_storage_class: ibmc-block-gold # Generate a KafkaCfg template mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.kafka","title":"Example Playbook"},{"location":"roles/kafka/#aws-msk-role-variables","text":"","title":"AWS MSK Role Variables"},{"location":"roles/kafka/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role.","title":"Prerequisites"},{"location":"roles/kafka/#kafka_version_1","text":"The version of Kafka to deploy for AWS MSK. Environment Variable: KAFKA_VERSION Default Value: 3.3.1","title":"kafka_version"},{"location":"roles/kafka/#kafka_cluster_name_1","text":"The name of the Kafka cluster that will be created Required Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka","title":"kafka_cluster_name"},{"location":"roles/kafka/#aws_region","text":"Required Environment Variable: AWS_REGION Default Value: None","title":"aws_region"},{"location":"roles/kafka/#vpc_id","text":"The AWS Virtual Private Cloud identifier (VPC ID) where the MSK instance will be hosted. Required Environment Variable: VPC_ID Default Value: None","title":"vpc_id"},{"location":"roles/kafka/#aws_msk_cidr_az1","text":"The CIDR address for the first Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ1 Default Value: None","title":"aws_msk_cidr_az1"},{"location":"roles/kafka/#aws_msk_cidr_az2","text":"The CIDR address for the second Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ2 Default Value: None","title":"aws_msk_cidr_az2"},{"location":"roles/kafka/#aws_msk_cidr_az3","text":"The CIDR address for the third Availability Zone subnet. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_CIDR_AZ3 Default Value: None","title":"aws_msk_cidr_az3"},{"location":"roles/kafka/#aws_msk_ingress_cidr","text":"The IPv4 CIDR address for ingress connection. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_INGRESS_CIDR Default Value: None","title":"aws_msk_ingress_cidr"},{"location":"roles/kafka/#aws_msk_egress_cidr","text":"The IPv4 CIDR address for egress connection. This information is found in the subnet details under your VPC. Required Environment Variable: AWS_MSK_EGRESS_CIDR Default Value: None","title":"aws_msk_egress_cidr"},{"location":"roles/kafka/#aws_kafka_user_name","text":"The name of the user to setup in the cluster for MAS. Required Environment Variable: AWS_KAFKA_USER_NAME Default Value: None","title":"aws_kafka_user_name"},{"location":"roles/kafka/#aws_kafka_user_password","text":"The password of the user to setup in the cluster for MAS. Optional Environment Variable: AWS_KAFKA_USER_PASSWORD Default Value: None","title":"aws_kafka_user_password"},{"location":"roles/kafka/#aws_msk_instance_type","text":"The type/flavor of your MSK instance. Optional Environment Variable: AWS_MSK_INSTANCE_TYPE Default Value: kafka.m5.large","title":"aws_msk_instance_type"},{"location":"roles/kafka/#aws_msk_volume_size","text":"The storage/volume size of your MSK instance. Optional Environment Variable: AWS_MSK_VOLUME_SIZE Default Value: 100","title":"aws_msk_volume_size"},{"location":"roles/kafka/#aws_msk_instance_number","text":"The number of broker/instances of your MSK instance. Optional Environment Variable: AWS_MSK_INSTANCE_NUMBER Default Value: 3","title":"aws_msk_instance_number"},{"location":"roles/kafka/#mas_instance_id_2","text":"The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/kafka/#mas_config_dir_2","text":"Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/kafka/#custom_labels_2","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/kafka/#aws_msk_secret","text":"AWS MSK Secret name. The secret name must begin with the prefix AmazonMSK_. If this is not set, then default secret name will be AmazonMSK_SECRET_{{kafka_cluster_name}} Optional Environment Variable: AWS_MSK_SECRET Default Value: AmazonMSK_SECRET_{{kafka_cluster_name}}'","title":"aws_msk_secret"},{"location":"roles/kafka/#example-playbook-to-install-aws-msk","text":"- hosts: localhost any_errors_fatal: true vars: aws_region: ca-central-1 aws_access_key_id: ***** aws_secret_access_key: ***** kafka_version: 3.3.1 kafka_provider: aws kafka_action: install kafka_cluster_name: msk-abcd0zyxw kafka_namespace: msk-abcd0zyxw vpc_id: vpc-07088da510b3c35c5 aws_kafka_user_name: mskuser-abcd0zyxw aws_msk_instance_type: kafka.t3.small aws_msk_volume_size: 100 aws_msk_instance_number: 3 aws_msk_cidr_az1: \"10.0.128.0/20\" aws_msk_cidr_az2: \"10.0.144.0/20\" aws_msk_cidr_az3: \"10.0.160.0/20\" aws_msk_ingress_cidr: \"10.0.0.0/16\" aws_msk_egress_cidr: \"10.0.0.0/16\" # Generate a KafkaCfg template mas_config_dir: /var/tmp/masconfigdir mas_instance_id: abcd0zyxw roles: - ibm.mas_devops.kafka","title":"Example Playbook to install AWS MSK"},{"location":"roles/kafka/#example-playbook-to-uninstall-aws-msk","text":"- hosts: localhost any_errors_fatal: true vars: aws_region: ca-central-1 aws_access_key_id: ***** aws_secret_access_key: ***** vpc_id: vpc-07088da510b3c35c5 kafka_provider: aws kafka_action: uninstall kafka_cluster_name: msk-abcd0zyxw aws_msk_cidr_az1: \"10.0.128.0/20\" aws_msk_cidr_az2: \"10.0.144.0/20\" aws_msk_cidr_az3: \"10.0.160.0/20\" roles: - ibm.mas_devops.kafka","title":"Example Playbook to uninstall AWS MSK"},{"location":"roles/kafka/#license","text":"EPL-2.0","title":"License"},{"location":"roles/key_rotation/","text":"key_rotation \u00a4 Create new apikey for user in cloud account and delete the existing one. Role Variables \u00a4 cluster_type \u00a4 Required. Specify the cluster type, supported values are roks , and rosa . Environment Variable: CLUSTER_TYPE Default Value: None Role Variables - ROKS \u00a4 ibmcloud_apikey \u00a4 Required. A new key will be created and this key will be deleted. Environment Variable: IBMCLOUD_APIKEY Default: None ibmcloud_keyname \u00a4 Required. A new key will be created and this key will be deleted. Environment Variable: IBMCLOUD_KEYNAME Default: None ibmcloud_output_keydir \u00a4 Optional. Environment Variable: IBMCLOUD_OUTPUT_KEYDIR Default: '/tmp' Role Variables - ROSA or IPI/AWS \u00a4 The following variables are used when cluster_type = rosa or cluster_type=ipe and cluster_platform=aws . aws_region \u00a4 Required when cluster_type = rosa or cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_REGION Default Value: us-east-1 aws_username \u00a4 Required. Environment Variable: AWS_USERNAME Default: None aws_access_key_id \u00a4 Required. A new key will be created and this key will be deleted. Environment Variable: AWS_ACCESS_KEY_ID Default: None aws_secret_access_key \u00a4 Required. A new key will be created and this key will be deleted. Environment Variable: AWS_SECRET_ACCESS_KEY Default: None Example Playbook \u00a4 - hosts: localhost vars: cluster_type: roks ibmcloud_apikey: ################ ibmcloud_keyname: myapikeyname roles: - ibm.mas_devops.key_rotation License \u00a4 EPL-2.0","title":"key_rotation"},{"location":"roles/key_rotation/#key_rotation","text":"Create new apikey for user in cloud account and delete the existing one.","title":"key_rotation"},{"location":"roles/key_rotation/#role-variables","text":"","title":"Role Variables"},{"location":"roles/key_rotation/#cluster_type","text":"Required. Specify the cluster type, supported values are roks , and rosa . Environment Variable: CLUSTER_TYPE Default Value: None","title":"cluster_type"},{"location":"roles/key_rotation/#role-variables-roks","text":"","title":"Role Variables - ROKS"},{"location":"roles/key_rotation/#ibmcloud_apikey","text":"Required. A new key will be created and this key will be deleted. Environment Variable: IBMCLOUD_APIKEY Default: None","title":"ibmcloud_apikey"},{"location":"roles/key_rotation/#ibmcloud_keyname","text":"Required. A new key will be created and this key will be deleted. Environment Variable: IBMCLOUD_KEYNAME Default: None","title":"ibmcloud_keyname"},{"location":"roles/key_rotation/#ibmcloud_output_keydir","text":"Optional. Environment Variable: IBMCLOUD_OUTPUT_KEYDIR Default: '/tmp'","title":"ibmcloud_output_keydir"},{"location":"roles/key_rotation/#role-variables-rosa-or-ipiaws","text":"The following variables are used when cluster_type = rosa or cluster_type=ipe and cluster_platform=aws .","title":"Role Variables - ROSA or IPI/AWS"},{"location":"roles/key_rotation/#aws_region","text":"Required when cluster_type = rosa or cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_REGION Default Value: us-east-1","title":"aws_region"},{"location":"roles/key_rotation/#aws_username","text":"Required. Environment Variable: AWS_USERNAME Default: None","title":"aws_username"},{"location":"roles/key_rotation/#aws_access_key_id","text":"Required. A new key will be created and this key will be deleted. Environment Variable: AWS_ACCESS_KEY_ID Default: None","title":"aws_access_key_id"},{"location":"roles/key_rotation/#aws_secret_access_key","text":"Required. A new key will be created and this key will be deleted. Environment Variable: AWS_SECRET_ACCESS_KEY Default: None","title":"aws_secret_access_key"},{"location":"roles/key_rotation/#example-playbook","text":"- hosts: localhost vars: cluster_type: roks ibmcloud_apikey: ################ ibmcloud_keyname: myapikeyname roles: - ibm.mas_devops.key_rotation","title":"Example Playbook"},{"location":"roles/key_rotation/#license","text":"EPL-2.0","title":"License"},{"location":"roles/kmodels/","text":"Kmodels \u00a4 ===== This role provides support to deploy Kmodels components for AI Broker Application: Install Kmodel controller Install istio Install Kmodel store Install Kmodel watcher Role Variables \u00a4 tenantName \u00a4 The tenant name for Kmodels role. Environment Variable: MAS_AIBROKER_TENANT_NAME Default Value: user storage_piplines_bucket \u00a4 The storage piplines bucket for Kmodels role. Environment Variable: MAS_AIBROKER_STORAGE_PIPELINES_BUCKET Default Value: `` storage_tenants_bucket \u00a4 The storage tenants bucket for Kmodels role. Environment Variable: MAS_AIBROKER_STORAGE_TENANTS_BUCKET Default Value: `` License \u00a4 EPL-2.0","title":"Kmodels"},{"location":"roles/kmodels/#kmodels","text":"===== This role provides support to deploy Kmodels components for AI Broker Application: Install Kmodel controller Install istio Install Kmodel store Install Kmodel watcher","title":"Kmodels"},{"location":"roles/kmodels/#role-variables","text":"","title":"Role Variables"},{"location":"roles/kmodels/#tenantname","text":"The tenant name for Kmodels role. Environment Variable: MAS_AIBROKER_TENANT_NAME Default Value: user","title":"tenantName"},{"location":"roles/kmodels/#storage_piplines_bucket","text":"The storage piplines bucket for Kmodels role. Environment Variable: MAS_AIBROKER_STORAGE_PIPELINES_BUCKET Default Value: ``","title":"storage_piplines_bucket"},{"location":"roles/kmodels/#storage_tenants_bucket","text":"The storage tenants bucket for Kmodels role. Environment Variable: MAS_AIBROKER_STORAGE_TENANTS_BUCKET Default Value: ``","title":"storage_tenants_bucket"},{"location":"roles/kmodels/#license","text":"EPL-2.0","title":"License"},{"location":"roles/mirror_case_prepare/","text":"mirror_case_prepare \u00a4 This role generates a mirror manifest file suitable for use with the oc mirror command (or the ibm.mas_devops.mirror_images role) from an IBM CASE bundle. Requirements \u00a4 The ibm-pak plugin must be installed. Role Variables \u00a4 case_name \u00a4 The name of the CASE bundle to prepare for mirroring. Required Environment Variable: CASE_NAME Default: None case_version \u00a4 The version of the CASE bundle to prepare for mirroring. Required Environment Variable: CASE_VERSION Default: None registry_public_host \u00a4 The public hostname for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None registry_public_port \u00a4 The public port for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None registry_prefix \u00a4 The prefix used for the target registry. The images will not be mirrored to the registry at this time but will define the final destination in the form: {host}:{port}/{prefix}/{reponame} Environment Variable: REGISTRY_PREFIX Default: None exclude_images \u00a4 A list of child CASE bundles to exclude from the mirroring process. Optional Environment Variable: None Default: None Role Variables - IBM Pak \u00a4 ibmpak_skip_verify \u00a4 Skip the certification verification when downloading CASE bundles with oc ibm-pak get . Optional Environment Variable: IBMPAK_SKIP_VERIFY Default: False ibmpak_skip_dependencies \u00a4 Skip downloading CASE bundle dependencies with oc ibm-pak get . Optional Environment Variable: IBMPAK_SKIP_DEPENDENCIES Default: False ibmpak_insecure \u00a4 Skip TLS/SSL verification when downloading CASE bundles with oc ibm-pak get . Optional Environment Variable: IBMPAK_INSECURE Default: False Example Playbook \u00a4 - hosts: localhost vars: case_name: ibm-mas case_version: 8.8.1 exclude_images: - ibm-truststore-mgr - ibm-sls - ibm-mas-assist - ibm-mas-iot - ibm-mas-manage registry_public_host: myregistry.com registry_public_port: 32500 registry_prefix: projectName roles: - ibm.mas_devops.mirror_case_prepare License \u00a4 EPL-2.0","title":"mirror_case_prepare"},{"location":"roles/mirror_case_prepare/#mirror_case_prepare","text":"This role generates a mirror manifest file suitable for use with the oc mirror command (or the ibm.mas_devops.mirror_images role) from an IBM CASE bundle.","title":"mirror_case_prepare"},{"location":"roles/mirror_case_prepare/#requirements","text":"The ibm-pak plugin must be installed.","title":"Requirements"},{"location":"roles/mirror_case_prepare/#role-variables","text":"","title":"Role Variables"},{"location":"roles/mirror_case_prepare/#case_name","text":"The name of the CASE bundle to prepare for mirroring. Required Environment Variable: CASE_NAME Default: None","title":"case_name"},{"location":"roles/mirror_case_prepare/#case_version","text":"The version of the CASE bundle to prepare for mirroring. Required Environment Variable: CASE_VERSION Default: None","title":"case_version"},{"location":"roles/mirror_case_prepare/#registry_public_host","text":"The public hostname for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None","title":"registry_public_host"},{"location":"roles/mirror_case_prepare/#registry_public_port","text":"The public port for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None","title":"registry_public_port"},{"location":"roles/mirror_case_prepare/#registry_prefix","text":"The prefix used for the target registry. The images will not be mirrored to the registry at this time but will define the final destination in the form: {host}:{port}/{prefix}/{reponame} Environment Variable: REGISTRY_PREFIX Default: None","title":"registry_prefix"},{"location":"roles/mirror_case_prepare/#exclude_images","text":"A list of child CASE bundles to exclude from the mirroring process. Optional Environment Variable: None Default: None","title":"exclude_images"},{"location":"roles/mirror_case_prepare/#role-variables-ibm-pak","text":"","title":"Role Variables - IBM Pak"},{"location":"roles/mirror_case_prepare/#ibmpak_skip_verify","text":"Skip the certification verification when downloading CASE bundles with oc ibm-pak get . Optional Environment Variable: IBMPAK_SKIP_VERIFY Default: False","title":"ibmpak_skip_verify"},{"location":"roles/mirror_case_prepare/#ibmpak_skip_dependencies","text":"Skip downloading CASE bundle dependencies with oc ibm-pak get . Optional Environment Variable: IBMPAK_SKIP_DEPENDENCIES Default: False","title":"ibmpak_skip_dependencies"},{"location":"roles/mirror_case_prepare/#ibmpak_insecure","text":"Skip TLS/SSL verification when downloading CASE bundles with oc ibm-pak get . Optional Environment Variable: IBMPAK_INSECURE Default: False","title":"ibmpak_insecure"},{"location":"roles/mirror_case_prepare/#example-playbook","text":"- hosts: localhost vars: case_name: ibm-mas case_version: 8.8.1 exclude_images: - ibm-truststore-mgr - ibm-sls - ibm-mas-assist - ibm-mas-iot - ibm-mas-manage registry_public_host: myregistry.com registry_public_port: 32500 registry_prefix: projectName roles: - ibm.mas_devops.mirror_case_prepare","title":"Example Playbook"},{"location":"roles/mirror_case_prepare/#license","text":"EPL-2.0","title":"License"},{"location":"roles/mirror_extras_prepare/","text":"mirror_extras_prepare \u00a4 This role generates a mirror manifest file suitable for use with the oc mirror command (or the ibm.mas_devops.mirror_images role) for a specific set of extra images. Available Extras \u00a4 Extra Versions Description catalog N/A Special extra package for mirroring the IBM Maximo Operator Catalog db2u 1.0.0, 1.0.1 Extra container images missing from the ibm-db2operator CASE bundle mongoce 4.2.6, 4.2.23, 4.4.21 Package containing all images required to use MongoCE Operator in the disconnected environment uds 1.0.0, 1.1.0, 1.2.0, 1.3.0 Extra container images missing from the ibm-uds CASE bundle wd 5.3.1 Extra container images missing from the ibm-watson-discovery CASE bundle odf 4.15 Extra images needed for ODF 4.15 Role Variables \u00a4 extras_name \u00a4 The name of the extras package to prepare for mirroring. Required Environment Variable: EXTRAS_NAME Default: None extras_version \u00a4 The version of the extras package to prepare for mirroring. Required Environment Variable: EXTRAS_VERSION Default: None registry_public_host \u00a4 The public hostname for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None registry_public_port \u00a4 The public port for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None registry_prefix \u00a4 The prefix used for the target registry. The images will not be mirrored to the registry at this time but will define the final destination in the form: {host}:{port}/{prefix}/{reponame} Environment Variable: REGISTRY_PREFIX Default: None Example Playbook \u00a4 - hosts: localhost vars: extras_name: mongoce extras_version: 4.2.6 registry_public_host: myocp-5f1320191125833da1cac8216c06779e-0000.us-south.containers.appdomain.cloud registry_public_port: 32500 registry_prefix: projectName roles: - ibm.mas_devops.mirror_extras_prepare License \u00a4 EPL-2.0","title":"mirror_extras_prepare"},{"location":"roles/mirror_extras_prepare/#mirror_extras_prepare","text":"This role generates a mirror manifest file suitable for use with the oc mirror command (or the ibm.mas_devops.mirror_images role) for a specific set of extra images.","title":"mirror_extras_prepare"},{"location":"roles/mirror_extras_prepare/#available-extras","text":"Extra Versions Description catalog N/A Special extra package for mirroring the IBM Maximo Operator Catalog db2u 1.0.0, 1.0.1 Extra container images missing from the ibm-db2operator CASE bundle mongoce 4.2.6, 4.2.23, 4.4.21 Package containing all images required to use MongoCE Operator in the disconnected environment uds 1.0.0, 1.1.0, 1.2.0, 1.3.0 Extra container images missing from the ibm-uds CASE bundle wd 5.3.1 Extra container images missing from the ibm-watson-discovery CASE bundle odf 4.15 Extra images needed for ODF 4.15","title":"Available Extras"},{"location":"roles/mirror_extras_prepare/#role-variables","text":"","title":"Role Variables"},{"location":"roles/mirror_extras_prepare/#extras_name","text":"The name of the extras package to prepare for mirroring. Required Environment Variable: EXTRAS_NAME Default: None","title":"extras_name"},{"location":"roles/mirror_extras_prepare/#extras_version","text":"The version of the extras package to prepare for mirroring. Required Environment Variable: EXTRAS_VERSION Default: None","title":"extras_version"},{"location":"roles/mirror_extras_prepare/#registry_public_host","text":"The public hostname for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None","title":"registry_public_host"},{"location":"roles/mirror_extras_prepare/#registry_public_port","text":"The public port for the target registry. The images will not be mirrored to the registry at this time, but to prepare the manifest we need to know the target destination. Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None","title":"registry_public_port"},{"location":"roles/mirror_extras_prepare/#registry_prefix","text":"The prefix used for the target registry. The images will not be mirrored to the registry at this time but will define the final destination in the form: {host}:{port}/{prefix}/{reponame} Environment Variable: REGISTRY_PREFIX Default: None","title":"registry_prefix"},{"location":"roles/mirror_extras_prepare/#example-playbook","text":"- hosts: localhost vars: extras_name: mongoce extras_version: 4.2.6 registry_public_host: myocp-5f1320191125833da1cac8216c06779e-0000.us-south.containers.appdomain.cloud registry_public_port: 32500 registry_prefix: projectName roles: - ibm.mas_devops.mirror_extras_prepare","title":"Example Playbook"},{"location":"roles/mirror_extras_prepare/#license","text":"EPL-2.0","title":"License"},{"location":"roles/mirror_images/","text":"mirror_images \u00a4 Supports mirroring specific images to the target mirror registry","title":"mirror_images"},{"location":"roles/mirror_images/#mirror_images","text":"Supports mirroring specific images to the target mirror registry","title":"mirror_images"},{"location":"roles/mirror_ocp/","text":"mirror_ocp \u00a4 This role supports mirroring the Red Hat Platform and selected content from the Red Hat operator catalogs . Only content in the Red Hat catalogs directly used by IBM Maximo Application Suite is mirrored. Four actions are supported: direct Directly mirror content to your target registry to-filesystem Mirror content to the local filesystem from-filesystem Mirror content from the local filesystem to your target registry Three Catalogs are mirrored, containing the following content: certified-operator-index \u00a4 crunchy-postgres-operator (required by ibm.mas_devops.uds role) gpu-operator-certified (required by ibm.mas_devops.nvidia_gpu role) kubeturbo-certified (required by ibm.mas_devops.kubeturbo role) ibm-metrics-operator (required by ibm.mas_devops.dro role) ibm-data-reporter-operator (required by ibm.mas_devops.dro role) community-operator-index \u00a4 grafana-operator (required by ibm.mas_devops.grafana role) opentelemetry-operator (required by ibm.mas_devops.opentelemetry role) strimzi-kafka-operator (required by ibm.mas_devops.kafka role) redhat-operator-index \u00a4 amq-streams (required by ibm.mas_devops.kafka role) openshift-pipelines-operator-rh (required by the MAS CLI) nfd (required by ibm.mas_devops.nvidia_gpu role) aws-efs-csi-driver-operator (required by ibm.mas_devops.ocp_efs role) local-storage-operator (required by ibm.mas_devops.ocs role) odf-operator (required by ibm.mas_devops.ocs role) openshift-cert-manager-operator (required by ibm.mas_devops.cert_manager role) lvms-operator (not directly used, but often used in SNO environments) Requirements \u00a4 oc tool must be installed oc-mirror plugin must be installed Role Variables \u00a4 mirror_mode \u00a4 Set the action to perform ( direct , to-filesystem , from-filesystem ) Required Environment Variable: MIRROR_MODE Default: None Role Variables - Mirror Actions \u00a4 mirror_working_dir \u00a4 Set the working directory for the mirror operations Required Environment Variable: MIRROR_WORKING_DIR Default: None mirror_redhat_platform \u00a4 Enable mirroring of the Red Hat platform images. Optional Environment Variable: MIRROR_REDHAT_PLATFORM Default: False mirror_redhat_operators \u00a4 Enable mirroring of selected content from the Red Hat operator catalogs. Optional Environment Variable: MIRROR_REDHAT_OPERATORS Default: False redhat_pullsecret \u00a4 Path to your Red Hat pull secret, available from: https://console.redhat.com/openshift/install/pull-secret . Required Environment Variable: REDHAT_PULLSECRET Default: None Role Variables - OpenShift Version \u00a4 ocp_release \u00a4 The Red Hat release you are mirroring content for, e.g. 4.12 . Required Environment Variable: OCP_RELEASE Default: None ocp_min_version \u00a4 The minimum version of the Red Hat release to mirror platform content for, e.g. 4.12.0 . Optional Environment Variable: OCP_MIN_VERSION Default: None ocp_max_version \u00a4 The maximimum version of the Red Hat release to mirror platform content for, e.g. 4.12.18 . Optional Environment Variable: OCP_MAX_VERSION Default: None Role Variables - Target Registry \u00a4 registry_public_host \u00a4 The public hostname for the target registry Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None registry_public_port \u00a4 The public port number for the target registry Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None registry_prefix \u00a4 The prefix used for the target registry. The images will not be mirrored to the registry at this time but will define the final destination in the form: {host}:{port}/{prefix}/{reponame} Environment Variable: REGISTRY_PREFIX Default: None registry_username \u00a4 The username for the target registry. Required Environment Variable: REGISTRY_USERNAME Default: None registry_password \u00a4 The password for the target registry. Required Environment Variable: REGISTRY_PASSWORD Default: None Example Playbook \u00a4 - hosts: localhost vars: registry_public_host: myregistry.mycompany.com registry_public_port: 5000 registry_prefix: projectName registry_username: user1 registry_password: 8934jk77s862! # Not a real password, don't worry security folks mirror_mode: direct mirror_working_dir: /tmp/mirror mirror_redhat_platform: false mirror_redhat_operators: true ocp_release: 4.15 redhat_pullsecret: ~/pull-secret.json roles: - ibm.mas_devops.mirror_ocp License \u00a4 EPL-2.0","title":"mirror_ocp"},{"location":"roles/mirror_ocp/#mirror_ocp","text":"This role supports mirroring the Red Hat Platform and selected content from the Red Hat operator catalogs . Only content in the Red Hat catalogs directly used by IBM Maximo Application Suite is mirrored. Four actions are supported: direct Directly mirror content to your target registry to-filesystem Mirror content to the local filesystem from-filesystem Mirror content from the local filesystem to your target registry Three Catalogs are mirrored, containing the following content:","title":"mirror_ocp"},{"location":"roles/mirror_ocp/#certified-operator-index","text":"crunchy-postgres-operator (required by ibm.mas_devops.uds role) gpu-operator-certified (required by ibm.mas_devops.nvidia_gpu role) kubeturbo-certified (required by ibm.mas_devops.kubeturbo role) ibm-metrics-operator (required by ibm.mas_devops.dro role) ibm-data-reporter-operator (required by ibm.mas_devops.dro role)","title":"certified-operator-index"},{"location":"roles/mirror_ocp/#community-operator-index","text":"grafana-operator (required by ibm.mas_devops.grafana role) opentelemetry-operator (required by ibm.mas_devops.opentelemetry role) strimzi-kafka-operator (required by ibm.mas_devops.kafka role)","title":"community-operator-index"},{"location":"roles/mirror_ocp/#redhat-operator-index","text":"amq-streams (required by ibm.mas_devops.kafka role) openshift-pipelines-operator-rh (required by the MAS CLI) nfd (required by ibm.mas_devops.nvidia_gpu role) aws-efs-csi-driver-operator (required by ibm.mas_devops.ocp_efs role) local-storage-operator (required by ibm.mas_devops.ocs role) odf-operator (required by ibm.mas_devops.ocs role) openshift-cert-manager-operator (required by ibm.mas_devops.cert_manager role) lvms-operator (not directly used, but often used in SNO environments)","title":"redhat-operator-index"},{"location":"roles/mirror_ocp/#requirements","text":"oc tool must be installed oc-mirror plugin must be installed","title":"Requirements"},{"location":"roles/mirror_ocp/#role-variables","text":"","title":"Role Variables"},{"location":"roles/mirror_ocp/#mirror_mode","text":"Set the action to perform ( direct , to-filesystem , from-filesystem ) Required Environment Variable: MIRROR_MODE Default: None","title":"mirror_mode"},{"location":"roles/mirror_ocp/#role-variables-mirror-actions","text":"","title":"Role Variables - Mirror Actions"},{"location":"roles/mirror_ocp/#mirror_working_dir","text":"Set the working directory for the mirror operations Required Environment Variable: MIRROR_WORKING_DIR Default: None","title":"mirror_working_dir"},{"location":"roles/mirror_ocp/#mirror_redhat_platform","text":"Enable mirroring of the Red Hat platform images. Optional Environment Variable: MIRROR_REDHAT_PLATFORM Default: False","title":"mirror_redhat_platform"},{"location":"roles/mirror_ocp/#mirror_redhat_operators","text":"Enable mirroring of selected content from the Red Hat operator catalogs. Optional Environment Variable: MIRROR_REDHAT_OPERATORS Default: False","title":"mirror_redhat_operators"},{"location":"roles/mirror_ocp/#redhat_pullsecret","text":"Path to your Red Hat pull secret, available from: https://console.redhat.com/openshift/install/pull-secret . Required Environment Variable: REDHAT_PULLSECRET Default: None","title":"redhat_pullsecret"},{"location":"roles/mirror_ocp/#role-variables-openshift-version","text":"","title":"Role Variables - OpenShift Version"},{"location":"roles/mirror_ocp/#ocp_release","text":"The Red Hat release you are mirroring content for, e.g. 4.12 . Required Environment Variable: OCP_RELEASE Default: None","title":"ocp_release"},{"location":"roles/mirror_ocp/#ocp_min_version","text":"The minimum version of the Red Hat release to mirror platform content for, e.g. 4.12.0 . Optional Environment Variable: OCP_MIN_VERSION Default: None","title":"ocp_min_version"},{"location":"roles/mirror_ocp/#ocp_max_version","text":"The maximimum version of the Red Hat release to mirror platform content for, e.g. 4.12.18 . Optional Environment Variable: OCP_MAX_VERSION Default: None","title":"ocp_max_version"},{"location":"roles/mirror_ocp/#role-variables-target-registry","text":"","title":"Role Variables - Target Registry"},{"location":"roles/mirror_ocp/#registry_public_host","text":"The public hostname for the target registry Required Environment Variable: REGISTRY_PUBLIC_HOST Default: None","title":"registry_public_host"},{"location":"roles/mirror_ocp/#registry_public_port","text":"The public port number for the target registry Required Environment Variable: REGISTRY_PUBLIC_PORT Default: None","title":"registry_public_port"},{"location":"roles/mirror_ocp/#registry_prefix","text":"The prefix used for the target registry. The images will not be mirrored to the registry at this time but will define the final destination in the form: {host}:{port}/{prefix}/{reponame} Environment Variable: REGISTRY_PREFIX Default: None","title":"registry_prefix"},{"location":"roles/mirror_ocp/#registry_username","text":"The username for the target registry. Required Environment Variable: REGISTRY_USERNAME Default: None","title":"registry_username"},{"location":"roles/mirror_ocp/#registry_password","text":"The password for the target registry. Required Environment Variable: REGISTRY_PASSWORD Default: None","title":"registry_password"},{"location":"roles/mirror_ocp/#example-playbook","text":"- hosts: localhost vars: registry_public_host: myregistry.mycompany.com registry_public_port: 5000 registry_prefix: projectName registry_username: user1 registry_password: 8934jk77s862! # Not a real password, don't worry security folks mirror_mode: direct mirror_working_dir: /tmp/mirror mirror_redhat_platform: false mirror_redhat_operators: true ocp_release: 4.15 redhat_pullsecret: ~/pull-secret.json roles: - ibm.mas_devops.mirror_ocp","title":"Example Playbook"},{"location":"roles/mirror_ocp/#license","text":"EPL-2.0","title":"License"},{"location":"roles/mongodb/","text":"mongodb \u00a4 This role currently supports provisioning of mongodb in three different providers: - community - aws (documentdb) - ibm If the selected provider is community then the MongoDB Community Kubernetes Operator will be configured and deployed into the specified namespace. By default a three member MongoDB replica set will be created. The cluster will bind six PVCs, these provide persistence for the data and system logs across the three nodes. Currently there is no support built-in for customizing the cluster beyond this configuration. Tip The role will generate a yaml file containing the definition of a Secret and MongoCfg resource that can be used to configure the deployed instance as the MAS system MongoDb. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/mongocfg-mongoce-system.yaml or used in conjunction with the suite_config role. Prerequisites \u00a4 To run this role with providers as ibm or aws you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role when provider is either ibm or aws . To run the docdb_secret_rotate MONGODB_ACTION when the provider is aws you must have already installed the Mongo Shell . This role will install a GrafanaDashboard used for monitoring the MongoDB instance when the provided is community and you have run the grafana role previously. If you did not run the grafana role then the GrafanaDashboard won't be installed. Role Variables - Common \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the MongoCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a MongoCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated MongoCfg resource definition. This can be used to manually configure a MAS instance to connect to the Mongo cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a MongoCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None mongodb_provider \u00a4 MongoDB provider, choose whether to use the MongoDb Community Edition Operator ( community ), IBM Cloud Database for MongoDb ( ibm ), or AWS DocumentDb ( aws ). Environment variable: MONGODB_PROVIDER Default Value: community mongodb_action \u00a4 Determines which action to perform w.r.t mongodb for a specified provider: Environment variable: MONGODB_ACTION Default Value: install Each provider supports a different set of actions: - community : install , uninstall , backup , restore - aws : install , uninstall , docdb_secret_rotate , destroy-data - ibm : install , uninstall , backup , restore , create-mongo-service-credentials Role Variables - CE Operator \u00a4 mongodb_namespace \u00a4 The namespace where the operator and MongoDb cluster will be deployed. Environment Variable: MONGODB_NAMESPACE Default Value: mongoce mongodb_version \u00a4 Defines the specific mongo version to be used. Best practice would be to use the version associated with the current Maximo Application Suite catalog. However, this value can currently be overridden to 4.4.21, 5.0.21, 5.0.23, 6.0.10, 6.0.12, 7.0.12 Important It is advised to never attempt a downgrade a MongoDB instance managed by the MAS Devops Ansible Collection. Also best practices should include creating scheduled backups of any MongoDB instance. Optional Environment Variable: MONGODB_VERSION Default Value: Automatically defined by the mongo version specified in the latest MAS case bundle available . mongodb_override_spec \u00a4 This forces the deploy to use the environment variables instead of maintaining spec settings for the existing installed MongoDB. By default this is False and if you upgrade or reinstall Mongo your existing settings will be preserved. Important It is advised you check your existing Mongo installation before using this. If you do not set the environment variables to match what you have in the spec or you use defaults you may find your members, memory, and cpu reset to the default values specified in this README. Unknown settings are not preserved in the spec. Optional Environment Variable: MONGODB_OVERRIDE_SPEC Default Value: false List of preserved settings mongodb_cpu_limits mongodb_mem_limits mongodb_cpu_requests mongodb_mem_requests mongodb_storage_class mongodb_storage_capacity_data mongodb_storage_capacity_logs mongodb_replicas mongodb_storage_class \u00a4 Required : The name of the storage class to configure the MongoDb operator to use for persistent storage in the MongoDb cluster. Storage class must support ReadWriteOnce(RWO) access mode. Environment Variable: MONGODB_STORAGE_CLASS Default Value: None mongodb_storage_capacity_data \u00a4 The size of the PVC that will be created for data storage in the cluster. Environment Variable: MONGODB_STORAGE_CAPACITY_DATA Default Value: 20Gi mongodb_storage_capacity_logs \u00a4 The size of the PVC that will be created for log storage in the cluster. Environment Variable: MONGODB_STORAGE_CAPACITY_LOGS Default Value: 20Gi mongodb_cpu_limits \u00a4 The CPU limits on the mongodb container. Environment Variable: MONGODB_CPU_LIMITS Default Value: 1 mongodb_mem_limits \u00a4 The Memory limits on the mongodb container. Environment Variable: MONGODB_MEM_LIMITS Default Value: 1Gi mongodb_cpu_requests \u00a4 The CPU requests on the mongodb container. Environment Variable: MONGODB_CPU_REQUESTS Default Value: 500m mongodb_mem_requests \u00a4 The Memory requests on the mongodb container. Environment Variable: MONGODB_MEM_REQUESTS Default Value: 1Gi mongodb_replicas \u00a4 The number of the mongodb replica set members. Default is 3. Set to 1 for SNO Cluster. - Environment Variable: MONGODB_REPLICAS - Default Value: 3 custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None mongodb_v5_upgrade \u00a4 Set this to true to confirm you want to upgrade your existing Mongo instance from version 4.2 or 4.4 to version 5. Optional Environment Variable: MONGODB_V5_UPGRADE Default Value: false mongodb_v6_upgrade \u00a4 Set this to true to confirm you want to upgrade your existing Mongo instance from version 5 to version 6. Optional Environment Variable: MONGODB_V6_UPGRADE Default Value: false mongodb_v7_upgrade \u00a4 Set this to true to confirm you want to upgrade your existing Mongo instance from version 6 to version 7. Optional Environment Variable: MONGODB_V7_UPGRADE Default Value: false masbr_confirm_cluster \u00a4 Set true or false to indicate the role whether to confirm the currently connected cluster before running the backup or restore job. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false masbr_copy_timeout_sec \u00a4 Set the transfer files timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) masbr_job_timezone \u00a4 Set the time zone for creating scheduled backup job. If not set a value for this variable, this role will use UTC time zone when creating a CronJob for running scheduled backup job. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None masbr_storage_type \u00a4 Set local or cloud to indicate this role to save the backup files to local file system or cloud object storage. Required Environment Variable: MASBR_STORAGE_TYPE Default: None masbr_storage_local_folder \u00a4 Set local path to save the backup files. Required only when MASBR_STORAGE_TYPE=local Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None masbr_storage_cloud_rclone_file \u00a4 Set the path of rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_FILE Default: None masbr_storage_cloud_rclone_name \u00a4 Set the configuration name defined in rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_NAME Default: None masbr_storage_cloud_bucket \u00a4 Set the object storage bucket name for saving the backup files Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_BUCKET Default: None masbr_slack_enabled \u00a4 Set true or false to indicate whether this role will send Slack notification messages of the backup and restore progress. Optional Environment Variable: MASBR_SLACK_ENABLED Default: false masbr_slack_level \u00a4 Set failure , info or verbose to indicate this role to send Slack notification messages in which backup and resore phases: Slack level Backup/Restore phases failure Failed , PartiallyFailed info Completed , Failed , PartiallyFailed verbose InProgress , Completed , Failed , PartiallyFailed Optional Environment Variable: MASBR_SLACK_LEVEL Default: info masbr_slack_token \u00a4 The Slack integration token. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_TOKEN Default: None masbr_slack_channel \u00a4 The Slack channel to send the notification messages to. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_CHANNEL Default: None masbr_slack_user \u00a4 The sender of the Slack notification message. Optional Environment Variable: MASBR_SLACK_USER Default: MASBR masbr_backup_type \u00a4 Set full or incr to indicate the role to create a full backup or incremental backup. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full masbr_backup_from_version \u00a4 Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). This variable is only valid when MASBR_BACKUP_TYPE=incr . If not set a value for this variable, this role will try to find the latest full backup version from the specified storage location. Optional Environment Variable: MASBR_BACKUP_FROM_VERSION Default: None masbr_backup_schedule \u00a4 Set Cron expression to create a scheduled backup. If not set a value for this varialbe, this role will create an on-demand backup. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None masbr_restore_from_version \u00a4 Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) Required only when MONGODB_ACTION=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Role Variables - IBM Cloud \u00a4 ibm_mongo_name \u00a4 Required. IBM Cloud Mongo database instance name. Environment Variable: IBM_MONGO_NAME Default Value: mongo-${MAS_INSTANCE_ID} ibm_mongo_admin_password \u00a4 Optional. Sets IBM Cloud Mongo database administrator user password. If not set, an auto-generated 20 character length string will be used. Environment Variable: IBM_MONGO_ADMIN_PASSWORD Default Value: None. ibm_mongo_admin_credentials_secret_name \u00a4 Secret for MongoDB Admin credentials. Secret Name: <mongo-name>-admin-credentials ibm_mongo_service_credentials_secret_name \u00a4 Secret for MongoDB Service credentials. Secret Name: <mongo-name>-service-credentials ibm_mongo_resourcegroup \u00a4 Required.IBM Cloud Resource Group under which resource group will be created. Environment Variable: IBM_MONGO_RESOURCEGROUP Default Value: Default ibm_mongo_region \u00a4 Required.IBM Cloud region where MongoDB resources will be created. Environment Variable: IBM_MONGO_REGION Default Value: us-east ibmcloud_apikey \u00a4 Required.IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY ibm_mongo_plan \u00a4 Plan name for this IBMCloud Service. Environment Variable: IBM_MONGO_PLAN Default Value: standard ibm_mongo_service \u00a4 IBMCloud Offering name for MongoDB Database Value: databases-for-mongodb ibm_mongo_service_endpoints \u00a4 MongoDB Service Endpoints type can be either public or private Environment Variable: IBM_MONGO_SERVICE_ENDPOINTS Default Value: public ibm_mongo_version \u00a4 Specify MongoDB version to be deployed Environment Variable: IBM_MONGO_VERSION Default Value: 4.2 ibm_mongo_memory \u00a4 Specify MongoDB Memory size Environment Variable: IBM_MONGO_MEMORY Default Value: 3840 ibm_mongo_disk \u00a4 Specify MongoDB Disk size Environment Variable: IBM_MONGO_DISK Default Value: 30720 ibm_mongo_cpu \u00a4 Specify MongoDB CPU Environment Variable: IBM_MONGO_CPU Default Value: 0 ibm_mongo_name \u00a4 Resource Name in IBMCloud for MongoDB Value: mongo-{{mas_instance_id}} ibm_mongo_backup_id \u00a4 Required only if is_restore is True CRN ID for backup resource Environment Variable: IBM_MONGO_BACKUP_ID Default Value: `` is_restore \u00a4 Whether want to restore from an existing backup resource or not. Environment Variable: IS_RESTORE Default Value: false restored_mongodb_service_name \u00a4 Required only If is_restore is True .MongoDB Service Name Environment Variable: RESTORED_MONGODB_SERVICE_NAME Role Variables - AWS DocumentDB \u00a4 aws_access_key_id \u00a4 Required.AWS Account Access Key Id Environment Variable: AWS_ACCESS_KEY_ID aws_secret_access_key \u00a4 Required.AWS Account Secret Access Key Environment Variable: AWS_SECRET_ACCESS_KEY aws_region \u00a4 Required.AWS Region where DocumentDB and other resources will be created Environment Variable: AWS_REGION Default Value: us-east-2 vpc_id \u00a4 Required.AWS VPC ID under which documentdb,subnets and security group will be created Environment Variable: VPC_ID docdb_cluster_name \u00a4 Required.DocumentDB Cluster Name Environment Variable: DOCDB_CLUSTER_NAME docdb_subnet_group_name \u00a4 DocumentDB Subnet Group Name Value: docdb-{{ docdb_cluster_name }} docdb_security_group_name \u00a4 DocumentDB Security Group Name Value: docdb-{{ docdb_cluster_name }} docdb_admin_credentials_secret_name \u00a4 DocumentDB Admin Credentials Secret Name Value: {{ docdb_cluster_name }}-admin-credentials docdb_engine_version \u00a4 DocumentDB Engine version Environment variable: DOCDB_ENGINE_VERSION Default Value: 4.0.0 docdb_master_username \u00a4 DocumentDB master username Environment variable: DOCDB_MASTER_USERNAME Default Value: docdbadmin docdb_instance_class \u00a4 DocumentDB Instance Class Environment variable: DOCDB_INSTANCE_CLASS Default Value: db.t3.medium docdb_instance_number \u00a4 Number of instances required for DocumentDB Environment variable: DOCDB_INSTANCE_NUMBER Default Value: 3 docdb_instance_identifier_prefix \u00a4 Required. Prefix for DocumentDB Instance name Environment variable: DOCDB_INSTANCE_IDENTIFIER_PREFIX docdb_ingress_cidr \u00a4 Required. IPv4 Address from which incoming connection requests will be allowed to DocumentDB cluster e.g Provide IPv4 CIDR address of VPC where ROSA cluster is installed Environment variable: DOCDB_INGRESS_CIDR docdb_egress_cidr \u00a4 Required. IPv4 Address at which outgoing connection requests will be allowed to DocumentDB cluster e.g Provide IPv4 CIDR address of VPC where ROSA cluster is installed Environment variable: DOCDB_EGRESS_CIDR docdb_cidr_az1: \u00a4 Required. Provide IPv4 CIDR address for the subnet to be created in one of the 3 availabilty zones of your VPC. If the subnet exists already then it must contain the tag of Name: {{ docdb_cluster_name }}, if the subnet doesn't exist already then one is created. Environment variable: DOCDB_CIDR_AZ1 docdb_cidr_az2: \u00a4 Required. Provide IPv4 CIDR address for the subnet to be created in one of the 3 availabilty zones of your VPC. If the subnet exists already then it must contain the tag of Name: {{ docdb_cluster_name }}, if the subnet doesn't exist already then one is created. Environment variable: DOCDB_CIDR_AZ2 docdb_cidr_az3: \u00a4 Required. Provide IPv4 CIDR address for the subnet to be created in one of the 3 availabilty zones of your VPC. If the subnet exists already then it must contain the tag of Name: {{ docdb_cluster_name }}, if the subnet doesn't exist already then one is created. Environment variable: DOCDB_CIDR_AZ3 AWS DocumentDB Secret Rotate role Variables \u00a4 docdb_mongo_instance_name \u00a4 Required. DocumentDB Instance Name Environment variable: DOCDB_MONGO_INSTANCE_NAME docdb_host \u00a4 Required. Any one Host Address out of multiple documentDB Instances Environment variable: DOCDB_HOST docdb_port \u00a4 Required. Corresponding port address of DocumentDB Instance Host Environment variable: DOCDB_PORT docdb_instance_username \u00a4 Required. Specify username for which password is being changed Environment variable: DOCDB_INSTANCE_USERNAME docdb_instance_password_old \u00a4 Required. Specify the old user password Environment variable: DOCDB_PASSWORD_OLD docdb_master_password \u00a4 Required. DocumentDB Master Username Environment variable: DOCDB_MASTER_PASSWORD docdb_master_username \u00a4 Required. DocumentDB Master Password Environment variable: DOCDB_MASTER_USERNAME AWS DocumentDB destroy-data action Variables \u00a4 mas_instance_id \u00a4 The specified MAS instance ID Required Environment Variable: MAS_INSTANCE_ID Default Value: None mongo_username \u00a4 Mongo Username Environment Variable: MONGO_USERNAME Default Value: None mongo_password \u00a4 Mongo password Environment Variable: MONGO_PASSWORD Default Value: None config \u00a4 Mongo Config, please refer to the below example playbook section for details Required Environment Variable: CONFIG Default Value: None certificates \u00a4 Mongo Certificates, please refer to the below example playbook section for details Required Environment Variable: CERTIFICATES Default Value: None Example Playbooks \u00a4 Install (CE Operator) \u00a4 - hosts: localhost any_errors_fatal: true vars: mongodb_storage_class: ibmc-block-gold mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.mongodb Backup (CE Operator) \u00a4 - hosts: localhost any_errors_fatal: true vars: mongodb_action: backup mas_instance_id: masinst1 masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.mongodb Restore (CE Operator) \u00a4 - hosts: localhost any_errors_fatal: true vars: mongodb_action: restore mas_instance_id: masinst1 masbr_restore_from_version: 20240621021316 masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.mongodb Install (IBM Cloud) \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: ibm ibmcloud_apikey: apikey**** ibmcloud_resource_group: mas-test roles: - ibm.mas_devops.mongodb Install (AWS DocumentDB) \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: aws mongodb_action: provision docdb_size: ~/docdb-config.yml docdb_cluster_name: test-db docdb_ingress_cidr: 10.0.0.0/16 docdb_egress_cidr: 10.0.0.0/16 docdb_cidr_az1: 10.0.0.0/26 docdb_cidr_az2: 10.0.0.64/26 docdb_cidr_az3: 10.0.0.128/26 docdb_instance_identifier_prefix: test-db-instance vpc_id: test-vpc-id aws_access_key_id: aws-key aws_secret_access_key: aws-access-key roles: - ibm.mas_devops.mongodb AWS DocumentDb Secret Rotation \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: aws mongodb_action: docdb_secret_rotate docdb_mongo_instance_name: test-db-instance db_host: aws.test1.host7283-***** db_port: 27017 docdb_master_username: admin docdb_master_password: pass*** docdb_instance_password_old: oldpass**** docdb_instance_username: testuser aws_access_key_id: aws-key aws_secret_access_key: aws-access-key roles: - ibm.mas_devops.mongodb AWS DocumentDb destroy-data action \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mongodb_provider: aws mongodb_action: destroy-data mongo_username: pqradmin mongo_password: xyzabc config: configDb: admin authMechanism: DEFAULT retryWrites: false hosts: - host: abc-0.pqr.databases.appdomain.cloud port: 32250 - host: abc-1.pqr.databases.appdomain.cloud port: 32250 - host: abc-2.pqr.databases.appdomain.cloud port: 32250 certificates: - alias: ca crt: | -----BEGIN CERTIFICATE----- MIIDDzCCAfegAwIBAgIJANEH58y2/kzHMA0GCSqGSIb3DQEBCwUAMB4xHDAaBgNV BAMME0lCTSBDbG91ZCBEYXRhYmFzZXMwHhcNMTgwNjI1MTQyOTAwWhcNMjgwNjIy MTQyOTAwWjAeMRwwGgYDVQQDDBNJQk0gQ2xvdWQgRGF0YWJhc2VzMIIBIjANBgkq 1eKI2FLzYKpoKBe5rcnrM7nHgNc/nCdEs5JecHb1dHv1QfPm6pzIxwIDAQABo1Aw TjAdBgNVHQ4EFgQUK3+XZo1wyKs+DEoYXbHruwSpXjgwHwYDVR0jBBgwFoAUK3+X Zo1wyKs+DEoYXbHruwSpXjgwDAYDVR0TBAUwAwEB/zANBgkqhkiG9w0BAQsFAAOC doqqgGIZ2nxCkp5/FXxF/TMb55vteTQwfgBy60jVVkbF7eVOWCv0KaNHPF5hrqbN i+3XjJ7/peF3xMvTMoy35DcT3E2ZeSVjouZs15O90kI3k2daS2OHJABW0vSj4nLz +PQzp/B9cQmOO8dCe049Q3oaUA== -----END CERTIFICATE----- roles: - ibm.mas_devops.mongodb Troubleshooting \u00a4 Important Please be cautious while performing any of the troubleshooting steps outlined below. It is important to understand that the MongoDB Community operator persists data within Persistent Volume Claims. These claims should not be removed inadvertent deletion of the mongoce namespace could result in data loss. MongoDB Replica Set Pods Will Not Start \u00a4 MongoDB 5 has introduced new platform specific requirements. Please consult the Platform Support Notes for detailed information. It is of particular importance to confirm that the AVX instruction set is exposed or available to the MongoDB workloads. This can easily be determined by entering any running pod on the same OpenShift cluster where MongoDB replica set members are failing to start. Once inside of a running pod the following command can be executed to confirm if the AVX instruction set is available: cat /proc/cpuinfo | grep flags | grep avx If avx is not found in the available flags then either the physical processor hosting the OpenShift cluster does not provide the AVX instruction set or the virtual host configuration is not exposing the AVX instruction set. If the latter is suspected the virtual hosting documentation should be referenced for details on how to expose the AVX instruction set. LDAP Authentication \u00a4 If authenticating via LDAP with PLAIN specified for authMechanism then configDb must be set to $external in the MongoCfg. The field configDb in the MongoCfg refers to the authentication database. CA Certificate Renewal \u00a4 Warning If the MongoDB CA Certificate expires the MongoDB replica set will become unusable. Replica set members will not be able to communicate with each other and client applications (i.e. Maximo Application Suite components) will not be to connect. In order to renew the CA Certificate used by the MongoDB replica set the following steps must be taken: Delete the CA Certificate resource Delete the MongoDB server Certificate resource Delete the Secrets resources associated with both the CA Certificate and Server Certificate Delete the Secret resource which contains the MongoDB configuration parameters Delete the ConfigMap resources which contains the CA certificate Delete the Secret resource which contains the sever certificate and private key The following steps illustrate the process required to renew the CA Certificate, sever certificate and reconfigure the MongoDB replica set with the new CA and server certificates. The first step is to stop the Mongo replica set and MongoDb CE Operator pod. oc project mongoce oc delete deployment mongodb-kubernetes-operator Important Make sure the MongoDB Community operator pod has terminated before proceeding. oc delete statefulset mas-mongo-ce Important Make sure all pods in the mongoce namespace have terminated before proceeding Remove expired CA Certificate and Server Certificate resources. Clean up MongoDB Community configuration and then run the mongodb role. oc delete certificate mongo-ca-crt oc delete certificate mongo-server oc delete secret mongo-ca-secret oc delete secret mongo-server-cert oc delete secret mas-mongo-ce-config oc delete configmap mas-mongo-ce-cert-map oc delete secret mas-mongo-ce-server-certificate-key export ROLE_NAME=mongodb ansible-playbook ibm.mas_devops.run_role Once the mongodb role has completed the MongoDb CE Operator pod and Mongo replica set should be configured. After the CA and server Certificates have been renewed you must ensure that that MongoCfg Suite CR is updated with the new CA Certificate. First obtain the CA Certificate from the Secret resource mongo-ca-secret . Then edit the Suite MongoCfg CR in the Maximo Application Suite core namespace. This is done by updating the appropriate certificate under .spec.certificates in the MongoCfg CR: spec: certificates: - alias: ca crt: | -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- If an IBM Suite Licensing Service (SLS) is also connecting to the MongoDB replica set the LicenseService CR must also be updated to reflect the new MongoDB CA. This can be added to the .spec.mongo.certificates section of the LicenseService CR. mongo: certificates: - alias: mongoca crt: | -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- Once the CA certificate has been updated for the MongoCfg and LicenseService CRs several pods in the core and SLS namespaces might need to be restarted to pick up the changes. This would include but is not limited to coreidp, coreapi, api-licensing. License \u00a4 EPL-2.0","title":"mongodb"},{"location":"roles/mongodb/#mongodb","text":"This role currently supports provisioning of mongodb in three different providers: - community - aws (documentdb) - ibm If the selected provider is community then the MongoDB Community Kubernetes Operator will be configured and deployed into the specified namespace. By default a three member MongoDB replica set will be created. The cluster will bind six PVCs, these provide persistence for the data and system logs across the three nodes. Currently there is no support built-in for customizing the cluster beyond this configuration. Tip The role will generate a yaml file containing the definition of a Secret and MongoCfg resource that can be used to configure the deployed instance as the MAS system MongoDb. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/mongocfg-mongoce-system.yaml or used in conjunction with the suite_config role.","title":"mongodb"},{"location":"roles/mongodb/#prerequisites","text":"To run this role with providers as ibm or aws you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role when provider is either ibm or aws . To run the docdb_secret_rotate MONGODB_ACTION when the provider is aws you must have already installed the Mongo Shell . This role will install a GrafanaDashboard used for monitoring the MongoDB instance when the provided is community and you have run the grafana role previously. If you did not run the grafana role then the GrafanaDashboard won't be installed.","title":"Prerequisites"},{"location":"roles/mongodb/#role-variables-common","text":"","title":"Role Variables - Common"},{"location":"roles/mongodb/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the MongoCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a MongoCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/mongodb/#mas_config_dir","text":"Local directory to save the generated MongoCfg resource definition. This can be used to manually configure a MAS instance to connect to the Mongo cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a MongoCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/mongodb/#mongodb_provider","text":"MongoDB provider, choose whether to use the MongoDb Community Edition Operator ( community ), IBM Cloud Database for MongoDb ( ibm ), or AWS DocumentDb ( aws ). Environment variable: MONGODB_PROVIDER Default Value: community","title":"mongodb_provider"},{"location":"roles/mongodb/#mongodb_action","text":"Determines which action to perform w.r.t mongodb for a specified provider: Environment variable: MONGODB_ACTION Default Value: install Each provider supports a different set of actions: - community : install , uninstall , backup , restore - aws : install , uninstall , docdb_secret_rotate , destroy-data - ibm : install , uninstall , backup , restore , create-mongo-service-credentials","title":"mongodb_action"},{"location":"roles/mongodb/#role-variables-ce-operator","text":"","title":"Role Variables - CE Operator"},{"location":"roles/mongodb/#mongodb_namespace","text":"The namespace where the operator and MongoDb cluster will be deployed. Environment Variable: MONGODB_NAMESPACE Default Value: mongoce","title":"mongodb_namespace"},{"location":"roles/mongodb/#mongodb_version","text":"Defines the specific mongo version to be used. Best practice would be to use the version associated with the current Maximo Application Suite catalog. However, this value can currently be overridden to 4.4.21, 5.0.21, 5.0.23, 6.0.10, 6.0.12, 7.0.12 Important It is advised to never attempt a downgrade a MongoDB instance managed by the MAS Devops Ansible Collection. Also best practices should include creating scheduled backups of any MongoDB instance. Optional Environment Variable: MONGODB_VERSION Default Value: Automatically defined by the mongo version specified in the latest MAS case bundle available .","title":"mongodb_version"},{"location":"roles/mongodb/#mongodb_override_spec","text":"This forces the deploy to use the environment variables instead of maintaining spec settings for the existing installed MongoDB. By default this is False and if you upgrade or reinstall Mongo your existing settings will be preserved. Important It is advised you check your existing Mongo installation before using this. If you do not set the environment variables to match what you have in the spec or you use defaults you may find your members, memory, and cpu reset to the default values specified in this README. Unknown settings are not preserved in the spec. Optional Environment Variable: MONGODB_OVERRIDE_SPEC Default Value: false List of preserved settings mongodb_cpu_limits mongodb_mem_limits mongodb_cpu_requests mongodb_mem_requests mongodb_storage_class mongodb_storage_capacity_data mongodb_storage_capacity_logs mongodb_replicas","title":"mongodb_override_spec"},{"location":"roles/mongodb/#mongodb_storage_class","text":"Required : The name of the storage class to configure the MongoDb operator to use for persistent storage in the MongoDb cluster. Storage class must support ReadWriteOnce(RWO) access mode. Environment Variable: MONGODB_STORAGE_CLASS Default Value: None","title":"mongodb_storage_class"},{"location":"roles/mongodb/#mongodb_storage_capacity_data","text":"The size of the PVC that will be created for data storage in the cluster. Environment Variable: MONGODB_STORAGE_CAPACITY_DATA Default Value: 20Gi","title":"mongodb_storage_capacity_data"},{"location":"roles/mongodb/#mongodb_storage_capacity_logs","text":"The size of the PVC that will be created for log storage in the cluster. Environment Variable: MONGODB_STORAGE_CAPACITY_LOGS Default Value: 20Gi","title":"mongodb_storage_capacity_logs"},{"location":"roles/mongodb/#mongodb_cpu_limits","text":"The CPU limits on the mongodb container. Environment Variable: MONGODB_CPU_LIMITS Default Value: 1","title":"mongodb_cpu_limits"},{"location":"roles/mongodb/#mongodb_mem_limits","text":"The Memory limits on the mongodb container. Environment Variable: MONGODB_MEM_LIMITS Default Value: 1Gi","title":"mongodb_mem_limits"},{"location":"roles/mongodb/#mongodb_cpu_requests","text":"The CPU requests on the mongodb container. Environment Variable: MONGODB_CPU_REQUESTS Default Value: 500m","title":"mongodb_cpu_requests"},{"location":"roles/mongodb/#mongodb_mem_requests","text":"The Memory requests on the mongodb container. Environment Variable: MONGODB_MEM_REQUESTS Default Value: 1Gi","title":"mongodb_mem_requests"},{"location":"roles/mongodb/#mongodb_replicas","text":"The number of the mongodb replica set members. Default is 3. Set to 1 for SNO Cluster. - Environment Variable: MONGODB_REPLICAS - Default Value: 3","title":"mongodb_replicas"},{"location":"roles/mongodb/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/mongodb/#mongodb_v5_upgrade","text":"Set this to true to confirm you want to upgrade your existing Mongo instance from version 4.2 or 4.4 to version 5. Optional Environment Variable: MONGODB_V5_UPGRADE Default Value: false","title":"mongodb_v5_upgrade"},{"location":"roles/mongodb/#mongodb_v6_upgrade","text":"Set this to true to confirm you want to upgrade your existing Mongo instance from version 5 to version 6. Optional Environment Variable: MONGODB_V6_UPGRADE Default Value: false","title":"mongodb_v6_upgrade"},{"location":"roles/mongodb/#mongodb_v7_upgrade","text":"Set this to true to confirm you want to upgrade your existing Mongo instance from version 6 to version 7. Optional Environment Variable: MONGODB_V7_UPGRADE Default Value: false","title":"mongodb_v7_upgrade"},{"location":"roles/mongodb/#masbr_confirm_cluster","text":"Set true or false to indicate the role whether to confirm the currently connected cluster before running the backup or restore job. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false","title":"masbr_confirm_cluster"},{"location":"roles/mongodb/#masbr_copy_timeout_sec","text":"Set the transfer files timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours)","title":"masbr_copy_timeout_sec"},{"location":"roles/mongodb/#masbr_job_timezone","text":"Set the time zone for creating scheduled backup job. If not set a value for this variable, this role will use UTC time zone when creating a CronJob for running scheduled backup job. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None","title":"masbr_job_timezone"},{"location":"roles/mongodb/#masbr_storage_type","text":"Set local or cloud to indicate this role to save the backup files to local file system or cloud object storage. Required Environment Variable: MASBR_STORAGE_TYPE Default: None","title":"masbr_storage_type"},{"location":"roles/mongodb/#masbr_storage_local_folder","text":"Set local path to save the backup files. Required only when MASBR_STORAGE_TYPE=local Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None","title":"masbr_storage_local_folder"},{"location":"roles/mongodb/#masbr_storage_cloud_rclone_file","text":"Set the path of rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_FILE Default: None","title":"masbr_storage_cloud_rclone_file"},{"location":"roles/mongodb/#masbr_storage_cloud_rclone_name","text":"Set the configuration name defined in rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_NAME Default: None","title":"masbr_storage_cloud_rclone_name"},{"location":"roles/mongodb/#masbr_storage_cloud_bucket","text":"Set the object storage bucket name for saving the backup files Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_BUCKET Default: None","title":"masbr_storage_cloud_bucket"},{"location":"roles/mongodb/#masbr_slack_enabled","text":"Set true or false to indicate whether this role will send Slack notification messages of the backup and restore progress. Optional Environment Variable: MASBR_SLACK_ENABLED Default: false","title":"masbr_slack_enabled"},{"location":"roles/mongodb/#masbr_slack_level","text":"Set failure , info or verbose to indicate this role to send Slack notification messages in which backup and resore phases: Slack level Backup/Restore phases failure Failed , PartiallyFailed info Completed , Failed , PartiallyFailed verbose InProgress , Completed , Failed , PartiallyFailed Optional Environment Variable: MASBR_SLACK_LEVEL Default: info","title":"masbr_slack_level"},{"location":"roles/mongodb/#masbr_slack_token","text":"The Slack integration token. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_TOKEN Default: None","title":"masbr_slack_token"},{"location":"roles/mongodb/#masbr_slack_channel","text":"The Slack channel to send the notification messages to. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_CHANNEL Default: None","title":"masbr_slack_channel"},{"location":"roles/mongodb/#masbr_slack_user","text":"The sender of the Slack notification message. Optional Environment Variable: MASBR_SLACK_USER Default: MASBR","title":"masbr_slack_user"},{"location":"roles/mongodb/#masbr_backup_type","text":"Set full or incr to indicate the role to create a full backup or incremental backup. Optional Environment Variable: MASBR_BACKUP_TYPE Default: full","title":"masbr_backup_type"},{"location":"roles/mongodb/#masbr_backup_from_version","text":"Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). This variable is only valid when MASBR_BACKUP_TYPE=incr . If not set a value for this variable, this role will try to find the latest full backup version from the specified storage location. Optional Environment Variable: MASBR_BACKUP_FROM_VERSION Default: None","title":"masbr_backup_from_version"},{"location":"roles/mongodb/#masbr_backup_schedule","text":"Set Cron expression to create a scheduled backup. If not set a value for this varialbe, this role will create an on-demand backup. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None","title":"masbr_backup_schedule"},{"location":"roles/mongodb/#masbr_restore_from_version","text":"Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) Required only when MONGODB_ACTION=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None","title":"masbr_restore_from_version"},{"location":"roles/mongodb/#role-variables-ibm-cloud","text":"","title":"Role Variables - IBM Cloud"},{"location":"roles/mongodb/#ibm_mongo_name","text":"Required. IBM Cloud Mongo database instance name. Environment Variable: IBM_MONGO_NAME Default Value: mongo-${MAS_INSTANCE_ID}","title":"ibm_mongo_name"},{"location":"roles/mongodb/#ibm_mongo_admin_password","text":"Optional. Sets IBM Cloud Mongo database administrator user password. If not set, an auto-generated 20 character length string will be used. Environment Variable: IBM_MONGO_ADMIN_PASSWORD Default Value: None.","title":"ibm_mongo_admin_password"},{"location":"roles/mongodb/#ibm_mongo_admin_credentials_secret_name","text":"Secret for MongoDB Admin credentials. Secret Name: <mongo-name>-admin-credentials","title":"ibm_mongo_admin_credentials_secret_name"},{"location":"roles/mongodb/#ibm_mongo_service_credentials_secret_name","text":"Secret for MongoDB Service credentials. Secret Name: <mongo-name>-service-credentials","title":"ibm_mongo_service_credentials_secret_name"},{"location":"roles/mongodb/#ibm_mongo_resourcegroup","text":"Required.IBM Cloud Resource Group under which resource group will be created. Environment Variable: IBM_MONGO_RESOURCEGROUP Default Value: Default","title":"ibm_mongo_resourcegroup"},{"location":"roles/mongodb/#ibm_mongo_region","text":"Required.IBM Cloud region where MongoDB resources will be created. Environment Variable: IBM_MONGO_REGION Default Value: us-east","title":"ibm_mongo_region"},{"location":"roles/mongodb/#ibmcloud_apikey","text":"Required.IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY","title":"ibmcloud_apikey"},{"location":"roles/mongodb/#ibm_mongo_plan","text":"Plan name for this IBMCloud Service. Environment Variable: IBM_MONGO_PLAN Default Value: standard","title":"ibm_mongo_plan"},{"location":"roles/mongodb/#ibm_mongo_service","text":"IBMCloud Offering name for MongoDB Database Value: databases-for-mongodb","title":"ibm_mongo_service"},{"location":"roles/mongodb/#ibm_mongo_service_endpoints","text":"MongoDB Service Endpoints type can be either public or private Environment Variable: IBM_MONGO_SERVICE_ENDPOINTS Default Value: public","title":"ibm_mongo_service_endpoints"},{"location":"roles/mongodb/#ibm_mongo_version","text":"Specify MongoDB version to be deployed Environment Variable: IBM_MONGO_VERSION Default Value: 4.2","title":"ibm_mongo_version"},{"location":"roles/mongodb/#ibm_mongo_memory","text":"Specify MongoDB Memory size Environment Variable: IBM_MONGO_MEMORY Default Value: 3840","title":"ibm_mongo_memory"},{"location":"roles/mongodb/#ibm_mongo_disk","text":"Specify MongoDB Disk size Environment Variable: IBM_MONGO_DISK Default Value: 30720","title":"ibm_mongo_disk"},{"location":"roles/mongodb/#ibm_mongo_cpu","text":"Specify MongoDB CPU Environment Variable: IBM_MONGO_CPU Default Value: 0","title":"ibm_mongo_cpu"},{"location":"roles/mongodb/#ibm_mongo_name_1","text":"Resource Name in IBMCloud for MongoDB Value: mongo-{{mas_instance_id}}","title":"ibm_mongo_name"},{"location":"roles/mongodb/#ibm_mongo_backup_id","text":"Required only if is_restore is True CRN ID for backup resource Environment Variable: IBM_MONGO_BACKUP_ID Default Value: ``","title":"ibm_mongo_backup_id"},{"location":"roles/mongodb/#is_restore","text":"Whether want to restore from an existing backup resource or not. Environment Variable: IS_RESTORE Default Value: false","title":"is_restore"},{"location":"roles/mongodb/#restored_mongodb_service_name","text":"Required only If is_restore is True .MongoDB Service Name Environment Variable: RESTORED_MONGODB_SERVICE_NAME","title":"restored_mongodb_service_name"},{"location":"roles/mongodb/#role-variables-aws-documentdb","text":"","title":"Role Variables - AWS DocumentDB"},{"location":"roles/mongodb/#aws_access_key_id","text":"Required.AWS Account Access Key Id Environment Variable: AWS_ACCESS_KEY_ID","title":"aws_access_key_id"},{"location":"roles/mongodb/#aws_secret_access_key","text":"Required.AWS Account Secret Access Key Environment Variable: AWS_SECRET_ACCESS_KEY","title":"aws_secret_access_key"},{"location":"roles/mongodb/#aws_region","text":"Required.AWS Region where DocumentDB and other resources will be created Environment Variable: AWS_REGION Default Value: us-east-2","title":"aws_region"},{"location":"roles/mongodb/#vpc_id","text":"Required.AWS VPC ID under which documentdb,subnets and security group will be created Environment Variable: VPC_ID","title":"vpc_id"},{"location":"roles/mongodb/#docdb_cluster_name","text":"Required.DocumentDB Cluster Name Environment Variable: DOCDB_CLUSTER_NAME","title":"docdb_cluster_name"},{"location":"roles/mongodb/#docdb_subnet_group_name","text":"DocumentDB Subnet Group Name Value: docdb-{{ docdb_cluster_name }}","title":"docdb_subnet_group_name"},{"location":"roles/mongodb/#docdb_security_group_name","text":"DocumentDB Security Group Name Value: docdb-{{ docdb_cluster_name }}","title":"docdb_security_group_name"},{"location":"roles/mongodb/#docdb_admin_credentials_secret_name","text":"DocumentDB Admin Credentials Secret Name Value: {{ docdb_cluster_name }}-admin-credentials","title":"docdb_admin_credentials_secret_name"},{"location":"roles/mongodb/#docdb_engine_version","text":"DocumentDB Engine version Environment variable: DOCDB_ENGINE_VERSION Default Value: 4.0.0","title":"docdb_engine_version"},{"location":"roles/mongodb/#docdb_master_username","text":"DocumentDB master username Environment variable: DOCDB_MASTER_USERNAME Default Value: docdbadmin","title":"docdb_master_username"},{"location":"roles/mongodb/#docdb_instance_class","text":"DocumentDB Instance Class Environment variable: DOCDB_INSTANCE_CLASS Default Value: db.t3.medium","title":"docdb_instance_class"},{"location":"roles/mongodb/#docdb_instance_number","text":"Number of instances required for DocumentDB Environment variable: DOCDB_INSTANCE_NUMBER Default Value: 3","title":"docdb_instance_number"},{"location":"roles/mongodb/#docdb_instance_identifier_prefix","text":"Required. Prefix for DocumentDB Instance name Environment variable: DOCDB_INSTANCE_IDENTIFIER_PREFIX","title":"docdb_instance_identifier_prefix"},{"location":"roles/mongodb/#docdb_ingress_cidr","text":"Required. IPv4 Address from which incoming connection requests will be allowed to DocumentDB cluster e.g Provide IPv4 CIDR address of VPC where ROSA cluster is installed Environment variable: DOCDB_INGRESS_CIDR","title":"docdb_ingress_cidr"},{"location":"roles/mongodb/#docdb_egress_cidr","text":"Required. IPv4 Address at which outgoing connection requests will be allowed to DocumentDB cluster e.g Provide IPv4 CIDR address of VPC where ROSA cluster is installed Environment variable: DOCDB_EGRESS_CIDR","title":"docdb_egress_cidr"},{"location":"roles/mongodb/#docdb_cidr_az1","text":"Required. Provide IPv4 CIDR address for the subnet to be created in one of the 3 availabilty zones of your VPC. If the subnet exists already then it must contain the tag of Name: {{ docdb_cluster_name }}, if the subnet doesn't exist already then one is created. Environment variable: DOCDB_CIDR_AZ1","title":"docdb_cidr_az1:"},{"location":"roles/mongodb/#docdb_cidr_az2","text":"Required. Provide IPv4 CIDR address for the subnet to be created in one of the 3 availabilty zones of your VPC. If the subnet exists already then it must contain the tag of Name: {{ docdb_cluster_name }}, if the subnet doesn't exist already then one is created. Environment variable: DOCDB_CIDR_AZ2","title":"docdb_cidr_az2:"},{"location":"roles/mongodb/#docdb_cidr_az3","text":"Required. Provide IPv4 CIDR address for the subnet to be created in one of the 3 availabilty zones of your VPC. If the subnet exists already then it must contain the tag of Name: {{ docdb_cluster_name }}, if the subnet doesn't exist already then one is created. Environment variable: DOCDB_CIDR_AZ3","title":"docdb_cidr_az3:"},{"location":"roles/mongodb/#aws-documentdb-secret-rotate-role-variables","text":"","title":"AWS DocumentDB Secret Rotate role Variables"},{"location":"roles/mongodb/#docdb_mongo_instance_name","text":"Required. DocumentDB Instance Name Environment variable: DOCDB_MONGO_INSTANCE_NAME","title":"docdb_mongo_instance_name"},{"location":"roles/mongodb/#docdb_host","text":"Required. Any one Host Address out of multiple documentDB Instances Environment variable: DOCDB_HOST","title":"docdb_host"},{"location":"roles/mongodb/#docdb_port","text":"Required. Corresponding port address of DocumentDB Instance Host Environment variable: DOCDB_PORT","title":"docdb_port"},{"location":"roles/mongodb/#docdb_instance_username","text":"Required. Specify username for which password is being changed Environment variable: DOCDB_INSTANCE_USERNAME","title":"docdb_instance_username"},{"location":"roles/mongodb/#docdb_instance_password_old","text":"Required. Specify the old user password Environment variable: DOCDB_PASSWORD_OLD","title":"docdb_instance_password_old"},{"location":"roles/mongodb/#docdb_master_password","text":"Required. DocumentDB Master Username Environment variable: DOCDB_MASTER_PASSWORD","title":"docdb_master_password"},{"location":"roles/mongodb/#docdb_master_username_1","text":"Required. DocumentDB Master Password Environment variable: DOCDB_MASTER_USERNAME","title":"docdb_master_username"},{"location":"roles/mongodb/#aws-documentdb-destroy-data-action-variables","text":"","title":"AWS DocumentDB destroy-data action Variables"},{"location":"roles/mongodb/#mas_instance_id_1","text":"The specified MAS instance ID Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/mongodb/#mongo_username","text":"Mongo Username Environment Variable: MONGO_USERNAME Default Value: None","title":"mongo_username"},{"location":"roles/mongodb/#mongo_password","text":"Mongo password Environment Variable: MONGO_PASSWORD Default Value: None","title":"mongo_password"},{"location":"roles/mongodb/#config","text":"Mongo Config, please refer to the below example playbook section for details Required Environment Variable: CONFIG Default Value: None","title":"config"},{"location":"roles/mongodb/#certificates","text":"Mongo Certificates, please refer to the below example playbook section for details Required Environment Variable: CERTIFICATES Default Value: None","title":"certificates"},{"location":"roles/mongodb/#example-playbooks","text":"","title":"Example Playbooks"},{"location":"roles/mongodb/#install-ce-operator","text":"- hosts: localhost any_errors_fatal: true vars: mongodb_storage_class: ibmc-block-gold mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.mongodb","title":"Install (CE Operator)"},{"location":"roles/mongodb/#backup-ce-operator","text":"- hosts: localhost any_errors_fatal: true vars: mongodb_action: backup mas_instance_id: masinst1 masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.mongodb","title":"Backup (CE Operator)"},{"location":"roles/mongodb/#restore-ce-operator","text":"- hosts: localhost any_errors_fatal: true vars: mongodb_action: restore mas_instance_id: masinst1 masbr_restore_from_version: 20240621021316 masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.mongodb","title":"Restore (CE Operator)"},{"location":"roles/mongodb/#install-ibm-cloud","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: ibm ibmcloud_apikey: apikey**** ibmcloud_resource_group: mas-test roles: - ibm.mas_devops.mongodb","title":"Install (IBM Cloud)"},{"location":"roles/mongodb/#install-aws-documentdb","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: aws mongodb_action: provision docdb_size: ~/docdb-config.yml docdb_cluster_name: test-db docdb_ingress_cidr: 10.0.0.0/16 docdb_egress_cidr: 10.0.0.0/16 docdb_cidr_az1: 10.0.0.0/26 docdb_cidr_az2: 10.0.0.64/26 docdb_cidr_az3: 10.0.0.128/26 docdb_instance_identifier_prefix: test-db-instance vpc_id: test-vpc-id aws_access_key_id: aws-key aws_secret_access_key: aws-access-key roles: - ibm.mas_devops.mongodb","title":"Install (AWS DocumentDB)"},{"location":"roles/mongodb/#aws-documentdb-secret-rotation","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig mongodb_provider: aws mongodb_action: docdb_secret_rotate docdb_mongo_instance_name: test-db-instance db_host: aws.test1.host7283-***** db_port: 27017 docdb_master_username: admin docdb_master_password: pass*** docdb_instance_password_old: oldpass**** docdb_instance_username: testuser aws_access_key_id: aws-key aws_secret_access_key: aws-access-key roles: - ibm.mas_devops.mongodb","title":"AWS DocumentDb Secret Rotation"},{"location":"roles/mongodb/#aws-documentdb-destroy-data-action","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mongodb_provider: aws mongodb_action: destroy-data mongo_username: pqradmin mongo_password: xyzabc config: configDb: admin authMechanism: DEFAULT retryWrites: false hosts: - host: abc-0.pqr.databases.appdomain.cloud port: 32250 - host: abc-1.pqr.databases.appdomain.cloud port: 32250 - host: abc-2.pqr.databases.appdomain.cloud port: 32250 certificates: - alias: ca crt: | -----BEGIN CERTIFICATE----- MIIDDzCCAfegAwIBAgIJANEH58y2/kzHMA0GCSqGSIb3DQEBCwUAMB4xHDAaBgNV BAMME0lCTSBDbG91ZCBEYXRhYmFzZXMwHhcNMTgwNjI1MTQyOTAwWhcNMjgwNjIy MTQyOTAwWjAeMRwwGgYDVQQDDBNJQk0gQ2xvdWQgRGF0YWJhc2VzMIIBIjANBgkq 1eKI2FLzYKpoKBe5rcnrM7nHgNc/nCdEs5JecHb1dHv1QfPm6pzIxwIDAQABo1Aw TjAdBgNVHQ4EFgQUK3+XZo1wyKs+DEoYXbHruwSpXjgwHwYDVR0jBBgwFoAUK3+X Zo1wyKs+DEoYXbHruwSpXjgwDAYDVR0TBAUwAwEB/zANBgkqhkiG9w0BAQsFAAOC doqqgGIZ2nxCkp5/FXxF/TMb55vteTQwfgBy60jVVkbF7eVOWCv0KaNHPF5hrqbN i+3XjJ7/peF3xMvTMoy35DcT3E2ZeSVjouZs15O90kI3k2daS2OHJABW0vSj4nLz +PQzp/B9cQmOO8dCe049Q3oaUA== -----END CERTIFICATE----- roles: - ibm.mas_devops.mongodb","title":"AWS DocumentDb destroy-data action"},{"location":"roles/mongodb/#troubleshooting","text":"Important Please be cautious while performing any of the troubleshooting steps outlined below. It is important to understand that the MongoDB Community operator persists data within Persistent Volume Claims. These claims should not be removed inadvertent deletion of the mongoce namespace could result in data loss.","title":"Troubleshooting"},{"location":"roles/mongodb/#mongodb-replica-set-pods-will-not-start","text":"MongoDB 5 has introduced new platform specific requirements. Please consult the Platform Support Notes for detailed information. It is of particular importance to confirm that the AVX instruction set is exposed or available to the MongoDB workloads. This can easily be determined by entering any running pod on the same OpenShift cluster where MongoDB replica set members are failing to start. Once inside of a running pod the following command can be executed to confirm if the AVX instruction set is available: cat /proc/cpuinfo | grep flags | grep avx If avx is not found in the available flags then either the physical processor hosting the OpenShift cluster does not provide the AVX instruction set or the virtual host configuration is not exposing the AVX instruction set. If the latter is suspected the virtual hosting documentation should be referenced for details on how to expose the AVX instruction set.","title":"MongoDB Replica Set Pods Will Not Start"},{"location":"roles/mongodb/#ldap-authentication","text":"If authenticating via LDAP with PLAIN specified for authMechanism then configDb must be set to $external in the MongoCfg. The field configDb in the MongoCfg refers to the authentication database.","title":"LDAP Authentication"},{"location":"roles/mongodb/#ca-certificate-renewal","text":"Warning If the MongoDB CA Certificate expires the MongoDB replica set will become unusable. Replica set members will not be able to communicate with each other and client applications (i.e. Maximo Application Suite components) will not be to connect. In order to renew the CA Certificate used by the MongoDB replica set the following steps must be taken: Delete the CA Certificate resource Delete the MongoDB server Certificate resource Delete the Secrets resources associated with both the CA Certificate and Server Certificate Delete the Secret resource which contains the MongoDB configuration parameters Delete the ConfigMap resources which contains the CA certificate Delete the Secret resource which contains the sever certificate and private key The following steps illustrate the process required to renew the CA Certificate, sever certificate and reconfigure the MongoDB replica set with the new CA and server certificates. The first step is to stop the Mongo replica set and MongoDb CE Operator pod. oc project mongoce oc delete deployment mongodb-kubernetes-operator Important Make sure the MongoDB Community operator pod has terminated before proceeding. oc delete statefulset mas-mongo-ce Important Make sure all pods in the mongoce namespace have terminated before proceeding Remove expired CA Certificate and Server Certificate resources. Clean up MongoDB Community configuration and then run the mongodb role. oc delete certificate mongo-ca-crt oc delete certificate mongo-server oc delete secret mongo-ca-secret oc delete secret mongo-server-cert oc delete secret mas-mongo-ce-config oc delete configmap mas-mongo-ce-cert-map oc delete secret mas-mongo-ce-server-certificate-key export ROLE_NAME=mongodb ansible-playbook ibm.mas_devops.run_role Once the mongodb role has completed the MongoDb CE Operator pod and Mongo replica set should be configured. After the CA and server Certificates have been renewed you must ensure that that MongoCfg Suite CR is updated with the new CA Certificate. First obtain the CA Certificate from the Secret resource mongo-ca-secret . Then edit the Suite MongoCfg CR in the Maximo Application Suite core namespace. This is done by updating the appropriate certificate under .spec.certificates in the MongoCfg CR: spec: certificates: - alias: ca crt: | -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- If an IBM Suite Licensing Service (SLS) is also connecting to the MongoDB replica set the LicenseService CR must also be updated to reflect the new MongoDB CA. This can be added to the .spec.mongo.certificates section of the LicenseService CR. mongo: certificates: - alias: mongoca crt: | -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- Once the CA certificate has been updated for the MongoCfg and LicenseService CRs several pods in the core and SLS namespaces might need to be restarted to pick up the changes. This would include but is not limited to coreidp, coreapi, api-licensing.","title":"CA Certificate Renewal"},{"location":"roles/mongodb/#license","text":"EPL-2.0","title":"License"},{"location":"roles/nvidia_gpu/","text":"nvidia_gpu \u00a4 This role installs the NVIDIA Graphical Processing Unit (GPU) operator and its prerequisite Node Feature Discovery (NFD) operator in an IBM Cloud Openshift cluster console. The role first installs the NFD operator and continues with the final step to install the NVIDIA GPU Operator. The NFD Operator is installed using the Red Hat Operators catalog source and the GPU operator is installed using the Certified Operators catalog source. Role Variables \u00a4 nfd_namespace \u00a4 The namespace where the node feature discovery operator will be deployed. Environment Variable: NFD_NAMESPACE Default Value: openshift-nfd nfd_channel \u00a4 The channel to subscribe to for the nfd operator installation and updates. Available channels may be found in the package manifest of nfd operator in openshift. Environment Variable: NFD_CHANNEL Default Value: stable gpu_namespace \u00a4 The namespace where the NVIDIA GPU operator will be deployed. For version 1.8.x, use of single namespace is not supported, therefore use openshift-operators . Environment Variable: GPU_NAMESPACE Default Value: nvidia-gpu-operator gpu_channel \u00a4 The channel to subscribe to for the gpu operator installation and updates. Available channels may be found in the package manifest of gpu-operator-certified operator in openshift. Environment Variable: GPU_CHANNEL Default Value: v24.9 gpu_driver_version \u00a4 By default, it will pull the latest version and the environment variable is not needed. If a specific version is needed (due to OS version compatibilities), specify the following environment variable. Environment Variable: GPU_DRIVER_VERSION Default Value: N/A See the attached links for more information and to decide which driver version to use. https://catalog.ngc.nvidia.com/orgs/nvidia/containers/driver/tags gpu_driver_repository_path \u00a4 The gpu driver repository. If using a different repository, you can set the value for this repo. We only support public repositories at the moment. Environment Variable: GPU_DRIVER_REPOSITORY_PATH Default Value: nvcr.io/nvidia For more information on the NVIDIA GPU and NFD operators, visit https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/install-gpu-ocp.html Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.nvidia_gpu License \u00a4 EPL-2.0","title":"nvidia_gpu"},{"location":"roles/nvidia_gpu/#nvidia_gpu","text":"This role installs the NVIDIA Graphical Processing Unit (GPU) operator and its prerequisite Node Feature Discovery (NFD) operator in an IBM Cloud Openshift cluster console. The role first installs the NFD operator and continues with the final step to install the NVIDIA GPU Operator. The NFD Operator is installed using the Red Hat Operators catalog source and the GPU operator is installed using the Certified Operators catalog source.","title":"nvidia_gpu"},{"location":"roles/nvidia_gpu/#role-variables","text":"","title":"Role Variables"},{"location":"roles/nvidia_gpu/#nfd_namespace","text":"The namespace where the node feature discovery operator will be deployed. Environment Variable: NFD_NAMESPACE Default Value: openshift-nfd","title":"nfd_namespace"},{"location":"roles/nvidia_gpu/#nfd_channel","text":"The channel to subscribe to for the nfd operator installation and updates. Available channels may be found in the package manifest of nfd operator in openshift. Environment Variable: NFD_CHANNEL Default Value: stable","title":"nfd_channel"},{"location":"roles/nvidia_gpu/#gpu_namespace","text":"The namespace where the NVIDIA GPU operator will be deployed. For version 1.8.x, use of single namespace is not supported, therefore use openshift-operators . Environment Variable: GPU_NAMESPACE Default Value: nvidia-gpu-operator","title":"gpu_namespace"},{"location":"roles/nvidia_gpu/#gpu_channel","text":"The channel to subscribe to for the gpu operator installation and updates. Available channels may be found in the package manifest of gpu-operator-certified operator in openshift. Environment Variable: GPU_CHANNEL Default Value: v24.9","title":"gpu_channel"},{"location":"roles/nvidia_gpu/#gpu_driver_version","text":"By default, it will pull the latest version and the environment variable is not needed. If a specific version is needed (due to OS version compatibilities), specify the following environment variable. Environment Variable: GPU_DRIVER_VERSION Default Value: N/A See the attached links for more information and to decide which driver version to use. https://catalog.ngc.nvidia.com/orgs/nvidia/containers/driver/tags","title":"gpu_driver_version"},{"location":"roles/nvidia_gpu/#gpu_driver_repository_path","text":"The gpu driver repository. If using a different repository, you can set the value for this repo. We only support public repositories at the moment. Environment Variable: GPU_DRIVER_REPOSITORY_PATH Default Value: nvcr.io/nvidia For more information on the NVIDIA GPU and NFD operators, visit https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/install-gpu-ocp.html","title":"gpu_driver_repository_path"},{"location":"roles/nvidia_gpu/#example-playbook","text":"- hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.nvidia_gpu","title":"Example Playbook"},{"location":"roles/nvidia_gpu/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_cluster_monitoring/","text":"ocp_cluster_monitoring \u00a4 Configures the OpenShift Container Platform Cluster Monitoring enabling two settings: OpenShift user defined project monitoring is enabled ( openshift-monitoring namespace) OpenShift monitoring stack is configured to use persistent storage ( openshift-monitoring namespace) Role Variables \u00a4 cluster_monitoring_action \u00a4 Inform the role whether to perform an install or an uninstall of the cluster monitoring stack. Optional Environment Variable: CLUSTER_MONITORING_ACTION Default: install prometheus_retention_period \u00a4 Adjust the retention period for Prometheus metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_RETENTION_PERIOD Default Value: 15d prometheus_storage_class \u00a4 Declare the storage class for Prometheus' metrics data persistent volume. Storage class must support ReadWriteOnce(RWO) access mode. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: PROMETHEUS_STORAGE_CLASS Default Value: ibmc-block-gold , ocs-storagecluster-ceph-rbd , or managed-premium (if available) prometheus_storage_size \u00a4 Adjust the size of the volume used to store metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_STORAGE_SIZE Default Value: 20Gi prometheus_alertmgr_storage_class \u00a4 Declare the storage class for AlertManager's persistent volume. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available) Note : Storage class must support ReadWriteMany(RWX) access mode. prometheus_alertmgr_storage_size \u00a4 Adjust the size of the volume used by AlertManager, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_SIZE Default Value: 20Gi prometheus_userworkload_retention_period \u00a4 Adjust the retention period for User Workload Prometheus metrics, this parameter applies only to the User Workload Prometheus instance. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_RETENTION_PERIOD Default Value: 15d prometheus_userworkload_storage_class \u00a4 Declare the storage class for User Workload Prometheus' metrics data persistent volume. Storage class must support ReadWriteOnce(RWO) access mode. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_CLASS Default Value: PROMETHEUS_STORAGE_CLASS prometheus_userworkload_storage_size \u00a4 Adjust the size of the volume used to store User Workload metrics. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_SIZE Default Value: 20Gi Example Playbook \u00a4 - hosts: localhost vars: prometheus_storage_class: \"ibmc-block-gold\" prometheus_alertmgr_storage_class: \"ibmc-file-gold-gid\" roles: - ibm.mas_devops.ocp_cluster_monitoring License \u00a4 EPL-2.0","title":"ocp_cluster_monitoring"},{"location":"roles/ocp_cluster_monitoring/#ocp_cluster_monitoring","text":"Configures the OpenShift Container Platform Cluster Monitoring enabling two settings: OpenShift user defined project monitoring is enabled ( openshift-monitoring namespace) OpenShift monitoring stack is configured to use persistent storage ( openshift-monitoring namespace)","title":"ocp_cluster_monitoring"},{"location":"roles/ocp_cluster_monitoring/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_cluster_monitoring/#cluster_monitoring_action","text":"Inform the role whether to perform an install or an uninstall of the cluster monitoring stack. Optional Environment Variable: CLUSTER_MONITORING_ACTION Default: install","title":"cluster_monitoring_action"},{"location":"roles/ocp_cluster_monitoring/#prometheus_retention_period","text":"Adjust the retention period for Prometheus metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_RETENTION_PERIOD Default Value: 15d","title":"prometheus_retention_period"},{"location":"roles/ocp_cluster_monitoring/#prometheus_storage_class","text":"Declare the storage class for Prometheus' metrics data persistent volume. Storage class must support ReadWriteOnce(RWO) access mode. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: PROMETHEUS_STORAGE_CLASS Default Value: ibmc-block-gold , ocs-storagecluster-ceph-rbd , or managed-premium (if available)","title":"prometheus_storage_class"},{"location":"roles/ocp_cluster_monitoring/#prometheus_storage_size","text":"Adjust the size of the volume used to store metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_STORAGE_SIZE Default Value: 20Gi","title":"prometheus_storage_size"},{"location":"roles/ocp_cluster_monitoring/#prometheus_alertmgr_storage_class","text":"Declare the storage class for AlertManager's persistent volume. Required if one of the known supported storage classes is not installed in the cluster. Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_CLASS Default Value: ibmc-file-gold-gid , ocs-storagecluster-cephfs , azurefiles-premium (if available) Note : Storage class must support ReadWriteMany(RWX) access mode.","title":"prometheus_alertmgr_storage_class"},{"location":"roles/ocp_cluster_monitoring/#prometheus_alertmgr_storage_size","text":"Adjust the size of the volume used by AlertManager, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Optional Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_SIZE Default Value: 20Gi","title":"prometheus_alertmgr_storage_size"},{"location":"roles/ocp_cluster_monitoring/#prometheus_userworkload_retention_period","text":"Adjust the retention period for User Workload Prometheus metrics, this parameter applies only to the User Workload Prometheus instance. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_RETENTION_PERIOD Default Value: 15d","title":"prometheus_userworkload_retention_period"},{"location":"roles/ocp_cluster_monitoring/#prometheus_userworkload_storage_class","text":"Declare the storage class for User Workload Prometheus' metrics data persistent volume. Storage class must support ReadWriteOnce(RWO) access mode. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_CLASS Default Value: PROMETHEUS_STORAGE_CLASS","title":"prometheus_userworkload_storage_class"},{"location":"roles/ocp_cluster_monitoring/#prometheus_userworkload_storage_size","text":"Adjust the size of the volume used to store User Workload metrics. Optional Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_SIZE Default Value: 20Gi","title":"prometheus_userworkload_storage_size"},{"location":"roles/ocp_cluster_monitoring/#example-playbook","text":"- hosts: localhost vars: prometheus_storage_class: \"ibmc-block-gold\" prometheus_alertmgr_storage_class: \"ibmc-file-gold-gid\" roles: - ibm.mas_devops.ocp_cluster_monitoring","title":"Example Playbook"},{"location":"roles/ocp_cluster_monitoring/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_config/","text":"ocp_config \u00a4 This role can perform the following configuration: Tune the IngressController to avoid request failures due to timeout for long running requests Update APIServer and IngressController to set a custom tlsSecurityProfile to accommodate ciphers supported by IBM Java Semeru runtime. This is required for allowing the Java applications using Semeru runtime to run in FIPS mode. The following cipers will be enabled: TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 ECDHE-ECDSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-GCM-SHA256 ECDHE-ECDSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-GCM-SHA384 ECDHE-ECDSA-CHACHA20-POLY1305 ECDHE-RSA-CHACHA20-POLY1305 DHE-RSA-AES128-GCM-SHA256 DHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-SHA Disable the default Red Hat CatalogSources : certified-operators community-operators redhat-operators Role Variables - API Server \u00a4 ocp_update_ciphers_for_semeru \u00a4 Set to True if you want to configure the API Server and Ingress Controller to use a custom set of ciphers that are compatible with IBM Java Semeru in FIPS mode. Optional Environment Variable: OCP_UPDATE_CIPHERS_FOR_SEMERU Default Value: False Role Variables - Ingress Controller \u00a4 ocp_ingress_update_timeouts \u00a4 Set to True if you want to customize the Ingress's client and server timeout values Optional Environment Variable: OCP_INGRESS_UPDATE_TIMEOUTS Default Value: False ocp_ingress_client_timeout \u00a4 Specifies how long a connection is held open while waiting for a client response Optional Environment Variable: OCP_INGRESS_CLIENT_TIMEOUT Default Value: 30s ocp_ingress_server_timeout \u00a4 Specifies how long a connection is held open while waiting for a server response Optional Environment Variable: OCP_INGRESS_SERVER_TIMEOUT Default Value: 30s Role Variables - OperatorHub \u00a4 ocp_operatorhub_disable_redhat_sources \u00a4 Set to True if you want to disable the default Red Hat catalog sources Optional Environment Variable: OCP_OPERATORHUB_DISABLE_REDHAT_SOURCES Default Value: False Note Setting this to False will not enable the default catalog sources if they are currently disabled, it will just instruct this role to take no action. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: ocp_update_ciphers_for_semeru: True ocp_ingress_update_timeouts: True ocp_ingress_client_timeout: 30s ocp_ingress_server_timeout: 30s ocp_operatorhub_disable_redhat_sources: True roles: - ibm.mas_devops.ocp_config License \u00a4 EPL-2.0","title":"ocp_config"},{"location":"roles/ocp_config/#ocp_config","text":"This role can perform the following configuration: Tune the IngressController to avoid request failures due to timeout for long running requests Update APIServer and IngressController to set a custom tlsSecurityProfile to accommodate ciphers supported by IBM Java Semeru runtime. This is required for allowing the Java applications using Semeru runtime to run in FIPS mode. The following cipers will be enabled: TLS_AES_128_GCM_SHA256 TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 ECDHE-ECDSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-GCM-SHA256 ECDHE-ECDSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-GCM-SHA384 ECDHE-ECDSA-CHACHA20-POLY1305 ECDHE-RSA-CHACHA20-POLY1305 DHE-RSA-AES128-GCM-SHA256 DHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-SHA ECDHE-RSA-AES256-SHA Disable the default Red Hat CatalogSources : certified-operators community-operators redhat-operators","title":"ocp_config"},{"location":"roles/ocp_config/#role-variables-api-server","text":"","title":"Role Variables - API Server"},{"location":"roles/ocp_config/#ocp_update_ciphers_for_semeru","text":"Set to True if you want to configure the API Server and Ingress Controller to use a custom set of ciphers that are compatible with IBM Java Semeru in FIPS mode. Optional Environment Variable: OCP_UPDATE_CIPHERS_FOR_SEMERU Default Value: False","title":"ocp_update_ciphers_for_semeru"},{"location":"roles/ocp_config/#role-variables-ingress-controller","text":"","title":"Role Variables - Ingress Controller"},{"location":"roles/ocp_config/#ocp_ingress_update_timeouts","text":"Set to True if you want to customize the Ingress's client and server timeout values Optional Environment Variable: OCP_INGRESS_UPDATE_TIMEOUTS Default Value: False","title":"ocp_ingress_update_timeouts"},{"location":"roles/ocp_config/#ocp_ingress_client_timeout","text":"Specifies how long a connection is held open while waiting for a client response Optional Environment Variable: OCP_INGRESS_CLIENT_TIMEOUT Default Value: 30s","title":"ocp_ingress_client_timeout"},{"location":"roles/ocp_config/#ocp_ingress_server_timeout","text":"Specifies how long a connection is held open while waiting for a server response Optional Environment Variable: OCP_INGRESS_SERVER_TIMEOUT Default Value: 30s","title":"ocp_ingress_server_timeout"},{"location":"roles/ocp_config/#role-variables-operatorhub","text":"","title":"Role Variables - OperatorHub"},{"location":"roles/ocp_config/#ocp_operatorhub_disable_redhat_sources","text":"Set to True if you want to disable the default Red Hat catalog sources Optional Environment Variable: OCP_OPERATORHUB_DISABLE_REDHAT_SOURCES Default Value: False Note Setting this to False will not enable the default catalog sources if they are currently disabled, it will just instruct this role to take no action.","title":"ocp_operatorhub_disable_redhat_sources"},{"location":"roles/ocp_config/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: ocp_update_ciphers_for_semeru: True ocp_ingress_update_timeouts: True ocp_ingress_client_timeout: 30s ocp_ingress_server_timeout: 30s ocp_operatorhub_disable_redhat_sources: True roles: - ibm.mas_devops.ocp_config","title":"Example Playbook"},{"location":"roles/ocp_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_contentsourcepolicy/","text":"ocp_contentsourcepolicy \u00a4 Installs an ImageContentSourcePolicy for IBM Maximo Application Suite's Maximo Operator Catalog. Optionally can also install a second ContentSourcePolicy suitable for the Red Hat Operator Catalogs created by mirror_ocp . Warning This role doesn't work on IBMCloud ROKS. IBM Cloud RedHat OpenShift Service does not implement support for ImageContentSourcePolicies . If you want to use image mirroring you must manually configure each worker node individually using the IBM Cloud command line tool. IBM Maximo Operator Catalog Content \u00a4 All content used in the MAS install is sourced from three registries: icr.io , cp.icr.io , & quay.io : icr.io/cpopen All IBM operators icr.io/ibm-truststore-mgr IBM truststore manager worker image icr.io/ibm-sls IBM SLS content icr.io/ibm-uds IBM UDS content icr.io/db2u IBM Db2 Universal operator content cp.icr.io/cp All IBM entitled container images quay.io/opencloudio IBM common services quay.io/mongodb MongoDb Community Edition Operator & associated container images quay.io/amlen Eclipse Amlen - Message Broker for IoT/Mobile/Web quay.io/ibmmas Non-product IBM Maximo Application Suite images (e.g. MAS CLI) Red Hat Operator Catalog Content \u00a4 All content from the subset of the Red Hat operator catalogs supported by mirror_ocp is sourced from eight registries: icr.io , docker.io , quay.io , gcr.io , ghcr.io , nvcr.io , registry.connect.redhat.com , and registry.redhat.io : icr.io/cpopen docker.io/grafana quay.io/community-operator-pipeline-prod quay.io/operator-pipeline-prod quay.io/openshift-community-operators quay.io/strimzi quay.io/rh-marketplace gcr.io/kubebuilder ghcr.io/grafana ghcr.io/open-telemetry nvcr.io/nvidia registry.connect.redhat.com/crunchydata registry.connect.redhat.com/nvidia registry.connect.redhat.com/turbonomic registry.connect.redhat.com/rh-marketplace registry.redhat.io/openshift4 registry.redhat.io/source-to-image registry.redhat.io/odf4 registry.redhat.io/cert-manager registry.redhat.io/rhceph registry.redhat.io/amq-streams registry.redhat.io/ubi8 registry.redhat.io/openshift-pipelines registry.redhat.io/openshift-serverless-1 registry.redhat.io/lvms4 Note A content source policy for this content is only configured when setup_redhat_catalogs is set to True . If you are managing the Red Hat Operator Catalogs yourself the content therein may well be different depending how you have configured mirroring. Role Variables \u00a4 setup_redhat_release \u00a4 Instruct the role to setup ContentSourcePolicy for the mirrored release content generated by mirror_ocp . This will create an additional policy named ibm-mas-redhat-release . Required Environment Variable: SETUP_REDHAT_RELEASE Default: False setup_redhat_catalogs \u00a4 Instruct the role to setup CatalogSources and ContentSourcePolicy for the mirror catalogs generated by mirror_ocp . This will create an additional policy named ibm-mas-redhat-catalogs . Required Environment Variable: SETUP_REDHAT_CATALOGS Default: False ocp_release \u00a4 The Red Hat release you are configuring an image content source policy for, e.g. 4.15 . Required if setup_redhat_catalogs is enabled (not required if only setup_redhat_release is used) Environment Variable: OCP_RELEASE Default: None Role Variables - Target Registry \u00a4 registry_private_host \u00a4 The private hostname for the target registry Required Environment Variable: REGISTRY_PRIVATE_HOST Default: None registry_private_port \u00a4 The private port number for the target registry Required Environment Variable: REGISTRY_PRIVATE_PORT Default: None registry_private_ca_file \u00a4 The CA certificate presented by the registry on it's private endpoint. Required Environment Variable: REGISTRY_PRIVATE_CA_FILE Default: None registry_username \u00a4 The username for the target registry. Required Environment Variable: REGISTRY_USERNAME Default: None registry_password \u00a4 The password for the target registry. Required Environment Variable: REGISTRY_PASSWORD Default: None redhat_catalogs_prefix \u00a4 The prefix amended to the catalog sources names. E.g: With a redhat_catalogs_prefix of \"ibm-mas\" then redhat/certified-operator-index would instead be created as redhat/ibm-mas-certified-operator-index Optional Environment Variable: REDHAT_CATALOGS_PREFIX Default: None Example Playbook \u00a4 - hosts: localhost vars: registry_private_host: myocp-5f1320191125833da1cac8216c06779e-0000.us-south.containers.appdomain.cloud registry_private_port: 32500 registry_private_ca_file: ~/registry-ca.crt registry_username: admin registry_password: 8934jk77s862! # Not a real password, don't worry security folks setup_redhat_catalogs: true roles: - ibm.mas_devops.ocp_contentsourcepolicy License \u00a4 EPL-2.0","title":"ocp_contentsourcepolicy"},{"location":"roles/ocp_contentsourcepolicy/#ocp_contentsourcepolicy","text":"Installs an ImageContentSourcePolicy for IBM Maximo Application Suite's Maximo Operator Catalog. Optionally can also install a second ContentSourcePolicy suitable for the Red Hat Operator Catalogs created by mirror_ocp . Warning This role doesn't work on IBMCloud ROKS. IBM Cloud RedHat OpenShift Service does not implement support for ImageContentSourcePolicies . If you want to use image mirroring you must manually configure each worker node individually using the IBM Cloud command line tool.","title":"ocp_contentsourcepolicy"},{"location":"roles/ocp_contentsourcepolicy/#ibm-maximo-operator-catalog-content","text":"All content used in the MAS install is sourced from three registries: icr.io , cp.icr.io , & quay.io : icr.io/cpopen All IBM operators icr.io/ibm-truststore-mgr IBM truststore manager worker image icr.io/ibm-sls IBM SLS content icr.io/ibm-uds IBM UDS content icr.io/db2u IBM Db2 Universal operator content cp.icr.io/cp All IBM entitled container images quay.io/opencloudio IBM common services quay.io/mongodb MongoDb Community Edition Operator & associated container images quay.io/amlen Eclipse Amlen - Message Broker for IoT/Mobile/Web quay.io/ibmmas Non-product IBM Maximo Application Suite images (e.g. MAS CLI)","title":"IBM Maximo Operator Catalog Content"},{"location":"roles/ocp_contentsourcepolicy/#red-hat-operator-catalog-content","text":"All content from the subset of the Red Hat operator catalogs supported by mirror_ocp is sourced from eight registries: icr.io , docker.io , quay.io , gcr.io , ghcr.io , nvcr.io , registry.connect.redhat.com , and registry.redhat.io : icr.io/cpopen docker.io/grafana quay.io/community-operator-pipeline-prod quay.io/operator-pipeline-prod quay.io/openshift-community-operators quay.io/strimzi quay.io/rh-marketplace gcr.io/kubebuilder ghcr.io/grafana ghcr.io/open-telemetry nvcr.io/nvidia registry.connect.redhat.com/crunchydata registry.connect.redhat.com/nvidia registry.connect.redhat.com/turbonomic registry.connect.redhat.com/rh-marketplace registry.redhat.io/openshift4 registry.redhat.io/source-to-image registry.redhat.io/odf4 registry.redhat.io/cert-manager registry.redhat.io/rhceph registry.redhat.io/amq-streams registry.redhat.io/ubi8 registry.redhat.io/openshift-pipelines registry.redhat.io/openshift-serverless-1 registry.redhat.io/lvms4 Note A content source policy for this content is only configured when setup_redhat_catalogs is set to True . If you are managing the Red Hat Operator Catalogs yourself the content therein may well be different depending how you have configured mirroring.","title":"Red Hat Operator Catalog Content"},{"location":"roles/ocp_contentsourcepolicy/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_contentsourcepolicy/#setup_redhat_release","text":"Instruct the role to setup ContentSourcePolicy for the mirrored release content generated by mirror_ocp . This will create an additional policy named ibm-mas-redhat-release . Required Environment Variable: SETUP_REDHAT_RELEASE Default: False","title":"setup_redhat_release"},{"location":"roles/ocp_contentsourcepolicy/#setup_redhat_catalogs","text":"Instruct the role to setup CatalogSources and ContentSourcePolicy for the mirror catalogs generated by mirror_ocp . This will create an additional policy named ibm-mas-redhat-catalogs . Required Environment Variable: SETUP_REDHAT_CATALOGS Default: False","title":"setup_redhat_catalogs"},{"location":"roles/ocp_contentsourcepolicy/#ocp_release","text":"The Red Hat release you are configuring an image content source policy for, e.g. 4.15 . Required if setup_redhat_catalogs is enabled (not required if only setup_redhat_release is used) Environment Variable: OCP_RELEASE Default: None","title":"ocp_release"},{"location":"roles/ocp_contentsourcepolicy/#role-variables-target-registry","text":"","title":"Role Variables - Target Registry"},{"location":"roles/ocp_contentsourcepolicy/#registry_private_host","text":"The private hostname for the target registry Required Environment Variable: REGISTRY_PRIVATE_HOST Default: None","title":"registry_private_host"},{"location":"roles/ocp_contentsourcepolicy/#registry_private_port","text":"The private port number for the target registry Required Environment Variable: REGISTRY_PRIVATE_PORT Default: None","title":"registry_private_port"},{"location":"roles/ocp_contentsourcepolicy/#registry_private_ca_file","text":"The CA certificate presented by the registry on it's private endpoint. Required Environment Variable: REGISTRY_PRIVATE_CA_FILE Default: None","title":"registry_private_ca_file"},{"location":"roles/ocp_contentsourcepolicy/#registry_username","text":"The username for the target registry. Required Environment Variable: REGISTRY_USERNAME Default: None","title":"registry_username"},{"location":"roles/ocp_contentsourcepolicy/#registry_password","text":"The password for the target registry. Required Environment Variable: REGISTRY_PASSWORD Default: None","title":"registry_password"},{"location":"roles/ocp_contentsourcepolicy/#redhat_catalogs_prefix","text":"The prefix amended to the catalog sources names. E.g: With a redhat_catalogs_prefix of \"ibm-mas\" then redhat/certified-operator-index would instead be created as redhat/ibm-mas-certified-operator-index Optional Environment Variable: REDHAT_CATALOGS_PREFIX Default: None","title":"redhat_catalogs_prefix"},{"location":"roles/ocp_contentsourcepolicy/#example-playbook","text":"- hosts: localhost vars: registry_private_host: myocp-5f1320191125833da1cac8216c06779e-0000.us-south.containers.appdomain.cloud registry_private_port: 32500 registry_private_ca_file: ~/registry-ca.crt registry_username: admin registry_password: 8934jk77s862! # Not a real password, don't worry security folks setup_redhat_catalogs: true roles: - ibm.mas_devops.ocp_contentsourcepolicy","title":"Example Playbook"},{"location":"roles/ocp_contentsourcepolicy/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_deprovision/","text":"ocp_deprovision \u00a4 Deprovision OCP cluster in Fyre, IBM Cloud, & ROSA. Role Variables \u00a4 cluster_type \u00a4 Required. Specify the cluster type, supported values are roks and quickburn . Required Environment Variable: CLUSTER_TYPE Default Value: None cluster_name \u00a4 Required. Specify the name of the cluster Required Environment Variable: CLUSTER_NAME Default Value: None Role Variables - ROKS \u00a4 ibmcloud_apikey \u00a4 The APIKey to be used by ibmcloud login comand. Required if cluster_type = roks Environment Variable: IBMCLOUD_APIKEY Default Value: None Role Variables - ROSA \u00a4 rosa_token \u00a4 The Token used to authenticate with the ROSA service. Required if cluster_type = rosa Environment Variable: ROSA_TOKEN Default Value: None Role Variables - FYRE \u00a4 fyre_username \u00a4 Username to authenticate with the Fyre API. Required if cluster_type = quickburn . Environment Variable: FYRE_USERNAME Default Value: None fyre_apikey \u00a4 API key to authenticate with the Fyre API. Required if cluster_type = quickburn . Environment Variable: FYRE_APIKEY Default Value: None fyre_site \u00a4 Site in Fyre where cluster had been provisioned Optional Environment Variable: FYRE_SITE Default Value: svl Role Variables - IPI \u00a4 The following variables are only used when cluster_type = ipi . ipi_install_dir \u00a4 The directory that is used to store the openshift-install executable, its configuration, & generated log files. Optional when cluster_type = aws-ipi Environment Variable: IPI_DIR Default Value: ~/openshift-install ipi_platform \u00a4 Platform the cluster was created on, any platform supported by openshift-install . Values allowed: aws and gcp . Required. Environment Variable: IPI_PLATFORM Default Value: None Role Variables - AWS \u00a4 The following variables are only used when cluster_type = ipi and ipi_platform = aws . aws_access_key_id \u00a4 AWS access key associated with an IAM user or role. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None aws_secret_access_key \u00a4 AWS secret access key associated with an IAM user or role. Make sure the access key has permissions to delete instances. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None Role Variables - GCP \u00a4 The following variables are only used when cluster_type = ipi and ipi_platform = gcp . gcp_service_account_file \u00a4 GCP service account file path. Make sure the service account has permissions to create instances. Required when cluster_type = ipi and ipi_platform = gcp Environment Variable: GOOGLE_APPLICATION_CREDENTIALS Default Value: None Example Playbook \u00a4 - hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_deprovision License \u00a4 EPL-2.0","title":"ocp_deprovision"},{"location":"roles/ocp_deprovision/#ocp_deprovision","text":"Deprovision OCP cluster in Fyre, IBM Cloud, & ROSA.","title":"ocp_deprovision"},{"location":"roles/ocp_deprovision/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_deprovision/#cluster_type","text":"Required. Specify the cluster type, supported values are roks and quickburn . Required Environment Variable: CLUSTER_TYPE Default Value: None","title":"cluster_type"},{"location":"roles/ocp_deprovision/#cluster_name","text":"Required. Specify the name of the cluster Required Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/ocp_deprovision/#role-variables-roks","text":"","title":"Role Variables - ROKS"},{"location":"roles/ocp_deprovision/#ibmcloud_apikey","text":"The APIKey to be used by ibmcloud login comand. Required if cluster_type = roks Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/ocp_deprovision/#role-variables-rosa","text":"","title":"Role Variables - ROSA"},{"location":"roles/ocp_deprovision/#rosa_token","text":"The Token used to authenticate with the ROSA service. Required if cluster_type = rosa Environment Variable: ROSA_TOKEN Default Value: None","title":"rosa_token"},{"location":"roles/ocp_deprovision/#role-variables-fyre","text":"","title":"Role Variables - FYRE"},{"location":"roles/ocp_deprovision/#fyre_username","text":"Username to authenticate with the Fyre API. Required if cluster_type = quickburn . Environment Variable: FYRE_USERNAME Default Value: None","title":"fyre_username"},{"location":"roles/ocp_deprovision/#fyre_apikey","text":"API key to authenticate with the Fyre API. Required if cluster_type = quickburn . Environment Variable: FYRE_APIKEY Default Value: None","title":"fyre_apikey"},{"location":"roles/ocp_deprovision/#fyre_site","text":"Site in Fyre where cluster had been provisioned Optional Environment Variable: FYRE_SITE Default Value: svl","title":"fyre_site"},{"location":"roles/ocp_deprovision/#role-variables-ipi","text":"The following variables are only used when cluster_type = ipi .","title":"Role Variables - IPI"},{"location":"roles/ocp_deprovision/#ipi_install_dir","text":"The directory that is used to store the openshift-install executable, its configuration, & generated log files. Optional when cluster_type = aws-ipi Environment Variable: IPI_DIR Default Value: ~/openshift-install","title":"ipi_install_dir"},{"location":"roles/ocp_deprovision/#ipi_platform","text":"Platform the cluster was created on, any platform supported by openshift-install . Values allowed: aws and gcp . Required. Environment Variable: IPI_PLATFORM Default Value: None","title":"ipi_platform"},{"location":"roles/ocp_deprovision/#role-variables-aws","text":"The following variables are only used when cluster_type = ipi and ipi_platform = aws .","title":"Role Variables - AWS"},{"location":"roles/ocp_deprovision/#aws_access_key_id","text":"AWS access key associated with an IAM user or role. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None","title":"aws_access_key_id"},{"location":"roles/ocp_deprovision/#aws_secret_access_key","text":"AWS secret access key associated with an IAM user or role. Make sure the access key has permissions to delete instances. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None","title":"aws_secret_access_key"},{"location":"roles/ocp_deprovision/#role-variables-gcp","text":"The following variables are only used when cluster_type = ipi and ipi_platform = gcp .","title":"Role Variables - GCP"},{"location":"roles/ocp_deprovision/#gcp_service_account_file","text":"GCP service account file path. Make sure the service account has permissions to create instances. Required when cluster_type = ipi and ipi_platform = gcp Environment Variable: GOOGLE_APPLICATION_CREDENTIALS Default Value: None","title":"gcp_service_account_file"},{"location":"roles/ocp_deprovision/#example-playbook","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_deprovision","title":"Example Playbook"},{"location":"roles/ocp_deprovision/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_efs/","text":"ocp_efs \u00a4 This role provides support to install aws-efs on aws using aws cli and connect that to ROSA using oc CLI. This role create a new inbound rule in the security group of the ec2 instance where rosa is installed and then creates a new EFS instance and adds access points and network mounts to access EFS from ROSA. Role Variables \u00a4 aws_access_key_id \u00a4 The AWS access key will be used to login to aws cli. Required Environment Variable: AWS_ACCESS_KEY_ID Default: None aws_secret_access_key \u00a4 The AWS access secret key will be used to login to aws cli. Required Environment Variable: AWS_SECRET_ACCESS_KEY Default: None AWS_region \u00a4 The aws region where you wish to provision the EFS instance. Required Environment Variable: AWS_DEFAULT_REGION Default: eu-west-2 cluster_name \u00a4 The name of the cluster we are going to attach the EFS storage to. Required Environment Variable: CLUSTER_NAME Default: None efs_unique_id \u00a4 Any unique identifier like mas instance id which will be used as EFS storage class name. If this parameter is not set, then cluster_name will be used Optional Environment Variable: EFS_UNIQUE_ID Default: None creation_token_prefix \u00a4 CreationTokens associated for AWS resources are built by concatenating creation_token_prefix and efs_unique_id. Optional Environment Variable: CREATION_TOKEN_PREFIX Default: 'mas_devops.' create_storage_class \u00a4 If true, a StorageClass for the EFS instance named efs<efs_unique_id> will be automatically created in the cluster. Optional Environment Variable: CREATE_STORAGE_CLASS . Unset implies true , otherwise Ansible's bool filter is used to interpret the value as a boolean. Default: true License \u00a4 EPL-2.0","title":"ocp_efs"},{"location":"roles/ocp_efs/#ocp_efs","text":"This role provides support to install aws-efs on aws using aws cli and connect that to ROSA using oc CLI. This role create a new inbound rule in the security group of the ec2 instance where rosa is installed and then creates a new EFS instance and adds access points and network mounts to access EFS from ROSA.","title":"ocp_efs"},{"location":"roles/ocp_efs/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_efs/#aws_access_key_id","text":"The AWS access key will be used to login to aws cli. Required Environment Variable: AWS_ACCESS_KEY_ID Default: None","title":"aws_access_key_id"},{"location":"roles/ocp_efs/#aws_secret_access_key","text":"The AWS access secret key will be used to login to aws cli. Required Environment Variable: AWS_SECRET_ACCESS_KEY Default: None","title":"aws_secret_access_key"},{"location":"roles/ocp_efs/#aws_region","text":"The aws region where you wish to provision the EFS instance. Required Environment Variable: AWS_DEFAULT_REGION Default: eu-west-2","title":"AWS_region"},{"location":"roles/ocp_efs/#cluster_name","text":"The name of the cluster we are going to attach the EFS storage to. Required Environment Variable: CLUSTER_NAME Default: None","title":"cluster_name"},{"location":"roles/ocp_efs/#efs_unique_id","text":"Any unique identifier like mas instance id which will be used as EFS storage class name. If this parameter is not set, then cluster_name will be used Optional Environment Variable: EFS_UNIQUE_ID Default: None","title":"efs_unique_id"},{"location":"roles/ocp_efs/#creation_token_prefix","text":"CreationTokens associated for AWS resources are built by concatenating creation_token_prefix and efs_unique_id. Optional Environment Variable: CREATION_TOKEN_PREFIX Default: 'mas_devops.'","title":"creation_token_prefix"},{"location":"roles/ocp_efs/#create_storage_class","text":"If true, a StorageClass for the EFS instance named efs<efs_unique_id> will be automatically created in the cluster. Optional Environment Variable: CREATE_STORAGE_CLASS . Unset implies true , otherwise Ansible's bool filter is used to interpret the value as a boolean. Default: true","title":"create_storage_class"},{"location":"roles/ocp_efs/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_github_oauth/","text":"ocp_github_oauth \u00a4 This role provides to support to configure cluster oauth using GitHub. Warning Make sure you have configured the oauth app in GitHub organization before use this role. When configuring make sure to use ibmgithub as the oauth id. Requires organization admin permission to perform this action. Role Variables \u00a4 oauth.github_client_secret_value Secret value provided by the GitHub oauth app configuration. ouath.github_client_id_value Client ID value provided by the GitHub oauth app configuration. oauth.github_hostname can be used to target public GitHub or an enterprise account (e.g. github.ibm.com) oauth.groups List of groups to be created and its cluster role bindings oauth.groups.name Defines the name of the group oauth.groups.users List of users to be added to the group oauth.groups.groups_cluster_rolebindings List of cluster role bindings to be created for the group oauth.organizations List of GitHub organizations where the authentication will be performed Example Playbook \u00a4 TODO: Add example License \u00a4 EPL-2.0","title":"ocp_github_oauth"},{"location":"roles/ocp_github_oauth/#ocp_github_oauth","text":"This role provides to support to configure cluster oauth using GitHub. Warning Make sure you have configured the oauth app in GitHub organization before use this role. When configuring make sure to use ibmgithub as the oauth id. Requires organization admin permission to perform this action.","title":"ocp_github_oauth"},{"location":"roles/ocp_github_oauth/#role-variables","text":"oauth.github_client_secret_value Secret value provided by the GitHub oauth app configuration. ouath.github_client_id_value Client ID value provided by the GitHub oauth app configuration. oauth.github_hostname can be used to target public GitHub or an enterprise account (e.g. github.ibm.com) oauth.groups List of groups to be created and its cluster role bindings oauth.groups.name Defines the name of the group oauth.groups.users List of users to be added to the group oauth.groups.groups_cluster_rolebindings List of cluster role bindings to be created for the group oauth.organizations List of GitHub organizations where the authentication will be performed","title":"Role Variables"},{"location":"roles/ocp_github_oauth/#example-playbook","text":"TODO: Add example","title":"Example Playbook"},{"location":"roles/ocp_github_oauth/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_login/","text":"ocp_login \u00a4 This role provides support to login to a cluster using the oc CLI by looking up cluster information from the infrastructure provider's APIs, it also supports setting ocp_server and ocp_token directly to support login to any Kubernetes cluster. Role Variables \u00a4 cluster_name \u00a4 The name of the cluster to login to. This will be used to lookup the actual login credentials of the system. Required unless ocp_server and ocp_token are set Environment Variable: CLUSTER_NAME Default: None cluster_type \u00a4 The type of cluster to login to ( roks , fyre , or rosa ) Required unless ocp_server and ocp_token are set Environment Variable: CLUSTER_TYPE Default: None ocp_server \u00a4 The OCP server address to perform oc login against Required unless cluster_name and cluster_type are set Environment Variable: OCP_SERVER Default: None ocp_token \u00a4 The login token to use for oc login Required unless cluster_name and cluster_type are set Environment Variable: OCP_TOKEN Default: None Role Variables - IBMCloud ROKS \u00a4 ibmcloud_apikey \u00a4 APIKey to be used by ibmcloud login comand Required when cluster_type is roks Environment Variable: IBMCLOUD_APIKEY Default: None ibmcloud_endpoint \u00a4 Override the default IBMCloud API endpoint. Optional Environment Variable: IBMCLOUD_ENDPOINT Default Value: https://cloud.ibm.com Role Variables - IBM DevIT Fyre \u00a4 fyre_username \u00a4 Your FYRE username Required when cluster_type is fyre Environment Variable: FYRE_APIKEY Default: None fyre_apikey \u00a4 Your FYRE API Key - Required when cluster_type is fyre - Environment Variable: FYRE_APIKEY - Default: None fyre_site \u00a4 Site where cluster had been provisioned in Fyre Optional Environment Variable: FYRE_SITE Default Value: svl enable_ipv6 \u00a4 Enable IPv6. This is for Fyre at RTP site only - Environment Variable: ENABLE_IPV6 - Default: False Role Variables - AWS ROSA \u00a4 rosa_token \u00a4 Your ROSA secure token. Required when cluster_type is rosa Environment Variable: ROSA_TOKEN Default: None rosa_cluster_admin_password \u00a4 The password for the cluster-admin account (created when the cluster was provisioned). Required when cluster_type is rosa Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default: None Example Playbooks \u00a4 Direct Login \u00a4 - hosts: localhost vars: ocp_server: xxxxx ocp_token: xxxxx roles: - ibm.mas_devops.ocp_login IBMCloud ROKS \u00a4 - hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx ibmcloud_resourcegroup: mygroup roles: - ibm.mas_devops.ocp_login AWS ROSA \u00a4 - hosts: localhost vars: cluster_name: mycluster cluster_type: rosa rosa_token: xxxxx rosa_cluster_admin_password: xxxxx roles: - ibm.mas_devops.ocp_login IBM DevIT Fyre \u00a4 - hosts: localhost vars: cluster_name: mycluster cluster_type: fyre fyre_username: xxxxx fyre_password: xxxxx roles: - ibm.mas_devops.ocp_login License \u00a4 EPL-2.0","title":"ocp_login"},{"location":"roles/ocp_login/#ocp_login","text":"This role provides support to login to a cluster using the oc CLI by looking up cluster information from the infrastructure provider's APIs, it also supports setting ocp_server and ocp_token directly to support login to any Kubernetes cluster.","title":"ocp_login"},{"location":"roles/ocp_login/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_login/#cluster_name","text":"The name of the cluster to login to. This will be used to lookup the actual login credentials of the system. Required unless ocp_server and ocp_token are set Environment Variable: CLUSTER_NAME Default: None","title":"cluster_name"},{"location":"roles/ocp_login/#cluster_type","text":"The type of cluster to login to ( roks , fyre , or rosa ) Required unless ocp_server and ocp_token are set Environment Variable: CLUSTER_TYPE Default: None","title":"cluster_type"},{"location":"roles/ocp_login/#ocp_server","text":"The OCP server address to perform oc login against Required unless cluster_name and cluster_type are set Environment Variable: OCP_SERVER Default: None","title":"ocp_server"},{"location":"roles/ocp_login/#ocp_token","text":"The login token to use for oc login Required unless cluster_name and cluster_type are set Environment Variable: OCP_TOKEN Default: None","title":"ocp_token"},{"location":"roles/ocp_login/#role-variables-ibmcloud-roks","text":"","title":"Role Variables - IBMCloud ROKS"},{"location":"roles/ocp_login/#ibmcloud_apikey","text":"APIKey to be used by ibmcloud login comand Required when cluster_type is roks Environment Variable: IBMCLOUD_APIKEY Default: None","title":"ibmcloud_apikey"},{"location":"roles/ocp_login/#ibmcloud_endpoint","text":"Override the default IBMCloud API endpoint. Optional Environment Variable: IBMCLOUD_ENDPOINT Default Value: https://cloud.ibm.com","title":"ibmcloud_endpoint"},{"location":"roles/ocp_login/#role-variables-ibm-devit-fyre","text":"","title":"Role Variables - IBM DevIT Fyre"},{"location":"roles/ocp_login/#fyre_username","text":"Your FYRE username Required when cluster_type is fyre Environment Variable: FYRE_APIKEY Default: None","title":"fyre_username"},{"location":"roles/ocp_login/#fyre_apikey","text":"Your FYRE API Key - Required when cluster_type is fyre - Environment Variable: FYRE_APIKEY - Default: None","title":"fyre_apikey"},{"location":"roles/ocp_login/#fyre_site","text":"Site where cluster had been provisioned in Fyre Optional Environment Variable: FYRE_SITE Default Value: svl","title":"fyre_site"},{"location":"roles/ocp_login/#enable_ipv6","text":"Enable IPv6. This is for Fyre at RTP site only - Environment Variable: ENABLE_IPV6 - Default: False","title":"enable_ipv6"},{"location":"roles/ocp_login/#role-variables-aws-rosa","text":"","title":"Role Variables - AWS ROSA"},{"location":"roles/ocp_login/#rosa_token","text":"Your ROSA secure token. Required when cluster_type is rosa Environment Variable: ROSA_TOKEN Default: None","title":"rosa_token"},{"location":"roles/ocp_login/#rosa_cluster_admin_password","text":"The password for the cluster-admin account (created when the cluster was provisioned). Required when cluster_type is rosa Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default: None","title":"rosa_cluster_admin_password"},{"location":"roles/ocp_login/#example-playbooks","text":"","title":"Example Playbooks"},{"location":"roles/ocp_login/#direct-login","text":"- hosts: localhost vars: ocp_server: xxxxx ocp_token: xxxxx roles: - ibm.mas_devops.ocp_login","title":"Direct Login"},{"location":"roles/ocp_login/#ibmcloud-roks","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: xxxxx ibmcloud_resourcegroup: mygroup roles: - ibm.mas_devops.ocp_login","title":"IBMCloud ROKS"},{"location":"roles/ocp_login/#aws-rosa","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: rosa rosa_token: xxxxx rosa_cluster_admin_password: xxxxx roles: - ibm.mas_devops.ocp_login","title":"AWS ROSA"},{"location":"roles/ocp_login/#ibm-devit-fyre","text":"- hosts: localhost vars: cluster_name: mycluster cluster_type: fyre fyre_username: xxxxx fyre_password: xxxxx roles: - ibm.mas_devops.ocp_login","title":"IBM DevIT Fyre"},{"location":"roles/ocp_login/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_node_config/","text":"ocp_node_config \u00a4 Use the following command to verify the effect of any labels and taints that you apply to a node: oc get pods --all-namespaces -o wide --field-selector spec.nodeName=xxx Role Variables - Node Selection \u00a4 Use either ocp_node_name or ocp_node_index to identify the node to be modified. If you specify both then the index will take priority over the name, if you specify neither then the role will fail to execute. ocp_node_name \u00a4 The name of the node to work with Optional Environment Variable: OCP_NODE_NAME Default: None ocp_node_index \u00a4 The index (in the list of nodes) of the node to work with. Note that the index starts at 0 (so use ocp_node_index=0 if you want to work with the first node). Optional Environment Variable: OCP_NODE_INDEX Default: None Role Variables - Node Labels \u00a4 ocp_node_label_keys \u00a4 A comma-seperated list of labels to add to the selected node Optional Environment Variable: OCP_NODE_LABEL_KEYS Default: None ocp_node_label_values \u00a4 A comma-seperated list of values for the labels being created Optional Environment Variable: OCP_NODE_LABEL_VALUES Default: None Role Variables - Node Taints \u00a4 ocp_node_taint_keys \u00a4 A comma-seperated list of taints to add to the selected node Optional Environment Variable: OCP_NODE_TAINT_KEYS Default: None ocp_node_taint_values \u00a4 A comma-seperated list of values for the taints being created Optional Environment Variable: OCP_NODE_TAINT_VALUES Default: None ocp_node_taint_effects \u00a4 A comma-seperated list of taint effects to set: NoSchedule : New pods will not be scheduled onto the node PreferNoSchedule : New pods try not to be scheduled onto the node NoExecute : New pods are not schedules onto the node, existing pods are removed Optional Environment Variable: OCP_NODE_TAINT_EFFECTS Default: None Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # Turn this worker node into a dedicated Db2 worker node ocp_node_name: \"10.172.168.89\" # Add the label that will be applied to all dedicated Db2 nodes # this will be used to direct Db2 workloads to these nodes ocp_node_label_keys: workload ocp_node_label_values: db2 # Set a taint preventing anything other than Db2 workloads for masinst1 running # on this node specific Db2 node ocp_node_taint_keys: dedicatedDb2Node ocp_node_taint_values: masinst1 ocp_node_taint_effects: NoExecute roles: - ibm.mas_devops.ocp_node_config License \u00a4 EPL-2.0","title":"ocp_node_config"},{"location":"roles/ocp_node_config/#ocp_node_config","text":"Use the following command to verify the effect of any labels and taints that you apply to a node: oc get pods --all-namespaces -o wide --field-selector spec.nodeName=xxx","title":"ocp_node_config"},{"location":"roles/ocp_node_config/#role-variables-node-selection","text":"Use either ocp_node_name or ocp_node_index to identify the node to be modified. If you specify both then the index will take priority over the name, if you specify neither then the role will fail to execute.","title":"Role Variables - Node Selection"},{"location":"roles/ocp_node_config/#ocp_node_name","text":"The name of the node to work with Optional Environment Variable: OCP_NODE_NAME Default: None","title":"ocp_node_name"},{"location":"roles/ocp_node_config/#ocp_node_index","text":"The index (in the list of nodes) of the node to work with. Note that the index starts at 0 (so use ocp_node_index=0 if you want to work with the first node). Optional Environment Variable: OCP_NODE_INDEX Default: None","title":"ocp_node_index"},{"location":"roles/ocp_node_config/#role-variables-node-labels","text":"","title":"Role Variables - Node Labels"},{"location":"roles/ocp_node_config/#ocp_node_label_keys","text":"A comma-seperated list of labels to add to the selected node Optional Environment Variable: OCP_NODE_LABEL_KEYS Default: None","title":"ocp_node_label_keys"},{"location":"roles/ocp_node_config/#ocp_node_label_values","text":"A comma-seperated list of values for the labels being created Optional Environment Variable: OCP_NODE_LABEL_VALUES Default: None","title":"ocp_node_label_values"},{"location":"roles/ocp_node_config/#role-variables-node-taints","text":"","title":"Role Variables - Node Taints"},{"location":"roles/ocp_node_config/#ocp_node_taint_keys","text":"A comma-seperated list of taints to add to the selected node Optional Environment Variable: OCP_NODE_TAINT_KEYS Default: None","title":"ocp_node_taint_keys"},{"location":"roles/ocp_node_config/#ocp_node_taint_values","text":"A comma-seperated list of values for the taints being created Optional Environment Variable: OCP_NODE_TAINT_VALUES Default: None","title":"ocp_node_taint_values"},{"location":"roles/ocp_node_config/#ocp_node_taint_effects","text":"A comma-seperated list of taint effects to set: NoSchedule : New pods will not be scheduled onto the node PreferNoSchedule : New pods try not to be scheduled onto the node NoExecute : New pods are not schedules onto the node, existing pods are removed Optional Environment Variable: OCP_NODE_TAINT_EFFECTS Default: None","title":"ocp_node_taint_effects"},{"location":"roles/ocp_node_config/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Turn this worker node into a dedicated Db2 worker node ocp_node_name: \"10.172.168.89\" # Add the label that will be applied to all dedicated Db2 nodes # this will be used to direct Db2 workloads to these nodes ocp_node_label_keys: workload ocp_node_label_values: db2 # Set a taint preventing anything other than Db2 workloads for masinst1 running # on this node specific Db2 node ocp_node_taint_keys: dedicatedDb2Node ocp_node_taint_values: masinst1 ocp_node_taint_effects: NoExecute roles: - ibm.mas_devops.ocp_node_config","title":"Example Playbook"},{"location":"roles/ocp_node_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_provision/","text":"ocp_provision \u00a4 Provision OCP cluster on IBM Cloud ROKS, ROSA, or DevIT Fyre. Fyre clusters will be automatically reconfigured to enable NFS storage. By default this is made available via the nfs-client storage class and supports both ReadWriteOnce and ReadWriteMany access modes. The image-registry-storage PVC used by the OpenShift image registry component will also be reconfigured to use this storage class. Role Variables \u00a4 cluster_type \u00a4 Specify the cluster type, supported values are fyre , roks , rosa , and ipi . Required Environment Variable: CLUSTER_TYPE Default Value: None cluster_name \u00a4 Specify the name of the cluster Required Environment Variable: CLUSTER_NAME Default Value: None ocp_version \u00a4 The version of OCP to use. A specific version can be set, minor and patch level versions can be used, e.g. 4.15 , or 4.15.16 . Additionally, two version aliases are available; default will auto-select the newest version of OCP currently supported by IBM Maximo Application Suite, rotate will auto-select a predetermined version of OCP currently supported by IBM Maximo Application Suite based on the day of the week. This latter option is primarily useful for testing purposes. Required Environment Variable: OCP_VERSION Default Value: None Note When using the IBMCloud Red Hat OpenShift Service (ROKS) the version must be followed by _openshift , e.g. 4.15_openshift or 4.15.16_openshift Role Variables - GPU Node Support \u00a4 ocp_provision_gpu \u00a4 Flag that determines if GPU worker nodes should be added during cluster creation (eg. needed for MVI application). This is currently only set up for ROKS clusters. Environment Variable: OCP_PROVISION_GPU Default Value: false gpu_workerpool_name \u00a4 The name of the gpu worker pool to added to or modify in the cluster. If already existing, use the existing name to avoid recreating another gpu worker pool unless that is the goal. Optional Environment Variable: GPU_WORKERPOOL_NAME Default Value: gpu gpu_workers \u00a4 The number of GPU worker nodes that will be deploy in the cluster. The node created will have mg4c.32x384.2xp100-GPU flavor. This variable depends on ocp_provision_gpu and is currently supported on ROKS clusters only. Optional Environment Variable: GPU_WORKERS Default Value: 1 compute_node_count \u00a4 The number of compute nodes (i.e. worker nodes) allocate to the OCP cluster. Optional Environment Variable: COMPUTE_NODE_COUNT Default Value: 3 controlplane_node_count \u00a4 The number of control plane nodes (i.e. master nodes) allocate to the OCP cluster. Optional Environment Variable: CONTROLPLANE_NODE_COUNT Default Value: 3 gpu_workerpool_name \u00a4 The name of the gpu worker pool to added to or modify in the cluster. If already existing, use the existing name to avoid recreating another gpu worker pool unless that is the goal. Environment Variable: GPU_WORKERPOOL_NAME Default Value: gpu Role Variables - ROKS \u00a4 The following variables are only used when cluster_type = roks . ibmcloud_apikey \u00a4 The APIKey to be used by ibmcloud login comand. Required if cluster_type = roks Environment Variable: IBMCLOUD_APIKEY Default Value: None ibmcloud_endpoint \u00a4 Override the default IBMCloud API endpoint. Optional Environment Variable: IBMCLOUD_ENDPOINT Default Value: https://cloud.ibm.com ibmcloud_resourcegroup \u00a4 The resource group to create the cluster inside. Optional Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default roks_zone \u00a4 IBM Cloud zone where the cluster should be provisioned. Optional Environment Variable: ROKS_ZONE Default Value: dal10 roks_flavor \u00a4 Worker node flavor Optional Environment Variable: ROKS_FLAVOR Default Value: b3c.16x64.300gb roks_workers \u00a4 Number of worker nodes for the roks cluster Optional Environment Variable: ROKS_WORKERS Default Value: 3 roks_flags \u00a4 Can be used to specify additional parameters for the cluster creation Optional Environment Variable: ROKS_FLAGS Default Value: None Role Variables - ROSA \u00a4 The following variables are only used when cluster_type = rosa . rosa_token \u00a4 Token to authenticate to the ROSA service. To obtain your API token login to the OpenShift cluster manager . Required if cluster_type = rosa . Environment Variable: ROSA_TOKEN Default Value: None rosa_cluster_admin_password \u00a4 Password to set up for the cluster-admin user account on the OCP instance. You will need this to log onto the cluster after it is provisioned. If this is not set then a password is auto-generated. Optional if cluster_type = rosa . Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default Value: None rosa_compute_nodes \u00a4 Number of compute nodes to deploy in the cluster. Optional Environment Variable: ROSA_COMPUTE_NODES Default Value: 3 rosa_compute_machine_type \u00a4 Worker nodes machine Optional Environment Variable: ROSA_COMPUTE_MACHINE_TYPE Default Value: m5.4xlarge rosa_config_dir \u00a4 Config directory to hold the rosa-{{cluster_name}}-details.yaml file that contains the api endpoint and cluster-admin details Optional Environment Variable: ROSA_CONFIG_DIR Default Value: None Role Variables - FYRE \u00a4 The following variables are only used when cluster_type = fyre . fyre_username \u00a4 Username to authenticate with Fyre API. Required if cluster_type = fyre . Environment Variable: FYRE_USERNAME Default Value: None fyre_apikey \u00a4 API key to authenticate with Fyre API. Required if cluster_type = fyre . Environment Variable: FYRE_APIKEY Default Value: None fyre_quota_type \u00a4 Type of quota to draw from when provisioning the cluster, valid options are quick_burn and product_group . Required if cluster_type = fyre . Environment Variable: FYRE_QUOTA_TYPE Default Value: quick_burn fyre_product_id \u00a4 The Product ID that the cluster will be associated with for accounting purposes. Required if cluster_type = fyre . Environment Variable: FYRE_PRODUCT_ID Default Value: None fyre_site \u00a4 Provide a site in Fyre where cluster will be provisioned Optional Environment Variable: FYRE_SITE Default Value: svl fyre_cluster_description \u00a4 Provide a description for the cluster. Optional Environment Variable: FYRE_CLUSTER_DESCRIPTION Default Value: None ocp_fips_enabled \u00a4 Set to true to provision a FIPS enabled cluster. Optional Environment Variable: OCP_FIPS_ENABLED Default Value: false fyre_cluster_size \u00a4 The name of one of Fyre's pre-defined cluster sizes to use for the new cluster. Required when cluster_type = fyre and fyre_quota_type = quick_burn . Environment Variable: FYRE_CLUSTER_SIZE Default Value: medium fyre_worker_count \u00a4 The number of worker nodes to provision in the cluster. Required when cluster_type = fyre and fyre_quota_type = product_group . Environment Variable: FYRE_WORKER_COUNT Default Value: 2 fyre_worker_cpu \u00a4 The amount of CPU to assign to each worker node (maximum value supported by FYRE 16). Required when cluster_type = fyre and fyre_quota_type = product_group . Environment Variable: FYRE_WORKER_CPU Default Value: 8 fyre_worker_memory \u00a4 The amount of memory to assign to each worker node (maximum value supported by FYRE 64). Required when cluster_type = fyre and fyre_quota_type = product_group . Environment Variable: FYRE_WORKER_MEMORY Default Value: 32 fyre_worker_additional_disks \u00a4 The size of additional disks in Gb added to each worker node, defined in a comma-seperated list, e.g. 400,400 will add two 400gb disks to each worker node. By default no additional disks will be attached. Optional Environment Variable: FYRE_WORKER_ADDITIONAL_DISKS Default Value: None fyre_nfs_setup \u00a4 Enables the use of NFS storage classes in the Fyre cluster. When enabled, the existing image registry PVC will be deleted and recreated configured to use the newly available NFS storage class. Optional Environment Variable: FYRE_NFS_SETUP Default Value: true fyre_nfs_image_registry_size \u00a4 Defines the image registry storage size when configured to use NFS. The size allocated cannot be superior of storage available in the Fyre Infrastructure node. Optional Environment Variable: FYRE_NFS_IMAGE_REGISTRY_SIZE Default: 100Gi enable_ipv6 \u00a4 Enable IPv6. This is for Fyre at RTP site only. Environment Variable: ENABLE_IPV6 Default: False Role Variables - IPI \u00a4 These variables are only used when cluster_type = ipi . Note IPI stands for Installer Provisioned Infrastructure . OpenShift offers two possible deployment methods: IPI and UPI (User Provisioned Infrastructure). The difference is the degree of automation and customization. IPI will not only deploy OpenShift but also all infrastructure components and configurations. ipi_platform \u00a4 Platform to create the cluster on. Technically, any platform supported by openshift-install should work here, but currently we have only specifically tested on aws and gcp , where aws is the default value. Optional when cluster_type = ipi Environment Variable: IPI_PLATFORM Default Value: aws ipi_region \u00a4 Platform region where OCP cluster will be created. Optional when cluster_type = ipi Environment Variable: IPI_REGION Default Value: us-east-1 ipi_base_domain \u00a4 Specify the base domain of the cluster that will be provisioned. Required when cluster_type = ipi Environment Variable: IPI_BASE_DOMAIN Default Value: None ipi_pull_secret_file \u00a4 Location of the file containing your Redhat OpenShift pull secret. This file can be obtained from the Red Hat Hybrid Cloud Console Required when cluster_type = ipi Environment Variable: IPI_PULL_SECRET_FILE Default Value: None ipi_dir \u00a4 The working directory that is used to perform the installation, it will contain the openshift-install executable, its configuration files, & any generated logs. Optional when cluster_type = ipi Environment Variable: IPI_DIR Default Value: ~/openshift-install sshKey \u00a4 Public SSH key value. It will be set in the OCP cluster nodes. Can be used to SSH into the OCP cluster nodes using a bastion. Optional when cluster_type = ipi Environment Variable: SSH_PUB_KEY ipi_controlplane_type \u00a4 Control plane node type. Optional when cluster_type = ipi Environment Variable: IPI_CONTROLPLANE_TYPE Default Value: m5.4xlarge ipi_controlplane_replicas \u00a4 The number of master nodes to provision to form the control plane of your cluster. Optional when cluster_type = ipi Environment Variable: IPI_CONTROLPLANE_REPLICAS Default Value: 3 ipi_compute_type \u00a4 Compute node type. Optional when cluster_type = ipi Environment Variable: IPI_COMPUTE_TYPE Default Value: m5.4xlarge ipi_compute_replicas \u00a4 The number of worker nodes to provsision in the cluster, providing your compute resource. Optional when cluster_type = ipi Environment Variable: IPI_COMPUTE_REPLICAS Default Value: 3 ipi_rootvolume_size \u00a4 The size of root volume in GiB. Optional when cluster_type = ipi Environment variable: IPI_ROOTVOLUME_SIZE Role Variables - AWS \u00a4 The following variables are only used when cluster_type = ipi and ipi_platform = aws . aws_access_key_id \u00a4 AWS access key associated with an IAM user or role. Make sure the access key has permissions to create instances. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None aws_secret_access_key \u00a4 AWS secret access key associated with an IAM user or role. Required when cluster_type = aws-ipi and ipi_platform = aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None Role Variables - GCP \u00a4 The following variables are only used when cluster_type = ipi and ipi_platform = gcp . gcp_service_account_file \u00a4 GCP service account file path. Make sure the service account has permissions to create instances. Required when cluster_type = ipi and ipi_platform = gcp Environment Variable: GOOGLE_APPLICATION_CREDENTIALS Default Value: None ipi_gcp_projectid \u00a4 GCP project id in which the cluster will be deployed. Required when cluster_type = ipi and ipi_platform = gcp Environment Variable: GOOGLE_PROJECTID Default Value: None Example Playbook \u00a4 - hosts: localhost vars: cluster_type: roks cluster_name: mycluster ocp_version: 4.10 ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_provision License \u00a4 EPL-2.0","title":"ocp_provision"},{"location":"roles/ocp_provision/#ocp_provision","text":"Provision OCP cluster on IBM Cloud ROKS, ROSA, or DevIT Fyre. Fyre clusters will be automatically reconfigured to enable NFS storage. By default this is made available via the nfs-client storage class and supports both ReadWriteOnce and ReadWriteMany access modes. The image-registry-storage PVC used by the OpenShift image registry component will also be reconfigured to use this storage class.","title":"ocp_provision"},{"location":"roles/ocp_provision/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_provision/#cluster_type","text":"Specify the cluster type, supported values are fyre , roks , rosa , and ipi . Required Environment Variable: CLUSTER_TYPE Default Value: None","title":"cluster_type"},{"location":"roles/ocp_provision/#cluster_name","text":"Specify the name of the cluster Required Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/ocp_provision/#ocp_version","text":"The version of OCP to use. A specific version can be set, minor and patch level versions can be used, e.g. 4.15 , or 4.15.16 . Additionally, two version aliases are available; default will auto-select the newest version of OCP currently supported by IBM Maximo Application Suite, rotate will auto-select a predetermined version of OCP currently supported by IBM Maximo Application Suite based on the day of the week. This latter option is primarily useful for testing purposes. Required Environment Variable: OCP_VERSION Default Value: None Note When using the IBMCloud Red Hat OpenShift Service (ROKS) the version must be followed by _openshift , e.g. 4.15_openshift or 4.15.16_openshift","title":"ocp_version"},{"location":"roles/ocp_provision/#role-variables-gpu-node-support","text":"","title":"Role Variables - GPU Node Support"},{"location":"roles/ocp_provision/#ocp_provision_gpu","text":"Flag that determines if GPU worker nodes should be added during cluster creation (eg. needed for MVI application). This is currently only set up for ROKS clusters. Environment Variable: OCP_PROVISION_GPU Default Value: false","title":"ocp_provision_gpu"},{"location":"roles/ocp_provision/#gpu_workerpool_name","text":"The name of the gpu worker pool to added to or modify in the cluster. If already existing, use the existing name to avoid recreating another gpu worker pool unless that is the goal. Optional Environment Variable: GPU_WORKERPOOL_NAME Default Value: gpu","title":"gpu_workerpool_name"},{"location":"roles/ocp_provision/#gpu_workers","text":"The number of GPU worker nodes that will be deploy in the cluster. The node created will have mg4c.32x384.2xp100-GPU flavor. This variable depends on ocp_provision_gpu and is currently supported on ROKS clusters only. Optional Environment Variable: GPU_WORKERS Default Value: 1","title":"gpu_workers"},{"location":"roles/ocp_provision/#compute_node_count","text":"The number of compute nodes (i.e. worker nodes) allocate to the OCP cluster. Optional Environment Variable: COMPUTE_NODE_COUNT Default Value: 3","title":"compute_node_count"},{"location":"roles/ocp_provision/#controlplane_node_count","text":"The number of control plane nodes (i.e. master nodes) allocate to the OCP cluster. Optional Environment Variable: CONTROLPLANE_NODE_COUNT Default Value: 3","title":"controlplane_node_count"},{"location":"roles/ocp_provision/#gpu_workerpool_name_1","text":"The name of the gpu worker pool to added to or modify in the cluster. If already existing, use the existing name to avoid recreating another gpu worker pool unless that is the goal. Environment Variable: GPU_WORKERPOOL_NAME Default Value: gpu","title":"gpu_workerpool_name"},{"location":"roles/ocp_provision/#role-variables-roks","text":"The following variables are only used when cluster_type = roks .","title":"Role Variables - ROKS"},{"location":"roles/ocp_provision/#ibmcloud_apikey","text":"The APIKey to be used by ibmcloud login comand. Required if cluster_type = roks Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/ocp_provision/#ibmcloud_endpoint","text":"Override the default IBMCloud API endpoint. Optional Environment Variable: IBMCLOUD_ENDPOINT Default Value: https://cloud.ibm.com","title":"ibmcloud_endpoint"},{"location":"roles/ocp_provision/#ibmcloud_resourcegroup","text":"The resource group to create the cluster inside. Optional Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/ocp_provision/#roks_zone","text":"IBM Cloud zone where the cluster should be provisioned. Optional Environment Variable: ROKS_ZONE Default Value: dal10","title":"roks_zone"},{"location":"roles/ocp_provision/#roks_flavor","text":"Worker node flavor Optional Environment Variable: ROKS_FLAVOR Default Value: b3c.16x64.300gb","title":"roks_flavor"},{"location":"roles/ocp_provision/#roks_workers","text":"Number of worker nodes for the roks cluster Optional Environment Variable: ROKS_WORKERS Default Value: 3","title":"roks_workers"},{"location":"roles/ocp_provision/#roks_flags","text":"Can be used to specify additional parameters for the cluster creation Optional Environment Variable: ROKS_FLAGS Default Value: None","title":"roks_flags"},{"location":"roles/ocp_provision/#role-variables-rosa","text":"The following variables are only used when cluster_type = rosa .","title":"Role Variables - ROSA"},{"location":"roles/ocp_provision/#rosa_token","text":"Token to authenticate to the ROSA service. To obtain your API token login to the OpenShift cluster manager . Required if cluster_type = rosa . Environment Variable: ROSA_TOKEN Default Value: None","title":"rosa_token"},{"location":"roles/ocp_provision/#rosa_cluster_admin_password","text":"Password to set up for the cluster-admin user account on the OCP instance. You will need this to log onto the cluster after it is provisioned. If this is not set then a password is auto-generated. Optional if cluster_type = rosa . Environment Variable: ROSA_CLUSTER_ADMIN_PASSWORD Default Value: None","title":"rosa_cluster_admin_password"},{"location":"roles/ocp_provision/#rosa_compute_nodes","text":"Number of compute nodes to deploy in the cluster. Optional Environment Variable: ROSA_COMPUTE_NODES Default Value: 3","title":"rosa_compute_nodes"},{"location":"roles/ocp_provision/#rosa_compute_machine_type","text":"Worker nodes machine Optional Environment Variable: ROSA_COMPUTE_MACHINE_TYPE Default Value: m5.4xlarge","title":"rosa_compute_machine_type"},{"location":"roles/ocp_provision/#rosa_config_dir","text":"Config directory to hold the rosa-{{cluster_name}}-details.yaml file that contains the api endpoint and cluster-admin details Optional Environment Variable: ROSA_CONFIG_DIR Default Value: None","title":"rosa_config_dir"},{"location":"roles/ocp_provision/#role-variables-fyre","text":"The following variables are only used when cluster_type = fyre .","title":"Role Variables - FYRE"},{"location":"roles/ocp_provision/#fyre_username","text":"Username to authenticate with Fyre API. Required if cluster_type = fyre . Environment Variable: FYRE_USERNAME Default Value: None","title":"fyre_username"},{"location":"roles/ocp_provision/#fyre_apikey","text":"API key to authenticate with Fyre API. Required if cluster_type = fyre . Environment Variable: FYRE_APIKEY Default Value: None","title":"fyre_apikey"},{"location":"roles/ocp_provision/#fyre_quota_type","text":"Type of quota to draw from when provisioning the cluster, valid options are quick_burn and product_group . Required if cluster_type = fyre . Environment Variable: FYRE_QUOTA_TYPE Default Value: quick_burn","title":"fyre_quota_type"},{"location":"roles/ocp_provision/#fyre_product_id","text":"The Product ID that the cluster will be associated with for accounting purposes. Required if cluster_type = fyre . Environment Variable: FYRE_PRODUCT_ID Default Value: None","title":"fyre_product_id"},{"location":"roles/ocp_provision/#fyre_site","text":"Provide a site in Fyre where cluster will be provisioned Optional Environment Variable: FYRE_SITE Default Value: svl","title":"fyre_site"},{"location":"roles/ocp_provision/#fyre_cluster_description","text":"Provide a description for the cluster. Optional Environment Variable: FYRE_CLUSTER_DESCRIPTION Default Value: None","title":"fyre_cluster_description"},{"location":"roles/ocp_provision/#ocp_fips_enabled","text":"Set to true to provision a FIPS enabled cluster. Optional Environment Variable: OCP_FIPS_ENABLED Default Value: false","title":"ocp_fips_enabled"},{"location":"roles/ocp_provision/#fyre_cluster_size","text":"The name of one of Fyre's pre-defined cluster sizes to use for the new cluster. Required when cluster_type = fyre and fyre_quota_type = quick_burn . Environment Variable: FYRE_CLUSTER_SIZE Default Value: medium","title":"fyre_cluster_size"},{"location":"roles/ocp_provision/#fyre_worker_count","text":"The number of worker nodes to provision in the cluster. Required when cluster_type = fyre and fyre_quota_type = product_group . Environment Variable: FYRE_WORKER_COUNT Default Value: 2","title":"fyre_worker_count"},{"location":"roles/ocp_provision/#fyre_worker_cpu","text":"The amount of CPU to assign to each worker node (maximum value supported by FYRE 16). Required when cluster_type = fyre and fyre_quota_type = product_group . Environment Variable: FYRE_WORKER_CPU Default Value: 8","title":"fyre_worker_cpu"},{"location":"roles/ocp_provision/#fyre_worker_memory","text":"The amount of memory to assign to each worker node (maximum value supported by FYRE 64). Required when cluster_type = fyre and fyre_quota_type = product_group . Environment Variable: FYRE_WORKER_MEMORY Default Value: 32","title":"fyre_worker_memory"},{"location":"roles/ocp_provision/#fyre_worker_additional_disks","text":"The size of additional disks in Gb added to each worker node, defined in a comma-seperated list, e.g. 400,400 will add two 400gb disks to each worker node. By default no additional disks will be attached. Optional Environment Variable: FYRE_WORKER_ADDITIONAL_DISKS Default Value: None","title":"fyre_worker_additional_disks"},{"location":"roles/ocp_provision/#fyre_nfs_setup","text":"Enables the use of NFS storage classes in the Fyre cluster. When enabled, the existing image registry PVC will be deleted and recreated configured to use the newly available NFS storage class. Optional Environment Variable: FYRE_NFS_SETUP Default Value: true","title":"fyre_nfs_setup"},{"location":"roles/ocp_provision/#fyre_nfs_image_registry_size","text":"Defines the image registry storage size when configured to use NFS. The size allocated cannot be superior of storage available in the Fyre Infrastructure node. Optional Environment Variable: FYRE_NFS_IMAGE_REGISTRY_SIZE Default: 100Gi","title":"fyre_nfs_image_registry_size"},{"location":"roles/ocp_provision/#enable_ipv6","text":"Enable IPv6. This is for Fyre at RTP site only. Environment Variable: ENABLE_IPV6 Default: False","title":"enable_ipv6"},{"location":"roles/ocp_provision/#role-variables-ipi","text":"These variables are only used when cluster_type = ipi . Note IPI stands for Installer Provisioned Infrastructure . OpenShift offers two possible deployment methods: IPI and UPI (User Provisioned Infrastructure). The difference is the degree of automation and customization. IPI will not only deploy OpenShift but also all infrastructure components and configurations.","title":"Role Variables - IPI"},{"location":"roles/ocp_provision/#ipi_platform","text":"Platform to create the cluster on. Technically, any platform supported by openshift-install should work here, but currently we have only specifically tested on aws and gcp , where aws is the default value. Optional when cluster_type = ipi Environment Variable: IPI_PLATFORM Default Value: aws","title":"ipi_platform"},{"location":"roles/ocp_provision/#ipi_region","text":"Platform region where OCP cluster will be created. Optional when cluster_type = ipi Environment Variable: IPI_REGION Default Value: us-east-1","title":"ipi_region"},{"location":"roles/ocp_provision/#ipi_base_domain","text":"Specify the base domain of the cluster that will be provisioned. Required when cluster_type = ipi Environment Variable: IPI_BASE_DOMAIN Default Value: None","title":"ipi_base_domain"},{"location":"roles/ocp_provision/#ipi_pull_secret_file","text":"Location of the file containing your Redhat OpenShift pull secret. This file can be obtained from the Red Hat Hybrid Cloud Console Required when cluster_type = ipi Environment Variable: IPI_PULL_SECRET_FILE Default Value: None","title":"ipi_pull_secret_file"},{"location":"roles/ocp_provision/#ipi_dir","text":"The working directory that is used to perform the installation, it will contain the openshift-install executable, its configuration files, & any generated logs. Optional when cluster_type = ipi Environment Variable: IPI_DIR Default Value: ~/openshift-install","title":"ipi_dir"},{"location":"roles/ocp_provision/#sshkey","text":"Public SSH key value. It will be set in the OCP cluster nodes. Can be used to SSH into the OCP cluster nodes using a bastion. Optional when cluster_type = ipi Environment Variable: SSH_PUB_KEY","title":"sshKey"},{"location":"roles/ocp_provision/#ipi_controlplane_type","text":"Control plane node type. Optional when cluster_type = ipi Environment Variable: IPI_CONTROLPLANE_TYPE Default Value: m5.4xlarge","title":"ipi_controlplane_type"},{"location":"roles/ocp_provision/#ipi_controlplane_replicas","text":"The number of master nodes to provision to form the control plane of your cluster. Optional when cluster_type = ipi Environment Variable: IPI_CONTROLPLANE_REPLICAS Default Value: 3","title":"ipi_controlplane_replicas"},{"location":"roles/ocp_provision/#ipi_compute_type","text":"Compute node type. Optional when cluster_type = ipi Environment Variable: IPI_COMPUTE_TYPE Default Value: m5.4xlarge","title":"ipi_compute_type"},{"location":"roles/ocp_provision/#ipi_compute_replicas","text":"The number of worker nodes to provsision in the cluster, providing your compute resource. Optional when cluster_type = ipi Environment Variable: IPI_COMPUTE_REPLICAS Default Value: 3","title":"ipi_compute_replicas"},{"location":"roles/ocp_provision/#ipi_rootvolume_size","text":"The size of root volume in GiB. Optional when cluster_type = ipi Environment variable: IPI_ROOTVOLUME_SIZE","title":"ipi_rootvolume_size"},{"location":"roles/ocp_provision/#role-variables-aws","text":"The following variables are only used when cluster_type = ipi and ipi_platform = aws .","title":"Role Variables - AWS"},{"location":"roles/ocp_provision/#aws_access_key_id","text":"AWS access key associated with an IAM user or role. Make sure the access key has permissions to create instances. Required when cluster_type = ipi and ipi_platform = aws Environment Variable: AWS_ACCESS_KEY_ID Default Value: None","title":"aws_access_key_id"},{"location":"roles/ocp_provision/#aws_secret_access_key","text":"AWS secret access key associated with an IAM user or role. Required when cluster_type = aws-ipi and ipi_platform = aws Environment Variable: AWS_SECRET_ACCESS_KEY Default Value: None","title":"aws_secret_access_key"},{"location":"roles/ocp_provision/#role-variables-gcp","text":"The following variables are only used when cluster_type = ipi and ipi_platform = gcp .","title":"Role Variables - GCP"},{"location":"roles/ocp_provision/#gcp_service_account_file","text":"GCP service account file path. Make sure the service account has permissions to create instances. Required when cluster_type = ipi and ipi_platform = gcp Environment Variable: GOOGLE_APPLICATION_CREDENTIALS Default Value: None","title":"gcp_service_account_file"},{"location":"roles/ocp_provision/#ipi_gcp_projectid","text":"GCP project id in which the cluster will be deployed. Required when cluster_type = ipi and ipi_platform = gcp Environment Variable: GOOGLE_PROJECTID Default Value: None","title":"ipi_gcp_projectid"},{"location":"roles/ocp_provision/#example-playbook","text":"- hosts: localhost vars: cluster_type: roks cluster_name: mycluster ocp_version: 4.10 ibmcloud_apikey: xxxxx roles: - ibm.mas_devops.ocp_provision","title":"Example Playbook"},{"location":"roles/ocp_provision/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_roks_upgrade_registry_storage/","text":"ocp_roks_upgrade_registry_storage \u00a4 This role will use IBMCloud APIs to upgrade the capacity of the volume backing the OCP cluster's image registry. The volume will be increased from the default capacity of 100GB to 400GB. This is needed if you intend to install all of the services available in CloudPak for Data because the 100GB volume is not large enough. Role Variables \u00a4 ibmcloud_apikey \u00a4 The APIKey to be used to modify the storage volume associated with the image registry. Environment Variable: IBMCLOUD_APIKEY Default Value: None Example Playbook \u00a4 - hosts: localhost roles: - ibm.mas_devops.ocp_roks_tuning License \u00a4 EPL-2.0","title":"ocp_roks_upgrade_registry_storage"},{"location":"roles/ocp_roks_upgrade_registry_storage/#ocp_roks_upgrade_registry_storage","text":"This role will use IBMCloud APIs to upgrade the capacity of the volume backing the OCP cluster's image registry. The volume will be increased from the default capacity of 100GB to 400GB. This is needed if you intend to install all of the services available in CloudPak for Data because the 100GB volume is not large enough.","title":"ocp_roks_upgrade_registry_storage"},{"location":"roles/ocp_roks_upgrade_registry_storage/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_roks_upgrade_registry_storage/#ibmcloud_apikey","text":"The APIKey to be used to modify the storage volume associated with the image registry. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/ocp_roks_upgrade_registry_storage/#example-playbook","text":"- hosts: localhost roles: - ibm.mas_devops.ocp_roks_tuning","title":"Example Playbook"},{"location":"roles/ocp_roks_upgrade_registry_storage/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_simulate_disconnected_network/","text":"ocp_simulate_disconnected_network \u00a4 Our goal is to modify the hosts file on each node (worker and master) to add a bogus entry that breaks DNS resolution for all the registries that we are going to mirror. This will simulate the cluster running in an air gap configuration, although network access will be possible elsewhere, the cluster will be unable to access the docker registries Important The host file (on Fyre) will look something like this, this role is (for now anyway) mainly focused on simulating an air gap cluster in Fyre, it may work on other cluster providers but that can not be guaranteed and may require modifications depending on the specific way OpenShift is set up. oc get nodes oc debug node/node1 sh-4.4# more /host/etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 172.30.55.8 image-registry.openshift-image-registry.svc image-registry.openshift-image-registry.svc.cluster.local # openshift-generated-node-resolver The default exclusions are: quay.io registry.redhat.io registry.connect.redhat.com gcr.io nvcr.io icr.io cp.icr.io docker-na-public.artifactory.swg-devops.com docker-na-proxy-svl.artifactory.swg-devops.com docker-na-proxy-rtp.artifactory.swg-devops.com These can be changed by setting airgap_network_exclusions explicitly.","title":"ocp_simulate_disconnected_network"},{"location":"roles/ocp_simulate_disconnected_network/#ocp_simulate_disconnected_network","text":"Our goal is to modify the hosts file on each node (worker and master) to add a bogus entry that breaks DNS resolution for all the registries that we are going to mirror. This will simulate the cluster running in an air gap configuration, although network access will be possible elsewhere, the cluster will be unable to access the docker registries Important The host file (on Fyre) will look something like this, this role is (for now anyway) mainly focused on simulating an air gap cluster in Fyre, it may work on other cluster providers but that can not be guaranteed and may require modifications depending on the specific way OpenShift is set up. oc get nodes oc debug node/node1 sh-4.4# more /host/etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 172.30.55.8 image-registry.openshift-image-registry.svc image-registry.openshift-image-registry.svc.cluster.local # openshift-generated-node-resolver The default exclusions are: quay.io registry.redhat.io registry.connect.redhat.com gcr.io nvcr.io icr.io cp.icr.io docker-na-public.artifactory.swg-devops.com docker-na-proxy-svl.artifactory.swg-devops.com docker-na-proxy-rtp.artifactory.swg-devops.com These can be changed by setting airgap_network_exclusions explicitly.","title":"ocp_simulate_disconnected_network"},{"location":"roles/ocp_upgrade/","text":"ocp_upgrade \u00a4 This role supports the upgrade of the Openshift Cluster version for master and worker nodes in IBM Cloud provider. Role Variables \u00a4 cluster_type \u00a4 Required. Specify the cluster type, only IBM Cloud Openshift Clusters are supported by this role at the moment. If you provide a different cluster type than roks , this role will fail. Environment Variable: CLUSTER_TYPE Default Value: None cluster_name \u00a4 Required. Specify the name of the cluster to be upgraded. Environment Variable: CLUSTER_NAME Default Value: None ocp_version_upgrade \u00a4 Required. Specify the target version of the Openshift to be upgraded. Environment Variable: OCP_VERSION_UPGRADE Default Value: None Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: cluster_name: my-ocp-cluster cluster_type: roks ocp_version_upgrade: 4.10_openshift roles: - ibm.mas_devops.ocp_upgrade License \u00a4 EPL-2.0","title":"ocp_upgrade"},{"location":"roles/ocp_upgrade/#ocp_upgrade","text":"This role supports the upgrade of the Openshift Cluster version for master and worker nodes in IBM Cloud provider.","title":"ocp_upgrade"},{"location":"roles/ocp_upgrade/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_upgrade/#cluster_type","text":"Required. Specify the cluster type, only IBM Cloud Openshift Clusters are supported by this role at the moment. If you provide a different cluster type than roks , this role will fail. Environment Variable: CLUSTER_TYPE Default Value: None","title":"cluster_type"},{"location":"roles/ocp_upgrade/#cluster_name","text":"Required. Specify the name of the cluster to be upgraded. Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/ocp_upgrade/#ocp_version_upgrade","text":"Required. Specify the target version of the Openshift to be upgraded. Environment Variable: OCP_VERSION_UPGRADE Default Value: None","title":"ocp_version_upgrade"},{"location":"roles/ocp_upgrade/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: cluster_name: my-ocp-cluster cluster_type: roks ocp_version_upgrade: 4.10_openshift roles: - ibm.mas_devops.ocp_upgrade","title":"Example Playbook"},{"location":"roles/ocp_upgrade/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_verify/","text":"ocp_verify \u00a4 This role will verify that the target OCP cluster is ready to be setup for MAS. For example, in IBMCloud ROKS we have seen delays of over an hour before the Red Hat Operator catalog is ready to use. This will cause attempts to install anything from that CatalogSource to fail as the timeouts built into the roles in this collection are designed to catch problems with an install, rather than a half-provisioned cluster that is not properly ready to use yet. Role Variables \u00a4 verify_cluster \u00a4 Enables verification that the cluster is healthy and ready to use. This check runs against the ClusterVersion resource and expects the Ready condition to be set to true. If the cluster is not ready within 1 hour the verification will fail. Optional Environment Variable: VERIFY_CLUSTER Default Value: True verify_catalogsources \u00a4 Enables verification that all installed catalog sources are healthy. If any CatalogSources are not reporting lastObservedState as READY after 30 minutes then the verification will fail. Optional Environment Variable: VERIFY_CATALOGSOURCES Default Value: True verify_subscriptions \u00a4 Enables verification that all operator subscriptions are up to date. If any Subscriptions are not reporting state as AtLatestKnown after 5 hours then the verification will fail. Optional Environment Variable: VERIFY_SUBSCRIPTIONS Default Value: True verify_workloads \u00a4 Enables verification that all operator subscriptions are up to date. If any Deployments or StatefulSets are not reporting updatedReplicas & availableReplicas equal to replicas after 10 hours then the verification will fail. Optional Environment Variable: VERIFY_WORKLOADS Default Value: True verify_ingress \u00a4 Enables verification that the cluster ingress TLS certificate can be ontained. This is required by a number of roles in the collection. Optional Environment Variable: VERIFY_INGRESS Default Value: True cluster_name \u00a4 Specify the name of the cluster, in some cluster setups this name is required to determine the name of the default router certificate. Optional, only used when verify_ingress is enabled Environment Variable: CLUSTER_NAME Default Value: None ocp_ingress_tls_secret_name \u00a4 Specify the name of the cluster's ingres tls secret which contains the default router certificate. Optional, only used when verify_ingress is enabled Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default Example Playbook \u00a4 - hosts: localhost vars: verify_cluster: True verify_catalogsources: True verify_subscriptions: True verify_workloads: True verify_ingress: True roles: - ibm.mas_devops.ocp_verify License \u00a4 EPL-2.0","title":"ocp_verify"},{"location":"roles/ocp_verify/#ocp_verify","text":"This role will verify that the target OCP cluster is ready to be setup for MAS. For example, in IBMCloud ROKS we have seen delays of over an hour before the Red Hat Operator catalog is ready to use. This will cause attempts to install anything from that CatalogSource to fail as the timeouts built into the roles in this collection are designed to catch problems with an install, rather than a half-provisioned cluster that is not properly ready to use yet.","title":"ocp_verify"},{"location":"roles/ocp_verify/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_verify/#verify_cluster","text":"Enables verification that the cluster is healthy and ready to use. This check runs against the ClusterVersion resource and expects the Ready condition to be set to true. If the cluster is not ready within 1 hour the verification will fail. Optional Environment Variable: VERIFY_CLUSTER Default Value: True","title":"verify_cluster"},{"location":"roles/ocp_verify/#verify_catalogsources","text":"Enables verification that all installed catalog sources are healthy. If any CatalogSources are not reporting lastObservedState as READY after 30 minutes then the verification will fail. Optional Environment Variable: VERIFY_CATALOGSOURCES Default Value: True","title":"verify_catalogsources"},{"location":"roles/ocp_verify/#verify_subscriptions","text":"Enables verification that all operator subscriptions are up to date. If any Subscriptions are not reporting state as AtLatestKnown after 5 hours then the verification will fail. Optional Environment Variable: VERIFY_SUBSCRIPTIONS Default Value: True","title":"verify_subscriptions"},{"location":"roles/ocp_verify/#verify_workloads","text":"Enables verification that all operator subscriptions are up to date. If any Deployments or StatefulSets are not reporting updatedReplicas & availableReplicas equal to replicas after 10 hours then the verification will fail. Optional Environment Variable: VERIFY_WORKLOADS Default Value: True","title":"verify_workloads"},{"location":"roles/ocp_verify/#verify_ingress","text":"Enables verification that the cluster ingress TLS certificate can be ontained. This is required by a number of roles in the collection. Optional Environment Variable: VERIFY_INGRESS Default Value: True","title":"verify_ingress"},{"location":"roles/ocp_verify/#cluster_name","text":"Specify the name of the cluster, in some cluster setups this name is required to determine the name of the default router certificate. Optional, only used when verify_ingress is enabled Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/ocp_verify/#ocp_ingress_tls_secret_name","text":"Specify the name of the cluster's ingres tls secret which contains the default router certificate. Optional, only used when verify_ingress is enabled Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default","title":"ocp_ingress_tls_secret_name"},{"location":"roles/ocp_verify/#example-playbook","text":"- hosts: localhost vars: verify_cluster: True verify_catalogsources: True verify_subscriptions: True verify_workloads: True verify_ingress: True roles: - ibm.mas_devops.ocp_verify","title":"Example Playbook"},{"location":"roles/ocp_verify/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocs/","text":"ocs \u00a4 This role provides support to install/update OpenShift Data Foundation Operator (ODF) (formerly called Openshift Container Storage (OCS)) and the Local Storage Operator (LSO). This role is not used by default when setting up IBM Cloud ROKS clusters because they are automatically provisioned with their own storage plugin already. The OCP Version is autodetected and if it is found to be at version 4.10 or less then the ocs operator is used, else the odf operator is used. Unfortunately, starting from OCP 4.8 IBM/Red Hat have decided to stop supporting OCS on IBMCloud ROKS. So this role is of limited value to users of ROKS going forward. If you attempt to install OpenShift Container Storage on ROKS via a Subscription channel you will be met by the following error as the admission webhook has been coded to prevent use of the OCS operator on IBM Cloud ROKS: Failed to apply object: \"admission webhook \"validate.managed.openshift.io\" denied the request: Installing OpenShift Data Foundation on IBM Cloud by using OperatorHub is not supported. You can install OpenShift Data Foundation by using the IBM Cloud add-on. For more information, see https://cloud.ibm.com/docs/openshift?topic=openshift-ocs-storage-prep. Role Variables \u00a4 lso_device_path \u00a4 Set this value with your actual local disk filepath to be used in the LocalVolume for block storage using the localblock storageclass. Environment Variable: LSO_DEVICE_PATH Default Value: /dev/vdb ocs_action \u00a4 Set this value to the required action for install or upgrade . If set to install, the local storage and OCF/ODF storage operators will be installed with their storage cluster. If set to upgrade, the operators and storage cluster will be updated based on the ocp version in the cluster. Environment Variable: OCS_ACTION Default Value: install Example Playbook \u00a4 --- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.ocs License \u00a4 EPL-2.0","title":"ocs"},{"location":"roles/ocs/#ocs","text":"This role provides support to install/update OpenShift Data Foundation Operator (ODF) (formerly called Openshift Container Storage (OCS)) and the Local Storage Operator (LSO). This role is not used by default when setting up IBM Cloud ROKS clusters because they are automatically provisioned with their own storage plugin already. The OCP Version is autodetected and if it is found to be at version 4.10 or less then the ocs operator is used, else the odf operator is used. Unfortunately, starting from OCP 4.8 IBM/Red Hat have decided to stop supporting OCS on IBMCloud ROKS. So this role is of limited value to users of ROKS going forward. If you attempt to install OpenShift Container Storage on ROKS via a Subscription channel you will be met by the following error as the admission webhook has been coded to prevent use of the OCS operator on IBM Cloud ROKS: Failed to apply object: \"admission webhook \"validate.managed.openshift.io\" denied the request: Installing OpenShift Data Foundation on IBM Cloud by using OperatorHub is not supported. You can install OpenShift Data Foundation by using the IBM Cloud add-on. For more information, see https://cloud.ibm.com/docs/openshift?topic=openshift-ocs-storage-prep.","title":"ocs"},{"location":"roles/ocs/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocs/#lso_device_path","text":"Set this value with your actual local disk filepath to be used in the LocalVolume for block storage using the localblock storageclass. Environment Variable: LSO_DEVICE_PATH Default Value: /dev/vdb","title":"lso_device_path"},{"location":"roles/ocs/#ocs_action","text":"Set this value to the required action for install or upgrade . If set to install, the local storage and OCF/ODF storage operators will be installed with their storage cluster. If set to upgrade, the operators and storage cluster will be updated based on the ocp version in the cluster. Environment Variable: OCS_ACTION Default Value: install","title":"ocs_action"},{"location":"roles/ocs/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.ocs","title":"Example Playbook"},{"location":"roles/ocs/#license","text":"EPL-2.0","title":"License"},{"location":"roles/odh/","text":"odh \u00a4 ===== This role provides support to deploy odh components for AI Broker Application: Install Red Hat OpenShift Serverless Operator Install Red Hat OpenShift Service Mesh Operator Install Authorino Operator Install Open Data Hub Operator Create DSCInitialization instance Create Data Science Cluster Create Create Data Science Pipelines Application Role Variables \u00a4 tenantName \u00a4 The tenant name for odh role. Environment Variable: MAS_AIBROKER_TENANT_NAME Default Value: user serverless_catalog_source \u00a4 The serverless catalog source for odh role. Environment Variable: SERVERLESS_CATALOG_SOURCE Default Value: redhat-operators serverless_channel \u00a4 The serverless_channel for odh role. Environment Variable: SERVERLESS_CHANNEL Default Value: stable service_mesh_channel \u00a4 The service mesh channel for odh role. Environment Variable: SERVICEMESH_CHANNEL Default Value: stable service_mesh_catalog_source \u00a4 The service mesh catalog source for odh role. Environment Variable: SERVICEMESH_CATALOG_SOURCE Default Value: redhat-operators authorino_catalog_source \u00a4 The authorino catalog source for odh role. Environment Variable: AUTHORINO_CATALOG_SOURCE Default Value: community-operators odh_channel \u00a4 The odh channel for odh role. Environment Variable: ODH_CHANNEL Default Value: fast odh_catalog_source \u00a4 The odh catalog source for odh role. Environment Variable: ODH_CATALOG_SOURCE Default Value: community-operators odh_operator_version \u00a4 The odh operator version for odh role. Environment Variable: ODH_OPERATOR_VERSION Default Value: opendatahub-operator.v2.11.1 License \u00a4 EPL-2.0","title":"odh"},{"location":"roles/odh/#odh","text":"===== This role provides support to deploy odh components for AI Broker Application: Install Red Hat OpenShift Serverless Operator Install Red Hat OpenShift Service Mesh Operator Install Authorino Operator Install Open Data Hub Operator Create DSCInitialization instance Create Data Science Cluster Create Create Data Science Pipelines Application","title":"odh"},{"location":"roles/odh/#role-variables","text":"","title":"Role Variables"},{"location":"roles/odh/#tenantname","text":"The tenant name for odh role. Environment Variable: MAS_AIBROKER_TENANT_NAME Default Value: user","title":"tenantName"},{"location":"roles/odh/#serverless_catalog_source","text":"The serverless catalog source for odh role. Environment Variable: SERVERLESS_CATALOG_SOURCE Default Value: redhat-operators","title":"serverless_catalog_source"},{"location":"roles/odh/#serverless_channel","text":"The serverless_channel for odh role. Environment Variable: SERVERLESS_CHANNEL Default Value: stable","title":"serverless_channel"},{"location":"roles/odh/#service_mesh_channel","text":"The service mesh channel for odh role. Environment Variable: SERVICEMESH_CHANNEL Default Value: stable","title":"service_mesh_channel"},{"location":"roles/odh/#service_mesh_catalog_source","text":"The service mesh catalog source for odh role. Environment Variable: SERVICEMESH_CATALOG_SOURCE Default Value: redhat-operators","title":"service_mesh_catalog_source"},{"location":"roles/odh/#authorino_catalog_source","text":"The authorino catalog source for odh role. Environment Variable: AUTHORINO_CATALOG_SOURCE Default Value: community-operators","title":"authorino_catalog_source"},{"location":"roles/odh/#odh_channel","text":"The odh channel for odh role. Environment Variable: ODH_CHANNEL Default Value: fast","title":"odh_channel"},{"location":"roles/odh/#odh_catalog_source","text":"The odh catalog source for odh role. Environment Variable: ODH_CATALOG_SOURCE Default Value: community-operators","title":"odh_catalog_source"},{"location":"roles/odh/#odh_operator_version","text":"The odh operator version for odh role. Environment Variable: ODH_OPERATOR_VERSION Default Value: opendatahub-operator.v2.11.1","title":"odh_operator_version"},{"location":"roles/odh/#license","text":"EPL-2.0","title":"License"},{"location":"roles/opentelemetry/","text":"opentelemetry \u00a4 Install and configure OpenTelemetry operator for IBM Maximo Application Suite (in openshift-operators namespace). Role Variables \u00a4 opentelemetry_action \u00a4 Inform the role whether to perform an install or an uninstall of Open Telemetry. Optional Environment Variable: OPENTELEMETRY_ACTION Default: install Example Playbook \u00a4 - hosts: localhost vars: opentelemetry_action: \"install\" roles: - ibm.mas_devops.opentelemetry License \u00a4 EPL-2.0","title":"opentelemetry"},{"location":"roles/opentelemetry/#opentelemetry","text":"Install and configure OpenTelemetry operator for IBM Maximo Application Suite (in openshift-operators namespace).","title":"opentelemetry"},{"location":"roles/opentelemetry/#role-variables","text":"","title":"Role Variables"},{"location":"roles/opentelemetry/#opentelemetry_action","text":"Inform the role whether to perform an install or an uninstall of Open Telemetry. Optional Environment Variable: OPENTELEMETRY_ACTION Default: install","title":"opentelemetry_action"},{"location":"roles/opentelemetry/#example-playbook","text":"- hosts: localhost vars: opentelemetry_action: \"install\" roles: - ibm.mas_devops.opentelemetry","title":"Example Playbook"},{"location":"roles/opentelemetry/#license","text":"EPL-2.0","title":"License"},{"location":"roles/registry/","text":"registry \u00a4 Create a Docker Registry running on RedHat OpenShift cluster. The registry will be backed by persistant storage, and accessible via either a clusterIP or loadbalancer service. This role can also be used to delete a docker registry on a cluster for a clean start. See usage below for more information. Usage \u00a4 If you set up the registry with a loadbalancer service you will be able to push to the registry via the cluster's hostname, but before you can use the registry you will need to install the registry's CA certificate and restart the Docker daemon so that your client trusts the new registry: CACERT=$(oc -n airgap-registry get secret airgap-registry-certificate -o jsonpath='{.data.ca\\.crt}' | base64 -d) DOMAIN=$(oc get ingress.config cluster -o jsonpath='{.spec.domain}') sudo mkdir -p /etc/docker/certs.d/$DOMAIN\\:32500/ sudo echo \"$CACERT\" > /etc/docker/certs.d/$DOMAIN\\:32500/ca.crt sudo service docker restart You can now use the registry as normal: DOMAIN=$(oc get ingress.config cluster -o jsonpath='{.spec.domain}') docker pull registry.access.redhat.com/ubi8/ubi-minimal docker tag registry.access.redhat.com/ubi8/ubi-minimal $DOMAIN:32500/ubi8/ubi-minimal docker push $DOMAIN:32500/ubi8/ubi-minimal If you set up the registry with a clusterip service you will only be able to push to the registry after using port forwarding: oc -n airgap-registry port-forward deployment/airgap-registry 9000:5000 docker pull registry.access.redhat.com/ubi8/ubi-minimal docker tag registry.access.redhat.com/ubi8/ubi-minimal localhost:9000/ubi8/ubi-minimal docker push localhost:9000/ubi8/ubi-minimal However, you will still need to set up Docker trust for the \"local\" registry: CACERT=$(oc -n airgap-registry get secret airgap-registry-certificate -o jsonpath='{.data.ca\\.crt}' | base64 -d) sudo mkdir -p /etc/docker/certs.d/$DOMAIN\\:32500/ sudo mkdir /etc/docker/certs.d/localhost\\:9000 sudo echo \"$CACERT\" > /etc/docker/certs.d/localhost\\:9000/ca.crt sudo service docker restart Usage for tear-down action \u00a4 This role can also be used to permanently delete a mirror registry from a given cluster by setting the registry_action to tear-down and specifying the corresponding registry_namespace , if not using the default value. Note that the tear-down action deletes the registry completely including the PVC storage and the registry namespace. To start up the registry again, the role needs to be run again with the registry_action on default or setup . Images previously stored in the registry before the tear-down will no longer be available and will need to be mirrored again once the registry setup has completed. Take precaution when using this function and expect that images can no longer be accessed from the registry that has been torn down. Note: Recreating the registry will also create a new ca cert for the new registry. An appropriate time to use this tear-down function is when the registry has too many images that are not being used or when there has been a shift to support newer versions but images of older versions are clogging the registry. The tear-down function frees the disk space and allows for a new registry to be setup. Role Variables \u00a4 registry_action \u00a4 The action to perform with this role. Can be set to tear-down to remove an existing registry and its namespace. Default is setup Optional Environment Variable: REGISTRY_ACTION Default Value: setup registry_namespace \u00a4 The namespace where the registry to run Optional Environment Variable: REGISTRY_NAMESPACE Default Value: airgap-registry registry_storage_class \u00a4 Required : The name of the storage class to configure the MongoDb operator to use for persistent storage in the MongoDb cluster. Storage class must support ReadWriteOnce(RWO) access mode. Required , unless running in IBM Cloud ROKS, where the storage class will default to ibmc-block-gold . Environment Variable: REGISTRY_STORAGE_CLASS Default Value: None registry_storage_capacity \u00a4 The size of the PVC that will be created for data storage in the cluster. Optional Environment Variable: REGISTRY_STORAGE_CAPACITY Default Value: 100Gi registry_service_type \u00a4 The type of service to set up in front of the registry, either loadbalancer or clusterip . Using loadbalancer will allow you to access the registry from outside of your cluster via the cluster domain on port 32500 . If you have other loadbalancers on the cluster that already claim port 32500 this role can not be usedbecause currently the loadbalancer port can not be customised. Optional Environment Variable: REGISTRY_SERVICE_TYPE Default Value: loadbalancer Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: registry_storage_class: ibmc-block-gold registry_storage_capacity: 500Gb registry_service_type: loadbalancer roles: - ibm.mas_devops.registry License \u00a4 EPL-2.0","title":"registry"},{"location":"roles/registry/#registry","text":"Create a Docker Registry running on RedHat OpenShift cluster. The registry will be backed by persistant storage, and accessible via either a clusterIP or loadbalancer service. This role can also be used to delete a docker registry on a cluster for a clean start. See usage below for more information.","title":"registry"},{"location":"roles/registry/#usage","text":"If you set up the registry with a loadbalancer service you will be able to push to the registry via the cluster's hostname, but before you can use the registry you will need to install the registry's CA certificate and restart the Docker daemon so that your client trusts the new registry: CACERT=$(oc -n airgap-registry get secret airgap-registry-certificate -o jsonpath='{.data.ca\\.crt}' | base64 -d) DOMAIN=$(oc get ingress.config cluster -o jsonpath='{.spec.domain}') sudo mkdir -p /etc/docker/certs.d/$DOMAIN\\:32500/ sudo echo \"$CACERT\" > /etc/docker/certs.d/$DOMAIN\\:32500/ca.crt sudo service docker restart You can now use the registry as normal: DOMAIN=$(oc get ingress.config cluster -o jsonpath='{.spec.domain}') docker pull registry.access.redhat.com/ubi8/ubi-minimal docker tag registry.access.redhat.com/ubi8/ubi-minimal $DOMAIN:32500/ubi8/ubi-minimal docker push $DOMAIN:32500/ubi8/ubi-minimal If you set up the registry with a clusterip service you will only be able to push to the registry after using port forwarding: oc -n airgap-registry port-forward deployment/airgap-registry 9000:5000 docker pull registry.access.redhat.com/ubi8/ubi-minimal docker tag registry.access.redhat.com/ubi8/ubi-minimal localhost:9000/ubi8/ubi-minimal docker push localhost:9000/ubi8/ubi-minimal However, you will still need to set up Docker trust for the \"local\" registry: CACERT=$(oc -n airgap-registry get secret airgap-registry-certificate -o jsonpath='{.data.ca\\.crt}' | base64 -d) sudo mkdir -p /etc/docker/certs.d/$DOMAIN\\:32500/ sudo mkdir /etc/docker/certs.d/localhost\\:9000 sudo echo \"$CACERT\" > /etc/docker/certs.d/localhost\\:9000/ca.crt sudo service docker restart","title":"Usage"},{"location":"roles/registry/#usage-for-tear-down-action","text":"This role can also be used to permanently delete a mirror registry from a given cluster by setting the registry_action to tear-down and specifying the corresponding registry_namespace , if not using the default value. Note that the tear-down action deletes the registry completely including the PVC storage and the registry namespace. To start up the registry again, the role needs to be run again with the registry_action on default or setup . Images previously stored in the registry before the tear-down will no longer be available and will need to be mirrored again once the registry setup has completed. Take precaution when using this function and expect that images can no longer be accessed from the registry that has been torn down. Note: Recreating the registry will also create a new ca cert for the new registry. An appropriate time to use this tear-down function is when the registry has too many images that are not being used or when there has been a shift to support newer versions but images of older versions are clogging the registry. The tear-down function frees the disk space and allows for a new registry to be setup.","title":"Usage for tear-down action"},{"location":"roles/registry/#role-variables","text":"","title":"Role Variables"},{"location":"roles/registry/#registry_action","text":"The action to perform with this role. Can be set to tear-down to remove an existing registry and its namespace. Default is setup Optional Environment Variable: REGISTRY_ACTION Default Value: setup","title":"registry_action"},{"location":"roles/registry/#registry_namespace","text":"The namespace where the registry to run Optional Environment Variable: REGISTRY_NAMESPACE Default Value: airgap-registry","title":"registry_namespace"},{"location":"roles/registry/#registry_storage_class","text":"Required : The name of the storage class to configure the MongoDb operator to use for persistent storage in the MongoDb cluster. Storage class must support ReadWriteOnce(RWO) access mode. Required , unless running in IBM Cloud ROKS, where the storage class will default to ibmc-block-gold . Environment Variable: REGISTRY_STORAGE_CLASS Default Value: None","title":"registry_storage_class"},{"location":"roles/registry/#registry_storage_capacity","text":"The size of the PVC that will be created for data storage in the cluster. Optional Environment Variable: REGISTRY_STORAGE_CAPACITY Default Value: 100Gi","title":"registry_storage_capacity"},{"location":"roles/registry/#registry_service_type","text":"The type of service to set up in front of the registry, either loadbalancer or clusterip . Using loadbalancer will allow you to access the registry from outside of your cluster via the cluster domain on port 32500 . If you have other loadbalancers on the cluster that already claim port 32500 this role can not be usedbecause currently the loadbalancer port can not be customised. Optional Environment Variable: REGISTRY_SERVICE_TYPE Default Value: loadbalancer","title":"registry_service_type"},{"location":"roles/registry/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: registry_storage_class: ibmc-block-gold registry_storage_capacity: 500Gb registry_service_type: loadbalancer roles: - ibm.mas_devops.registry","title":"Example Playbook"},{"location":"roles/registry/#license","text":"EPL-2.0","title":"License"},{"location":"roles/sls/","text":"sls \u00a4 Install IBM Suite License Service and generate a configuration that can be directly applied to IBM Maximo Application Suite. The role assumes that you have already installed the Certificate Manager in the target cluster. This action is performed by the cert_manager role if you want to use this collection to install the cert-manager operator. Role Variables \u00a4 sls_action \u00a4 Inform the role whether to perform an install or uninstall of the Suite License Service. Optional Environment Variable: SLS_ACTION Default: install Role Variables - Installation \u00a4 If sls_url is set then the role will skip the installation of an SLS instance and simply generate the SLSCfg resource for the SLS instance defined. artifactory_username \u00a4 Provide your artifactory username, primarily used to update the image pull secret in development. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None artifactory_token \u00a4 Provide your artifactory api key, primarily used to update the image pull secret in development. Optional Environment Variable: ARTIFACTORY_TOKEN Default: None sls_catalog_source \u00a4 Defines the OLM catalog to be used to install SLS. Set to ibm-sls-operators if you want to deploy pre-release development builds of SLS or leave as the default ibm-operator-catalog for the released versions. Optional Environment Variable: SLS_CATALOG_SOURCE Default: ibm-operator-catalog sls_channel \u00a4 The SLS OLM subscription channel to be installed. Optional Environment Variable: SLS_CHANNEL Default: 3.x sls_namespace \u00a4 Define the namespace where sls must be installed. Optional Environment Variable: SLS_NAMESPACE Default: ibm-sls sls_icr_cpopen \u00a4 The container registry source for all container images deployed by the SLS operator. From SLS 3.8.0 onwards this will be the only variable to set the registry. Override to use development images. Optional Environment Variable: SLS_ICR_CPOPEN Default: icr.io/cpopen sls_instance_name \u00a4 Defines the instance ID to be used for SLS installation. Optional Environment Variable: SLS_INSTANCE_NAME Default: sls sls_icr_cp [Deprecated in SLS 3.8.0] \u00a4 The container registry source for all container images deployed by the SLS operator. The api-licensing container image has moved to icr.io/cpopen in SLS 3.8.0. Set this variable for SLS 3.7.0 and lower. Override to use development images. Optional Environment Variable: SLS_ICR_CP Default: cp.icr.io/cp ibm_entitlement_key [Deprecated in SLS 3.8.0] \u00a4 Provide your IBM entitlement key . This is now deprecated in SLS 3.8.0. Provide this only for versions up to 3.7.0. Required unless sls_url is provided Environment Variable: IBM_ENTITLEMENT_KEY Default: None sls_entitlement_username [Deprecated in SLS 3.8.0] \u00a4 Username for entitled registry. This username will be used to create the image pull secret. This is now deprecated in SLS 3.8.0. Provide this only for versions up to 3.7.0. Optional Environment Variable: SLS_ENTITLEMENT_USERNAME Default: cp sls_entitlement_key [Deprecated in SLS 3.8.0] \u00a4 An IBM entitlement key specific for SLS installation, primarily used to override ibm_entitlement_key in development. This is now deprecated in SLS 3.8.0. Provide this only for versions up to 3.7.0. Optional Environment Variable: SLS_ENTITLEMENT_KEY Default: None Role Variables - Configuration \u00a4 sls_domain \u00a4 SLS can be configured to be externally accessible through a route by setting the domain. Set the domain if SLS is used by application suites that are installed in separate OpenShift clusters. Optional Environment Variable: SLS_DOMAIN Default: None sls_auth_enforce \u00a4 Determines whether authorization is enforced. If set to true, clients must use mTLS with certificates generated from the client registration flow for SLS API calls. Optional Environment Variable: SLS_AUTH_ENFORCE Default: True sls_mongo_retrywrites \u00a4 Set to true if MongoDB support retryable writes. In case if retryable writes is not supported (like in case of Amazon DocumentDB), set to false Optional Environment Variable: SLS_MONGO_RETRYWRITES Default: true sls_compliance_enforce \u00a4 Determines whether compliance is enforced. If there are not enough tokens to support the request. If compliance is not enforced, license checkout requests will be allowed even if there are not enough tokens to support the request. Optional Environment Variable: SLS_COMPLIANCE_ENFORCE Default: True sls_registration_open \u00a4 Determines whether registration is open. If set to true, clients will be able to register themselves with SLS and use SLS APIs. Optional Environment Variable: SLS_REGISTRATION_OPEN Default: True sls_mongodb_cfg_file \u00a4 Location of a MAS MongoCfg definition (as generated by the mongodb role). If provided the role will use the information in that config file to configure SLS. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: SLS_MONGODB_CFG_FILE Default: None sls_mongodb.hosts \u00a4 Defines list of host and port pair for MongoDb to be used with SLS. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: None Default: None sls_mongodb.certificates \u00a4 Defines list of Certificates for MongoDb to be used with SLS. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: None Default: None sls_mongodb.username \u00a4 Defines the MongoDB Username. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: None Default: None sls_mongodb.password \u00a4 Defines the MongoDb Password. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: None Default: None mas_pod_templates_dir \u00a4 Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-sls-licenseservice.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the LicenseService spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Role Variables - Bootstrap [SLS 3.7.0 and higher] \u00a4 entitlement_file \u00a4 Defines the License File to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on. Optional Environment Variable: SLS_ENTITLEMENT_FILE Default: None Role Variables - Bootstrap [Partly deprecated in SLS 3.7.0] \u00a4 bootstrap.license_file [Deprecated in SLS 3.7.0] \u00a4 Defines the License File to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on. Note: this variable used to be called bootstrap.entitlement_file and defaulted to {{mas_config_dir}}/entitlement.lic , this is no longer the case and SLS_LICENSE_FILE has to be set in order to bootstrap. This is now deprecated in SLS 3.7.0. Use this only for versions up to 3.6.0. Optional Environment Variable: SLS_LICENSE_FILE Default: None bootstrap.license_id [Deprecated in SLS 3.7.0] \u00a4 Defines the License Id to be used to bootstrap SLS. This must be set when bootstrap.license_file is also set and should match the licenseId from the license file. Don't set if you wish to setup entitlement later on. Note: this is now deprecated in SLS 3.7.0. Use this only for versions up to 3.6.0. Optional unless bootstrap.license_file is set Environment Variable: SLS_LICENSE_ID Default: None bootstrap.registration_key \u00a4 Defines the Registration Key to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on Optional Environment Variable: SLS_REGISTRATION_KEY Default: None Role Variables - SLSCfg \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the SlsCfg configuration will target. Optional, if this or mas_config_dir are not set then the role will not generate a SlsCfg template Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated SlsCfg resource definition. This can be used to manually configure a MAS instance to connect to SLS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a SlsCfg template. Optional, if this or mas_config_dir are not set then the role will not generate a SlsCfg template Environment Variable: MAS_CONFIG_DIR Default Value: None sls_url \u00a4 The URL of the LicenseService to be called when the Maximo Application Suite is registered with SLS. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_URL Default Value: None mas_license_sync_frequency \u00a4 The sync frequency of user license sync cronjob between Maximo Application Suite and SLS. Optional Environment Variable: MAS_LICENSE_SYNC_FREQUENCY Default Value: */30 * * * * sls_tls_crt \u00a4 The TLS CA certificate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. Takes precedence over sls_tls_crt_local_file_path . Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_TLS_CERT Default Value: None sls_tls_crt_local_file_path \u00a4 The path on the local system to a file containing the TLS CA certificate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. This variable is only used if sls_tls_crt has not been set. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_TLS_CERT_LOCAL_FILE_PATH Default Value: None sls_registration_key \u00a4 The Registration key of the LicenseService instance to be used when the Maximo Application Suite is registered with SLS. Optional Environment Variable: SLS_REGISTRATION_KEY Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None mas_pod_templates_dir \u00a4 Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-slscfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the SlsCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Example Playbook \u00a4 Install and generate a configuration [up to SLS 3.6.0] \u00a4 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxx mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" bootstrap: license_id: \"aa78dd65ef10\" license_file: \"/etc/mas/entitlement.lic\" registration_key: xxxx roles: - ibm.mas_devops.sls Install and upload license file [SLS 3.7.0] \u00a4 - hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxx mas_instance_id: inst1 sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" entitlement_file: \"/etc/mas/entitlement.lic\" roles: - ibm.mas_devops.sls Install and upload license file [from SLS 3.8.0] \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" entitlement_file: \"/etc/mas/entitlement.lic\" roles: - ibm.mas_devops.sls Generate a configuration for an existing installation \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_tls_crt_local_file_path: \"/home/me/sls.crt\" slscfg_url: \"https://xxx\" slscfg_registration_key: \"xxx\" roles: - ibm.mas_devops.sls License \u00a4 EPL-2.0","title":"sls"},{"location":"roles/sls/#sls","text":"Install IBM Suite License Service and generate a configuration that can be directly applied to IBM Maximo Application Suite. The role assumes that you have already installed the Certificate Manager in the target cluster. This action is performed by the cert_manager role if you want to use this collection to install the cert-manager operator.","title":"sls"},{"location":"roles/sls/#role-variables","text":"","title":"Role Variables"},{"location":"roles/sls/#sls_action","text":"Inform the role whether to perform an install or uninstall of the Suite License Service. Optional Environment Variable: SLS_ACTION Default: install","title":"sls_action"},{"location":"roles/sls/#role-variables-installation","text":"If sls_url is set then the role will skip the installation of an SLS instance and simply generate the SLSCfg resource for the SLS instance defined.","title":"Role Variables - Installation"},{"location":"roles/sls/#artifactory_username","text":"Provide your artifactory username, primarily used to update the image pull secret in development. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None","title":"artifactory_username"},{"location":"roles/sls/#artifactory_token","text":"Provide your artifactory api key, primarily used to update the image pull secret in development. Optional Environment Variable: ARTIFACTORY_TOKEN Default: None","title":"artifactory_token"},{"location":"roles/sls/#sls_catalog_source","text":"Defines the OLM catalog to be used to install SLS. Set to ibm-sls-operators if you want to deploy pre-release development builds of SLS or leave as the default ibm-operator-catalog for the released versions. Optional Environment Variable: SLS_CATALOG_SOURCE Default: ibm-operator-catalog","title":"sls_catalog_source"},{"location":"roles/sls/#sls_channel","text":"The SLS OLM subscription channel to be installed. Optional Environment Variable: SLS_CHANNEL Default: 3.x","title":"sls_channel"},{"location":"roles/sls/#sls_namespace","text":"Define the namespace where sls must be installed. Optional Environment Variable: SLS_NAMESPACE Default: ibm-sls","title":"sls_namespace"},{"location":"roles/sls/#sls_icr_cpopen","text":"The container registry source for all container images deployed by the SLS operator. From SLS 3.8.0 onwards this will be the only variable to set the registry. Override to use development images. Optional Environment Variable: SLS_ICR_CPOPEN Default: icr.io/cpopen","title":"sls_icr_cpopen"},{"location":"roles/sls/#sls_instance_name","text":"Defines the instance ID to be used for SLS installation. Optional Environment Variable: SLS_INSTANCE_NAME Default: sls","title":"sls_instance_name"},{"location":"roles/sls/#sls_icr_cp-deprecated-in-sls-380","text":"The container registry source for all container images deployed by the SLS operator. The api-licensing container image has moved to icr.io/cpopen in SLS 3.8.0. Set this variable for SLS 3.7.0 and lower. Override to use development images. Optional Environment Variable: SLS_ICR_CP Default: cp.icr.io/cp","title":"sls_icr_cp [Deprecated in SLS 3.8.0]"},{"location":"roles/sls/#ibm_entitlement_key-deprecated-in-sls-380","text":"Provide your IBM entitlement key . This is now deprecated in SLS 3.8.0. Provide this only for versions up to 3.7.0. Required unless sls_url is provided Environment Variable: IBM_ENTITLEMENT_KEY Default: None","title":"ibm_entitlement_key [Deprecated in SLS 3.8.0]"},{"location":"roles/sls/#sls_entitlement_username-deprecated-in-sls-380","text":"Username for entitled registry. This username will be used to create the image pull secret. This is now deprecated in SLS 3.8.0. Provide this only for versions up to 3.7.0. Optional Environment Variable: SLS_ENTITLEMENT_USERNAME Default: cp","title":"sls_entitlement_username [Deprecated in SLS 3.8.0]"},{"location":"roles/sls/#sls_entitlement_key-deprecated-in-sls-380","text":"An IBM entitlement key specific for SLS installation, primarily used to override ibm_entitlement_key in development. This is now deprecated in SLS 3.8.0. Provide this only for versions up to 3.7.0. Optional Environment Variable: SLS_ENTITLEMENT_KEY Default: None","title":"sls_entitlement_key [Deprecated in SLS 3.8.0]"},{"location":"roles/sls/#role-variables-configuration","text":"","title":"Role Variables - Configuration"},{"location":"roles/sls/#sls_domain","text":"SLS can be configured to be externally accessible through a route by setting the domain. Set the domain if SLS is used by application suites that are installed in separate OpenShift clusters. Optional Environment Variable: SLS_DOMAIN Default: None","title":"sls_domain"},{"location":"roles/sls/#sls_auth_enforce","text":"Determines whether authorization is enforced. If set to true, clients must use mTLS with certificates generated from the client registration flow for SLS API calls. Optional Environment Variable: SLS_AUTH_ENFORCE Default: True","title":"sls_auth_enforce"},{"location":"roles/sls/#sls_mongo_retrywrites","text":"Set to true if MongoDB support retryable writes. In case if retryable writes is not supported (like in case of Amazon DocumentDB), set to false Optional Environment Variable: SLS_MONGO_RETRYWRITES Default: true","title":"sls_mongo_retrywrites"},{"location":"roles/sls/#sls_compliance_enforce","text":"Determines whether compliance is enforced. If there are not enough tokens to support the request. If compliance is not enforced, license checkout requests will be allowed even if there are not enough tokens to support the request. Optional Environment Variable: SLS_COMPLIANCE_ENFORCE Default: True","title":"sls_compliance_enforce"},{"location":"roles/sls/#sls_registration_open","text":"Determines whether registration is open. If set to true, clients will be able to register themselves with SLS and use SLS APIs. Optional Environment Variable: SLS_REGISTRATION_OPEN Default: True","title":"sls_registration_open"},{"location":"roles/sls/#sls_mongodb_cfg_file","text":"Location of a MAS MongoCfg definition (as generated by the mongodb role). If provided the role will use the information in that config file to configure SLS. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: SLS_MONGODB_CFG_FILE Default: None","title":"sls_mongodb_cfg_file"},{"location":"roles/sls/#sls_mongodbhosts","text":"Defines list of host and port pair for MongoDb to be used with SLS. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: None Default: None","title":"sls_mongodb.hosts"},{"location":"roles/sls/#sls_mongodbcertificates","text":"Defines list of Certificates for MongoDb to be used with SLS. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: None Default: None","title":"sls_mongodb.certificates"},{"location":"roles/sls/#sls_mongodbusername","text":"Defines the MongoDB Username. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: None Default: None","title":"sls_mongodb.username"},{"location":"roles/sls/#sls_mongodbpassword","text":"Defines the MongoDb Password. Either sls_mongodb_cfg_file or the sls_mongodb object are required to install SLS. Environment Variable: None Default: None","title":"sls_mongodb.password"},{"location":"roles/sls/#mas_pod_templates_dir","text":"Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-sls-licenseservice.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the LicenseService spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None","title":"mas_pod_templates_dir"},{"location":"roles/sls/#role-variables-bootstrap-sls-370-and-higher","text":"","title":"Role Variables - Bootstrap [SLS 3.7.0 and higher]"},{"location":"roles/sls/#entitlement_file","text":"Defines the License File to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on. Optional Environment Variable: SLS_ENTITLEMENT_FILE Default: None","title":"entitlement_file"},{"location":"roles/sls/#role-variables-bootstrap-partly-deprecated-in-sls-370","text":"","title":"Role Variables - Bootstrap [Partly deprecated in SLS 3.7.0]"},{"location":"roles/sls/#bootstraplicense_file-deprecated-in-sls-370","text":"Defines the License File to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on. Note: this variable used to be called bootstrap.entitlement_file and defaulted to {{mas_config_dir}}/entitlement.lic , this is no longer the case and SLS_LICENSE_FILE has to be set in order to bootstrap. This is now deprecated in SLS 3.7.0. Use this only for versions up to 3.6.0. Optional Environment Variable: SLS_LICENSE_FILE Default: None","title":"bootstrap.license_file [Deprecated in SLS 3.7.0]"},{"location":"roles/sls/#bootstraplicense_id-deprecated-in-sls-370","text":"Defines the License Id to be used to bootstrap SLS. This must be set when bootstrap.license_file is also set and should match the licenseId from the license file. Don't set if you wish to setup entitlement later on. Note: this is now deprecated in SLS 3.7.0. Use this only for versions up to 3.6.0. Optional unless bootstrap.license_file is set Environment Variable: SLS_LICENSE_ID Default: None","title":"bootstrap.license_id [Deprecated in SLS 3.7.0]"},{"location":"roles/sls/#bootstrapregistration_key","text":"Defines the Registration Key to be used to bootstrap SLS. Don't set if you wish to setup entitlement later on Optional Environment Variable: SLS_REGISTRATION_KEY Default: None","title":"bootstrap.registration_key"},{"location":"roles/sls/#role-variables-slscfg","text":"","title":"Role Variables - SLSCfg"},{"location":"roles/sls/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the SlsCfg configuration will target. Optional, if this or mas_config_dir are not set then the role will not generate a SlsCfg template Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/sls/#mas_config_dir","text":"Local directory to save the generated SlsCfg resource definition. This can be used to manually configure a MAS instance to connect to SLS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a SlsCfg template. Optional, if this or mas_config_dir are not set then the role will not generate a SlsCfg template Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/sls/#sls_url","text":"The URL of the LicenseService to be called when the Maximo Application Suite is registered with SLS. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_URL Default Value: None","title":"sls_url"},{"location":"roles/sls/#mas_license_sync_frequency","text":"The sync frequency of user license sync cronjob between Maximo Application Suite and SLS. Optional Environment Variable: MAS_LICENSE_SYNC_FREQUENCY Default Value: */30 * * * *","title":"mas_license_sync_frequency"},{"location":"roles/sls/#sls_tls_crt","text":"The TLS CA certificate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. Takes precedence over sls_tls_crt_local_file_path . Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_TLS_CERT Default Value: None","title":"sls_tls_crt"},{"location":"roles/sls/#sls_tls_crt_local_file_path","text":"The path on the local system to a file containing the TLS CA certificate of the LicenseService to be used when the Maximo Application Suite is registered with SLS. This variable is only used if sls_tls_crt has not been set. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: SLS_TLS_CERT_LOCAL_FILE_PATH Default Value: None","title":"sls_tls_crt_local_file_path"},{"location":"roles/sls/#sls_registration_key","text":"The Registration key of the LicenseService instance to be used when the Maximo Application Suite is registered with SLS. Optional Environment Variable: SLS_REGISTRATION_KEY Default Value: None","title":"sls_registration_key"},{"location":"roles/sls/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/sls/#mas_pod_templates_dir_1","text":"Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-slscfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the SlsCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None","title":"mas_pod_templates_dir"},{"location":"roles/sls/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/sls/#install-and-generate-a-configuration-up-to-sls-360","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxx mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" bootstrap: license_id: \"aa78dd65ef10\" license_file: \"/etc/mas/entitlement.lic\" registration_key: xxxx roles: - ibm.mas_devops.sls","title":"Install and generate a configuration [up to SLS 3.6.0]"},{"location":"roles/sls/#install-and-upload-license-file-sls-370","text":"- hosts: localhost any_errors_fatal: true vars: ibm_entitlement_key: xxxx mas_instance_id: inst1 sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" entitlement_file: \"/etc/mas/entitlement.lic\" roles: - ibm.mas_devops.sls","title":"Install and upload license file [SLS 3.7.0]"},{"location":"roles/sls/#install-and-upload-license-file-from-sls-380","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" entitlement_file: \"/etc/mas/entitlement.lic\" roles: - ibm.mas_devops.sls","title":"Install and upload license file [from SLS 3.8.0]"},{"location":"roles/sls/#generate-a-configuration-for-an-existing-installation","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me/masconfig sls_tls_crt_local_file_path: \"/home/me/sls.crt\" slscfg_url: \"https://xxx\" slscfg_registration_key: \"xxx\" roles: - ibm.mas_devops.sls","title":"Generate a configuration for an existing installation"},{"location":"roles/sls/#license","text":"EPL-2.0","title":"License"},{"location":"roles/smtp/","text":"smtp \u00a4 Generate an SMTP configuration that can be directly applied to IBM Maximo Application Suite. The role supports the Twilio SendGrid email provider. Twilio SendGrid Prior to running this role, you must create an account with SendGrid. The SendGrid account needs to support creating subusers. Tip The role will generate a yaml file containing the definition of a Secret and SmtpCfg resource that can be used to configure the smtp email provider for MAS. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/smtp-$MAS_INSTANCE_ID.yml\" or used in conjunction with the suite_config role. This role will create a subuser that must be validated. An email with a validation link will be sent to the primary email address. You need to validate the subuser using this link. If validation fails, you can resend the email using the SendGrid admin UI. Role Variables \u00a4 smtp_type \u00a4 Required. Specify the smtp provider. Currently the supported value is sendgrid . Environment Variable: SMTP_TYPE Default: None mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the SmtpCfg configuration will target. Required. If this or mas_config_dir are not set then the role will not generate a SmtpCfg template Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated SmtpCfg resource definition. This can be used to manually configure a MAS instance to connect to smtp provider, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a SmtpCfg template. Required. if this or mas_config_dir are not set then the role will not generate a SmtpCfg template Environment Variable: MAS_CONFIG_DIR Default Value: None mas_pod_templates_dir \u00a4 Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-smtpcfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the SmtpCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None sendgrid_primary_username \u00a4 Required. Username of the existing SendGrid account. Environment Variable: SMTP_PRIMARY_USERNAME Default: None sendgrid_primary_email \u00a4 Required. Email of the existing SendGrid account. Environment Variable: SMTP_PRIMARY_EMAIL Default: None sendgrid_subuser_email \u00a4 Required. Email of the SendGrid subuser. This role creates a SendGrid subuser for sending emails subusers Environment Variable: SMTP_SUBUSER_EMAIL Default: None sendgrid_defaultrecipientemail \u00a4 Required. Default destination email address. Environment Variable: SENDGRID_DEFAULTRECIPIENTEMAIL Default: None sendgrid_primary_apikey \u00a4 Required. Apikey of the existing SendGrid account. Environment Variable: SENDGRID_PRIMARY_APIKEY Default: None sendgrid_ips \u00a4 Required. ips of the existing SendGrid account. The primary SendGrid account has one or more IP Addresses associated with it. Specify the list of SendGrid IP Addresses to associate with the subuser. Environment Variable: SENDGRID_IPS Default: None sendgrid_subuser_username \u00a4 Optional. Username of the SendGrid subuser. This role creates a SendGrid subuser for sending emails subusers Environment Variable: SMTP_SUBUSER_USERNAME Default: ibm-mas_$MAS_INSTANCE_ID sendgrid_defaultsendername \u00a4 Optional. Easily readable name displayed in emails sent by the subuser Environment Variable: SENDGRID_DEFAULTSENDERNAME Default: '' sendgrid_defaultshouldemailpasswords \u00a4 Optional. Flag to indicate if the password should be sent by email. Environment Variable: SENDGRID_DEFAULTSHOULDEMAILPASSWORDS Default: false sendgrid_configscope \u00a4 Optional Environment Variable: SENDGRID_CONFIGSCOPE Default: system sendgrid_hostname \u00a4 Optional Environment Variable: SENDGRID_HOSTNAME Default: smtp.sendgrid.net sendgrid_port \u00a4 Optional Environment Variable: SENDGRID_PORT Default: 465 sendgrid_security \u00a4 Optional Environment Variable: SENDGRID_SECURITY Default: SSL sendgrid_authentication \u00a4 Optional Environment Variable: SENDGRID_AUTHENTICATION Default: true sendgrid_api_url \u00a4 Optional. The api URL of the smtp email service. This URL is used for REST calls. Environment Variable: SENDGRID_API_URL Default: api.sendgrid.com custom_labels \u00a4 Optional. List of comma separated key=value pairs for setting custom labels on instance specific resources. Environment Variable: CUSTOM_LABELS Default: None sendgrid_debug \u00a4 Optional. When set to True, the results of the SendGrid REST calls will be displayed in the log. Environment Variable: SENDGRID_DEBUG Default: False Example Playbook \u00a4 Generate a configuration \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me/masconfig smtp_type: sendgrid sendgrid_primary_username: myusername sendgrid_primary_email: myemail@mydomain sendgrid_subuser_username: mysubusername sendgrid_subuser_email: mysubuser@mydomain sendgrid_defaultrecipientemail: myemail@mydomain sendgrid_defaultsendername: 'My Name' sendgrid_primary_apikey: xxxx sendgrid_ips: '[\"XXX.XXX.XXX.XXX\"]' roles: - ibm.mas_devops.smtp License \u00a4 EPL-2.0","title":"smtp"},{"location":"roles/smtp/#smtp","text":"Generate an SMTP configuration that can be directly applied to IBM Maximo Application Suite. The role supports the Twilio SendGrid email provider. Twilio SendGrid Prior to running this role, you must create an account with SendGrid. The SendGrid account needs to support creating subusers. Tip The role will generate a yaml file containing the definition of a Secret and SmtpCfg resource that can be used to configure the smtp email provider for MAS. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/smtp-$MAS_INSTANCE_ID.yml\" or used in conjunction with the suite_config role. This role will create a subuser that must be validated. An email with a validation link will be sent to the primary email address. You need to validate the subuser using this link. If validation fails, you can resend the email using the SendGrid admin UI.","title":"smtp"},{"location":"roles/smtp/#role-variables","text":"","title":"Role Variables"},{"location":"roles/smtp/#smtp_type","text":"Required. Specify the smtp provider. Currently the supported value is sendgrid . Environment Variable: SMTP_TYPE Default: None","title":"smtp_type"},{"location":"roles/smtp/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the SmtpCfg configuration will target. Required. If this or mas_config_dir are not set then the role will not generate a SmtpCfg template Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/smtp/#mas_config_dir","text":"Local directory to save the generated SmtpCfg resource definition. This can be used to manually configure a MAS instance to connect to smtp provider, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a SmtpCfg template. Required. if this or mas_config_dir are not set then the role will not generate a SmtpCfg template Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/smtp/#mas_pod_templates_dir","text":"Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-smtpcfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the SmtpCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None","title":"mas_pod_templates_dir"},{"location":"roles/smtp/#sendgrid_primary_username","text":"Required. Username of the existing SendGrid account. Environment Variable: SMTP_PRIMARY_USERNAME Default: None","title":"sendgrid_primary_username"},{"location":"roles/smtp/#sendgrid_primary_email","text":"Required. Email of the existing SendGrid account. Environment Variable: SMTP_PRIMARY_EMAIL Default: None","title":"sendgrid_primary_email"},{"location":"roles/smtp/#sendgrid_subuser_email","text":"Required. Email of the SendGrid subuser. This role creates a SendGrid subuser for sending emails subusers Environment Variable: SMTP_SUBUSER_EMAIL Default: None","title":"sendgrid_subuser_email"},{"location":"roles/smtp/#sendgrid_defaultrecipientemail","text":"Required. Default destination email address. Environment Variable: SENDGRID_DEFAULTRECIPIENTEMAIL Default: None","title":"sendgrid_defaultrecipientemail"},{"location":"roles/smtp/#sendgrid_primary_apikey","text":"Required. Apikey of the existing SendGrid account. Environment Variable: SENDGRID_PRIMARY_APIKEY Default: None","title":"sendgrid_primary_apikey"},{"location":"roles/smtp/#sendgrid_ips","text":"Required. ips of the existing SendGrid account. The primary SendGrid account has one or more IP Addresses associated with it. Specify the list of SendGrid IP Addresses to associate with the subuser. Environment Variable: SENDGRID_IPS Default: None","title":"sendgrid_ips"},{"location":"roles/smtp/#sendgrid_subuser_username","text":"Optional. Username of the SendGrid subuser. This role creates a SendGrid subuser for sending emails subusers Environment Variable: SMTP_SUBUSER_USERNAME Default: ibm-mas_$MAS_INSTANCE_ID","title":"sendgrid_subuser_username"},{"location":"roles/smtp/#sendgrid_defaultsendername","text":"Optional. Easily readable name displayed in emails sent by the subuser Environment Variable: SENDGRID_DEFAULTSENDERNAME Default: ''","title":"sendgrid_defaultsendername"},{"location":"roles/smtp/#sendgrid_defaultshouldemailpasswords","text":"Optional. Flag to indicate if the password should be sent by email. Environment Variable: SENDGRID_DEFAULTSHOULDEMAILPASSWORDS Default: false","title":"sendgrid_defaultshouldemailpasswords"},{"location":"roles/smtp/#sendgrid_configscope","text":"Optional Environment Variable: SENDGRID_CONFIGSCOPE Default: system","title":"sendgrid_configscope"},{"location":"roles/smtp/#sendgrid_hostname","text":"Optional Environment Variable: SENDGRID_HOSTNAME Default: smtp.sendgrid.net","title":"sendgrid_hostname"},{"location":"roles/smtp/#sendgrid_port","text":"Optional Environment Variable: SENDGRID_PORT Default: 465","title":"sendgrid_port"},{"location":"roles/smtp/#sendgrid_security","text":"Optional Environment Variable: SENDGRID_SECURITY Default: SSL","title":"sendgrid_security"},{"location":"roles/smtp/#sendgrid_authentication","text":"Optional Environment Variable: SENDGRID_AUTHENTICATION Default: true","title":"sendgrid_authentication"},{"location":"roles/smtp/#sendgrid_api_url","text":"Optional. The api URL of the smtp email service. This URL is used for REST calls. Environment Variable: SENDGRID_API_URL Default: api.sendgrid.com","title":"sendgrid_api_url"},{"location":"roles/smtp/#custom_labels","text":"Optional. List of comma separated key=value pairs for setting custom labels on instance specific resources. Environment Variable: CUSTOM_LABELS Default: None","title":"custom_labels"},{"location":"roles/smtp/#sendgrid_debug","text":"Optional. When set to True, the results of the SendGrid REST calls will be displayed in the log. Environment Variable: SENDGRID_DEBUG Default: False","title":"sendgrid_debug"},{"location":"roles/smtp/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/smtp/#generate-a-configuration","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me/masconfig smtp_type: sendgrid sendgrid_primary_username: myusername sendgrid_primary_email: myemail@mydomain sendgrid_subuser_username: mysubusername sendgrid_subuser_email: mysubuser@mydomain sendgrid_defaultrecipientemail: myemail@mydomain sendgrid_defaultsendername: 'My Name' sendgrid_primary_apikey: xxxx sendgrid_ips: '[\"XXX.XXX.XXX.XXX\"]' roles: - ibm.mas_devops.smtp","title":"Generate a configuration"},{"location":"roles/smtp/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_backup_restore/","text":"Backup and Restore MAS Applications \u00a4 Overview \u00a4 This role supports backing up and restoring the data for below MAS applications: manage : Manage namespace resources, persistent volume data (e.g. attachments) iot : IoT namespace resources monitor : Monitor namespace resources health : Health namespace resources, Watson Studio project asset optimizer : Optimizer namespace resources visualinspection : Visual Inspection namespace resources, persistent volume data (e.g. image datasets, models) Supports creating on-demand or scheduled backup jobs for taking full or incremental backups, and optionally creating Kubernetes jobs for running the backup/restore process. Important An application backup can only be restored to an instance with the same MAS instance ID. Role Variables - General \u00a4 masbr_action \u00a4 Set backup or restore to indicate the role to create a backup or restore job. Required Environment Variable: MAS_BR_ACTION Default: None mas_app_id \u00a4 Defines the MAS application ID ( manage , iot , monitor , health , optimizer , or visualinspection ) for the backup or restore action. Required Environment Variable: MAS_APP_ID Default: None mas_instance_id \u00a4 Defines the MAS instance ID for the backup or restore action. Required Environment Variable: MAS_INSTANCE_ID Default: None mas_workspace_id \u00a4 Defines the MAS workspace ID for the backup or restore action. Required Environment Variable: MAS_WORKSPACE_ID Default: None masbr_confirm_cluster \u00a4 Set true or false to indicate the role whether to confirm the currently connected cluster before running the backup or restore job. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false masbr_copy_timeout_sec \u00a4 Set the transfer files timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) masbr_job_timezone \u00a4 Set the time zone for creating scheduled backup job. If not set a value for this variable, this role will use UTC time zone when creating a CronJob for running scheduled backup job. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None masbr_storage_type \u00a4 Set local or cloud to indicate this role to save the backup files to local file system or cloud object storage. Required Environment Variable: MASBR_STORAGE_TYPE Default: None masbr_storage_local_folder \u00a4 Set local path to save the backup files. Required only when MASBR_STORAGE_TYPE=local Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None masbr_storage_cloud_rclone_file \u00a4 Set the path of rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_FILE Default: None masbr_storage_cloud_rclone_name \u00a4 Set the configuration name defined in rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_NAME Default: None masbr_storage_cloud_bucket \u00a4 Set the object storage bucket name for saving the backup files Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_BUCKET Default: None masbr_slack_enabled \u00a4 Set true or false to indicate whether this role will send Slack notification messages of the backup and restore progress. Optional Environment Variable: MASBR_SLACK_ENABLED Default: false masbr_slack_level \u00a4 Set failure , info or verbose to indicate this role to send Slack notification messages in which backup and resore phases: Slack level Backup/Restore phases failure Failed , PartiallyFailed info Completed , Failed , PartiallyFailed verbose InProgress , Completed , Failed , PartiallyFailed Optional Environment Variable: MASBR_SLACK_LEVEL Default: info masbr_slack_token \u00a4 The Slack integration token. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_TOKEN Default: None masbr_slack_channel \u00a4 The Slack channel to send the notification messages to. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_CHANNEL Default: None masbr_slack_user \u00a4 The sender of the Slack notification message. Optional Environment Variable: MASBR_SLACK_USER Default: MASBR Role Variables - Backup \u00a4 masbr_backup_type \u00a4 Set full or incr to indicate the role to create a full backup or incremental backup. Only supports creating incremental backup for persistent volume data, this role will always create a full backup for other type of data regardless of whether this variable be set to incr . Optional Environment Variable: MASBR_BACKUP_TYPE Default: full masbr_backup_data \u00a4 Set the types of data to be backed up, multiple data types are separated by commas (e.g. namespace,pv ). If not set a value for this variable, this role will back up all types of data that supported by the specified MAS application. The data types supported by each MAS applications: MAS App Name MAS App ID Data types Manage manage namespace , pv IoT iot namespace Monitor monitor namespace Health health namespace , wsl Optimizer optimizer namespace Visual Inspection visualinspection namespace , pv Optional Environment Variable: MASBR_BACKUP_DATA Default: None masbr_backup_from_version \u00a4 Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). This variable is only valid when MASBR_BACKUP_TYPE=incr . If not set a value for this variable, this role will try to find the latest full backup version from the specified storage location. Optional Environment Variable: MASBR_BACKUP_FROM_VERSION Default: None masbr_backup_schedule \u00a4 Set Cron expression to create a scheduled backup. If not set a value for this varialbe, this role will create an on-demand backup. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None Role Variables - Restore \u00a4 masbr_restore_from_version \u00a4 Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) Required only when MAS_BR_ACTION=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None masbr_restore_data \u00a4 Set the types of data to be restored, multiple data types are separated by commas (e.g. namespace,pv ). If not set a value for this variable, this role will restore all types of data that supported by the specified MAS application. The data types supported by each MAS applications: MAS App Name MAS App ID Data types Manage manage namespace , pv IoT iot namespace Monitor monitor namespace Health health namespace , wsl Optimizer optimizer namespace Visual Inspection visualinspection namespace , pv Optional Environment Variable: MASBR_RESTORE_DATA Default: None Role Variables - Manage \u00a4 masbr_manage_pvc_paths \u00a4 Set the Manage PVC paths to use in backup and restore. The PVC path is in the format of <pvcName>:<mountPath>/<subPath> . Multiple PVC paths are separated by commas (e.g. manage-doclinks1-pvc:/mnt/doclinks1/attachments,manage-doclinks2-pvc:/mnt/doclinks2 ). The <pvcName> and <mountPath> are defined in the ManageWorkspace CRD instance spec.settings.deployment.persistentVolumes : persistentVolumes: - accessModes: - ReadWriteMany mountPath: /mnt/doclinks1 pvcName: manage-doclinks1-pvc size: '20' storageClassName: ocs-storagecluster-cephfs volumeName: '' - accessModes: - ReadWriteMany mountPath: /mnt/doclinks2 pvcName: manage-doclinks2-pvc size: '20' storageClassName: ocs-storagecluster-cephfs volumeName: '' If not set a value for this variable, this role will not backup and restore persistent valumne data for Manage. Optional Environment Variable: MASBR_MANAGE_PVC_PATHS Default: None Example Playbook \u00a4 Backup \u00a4 Backup Manage attachments, note that this does not include backup of any data in Db2, see the backup action in the db2 role. - hosts: localhost any_errors_fatal: true vars: masbr_action: backup mas_instance_id: main mas_workspace_id: ws1 mas_app_id: manage masbr_backup_data: pv masbr_manage_pvc_paths: \"manage-doclinks1-pvc:/mnt/doclinks1\" masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_app_backup_restore Restore \u00a4 Restore Manage attachments, note that this does not include restore of any data in Db2, see the restore action in the db2 role. - hosts: localhost any_errors_fatal: true vars: masbr_action: restore masbr_restore_from_version: 20240621021316 mas_instance_id: main mas_workspace_id: ws1 mas_app_id: manage masbr_backup_data: pv masbr_manage_pvc_paths: \"manage-doclinks1-pvc:/mnt/doclinks1\" masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_app_backup_restore License \u00a4 EPL-2.0","title":"suite_app_backup_restore"},{"location":"roles/suite_app_backup_restore/#backup-and-restore-mas-applications","text":"","title":"Backup and Restore MAS Applications"},{"location":"roles/suite_app_backup_restore/#overview","text":"This role supports backing up and restoring the data for below MAS applications: manage : Manage namespace resources, persistent volume data (e.g. attachments) iot : IoT namespace resources monitor : Monitor namespace resources health : Health namespace resources, Watson Studio project asset optimizer : Optimizer namespace resources visualinspection : Visual Inspection namespace resources, persistent volume data (e.g. image datasets, models) Supports creating on-demand or scheduled backup jobs for taking full or incremental backups, and optionally creating Kubernetes jobs for running the backup/restore process. Important An application backup can only be restored to an instance with the same MAS instance ID.","title":"Overview"},{"location":"roles/suite_app_backup_restore/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/suite_app_backup_restore/#masbr_action","text":"Set backup or restore to indicate the role to create a backup or restore job. Required Environment Variable: MAS_BR_ACTION Default: None","title":"masbr_action"},{"location":"roles/suite_app_backup_restore/#mas_app_id","text":"Defines the MAS application ID ( manage , iot , monitor , health , optimizer , or visualinspection ) for the backup or restore action. Required Environment Variable: MAS_APP_ID Default: None","title":"mas_app_id"},{"location":"roles/suite_app_backup_restore/#mas_instance_id","text":"Defines the MAS instance ID for the backup or restore action. Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_app_backup_restore/#mas_workspace_id","text":"Defines the MAS workspace ID for the backup or restore action. Required Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/suite_app_backup_restore/#masbr_confirm_cluster","text":"Set true or false to indicate the role whether to confirm the currently connected cluster before running the backup or restore job. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false","title":"masbr_confirm_cluster"},{"location":"roles/suite_app_backup_restore/#masbr_copy_timeout_sec","text":"Set the transfer files timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours)","title":"masbr_copy_timeout_sec"},{"location":"roles/suite_app_backup_restore/#masbr_job_timezone","text":"Set the time zone for creating scheduled backup job. If not set a value for this variable, this role will use UTC time zone when creating a CronJob for running scheduled backup job. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None","title":"masbr_job_timezone"},{"location":"roles/suite_app_backup_restore/#masbr_storage_type","text":"Set local or cloud to indicate this role to save the backup files to local file system or cloud object storage. Required Environment Variable: MASBR_STORAGE_TYPE Default: None","title":"masbr_storage_type"},{"location":"roles/suite_app_backup_restore/#masbr_storage_local_folder","text":"Set local path to save the backup files. Required only when MASBR_STORAGE_TYPE=local Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None","title":"masbr_storage_local_folder"},{"location":"roles/suite_app_backup_restore/#masbr_storage_cloud_rclone_file","text":"Set the path of rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_FILE Default: None","title":"masbr_storage_cloud_rclone_file"},{"location":"roles/suite_app_backup_restore/#masbr_storage_cloud_rclone_name","text":"Set the configuration name defined in rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_NAME Default: None","title":"masbr_storage_cloud_rclone_name"},{"location":"roles/suite_app_backup_restore/#masbr_storage_cloud_bucket","text":"Set the object storage bucket name for saving the backup files Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_BUCKET Default: None","title":"masbr_storage_cloud_bucket"},{"location":"roles/suite_app_backup_restore/#masbr_slack_enabled","text":"Set true or false to indicate whether this role will send Slack notification messages of the backup and restore progress. Optional Environment Variable: MASBR_SLACK_ENABLED Default: false","title":"masbr_slack_enabled"},{"location":"roles/suite_app_backup_restore/#masbr_slack_level","text":"Set failure , info or verbose to indicate this role to send Slack notification messages in which backup and resore phases: Slack level Backup/Restore phases failure Failed , PartiallyFailed info Completed , Failed , PartiallyFailed verbose InProgress , Completed , Failed , PartiallyFailed Optional Environment Variable: MASBR_SLACK_LEVEL Default: info","title":"masbr_slack_level"},{"location":"roles/suite_app_backup_restore/#masbr_slack_token","text":"The Slack integration token. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_TOKEN Default: None","title":"masbr_slack_token"},{"location":"roles/suite_app_backup_restore/#masbr_slack_channel","text":"The Slack channel to send the notification messages to. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_CHANNEL Default: None","title":"masbr_slack_channel"},{"location":"roles/suite_app_backup_restore/#masbr_slack_user","text":"The sender of the Slack notification message. Optional Environment Variable: MASBR_SLACK_USER Default: MASBR","title":"masbr_slack_user"},{"location":"roles/suite_app_backup_restore/#role-variables-backup","text":"","title":"Role Variables - Backup"},{"location":"roles/suite_app_backup_restore/#masbr_backup_type","text":"Set full or incr to indicate the role to create a full backup or incremental backup. Only supports creating incremental backup for persistent volume data, this role will always create a full backup for other type of data regardless of whether this variable be set to incr . Optional Environment Variable: MASBR_BACKUP_TYPE Default: full","title":"masbr_backup_type"},{"location":"roles/suite_app_backup_restore/#masbr_backup_data","text":"Set the types of data to be backed up, multiple data types are separated by commas (e.g. namespace,pv ). If not set a value for this variable, this role will back up all types of data that supported by the specified MAS application. The data types supported by each MAS applications: MAS App Name MAS App ID Data types Manage manage namespace , pv IoT iot namespace Monitor monitor namespace Health health namespace , wsl Optimizer optimizer namespace Visual Inspection visualinspection namespace , pv Optional Environment Variable: MASBR_BACKUP_DATA Default: None","title":"masbr_backup_data"},{"location":"roles/suite_app_backup_restore/#masbr_backup_from_version","text":"Set the full backup version to use in the incremental backup, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ). This variable is only valid when MASBR_BACKUP_TYPE=incr . If not set a value for this variable, this role will try to find the latest full backup version from the specified storage location. Optional Environment Variable: MASBR_BACKUP_FROM_VERSION Default: None","title":"masbr_backup_from_version"},{"location":"roles/suite_app_backup_restore/#masbr_backup_schedule","text":"Set Cron expression to create a scheduled backup. If not set a value for this varialbe, this role will create an on-demand backup. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None","title":"masbr_backup_schedule"},{"location":"roles/suite_app_backup_restore/#role-variables-restore","text":"","title":"Role Variables - Restore"},{"location":"roles/suite_app_backup_restore/#masbr_restore_from_version","text":"Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) Required only when MAS_BR_ACTION=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None","title":"masbr_restore_from_version"},{"location":"roles/suite_app_backup_restore/#masbr_restore_data","text":"Set the types of data to be restored, multiple data types are separated by commas (e.g. namespace,pv ). If not set a value for this variable, this role will restore all types of data that supported by the specified MAS application. The data types supported by each MAS applications: MAS App Name MAS App ID Data types Manage manage namespace , pv IoT iot namespace Monitor monitor namespace Health health namespace , wsl Optimizer optimizer namespace Visual Inspection visualinspection namespace , pv Optional Environment Variable: MASBR_RESTORE_DATA Default: None","title":"masbr_restore_data"},{"location":"roles/suite_app_backup_restore/#role-variables-manage","text":"","title":"Role Variables - Manage"},{"location":"roles/suite_app_backup_restore/#masbr_manage_pvc_paths","text":"Set the Manage PVC paths to use in backup and restore. The PVC path is in the format of <pvcName>:<mountPath>/<subPath> . Multiple PVC paths are separated by commas (e.g. manage-doclinks1-pvc:/mnt/doclinks1/attachments,manage-doclinks2-pvc:/mnt/doclinks2 ). The <pvcName> and <mountPath> are defined in the ManageWorkspace CRD instance spec.settings.deployment.persistentVolumes : persistentVolumes: - accessModes: - ReadWriteMany mountPath: /mnt/doclinks1 pvcName: manage-doclinks1-pvc size: '20' storageClassName: ocs-storagecluster-cephfs volumeName: '' - accessModes: - ReadWriteMany mountPath: /mnt/doclinks2 pvcName: manage-doclinks2-pvc size: '20' storageClassName: ocs-storagecluster-cephfs volumeName: '' If not set a value for this variable, this role will not backup and restore persistent valumne data for Manage. Optional Environment Variable: MASBR_MANAGE_PVC_PATHS Default: None","title":"masbr_manage_pvc_paths"},{"location":"roles/suite_app_backup_restore/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_app_backup_restore/#backup","text":"Backup Manage attachments, note that this does not include backup of any data in Db2, see the backup action in the db2 role. - hosts: localhost any_errors_fatal: true vars: masbr_action: backup mas_instance_id: main mas_workspace_id: ws1 mas_app_id: manage masbr_backup_data: pv masbr_manage_pvc_paths: \"manage-doclinks1-pvc:/mnt/doclinks1\" masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_app_backup_restore","title":"Backup"},{"location":"roles/suite_app_backup_restore/#restore","text":"Restore Manage attachments, note that this does not include restore of any data in Db2, see the restore action in the db2 role. - hosts: localhost any_errors_fatal: true vars: masbr_action: restore masbr_restore_from_version: 20240621021316 mas_instance_id: main mas_workspace_id: ws1 mas_app_id: manage masbr_backup_data: pv masbr_manage_pvc_paths: \"manage-doclinks1-pvc:/mnt/doclinks1\" masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_app_backup_restore","title":"Restore"},{"location":"roles/suite_app_backup_restore/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_config/","text":"suite_app_config \u00a4 This role is used to configure specific components of the application workspace after the application has been installed in the Maximo Application Suite. Role Variables - General \u00a4 mas_instance_id \u00a4 Defines the instance id that was used for the MAS installation Required Environment Variable: MAS_INSTANCE_ID Default: None mas_app_id \u00a4 Defines the application that is will be configured, valid settings are: assist , hputilities , iot , manage , monitor , optimizer , predict , and visualinspection . Required Environment Variable: MAS_APP_ID Default: None mas_workspace_id \u00a4 MAS application workspace to use to configure app components Required Environment Variable: MAS_WORKSPACE_ID Default: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default: None Role Variables - Workspace Configuration \u00a4 mas_appws_spec \u00a4 The application workspace deployment spec used to configure various aspects of the application workspace configuration. Note that use of this will override anything set in mas_appws_components Optional Environment Variable: MAS_APPWS_SPEC Default: defaults are specified in vars/defaultspecs/{mas_app_id}.yml mas_appws_bindings_jdbc \u00a4 Set the binding scope for the application workspace's JDBC binding ( system , application , workspace , or workspace-application ) Optional Environment Variable: MAS_APPWS_BINDINGS_JDBC Default: system mas_appws_components \u00a4 Defines the app components and versions to configure in the application workspace. Takes the form of key=value pairs seperated by a comma i.e. To install health within Manage set base=latest,health=latest Optional Environment Variable: MAS_APPWS_COMPONENTS Default: Application specific mas_pod_templates_dir \u00a4 This role will look for a configuration files named: ibm-mas-manage-manageworkspace.yml ibm-mas-manage-imagestitching.yml ibm-mas-manage-slackproxy.yml ibm-mas-manage-healthextworkspace.yml The content of the configuration file should be the yaml block that you wish to be inserted into the ManageWorkspace CR. ibm-mas-manage-manageworkspace.yml will be inserted into the ManageWorkspace CR spec -> podTemplates whereas the component ones e.g, ibm-mas-manage-imagestitching.yml will be under spec -> components -> civil -> podTemplates . The ibm-mas-manage-ws operator will then pass this on to the corresponding component CR when available. This is an example of one of the components (civil) - refer to the BestEfforts reference configuration in the MAS CLI . For full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Role Variables - Predict Configuration \u00a4 mas_appws_settings_deployment_size \u00a4 Controls the workload size of predict containers. Avaliable options are developer , small , medium and small | Deployment_size | Replica | | ---------------------- | :--: | | developer | 1 | | small | 2 | | medium | 3 | Optional, only supported when configuring Predict Environment Variable: MAS_APPWS_SETTINGS_DEPLOYMENT_SIZE Default: small Role Variables - Watson Studio Local \u00a4 These variables are only used when using this role to configure Predict , or Health & Predict Utilities . cpd_wsl_project_id \u00a4 The ID of the analytics project created in Watson Studio and used to configure hputilities application. Required unless cpd_wsl_project_name and mas_config_dir are set. Environment Variable: CPD_WSL_PROJECT_ID Default: None cpd_wsl_project_name \u00a4 Specifies the name of the file in mas_config_dir where the id of the analytics project is saved. Must be used in conjunction with mas_config_dir as an alternative to cpd_wsl_project_id . Optional Environment Variable: CPD_WSL_PROJECT_NAME Default Value: wsl-mas-${mas_instance_id}-hputilities mas_config_dir \u00a4 Local directory where generated resource definitions are saved into. Used in conjunction with cpd_wsl_project_name to retrieve the ID of a Watson Studio project previously created by the cp4d_service role. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None Role Variables - Watson Machine Learning \u00a4 These variables are only used when using this role to configure Predict . cpd_product_version \u00a4 The version of Cloud Pak for Data installed in the cluster, which is used to infer the version of Watson Machine Learning that must be passed into the Predict workspace configuration. Required Environment Variable: CPD_PRODUCT_VERSION Default: None cpd_wml_instance_id \u00a4 Identifier of wml instance to be configured in Predict. Optional Environment Variable: CPD_WML_INSTANCE_ID Default: openshift cpd_wml_url \u00a4 URL to access WML service (same as Cloud Pak for Data URL). Optional Environment Variable: CPD_WML_URL Default: https://internal-nginx-svc.ibm-cpd.svc:12443 (assumes CPD WML is installed the ibm-cpd namespace) Role Variables - Manage Workspace \u00a4 Manage - Health Integration variables \u00a4 mas_appws_bindings_health_wsl_flag \u00a4 Boolean value indicating if Watson Studio must be bound to Manage. It is expected a system level WatsonStudioCfg applied in the cluster. Optional Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL_FLAG Default: false mas_appws_bindings_health_wsl \u00a4 Set as system to indicate Watson Studio must be installed and bound to Health Optional Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL Default: None mas_app_settings_aio_flag \u00a4 Flag indicating if Asset Investment Optimization (AIO) resource must be loaded or not. It can be loaded only when Optimizer application is installed. Optional Only supported when Optimizer application is installed. Environment Variable: MAS_APP_SETTINGS_AIO_FLAG Default: true Manage - DB2 settings variables \u00a4 mas_app_settings_db_schema \u00a4 Name of the schema where Manage database lives in. Code also supports deprecated mas_app_settings_db2_schema variable name. Optional Environment Variable: MAS_APP_SETTINGS_DB_SCHEMA Default: maximo mas_app_settings_demodata \u00a4 Flag indicating if manage demodata should be loaded or not. Optional Environment Variable: MAS_APP_SETTINGS_DEMODATA Default: false (do not load demodata) mas_app_settings_db2vargraphic \u00a4 Optional. Flag indicating if VARGRAPHIC (if true) or VARCHAR (if false) is used. Details: https://www.ibm.com/docs/en/mas-cd/continuous-delivery?topic=deploy-language-support Default: true mas_app_settings_tablespace \u00a4 Name of the Manage database tablespace Optional Environment Variable: MAS_APP_SETTINGS_TABLESPACE Default: MAXDATA mas_app_settings_indexspace \u00a4 Name of the Manage database indexspace Optional Environment Variable: MAS_APP_SETTINGS_INDEXSPACE Default: MAXINDEX Manage - Persistent Volumes variables \u00a4 mas_app_settings_persistent_volumes_flag \u00a4 Flag indicating if persistent volumes should be configured by default during Manage Workspace activation. There are two defaulted File Storage Persistent Volumes Claim resources that will be created out of the box for Manage if this flag is set to true : /DOCLINKS : Persistent volume used to store doclinks/attachments. /bim : Persistent volume used to store Building Information Models related artifacts (models, docs and import). Optional Environment Variable: MAS_APP_SETTINGS_PERSISTENT_VOLUMES_FLAG Default: false JMS queues \u00a4 The following properties can be defined to customize the persistent volumes for the JMS queues setup for Manage. mas_app_settings_jms_queue_pvc_storage_class \u00a4 Provide the persistent volume storage class to be used for JMS queue configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) access modes are supported. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes. mas_app_settings_jms_queue_pvc_name \u00a4 Provide the persistent volume claim name to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_NAME Default: manage-jms mas_app_settings_jms_queue_pvc_size \u00a4 Provide the persistent volume claim size to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_SIZE Default: 20Gi mas_app_settings_jms_queue_mount_path \u00a4 Provide the persistent volume storage mount path to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_MOUNT_PATH Default: /jms mas_app_settings_jms_queue_pvc_accessmode \u00a4 Provide the persistent volume storage access-mode to be used for JMS queue configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_ACCESSMODE Default: ReadWriteMany mas_app_settings_default_jms \u00a4 Set this to true if you want to have JMS continuous queues configured Optional Environment Variable: MAS_APP_SETTINGS_DEFAULT_JMS Default: false Doclinks/Attachments \u00a4 The following properties can be defined to customize the persistent volumes for the Doclinks/Attachments setup for Manage. mas_app_settings_doclinks_pvc_storage_class \u00a4 Provide the persistent volume storage class to be used for doclinks/attachments configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes. mas_app_settings_doclinks_pvc_name \u00a4 Provide the persistent volume claim name to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_NAME Default: manage-doclinks mas_app_settings_doclinks_pvc_size \u00a4 Provide the persistent volume claim size to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_SIZE Default: 20Gi mas_app_settings_doclinks_mount_path \u00a4 Provide the persistent volume storage mount path to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_MOUNT_PATH Default: /DOCLINKS mas_app_settings_doclinks_pvc_accessmode \u00a4 Provide the persistent volume storage access-mode to be used for doclinks/attachments configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_ACCESSMODE Default: ReadWriteMany BIM (Building Information Models) \u00a4 The following properties can be defined to customize the persistent volumes for the Building Information Models setup for Manage. mas_app_settings_bim_pvc_storage_class \u00a4 Provide the persistent volume storage class to be used for Building Information Models configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. - Optional - Environment Variable: MAS_APP_SETTINGS_BIM_PVC_STORAGE_CLASS - Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes. mas_app_settings_bim_pvc_name \u00a4 Provide the persistent volume claim name to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_NAME Default: manage-bim mas_app_settings_bim_pvc_size \u00a4 Provide the persistent volume claim size to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_SIZE Default: 20Gi mas_app_settings_bim_mount_path \u00a4 Provide the persistent volume storage mount path to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_MOUNT_PATH Default: /bim mas_app_settings_bim_pvc_accessmode \u00a4 Provide the persistent volume storage access-mode to be used for Building Information Models configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_ACCESSMODE Default: ReadWriteMany Manage - Supported languages variables \u00a4 mas_app_settings_base_lang \u00a4 Provide the base language for Manage application. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. Optional Environment Variable: MAS_APP_SETTINGS_BASE_LANG Default: EN (English) mas_app_settings_secondary_langs \u00a4 Provide a list of additional secondary languages for Manage application. Note: The more languages you add, the longer Manage will take to install and activate. Export the MAS_APP_SETTINGS_SECONDARY_LANGS variable with the language codes as comma-separated values. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. Optional Environment Variable: MAS_APP_SETTINGS_SECONDARY_LANGS Default: None For example, use the following to enable Manage application with Arabic, Deutsch and Japanese as secondary languages: export MAS_APP_SETTINGS_SECONDARY_LANGS='AR,DE,JA' Manage - Server Bundle configuration variables \u00a4 mas_app_settings_server_bundles_size \u00a4 Optional. Provides different flavors of server bundle configuration to handle workload for Manage application. For more details about Manage application server bundle configuration, refer to Setting the server bundles for Manage application . Currently supported server bundle sizes are: dev - Deploys Manage with the default server bundle configuration. i.e just 1 bundle pod handling all Manage application workload. small - Deploys Manage with the most common deployment configuration. i.e 4 bundle pods, each one handling workload for each main capabilities: mea , cron , report and ui jms - Can be used for Manage 8.4 and above. Same server bundle configuration as small and includes jms bundle pod. Enabling JMS pod workload will also configure Manage to use default JMS messaging queues to be stored in /{{ mas_app_settings_jms_queue_mount_path }}/jmsstore persistent volume mount path. snojms - Can be used for Manage 8.4 and above. Includes all and jms bundle pods. Enabling JMS pod workload will also configure Manage to use default JMS messaging queues to be stored in /{{ mas_app_settings_jms_queue_mount_path }}/jmsstore persistent volume mount path. Environment Variable: MAS_APP_SETTINGS_SERVER_BUNDLES_SIZE Default: dev Manage - Customization Archive settings variables \u00a4 mas_app_settings_customization_archive_url \u00a4 Provide a custom archive/file path to be included as part of Manage deployment. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOMIZATION_ARCHIVE_URL Default: None mas_app_settings_customization_archive_name \u00a4 Provide a custom archive file name to be associated with the archive/file path provided. Only used when mas_app_settings_customization_archive_url is defined. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOMIZATION_ARCHIVE_NAME Default: manage-custom-archive Manage - Database encryption settings variables \u00a4 mas_app_settings_crypto_key \u00a4 This defines the MXE_SECURITY_CRYPTO_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Optional Environment Variable: MAS_APP_SETTINGS_CRYPTO_KEY Default: Auto-generated mas_app_settings_cryptox_key \u00a4 This defines the MXE_SECURITY_CRYPTOX_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Required if mas_app_settings_crypto_key is set. Environment Variable: MAS_APP_SETTINGS_CRYPTOX_KEY Default: Auto-generated mas_app_settings_old_crypto_key \u00a4 This defines the MXE_SECURITY_OLD_CRYPTO_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Optional Environment Variable: MAS_APP_SETTINGS_OLD_CRYPTO_KEY Default: None mas_app_settings_cryptox_key \u00a4 This defines the MXE_SECURITY_OLD_CRYPTOX_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Required if mas_app_settings_old_crypto_key is set. Environment Variable: MAS_APP_SETTINGS_OLD_CRYPTOX_KEY Default: None mas_app_settings_override_encryption_secrets_flag \u00a4 Set this to true if you want to override existing Manage database encryption keys i.e Manage database reencryption scenario. A backup of the secret holding the original encryption keys will be taken prior overriding it with the new defined keys. If set to false , then the database encryption keys will only be created with the defined keys if no existing database encryption keys are found under the target Manage instance i.e new Manage installations. Optional Environment Variable: MAS_APP_SETTINGS_OVERRIDE_ENCRYPTION_SECRETS_FLAG Default: False Manage - Server Timezone setting variable \u00a4 mas_app_settings_server_timezone \u00a4 Sets the Manage server timezone. If you also want to have the Manage's DB2 database aligned with the same timezone, you must set DB2_TIMEZONE while provisioning the corresponding DB2 instance using db2 role. Optional Environment Variable: MAS_APP_SETTINGS_SERVER_TIMEZONE Default: GMT Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS workspace configuration mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" mas_appws_spec: bindings: jdbc: \"{{ mas_appws_jdbc_binding | default( 'system' , true) }}\" roles: - ibm.mas_devops.suite_app_config License \u00a4 EPL-2.0","title":"suite_app_config"},{"location":"roles/suite_app_config/#suite_app_config","text":"This role is used to configure specific components of the application workspace after the application has been installed in the Maximo Application Suite.","title":"suite_app_config"},{"location":"roles/suite_app_config/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/suite_app_config/#mas_instance_id","text":"Defines the instance id that was used for the MAS installation Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_app_config/#mas_app_id","text":"Defines the application that is will be configured, valid settings are: assist , hputilities , iot , manage , monitor , optimizer , predict , and visualinspection . Required Environment Variable: MAS_APP_ID Default: None","title":"mas_app_id"},{"location":"roles/suite_app_config/#mas_workspace_id","text":"MAS application workspace to use to configure app components Required Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/suite_app_config/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default: None","title":"custom_labels"},{"location":"roles/suite_app_config/#role-variables-workspace-configuration","text":"","title":"Role Variables - Workspace Configuration"},{"location":"roles/suite_app_config/#mas_appws_spec","text":"The application workspace deployment spec used to configure various aspects of the application workspace configuration. Note that use of this will override anything set in mas_appws_components Optional Environment Variable: MAS_APPWS_SPEC Default: defaults are specified in vars/defaultspecs/{mas_app_id}.yml","title":"mas_appws_spec"},{"location":"roles/suite_app_config/#mas_appws_bindings_jdbc","text":"Set the binding scope for the application workspace's JDBC binding ( system , application , workspace , or workspace-application ) Optional Environment Variable: MAS_APPWS_BINDINGS_JDBC Default: system","title":"mas_appws_bindings_jdbc"},{"location":"roles/suite_app_config/#mas_appws_components","text":"Defines the app components and versions to configure in the application workspace. Takes the form of key=value pairs seperated by a comma i.e. To install health within Manage set base=latest,health=latest Optional Environment Variable: MAS_APPWS_COMPONENTS Default: Application specific","title":"mas_appws_components"},{"location":"roles/suite_app_config/#mas_pod_templates_dir","text":"This role will look for a configuration files named: ibm-mas-manage-manageworkspace.yml ibm-mas-manage-imagestitching.yml ibm-mas-manage-slackproxy.yml ibm-mas-manage-healthextworkspace.yml The content of the configuration file should be the yaml block that you wish to be inserted into the ManageWorkspace CR. ibm-mas-manage-manageworkspace.yml will be inserted into the ManageWorkspace CR spec -> podTemplates whereas the component ones e.g, ibm-mas-manage-imagestitching.yml will be under spec -> components -> civil -> podTemplates . The ibm-mas-manage-ws operator will then pass this on to the corresponding component CR when available. This is an example of one of the components (civil) - refer to the BestEfforts reference configuration in the MAS CLI . For full documentation of the supported options refer to the Customizing Pod Templates in the product documentation.","title":"mas_pod_templates_dir"},{"location":"roles/suite_app_config/#role-variables-predict-configuration","text":"","title":"Role Variables - Predict Configuration"},{"location":"roles/suite_app_config/#mas_appws_settings_deployment_size","text":"Controls the workload size of predict containers. Avaliable options are developer , small , medium and small | Deployment_size | Replica | | ---------------------- | :--: | | developer | 1 | | small | 2 | | medium | 3 | Optional, only supported when configuring Predict Environment Variable: MAS_APPWS_SETTINGS_DEPLOYMENT_SIZE Default: small","title":"mas_appws_settings_deployment_size"},{"location":"roles/suite_app_config/#role-variables-watson-studio-local","text":"These variables are only used when using this role to configure Predict , or Health & Predict Utilities .","title":"Role Variables - Watson Studio Local"},{"location":"roles/suite_app_config/#cpd_wsl_project_id","text":"The ID of the analytics project created in Watson Studio and used to configure hputilities application. Required unless cpd_wsl_project_name and mas_config_dir are set. Environment Variable: CPD_WSL_PROJECT_ID Default: None","title":"cpd_wsl_project_id"},{"location":"roles/suite_app_config/#cpd_wsl_project_name","text":"Specifies the name of the file in mas_config_dir where the id of the analytics project is saved. Must be used in conjunction with mas_config_dir as an alternative to cpd_wsl_project_id . Optional Environment Variable: CPD_WSL_PROJECT_NAME Default Value: wsl-mas-${mas_instance_id}-hputilities","title":"cpd_wsl_project_name"},{"location":"roles/suite_app_config/#mas_config_dir","text":"Local directory where generated resource definitions are saved into. Used in conjunction with cpd_wsl_project_name to retrieve the ID of a Watson Studio project previously created by the cp4d_service role. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/suite_app_config/#role-variables-watson-machine-learning","text":"These variables are only used when using this role to configure Predict .","title":"Role Variables - Watson Machine Learning"},{"location":"roles/suite_app_config/#cpd_product_version","text":"The version of Cloud Pak for Data installed in the cluster, which is used to infer the version of Watson Machine Learning that must be passed into the Predict workspace configuration. Required Environment Variable: CPD_PRODUCT_VERSION Default: None","title":"cpd_product_version"},{"location":"roles/suite_app_config/#cpd_wml_instance_id","text":"Identifier of wml instance to be configured in Predict. Optional Environment Variable: CPD_WML_INSTANCE_ID Default: openshift","title":"cpd_wml_instance_id"},{"location":"roles/suite_app_config/#cpd_wml_url","text":"URL to access WML service (same as Cloud Pak for Data URL). Optional Environment Variable: CPD_WML_URL Default: https://internal-nginx-svc.ibm-cpd.svc:12443 (assumes CPD WML is installed the ibm-cpd namespace)","title":"cpd_wml_url"},{"location":"roles/suite_app_config/#role-variables-manage-workspace","text":"","title":"Role Variables - Manage Workspace"},{"location":"roles/suite_app_config/#manage-health-integration-variables","text":"","title":"Manage - Health Integration variables"},{"location":"roles/suite_app_config/#mas_appws_bindings_health_wsl_flag","text":"Boolean value indicating if Watson Studio must be bound to Manage. It is expected a system level WatsonStudioCfg applied in the cluster. Optional Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL_FLAG Default: false","title":"mas_appws_bindings_health_wsl_flag"},{"location":"roles/suite_app_config/#mas_appws_bindings_health_wsl","text":"Set as system to indicate Watson Studio must be installed and bound to Health Optional Environment Variable: MAS_APPWS_BINDINGS_HEALTH_WSL Default: None","title":"mas_appws_bindings_health_wsl"},{"location":"roles/suite_app_config/#mas_app_settings_aio_flag","text":"Flag indicating if Asset Investment Optimization (AIO) resource must be loaded or not. It can be loaded only when Optimizer application is installed. Optional Only supported when Optimizer application is installed. Environment Variable: MAS_APP_SETTINGS_AIO_FLAG Default: true","title":"mas_app_settings_aio_flag"},{"location":"roles/suite_app_config/#manage-db2-settings-variables","text":"","title":"Manage - DB2 settings variables"},{"location":"roles/suite_app_config/#mas_app_settings_db_schema","text":"Name of the schema where Manage database lives in. Code also supports deprecated mas_app_settings_db2_schema variable name. Optional Environment Variable: MAS_APP_SETTINGS_DB_SCHEMA Default: maximo","title":"mas_app_settings_db_schema"},{"location":"roles/suite_app_config/#mas_app_settings_demodata","text":"Flag indicating if manage demodata should be loaded or not. Optional Environment Variable: MAS_APP_SETTINGS_DEMODATA Default: false (do not load demodata)","title":"mas_app_settings_demodata"},{"location":"roles/suite_app_config/#mas_app_settings_db2vargraphic","text":"Optional. Flag indicating if VARGRAPHIC (if true) or VARCHAR (if false) is used. Details: https://www.ibm.com/docs/en/mas-cd/continuous-delivery?topic=deploy-language-support Default: true","title":"mas_app_settings_db2vargraphic"},{"location":"roles/suite_app_config/#mas_app_settings_tablespace","text":"Name of the Manage database tablespace Optional Environment Variable: MAS_APP_SETTINGS_TABLESPACE Default: MAXDATA","title":"mas_app_settings_tablespace"},{"location":"roles/suite_app_config/#mas_app_settings_indexspace","text":"Name of the Manage database indexspace Optional Environment Variable: MAS_APP_SETTINGS_INDEXSPACE Default: MAXINDEX","title":"mas_app_settings_indexspace"},{"location":"roles/suite_app_config/#manage-persistent-volumes-variables","text":"","title":"Manage - Persistent Volumes variables"},{"location":"roles/suite_app_config/#mas_app_settings_persistent_volumes_flag","text":"Flag indicating if persistent volumes should be configured by default during Manage Workspace activation. There are two defaulted File Storage Persistent Volumes Claim resources that will be created out of the box for Manage if this flag is set to true : /DOCLINKS : Persistent volume used to store doclinks/attachments. /bim : Persistent volume used to store Building Information Models related artifacts (models, docs and import). Optional Environment Variable: MAS_APP_SETTINGS_PERSISTENT_VOLUMES_FLAG Default: false","title":"mas_app_settings_persistent_volumes_flag"},{"location":"roles/suite_app_config/#jms-queues","text":"The following properties can be defined to customize the persistent volumes for the JMS queues setup for Manage.","title":"JMS queues"},{"location":"roles/suite_app_config/#mas_app_settings_jms_queue_pvc_storage_class","text":"Provide the persistent volume storage class to be used for JMS queue configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) access modes are supported. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes.","title":"mas_app_settings_jms_queue_pvc_storage_class"},{"location":"roles/suite_app_config/#mas_app_settings_jms_queue_pvc_name","text":"Provide the persistent volume claim name to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_NAME Default: manage-jms","title":"mas_app_settings_jms_queue_pvc_name"},{"location":"roles/suite_app_config/#mas_app_settings_jms_queue_pvc_size","text":"Provide the persistent volume claim size to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_SIZE Default: 20Gi","title":"mas_app_settings_jms_queue_pvc_size"},{"location":"roles/suite_app_config/#mas_app_settings_jms_queue_mount_path","text":"Provide the persistent volume storage mount path to be used for JMS queue configuration. Note: JMS configuration will only be done if mas_app_settings_server_bundles_size property is set to jms . Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_MOUNT_PATH Default: /jms","title":"mas_app_settings_jms_queue_mount_path"},{"location":"roles/suite_app_config/#mas_app_settings_jms_queue_pvc_accessmode","text":"Provide the persistent volume storage access-mode to be used for JMS queue configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_JMS_QUEUE_PVC_ACCESSMODE Default: ReadWriteMany","title":"mas_app_settings_jms_queue_pvc_accessmode"},{"location":"roles/suite_app_config/#mas_app_settings_default_jms","text":"Set this to true if you want to have JMS continuous queues configured Optional Environment Variable: MAS_APP_SETTINGS_DEFAULT_JMS Default: false","title":"mas_app_settings_default_jms"},{"location":"roles/suite_app_config/#doclinksattachments","text":"The following properties can be defined to customize the persistent volumes for the Doclinks/Attachments setup for Manage.","title":"Doclinks/Attachments"},{"location":"roles/suite_app_config/#mas_app_settings_doclinks_pvc_storage_class","text":"Provide the persistent volume storage class to be used for doclinks/attachments configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_STORAGE_CLASS Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes.","title":"mas_app_settings_doclinks_pvc_storage_class"},{"location":"roles/suite_app_config/#mas_app_settings_doclinks_pvc_name","text":"Provide the persistent volume claim name to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_NAME Default: manage-doclinks","title":"mas_app_settings_doclinks_pvc_name"},{"location":"roles/suite_app_config/#mas_app_settings_doclinks_pvc_size","text":"Provide the persistent volume claim size to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_SIZE Default: 20Gi","title":"mas_app_settings_doclinks_pvc_size"},{"location":"roles/suite_app_config/#mas_app_settings_doclinks_mount_path","text":"Provide the persistent volume storage mount path to be used for doclinks/attachments configuration. Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_MOUNT_PATH Default: /DOCLINKS","title":"mas_app_settings_doclinks_mount_path"},{"location":"roles/suite_app_config/#mas_app_settings_doclinks_pvc_accessmode","text":"Provide the persistent volume storage access-mode to be used for doclinks/attachments configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_DOCLINKS_PVC_ACCESSMODE Default: ReadWriteMany","title":"mas_app_settings_doclinks_pvc_accessmode"},{"location":"roles/suite_app_config/#bim-building-information-models","text":"The following properties can be defined to customize the persistent volumes for the Building Information Models setup for Manage.","title":"BIM (Building Information Models)"},{"location":"roles/suite_app_config/#mas_app_settings_bim_pvc_storage_class","text":"Provide the persistent volume storage class to be used for Building Information Models configuration. Both ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class) are supported. - Optional - Environment Variable: MAS_APP_SETTINGS_BIM_PVC_STORAGE_CLASS - Default: None - If not set, a default storage class will be auto defined accordingly to your cluster's available storage classes.","title":"mas_app_settings_bim_pvc_storage_class"},{"location":"roles/suite_app_config/#mas_app_settings_bim_pvc_name","text":"Provide the persistent volume claim name to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_NAME Default: manage-bim","title":"mas_app_settings_bim_pvc_name"},{"location":"roles/suite_app_config/#mas_app_settings_bim_pvc_size","text":"Provide the persistent volume claim size to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_SIZE Default: 20Gi","title":"mas_app_settings_bim_pvc_size"},{"location":"roles/suite_app_config/#mas_app_settings_bim_mount_path","text":"Provide the persistent volume storage mount path to be used for Building Information Models configuration. Optional Environment Variable: MAS_APP_SETTINGS_BIM_MOUNT_PATH Default: /bim","title":"mas_app_settings_bim_mount_path"},{"location":"roles/suite_app_config/#mas_app_settings_bim_pvc_accessmode","text":"Provide the persistent volume storage access-mode to be used for Building Information Models configuration. Typically you would either choose between ReadWriteOnce (if using a block storage class) or ReadWriteMany (if using file storage class). Optional Environment Variable: MAS_APP_SETTINGS_BIM_PVC_ACCESSMODE Default: ReadWriteMany","title":"mas_app_settings_bim_pvc_accessmode"},{"location":"roles/suite_app_config/#manage-supported-languages-variables","text":"","title":"Manage - Supported languages variables"},{"location":"roles/suite_app_config/#mas_app_settings_base_lang","text":"Provide the base language for Manage application. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. Optional Environment Variable: MAS_APP_SETTINGS_BASE_LANG Default: EN (English)","title":"mas_app_settings_base_lang"},{"location":"roles/suite_app_config/#mas_app_settings_secondary_langs","text":"Provide a list of additional secondary languages for Manage application. Note: The more languages you add, the longer Manage will take to install and activate. Export the MAS_APP_SETTINGS_SECONDARY_LANGS variable with the language codes as comma-separated values. For a full list of supported languages for Manage application and its corresponding language codes, please refer to Language Support documentation. Optional Environment Variable: MAS_APP_SETTINGS_SECONDARY_LANGS Default: None For example, use the following to enable Manage application with Arabic, Deutsch and Japanese as secondary languages: export MAS_APP_SETTINGS_SECONDARY_LANGS='AR,DE,JA'","title":"mas_app_settings_secondary_langs"},{"location":"roles/suite_app_config/#manage-server-bundle-configuration-variables","text":"","title":"Manage - Server Bundle configuration variables"},{"location":"roles/suite_app_config/#mas_app_settings_server_bundles_size","text":"Optional. Provides different flavors of server bundle configuration to handle workload for Manage application. For more details about Manage application server bundle configuration, refer to Setting the server bundles for Manage application . Currently supported server bundle sizes are: dev - Deploys Manage with the default server bundle configuration. i.e just 1 bundle pod handling all Manage application workload. small - Deploys Manage with the most common deployment configuration. i.e 4 bundle pods, each one handling workload for each main capabilities: mea , cron , report and ui jms - Can be used for Manage 8.4 and above. Same server bundle configuration as small and includes jms bundle pod. Enabling JMS pod workload will also configure Manage to use default JMS messaging queues to be stored in /{{ mas_app_settings_jms_queue_mount_path }}/jmsstore persistent volume mount path. snojms - Can be used for Manage 8.4 and above. Includes all and jms bundle pods. Enabling JMS pod workload will also configure Manage to use default JMS messaging queues to be stored in /{{ mas_app_settings_jms_queue_mount_path }}/jmsstore persistent volume mount path. Environment Variable: MAS_APP_SETTINGS_SERVER_BUNDLES_SIZE Default: dev","title":"mas_app_settings_server_bundles_size"},{"location":"roles/suite_app_config/#manage-customization-archive-settings-variables","text":"","title":"Manage - Customization Archive settings variables"},{"location":"roles/suite_app_config/#mas_app_settings_customization_archive_url","text":"Provide a custom archive/file path to be included as part of Manage deployment. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOMIZATION_ARCHIVE_URL Default: None","title":"mas_app_settings_customization_archive_url"},{"location":"roles/suite_app_config/#mas_app_settings_customization_archive_name","text":"Provide a custom archive file name to be associated with the archive/file path provided. Only used when mas_app_settings_customization_archive_url is defined. Optional Environment Variable: MAS_APP_SETTINGS_CUSTOMIZATION_ARCHIVE_NAME Default: manage-custom-archive","title":"mas_app_settings_customization_archive_name"},{"location":"roles/suite_app_config/#manage-database-encryption-settings-variables","text":"","title":"Manage - Database encryption settings variables"},{"location":"roles/suite_app_config/#mas_app_settings_crypto_key","text":"This defines the MXE_SECURITY_CRYPTO_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Optional Environment Variable: MAS_APP_SETTINGS_CRYPTO_KEY Default: Auto-generated","title":"mas_app_settings_crypto_key"},{"location":"roles/suite_app_config/#mas_app_settings_cryptox_key","text":"This defines the MXE_SECURITY_CRYPTOX_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Required if mas_app_settings_crypto_key is set. Environment Variable: MAS_APP_SETTINGS_CRYPTOX_KEY Default: Auto-generated","title":"mas_app_settings_cryptox_key"},{"location":"roles/suite_app_config/#mas_app_settings_old_crypto_key","text":"This defines the MXE_SECURITY_OLD_CRYPTO_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Optional Environment Variable: MAS_APP_SETTINGS_OLD_CRYPTO_KEY Default: None","title":"mas_app_settings_old_crypto_key"},{"location":"roles/suite_app_config/#mas_app_settings_cryptox_key_1","text":"This defines the MXE_SECURITY_OLD_CRYPTOX_KEY value if you want to customize your Manage database encryption keys. For more details, refer to Manage database encryption documentation. Required if mas_app_settings_old_crypto_key is set. Environment Variable: MAS_APP_SETTINGS_OLD_CRYPTOX_KEY Default: None","title":"mas_app_settings_cryptox_key"},{"location":"roles/suite_app_config/#mas_app_settings_override_encryption_secrets_flag","text":"Set this to true if you want to override existing Manage database encryption keys i.e Manage database reencryption scenario. A backup of the secret holding the original encryption keys will be taken prior overriding it with the new defined keys. If set to false , then the database encryption keys will only be created with the defined keys if no existing database encryption keys are found under the target Manage instance i.e new Manage installations. Optional Environment Variable: MAS_APP_SETTINGS_OVERRIDE_ENCRYPTION_SECRETS_FLAG Default: False","title":"mas_app_settings_override_encryption_secrets_flag"},{"location":"roles/suite_app_config/#manage-server-timezone-setting-variable","text":"","title":"Manage - Server Timezone setting variable"},{"location":"roles/suite_app_config/#mas_app_settings_server_timezone","text":"Sets the Manage server timezone. If you also want to have the Manage's DB2 database aligned with the same timezone, you must set DB2_TIMEZONE while provisioning the corresponding DB2 instance using db2 role. Optional Environment Variable: MAS_APP_SETTINGS_SERVER_TIMEZONE Default: GMT","title":"mas_app_settings_server_timezone"},{"location":"roles/suite_app_config/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS workspace configuration mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" mas_appws_spec: bindings: jdbc: \"{{ mas_appws_jdbc_binding | default( 'system' , true) }}\" roles: - ibm.mas_devops.suite_app_config","title":"Example Playbook"},{"location":"roles/suite_app_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_install/","text":"suite_app_install \u00a4 This role is used to install a specified application in Maximo Application Suite. Role Variables - General \u00a4 mas_instance_id \u00a4 Defines the instance id that was used for the MAS installation Required Environment Variable: MAS_INSTANCE_ID Default: None mas_app_id \u00a4 Defines the kind of application that is intended for installation such as assist , health , iot , manage , monitor , mso , predict , or safety Optional Environment Variable: MAS_APP_ID Default: None mas_app_catalog_source \u00a4 Defines the catalog to be used to install the MAS app. You can set it to ibm-operator-catalog for release install or ibm-mas-{mas_app_id}-operators for development, where {mas_app_id} will be manage for the Manage and Health app installation, for example. Optional Environment Variable: MAS_APP_CATALOG_SOURCE Default: ibm-operator-catalog mas_app_channel \u00a4 Defines which channel of the MAS application to subscribe to Required Environment Variable: MAS_APP_CHANNEL Default: None custom_labels \u00a4 Optional. List of comma separated key=value pairs for setting custom labels on instance specific resources. Environment Variable: CUSTOM_LABELS Default: None Role Variables - Pre-Release Support \u00a4 artifactory_username \u00a4 Required when using this role for development versions of the MAS application. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None artifactory_token \u00a4 Required when using this role for development versions of the MAS application Optional Environment Variable: ARTIFACTORY_TOKEN Default: None mas_entitlement_username \u00a4 Username for entitled registry. This username will be used to create the image pull secret. Set to cp when installing release or use your w3Id for dev Optional Environment Variable: MAS_ENTITLEMENT_USERNAME Default: None mas_entitlement_key \u00a4 API Key for entitled registry. This password will be used to create the image pull secret. Set to with IBM entitlement key when installing release or use your artifactory apikey for dev. Optional Environment Variable: MAS_ENTITLEMENT_KEY Default: None Role Variables - Application Configuration \u00a4 mas_app_spec \u00a4 Use of mas_app_spec will override all other application configuration variables. Optional Environment Variable: None Default: defaults are specified in vars/defaultspecs/{{mas_app_id}}.yml mas_app_bindings_jdbc \u00a4 Set the binding scope for the application's JDBC binding ( system or application ) Optional Environment Variable: MAS_APP_BINDINGS_JDBC Default: system mas_app_plan \u00a4 Optional : Defines what plan will be used in application install. Environment Variable: MAS_APP_PLAN Default: Application-specific, see details below. Application Support: Optimizer v8.2+: full and limited are supported, defaults to full mas_pod_templates_dir \u00a4 Provide the directory where supported pod templates configuration files are defined. For application specifics read the information for `mas_pod_templates_dir`` below. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None Role Variables - Visual Inspection Configuration \u00a4 mas_app_settings_visualinspection_storage_class \u00a4 Optional : Storage class used for user data. This must support ReadWriteMany(RWX). Environment Variable: MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_CLASS Default: Auto-selected from storage classes installed in the cluster. mas_app_settings_visualinspection_storage_size \u00a4 Optional : Size of data persistent volume. Environment Variable: MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_SIZE Default: 100Gi Role Variables - IoT Configuration \u00a4 mas_app_settings_iot_deployment_size \u00a4 Optional : The IoT deployment size, one of dev , small or large . Environment Variable: MAS_APP_SETTINGS_IOT_DEPLOYMENT_SIZE Default: small Application Support: IoT 8.6+ mas_app_settings_iot_fpl_pvc_storage_class \u00a4 Optional : The persistent volume storage class used by the iot fpl component for transient state storage. The storage class can be used to dynamically provision a persistent volume with access mode RWO (ReadWriteOnce). - Environment Variable: MAS_APP_SETTINGS_IOT_FPL_PVC_STORAGE_CLASS - Default: Auto-selected from storage classes installed in the cluster. - Application Support: - IoT 8.6+ mas_app_settings_iot_fpl_router_pvc_size \u00a4 Optional : The persistent volume size used by the iot fpl pipeline router for transient state storage Environment Variable: MAS_APP_SETTINGS_IOT_FPL_ROUTER_PVC_SIZE Default: 100Gi. Application Support: IoT 8.6+ mas_app_settings_iot_fpl_executor_pvc_size \u00a4 Optional : The persistent volume size used by the iot fpl pipeline router for transient state storage Environment Variable: MAS_APP_SETTINGS_IOT_FPL_EXECUTOR_PVC_SIZE Default: 100Gi. Application Support: IoT 8.6+ mas_app_settings_iot_mqttbroker_pvc_storage_class \u00a4 Optional : The persistent volume storage class used by the iot mqtt broker (messagesight) The storage class can be used to dynamically provision a persistent volume with access mode RWO (ReadWriteOnce). - Environment Variable: MAS_APP_SETTINGS_IOT_MQTTBROKER_PVC_STORAGE_CLASS - Default: Auto-selected from storage classes installed in the cluster, if a default compatible one is found. - Application Support: - IoT 8.3+ mas_app_settings_iot_mqttbroker_pvc_size \u00a4 Optional : The persistent volume size used by the iot mqtt broker (messagesight) Environment Variable: MAS_APP_SETTINGS_IOT_MQTTBROKER_PVC_SIZE Default: 100Gi. Application Support: IoT 8.3+ mas_pod_templates_dir \u00a4 This role will look for a configuration files named: ibm-mas-iot-iot.yml ibm-mas-iot-actions.yml ibm-mas-iot-auth.yml ibm-mas-iot-datapower.yml ibm-mas-iot-devops.yml ibm-mas-iot-dm.yml ibm-mas-iot-dsc.yml ibm-mas-iot-edgeconfig.yml ibm-mas-iot-fpl.yml ibm-mas-iot-guardian.yml ibm-mas-iot-mbgx.yml ibm-mas-iot-mfgx.yml ibm-mas-iot-monitor.yml ibm-mas-iot-orgmgmt.yml ibm-mas-iot-provision.yml ibm-mas-iot-registry.yml ibm-mas-iot-state.yml ibm-mas-iot-webui.yml The content of the configuration file should be the yaml block that you wish to be inserted into the IoT CR. ibm-mas-iot-iot.yml will be inserted into the main IoT CR spec -> podTemplates whereas the component ones e.g, ibm-mas-iot-actions.yml will be under spec -> components -> {componentName} -> podTemplates . The ibm-mas-iot operator will then pass this on to the corresponding component CR when available. This is an example of one of the components (actions) - refer to the BestEfforts reference configuration in the MAS CLI . For full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Role Variables - Manage Configuration \u00a4 mas_pod_templates_dir \u00a4 This role will look for a configuration files named for manage: ibm-mas-manage-manageapp.yml The content of the configuration file should be the yaml block that you wish to be inserted into the ManageApp CR. ibm-mas-manage-manageapp.yml will be inserted into the ManageApp CR spec -> podTemplates . The ibm-mas-manage operator will then pass this on to the corresponding deployments when available. For full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Role Variables - Monitor Configuration \u00a4 mas_app_settings_monitor_deployment_size \u00a4 Optional, The Monitor deployment size, one of dev , small or large . Environment Variable: MAS_APP_SETTINGS_MONITOR_DEPLOYMENT_SIZE Default: dev Application Support: Monitor 8.6+ Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # Choose which catalog source to use for the MAS install, default to the IBM operator catalog mas_app_catalog_source: \"{{ lookup('env', 'MAS_APP_CATALOG_SOURCE') | default('ibm-operator-catalog', true) }}\" # Which MAS channel to subscribe to mas_app_channel: \"{{ lookup('env', 'MAS_APP_CHANNEL') | default('8.x', true) }}\" # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS configuration - Entitlement mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') | default('cp', true) }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" # Application Configuration - Spec mas_app_spec: bindings: jdbc: system mongo: system kafka: system settings: messagesight: storage: class: block1000p size: 100Gi deployment: size: medium # Application Configuration - Install Plan mas_app_plan: \"{{ lookup('env', 'MAS_APP_PLAN') | default('full', true) }}\" roles: - ibm.mas_devops.suite_app_install License \u00a4 EPL-2.0","title":"suite_app_install"},{"location":"roles/suite_app_install/#suite_app_install","text":"This role is used to install a specified application in Maximo Application Suite.","title":"suite_app_install"},{"location":"roles/suite_app_install/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/suite_app_install/#mas_instance_id","text":"Defines the instance id that was used for the MAS installation Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_app_install/#mas_app_id","text":"Defines the kind of application that is intended for installation such as assist , health , iot , manage , monitor , mso , predict , or safety Optional Environment Variable: MAS_APP_ID Default: None","title":"mas_app_id"},{"location":"roles/suite_app_install/#mas_app_catalog_source","text":"Defines the catalog to be used to install the MAS app. You can set it to ibm-operator-catalog for release install or ibm-mas-{mas_app_id}-operators for development, where {mas_app_id} will be manage for the Manage and Health app installation, for example. Optional Environment Variable: MAS_APP_CATALOG_SOURCE Default: ibm-operator-catalog","title":"mas_app_catalog_source"},{"location":"roles/suite_app_install/#mas_app_channel","text":"Defines which channel of the MAS application to subscribe to Required Environment Variable: MAS_APP_CHANNEL Default: None","title":"mas_app_channel"},{"location":"roles/suite_app_install/#custom_labels","text":"Optional. List of comma separated key=value pairs for setting custom labels on instance specific resources. Environment Variable: CUSTOM_LABELS Default: None","title":"custom_labels"},{"location":"roles/suite_app_install/#role-variables-pre-release-support","text":"","title":"Role Variables - Pre-Release Support"},{"location":"roles/suite_app_install/#artifactory_username","text":"Required when using this role for development versions of the MAS application. Optional Environment Variable: ARTIFACTORY_USERNAME Default: None","title":"artifactory_username"},{"location":"roles/suite_app_install/#artifactory_token","text":"Required when using this role for development versions of the MAS application Optional Environment Variable: ARTIFACTORY_TOKEN Default: None","title":"artifactory_token"},{"location":"roles/suite_app_install/#mas_entitlement_username","text":"Username for entitled registry. This username will be used to create the image pull secret. Set to cp when installing release or use your w3Id for dev Optional Environment Variable: MAS_ENTITLEMENT_USERNAME Default: None","title":"mas_entitlement_username"},{"location":"roles/suite_app_install/#mas_entitlement_key","text":"API Key for entitled registry. This password will be used to create the image pull secret. Set to with IBM entitlement key when installing release or use your artifactory apikey for dev. Optional Environment Variable: MAS_ENTITLEMENT_KEY Default: None","title":"mas_entitlement_key"},{"location":"roles/suite_app_install/#role-variables-application-configuration","text":"","title":"Role Variables - Application Configuration"},{"location":"roles/suite_app_install/#mas_app_spec","text":"Use of mas_app_spec will override all other application configuration variables. Optional Environment Variable: None Default: defaults are specified in vars/defaultspecs/{{mas_app_id}}.yml","title":"mas_app_spec"},{"location":"roles/suite_app_install/#mas_app_bindings_jdbc","text":"Set the binding scope for the application's JDBC binding ( system or application ) Optional Environment Variable: MAS_APP_BINDINGS_JDBC Default: system","title":"mas_app_bindings_jdbc"},{"location":"roles/suite_app_install/#mas_app_plan","text":"Optional : Defines what plan will be used in application install. Environment Variable: MAS_APP_PLAN Default: Application-specific, see details below. Application Support: Optimizer v8.2+: full and limited are supported, defaults to full","title":"mas_app_plan"},{"location":"roles/suite_app_install/#mas_pod_templates_dir","text":"Provide the directory where supported pod templates configuration files are defined. For application specifics read the information for `mas_pod_templates_dir`` below. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None","title":"mas_pod_templates_dir"},{"location":"roles/suite_app_install/#role-variables-visual-inspection-configuration","text":"","title":"Role Variables - Visual Inspection Configuration"},{"location":"roles/suite_app_install/#mas_app_settings_visualinspection_storage_class","text":"Optional : Storage class used for user data. This must support ReadWriteMany(RWX). Environment Variable: MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_CLASS Default: Auto-selected from storage classes installed in the cluster.","title":"mas_app_settings_visualinspection_storage_class"},{"location":"roles/suite_app_install/#mas_app_settings_visualinspection_storage_size","text":"Optional : Size of data persistent volume. Environment Variable: MAS_APP_SETTINGS_VISUALINSPECTION_STORAGE_SIZE Default: 100Gi","title":"mas_app_settings_visualinspection_storage_size"},{"location":"roles/suite_app_install/#role-variables-iot-configuration","text":"","title":"Role Variables - IoT Configuration"},{"location":"roles/suite_app_install/#mas_app_settings_iot_deployment_size","text":"Optional : The IoT deployment size, one of dev , small or large . Environment Variable: MAS_APP_SETTINGS_IOT_DEPLOYMENT_SIZE Default: small Application Support: IoT 8.6+","title":"mas_app_settings_iot_deployment_size"},{"location":"roles/suite_app_install/#mas_app_settings_iot_fpl_pvc_storage_class","text":"Optional : The persistent volume storage class used by the iot fpl component for transient state storage. The storage class can be used to dynamically provision a persistent volume with access mode RWO (ReadWriteOnce). - Environment Variable: MAS_APP_SETTINGS_IOT_FPL_PVC_STORAGE_CLASS - Default: Auto-selected from storage classes installed in the cluster. - Application Support: - IoT 8.6+","title":"mas_app_settings_iot_fpl_pvc_storage_class"},{"location":"roles/suite_app_install/#mas_app_settings_iot_fpl_router_pvc_size","text":"Optional : The persistent volume size used by the iot fpl pipeline router for transient state storage Environment Variable: MAS_APP_SETTINGS_IOT_FPL_ROUTER_PVC_SIZE Default: 100Gi. Application Support: IoT 8.6+","title":"mas_app_settings_iot_fpl_router_pvc_size"},{"location":"roles/suite_app_install/#mas_app_settings_iot_fpl_executor_pvc_size","text":"Optional : The persistent volume size used by the iot fpl pipeline router for transient state storage Environment Variable: MAS_APP_SETTINGS_IOT_FPL_EXECUTOR_PVC_SIZE Default: 100Gi. Application Support: IoT 8.6+","title":"mas_app_settings_iot_fpl_executor_pvc_size"},{"location":"roles/suite_app_install/#mas_app_settings_iot_mqttbroker_pvc_storage_class","text":"Optional : The persistent volume storage class used by the iot mqtt broker (messagesight) The storage class can be used to dynamically provision a persistent volume with access mode RWO (ReadWriteOnce). - Environment Variable: MAS_APP_SETTINGS_IOT_MQTTBROKER_PVC_STORAGE_CLASS - Default: Auto-selected from storage classes installed in the cluster, if a default compatible one is found. - Application Support: - IoT 8.3+","title":"mas_app_settings_iot_mqttbroker_pvc_storage_class"},{"location":"roles/suite_app_install/#mas_app_settings_iot_mqttbroker_pvc_size","text":"Optional : The persistent volume size used by the iot mqtt broker (messagesight) Environment Variable: MAS_APP_SETTINGS_IOT_MQTTBROKER_PVC_SIZE Default: 100Gi. Application Support: IoT 8.3+","title":"mas_app_settings_iot_mqttbroker_pvc_size"},{"location":"roles/suite_app_install/#mas_pod_templates_dir_1","text":"This role will look for a configuration files named: ibm-mas-iot-iot.yml ibm-mas-iot-actions.yml ibm-mas-iot-auth.yml ibm-mas-iot-datapower.yml ibm-mas-iot-devops.yml ibm-mas-iot-dm.yml ibm-mas-iot-dsc.yml ibm-mas-iot-edgeconfig.yml ibm-mas-iot-fpl.yml ibm-mas-iot-guardian.yml ibm-mas-iot-mbgx.yml ibm-mas-iot-mfgx.yml ibm-mas-iot-monitor.yml ibm-mas-iot-orgmgmt.yml ibm-mas-iot-provision.yml ibm-mas-iot-registry.yml ibm-mas-iot-state.yml ibm-mas-iot-webui.yml The content of the configuration file should be the yaml block that you wish to be inserted into the IoT CR. ibm-mas-iot-iot.yml will be inserted into the main IoT CR spec -> podTemplates whereas the component ones e.g, ibm-mas-iot-actions.yml will be under spec -> components -> {componentName} -> podTemplates . The ibm-mas-iot operator will then pass this on to the corresponding component CR when available. This is an example of one of the components (actions) - refer to the BestEfforts reference configuration in the MAS CLI . For full documentation of the supported options refer to the Customizing Pod Templates in the product documentation.","title":"mas_pod_templates_dir"},{"location":"roles/suite_app_install/#role-variables-manage-configuration","text":"","title":"Role Variables - Manage Configuration"},{"location":"roles/suite_app_install/#mas_pod_templates_dir_2","text":"This role will look for a configuration files named for manage: ibm-mas-manage-manageapp.yml The content of the configuration file should be the yaml block that you wish to be inserted into the ManageApp CR. ibm-mas-manage-manageapp.yml will be inserted into the ManageApp CR spec -> podTemplates . The ibm-mas-manage operator will then pass this on to the corresponding deployments when available. For full documentation of the supported options refer to the Customizing Pod Templates in the product documentation.","title":"mas_pod_templates_dir"},{"location":"roles/suite_app_install/#role-variables-monitor-configuration","text":"","title":"Role Variables - Monitor Configuration"},{"location":"roles/suite_app_install/#mas_app_settings_monitor_deployment_size","text":"Optional, The Monitor deployment size, one of dev , small or large . Environment Variable: MAS_APP_SETTINGS_MONITOR_DEPLOYMENT_SIZE Default: dev Application Support: Monitor 8.6+","title":"mas_app_settings_monitor_deployment_size"},{"location":"roles/suite_app_install/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Choose which catalog source to use for the MAS install, default to the IBM operator catalog mas_app_catalog_source: \"{{ lookup('env', 'MAS_APP_CATALOG_SOURCE') | default('ibm-operator-catalog', true) }}\" # Which MAS channel to subscribe to mas_app_channel: \"{{ lookup('env', 'MAS_APP_CHANNEL') | default('8.x', true) }}\" # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS configuration - Entitlement mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') | default('cp', true) }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" # Application Configuration - Spec mas_app_spec: bindings: jdbc: system mongo: system kafka: system settings: messagesight: storage: class: block1000p size: 100Gi deployment: size: medium # Application Configuration - Install Plan mas_app_plan: \"{{ lookup('env', 'MAS_APP_PLAN') | default('full', true) }}\" roles: - ibm.mas_devops.suite_app_install","title":"Example Playbook"},{"location":"roles/suite_app_install/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_rollback/","text":"suite_app_rollback \u00a4 This role is to roll back Maximo Application Suite Applications to an earlier version. Currently, this is designed for Manage Application only. Rollback is possible only in 8.7 and later. From 8.7 onwards, every version comes with a set of supported versions to which the Application can be rolled back. For example, you can roll back Manage Application from 8.7.x to 8.7.0. This role will rollback the version for an an installed MAS application after validating: - That the specificied version of application is compatible to rollback from the current version. - That the specificied version of application is compatible with the running MAS core platform. It will rollback the Manage Application to the desired version. It will validate that the Manage Application has been successfully reconciled at the rolled back version. Role Variables \u00a4 mas_instance_id \u00a4 Set the instance ID for the MAS installation where you wish to rollback the application. Required Environment Variable: MAS_INSTANCE_ID Default Value: None rollback_mas_app \u00a4 When set to true will ensure that the role performs rollback operation. optional Environment Variable: ROLLBACK_MAS_APP Default: True verify_app_version \u00a4 When set to true will ensure that the role checks the current Manage Appliation version matches with specified version. optional Environment Variable: VERIFY_APP_VERSION Default: False mas_app_version \u00a4 The version you wish to rollback to. Built-in validation will ensure that the rollback will only proceed if a supportable rollback path is chosen. It is required when any of the ROLLBACK_MAS_APP and VERIFY_APP_VERSION variables is set to true . Required Environment Variable: MAS_APP_VERSION Default Value: None mas_app_id \u00a4 The name of the Maximo Application Suite Application. This will be used to lookup for application namespace and resources. Please be aware that at present, 'manage' is the only supported value for this variable. Required Environment Variable: MAS_APP_ID Default Value: None Example Playbook \u00a4 Automatic Target Selection \u00a4 Running this playbook will rollback Manage Application to the 8.7.1 version. If you run this playbook when you are already on the same version it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: manage mas_app_version: 8.7.1 roles: - ibm.mas_devops.suite_app_rollback Verify Manage App version \u00a4 Running this playbook will attempt to verify the current version of Manage Application matches with the specified version. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: manage mas_app_version: 8.7.1 rollback_mas_app: False verify_app_version: True roles: - ibm.mas_devops.suite_app_rollback","title":"suite_app_rollback"},{"location":"roles/suite_app_rollback/#suite_app_rollback","text":"This role is to roll back Maximo Application Suite Applications to an earlier version. Currently, this is designed for Manage Application only. Rollback is possible only in 8.7 and later. From 8.7 onwards, every version comes with a set of supported versions to which the Application can be rolled back. For example, you can roll back Manage Application from 8.7.x to 8.7.0. This role will rollback the version for an an installed MAS application after validating: - That the specificied version of application is compatible to rollback from the current version. - That the specificied version of application is compatible with the running MAS core platform. It will rollback the Manage Application to the desired version. It will validate that the Manage Application has been successfully reconciled at the rolled back version.","title":"suite_app_rollback"},{"location":"roles/suite_app_rollback/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_rollback/#mas_instance_id","text":"Set the instance ID for the MAS installation where you wish to rollback the application. Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_app_rollback/#rollback_mas_app","text":"When set to true will ensure that the role performs rollback operation. optional Environment Variable: ROLLBACK_MAS_APP Default: True","title":"rollback_mas_app"},{"location":"roles/suite_app_rollback/#verify_app_version","text":"When set to true will ensure that the role checks the current Manage Appliation version matches with specified version. optional Environment Variable: VERIFY_APP_VERSION Default: False","title":"verify_app_version"},{"location":"roles/suite_app_rollback/#mas_app_version","text":"The version you wish to rollback to. Built-in validation will ensure that the rollback will only proceed if a supportable rollback path is chosen. It is required when any of the ROLLBACK_MAS_APP and VERIFY_APP_VERSION variables is set to true . Required Environment Variable: MAS_APP_VERSION Default Value: None","title":"mas_app_version"},{"location":"roles/suite_app_rollback/#mas_app_id","text":"The name of the Maximo Application Suite Application. This will be used to lookup for application namespace and resources. Please be aware that at present, 'manage' is the only supported value for this variable. Required Environment Variable: MAS_APP_ID Default Value: None","title":"mas_app_id"},{"location":"roles/suite_app_rollback/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_app_rollback/#automatic-target-selection","text":"Running this playbook will rollback Manage Application to the 8.7.1 version. If you run this playbook when you are already on the same version it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: manage mas_app_version: 8.7.1 roles: - ibm.mas_devops.suite_app_rollback","title":"Automatic Target Selection"},{"location":"roles/suite_app_rollback/#verify-manage-app-version","text":"Running this playbook will attempt to verify the current version of Manage Application matches with the specified version. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: manage mas_app_version: 8.7.1 rollback_mas_app: False verify_app_version: True roles: - ibm.mas_devops.suite_app_rollback","title":"Verify Manage App version"},{"location":"roles/suite_app_uninstall/","text":"suite_app_uninstall \u00a4 This role is used to uninstall a specified application in Maximo Application Suite. Role Variables - General \u00a4 mas_instance_id \u00a4 Defines the MAS instance id from which an appplication will be uninstalled Required Environment Variable: MAS_INSTANCE_ID Default: None mas_app_id \u00a4 Defines the kind of application that will be uninstalled such as assist , health , hputilities , iot , manage , monitor , mso , optimizer , predict , safety or visualinspection Required Environment Variable: MAS_APP_ID Default: None Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" roles: - ibm.mas_devops.suite_app_uninstall License \u00a4 EPL-2.0","title":"suite_app_uninstall"},{"location":"roles/suite_app_uninstall/#suite_app_uninstall","text":"This role is used to uninstall a specified application in Maximo Application Suite.","title":"suite_app_uninstall"},{"location":"roles/suite_app_uninstall/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/suite_app_uninstall/#mas_instance_id","text":"Defines the MAS instance id from which an appplication will be uninstalled Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_app_uninstall/#mas_app_id","text":"Defines the kind of application that will be uninstalled such as assist , health , hputilities , iot , manage , monitor , mso , optimizer , predict , safety or visualinspection Required Environment Variable: MAS_APP_ID Default: None","title":"mas_app_id"},{"location":"roles/suite_app_uninstall/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" roles: - ibm.mas_devops.suite_app_uninstall","title":"Example Playbook"},{"location":"roles/suite_app_uninstall/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_upgrade/","text":"suite_app_upgrade \u00a4 This role will upgrade the subscription channel for an an installed MAS application after validating: That the application is installed and in a healthy state That the new version of the application can be upgraded to from the existing version That the new version of the application is compatible with the running MAS core platform Role Variables \u00a4 mas_instance_id \u00a4 Set the instance ID for the MAS installation where you wish to upgrade the application. Required Environment Variable: MAS_INSTANCE_ID Default Value: None mas_app_channel \u00a4 Select the subscription channel you wish to upgrade to. Built-in validation will ensure that the upgrade will only proceed if a supportable upgrade path is chosen. Required Environment Variable: MAS_APP_CHANNEL Default Value: None mas_upgrade_dryrun \u00a4 When set to true will ensure that the role only preforms upgrade validation checks and does not make any changes to the target installation. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: False skip_compatibility_check \u00a4 When set to true will skip compatibility check before the upgrade install. By default, compatibility check will be performed to validate the specific target mas_app_channel is valid or not based on the existing mas and apps version. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: False Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: iot mas_app_channel: 8.5.x roles: - ibm.mas_devops.suite_app_upgrade","title":"suite_app_upgrade"},{"location":"roles/suite_app_upgrade/#suite_app_upgrade","text":"This role will upgrade the subscription channel for an an installed MAS application after validating: That the application is installed and in a healthy state That the new version of the application can be upgraded to from the existing version That the new version of the application is compatible with the running MAS core platform","title":"suite_app_upgrade"},{"location":"roles/suite_app_upgrade/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_upgrade/#mas_instance_id","text":"Set the instance ID for the MAS installation where you wish to upgrade the application. Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_app_upgrade/#mas_app_channel","text":"Select the subscription channel you wish to upgrade to. Built-in validation will ensure that the upgrade will only proceed if a supportable upgrade path is chosen. Required Environment Variable: MAS_APP_CHANNEL Default Value: None","title":"mas_app_channel"},{"location":"roles/suite_app_upgrade/#mas_upgrade_dryrun","text":"When set to true will ensure that the role only preforms upgrade validation checks and does not make any changes to the target installation. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: False","title":"mas_upgrade_dryrun"},{"location":"roles/suite_app_upgrade/#skip_compatibility_check","text":"When set to true will skip compatibility check before the upgrade install. By default, compatibility check will be performed to validate the specific target mas_app_channel is valid or not based on the existing mas and apps version. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: False","title":"skip_compatibility_check"},{"location":"roles/suite_app_upgrade/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_app_id: iot mas_app_channel: 8.5.x roles: - ibm.mas_devops.suite_app_upgrade","title":"Example Playbook"},{"location":"roles/suite_app_verify/","text":"suite_app_verify \u00a4 Verify if a MAS application is ready to use. This is done by verifying if Workspace CR is in READY state. If CR is not in READY state, it repeats verification every minute for ten minutes (you can override this rule and add more time if needed, check Role Variables section for more details) Role Variables \u00a4 Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev mas_app_ws_apiversion: apps.mas.ibm.com/v1 mas_app_ws_kind: ManageWorkspace mas_app_namespace: mas-masinst1-manage roles: - ibm.mas_devops.suite_app_verify License \u00a4 EPL-2.0","title":"suite_app_verify"},{"location":"roles/suite_app_verify/#suite_app_verify","text":"Verify if a MAS application is ready to use. This is done by verifying if Workspace CR is in READY state. If CR is not in READY state, it repeats verification every minute for ten minutes (you can override this rule and add more time if needed, check Role Variables section for more details)","title":"suite_app_verify"},{"location":"roles/suite_app_verify/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_verify/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev mas_app_ws_apiversion: apps.mas.ibm.com/v1 mas_app_ws_kind: ManageWorkspace mas_app_namespace: mas-masinst1-manage roles: - ibm.mas_devops.suite_app_verify","title":"Example Playbook"},{"location":"roles/suite_app_verify/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_backup_restore/","text":"Backup and Restore MAS Core \u00a4 Overview \u00a4 This role supports backing up and restoring MAS Core namespace resources; supports creating on-demand or scheduled backup jobs for taking full or incremental backups, and optionally creating Kubernetes jobs for running the backup/restore process. Important A backup can only be restored to an instance with the same MAS instance ID. Role Variables - General \u00a4 masbr_action \u00a4 Set backup or restore to indicate the role to create a backup or restore job. Required Environment Variable: MAS_BR_ACTION Default: None mas_instance_id \u00a4 Defines the MAS instance ID for the backup or restore action. Required Environment Variable: MAS_INSTANCE_ID Default: None masbr_confirm_cluster \u00a4 Set true or false to indicate the role whether to confirm the currently connected cluster before running the backup or restore job. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false masbr_copy_timeout_sec \u00a4 Set the transfer files timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours) masbr_job_timezone \u00a4 Set the time zone for creating scheduled backup job. If not set a value for this variable, this role will use UTC time zone when creating a CronJob for running scheduled backup job. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None masbr_storage_type \u00a4 Set local or cloud to indicate this role to save the backup files to local file system or cloud object storage. Required Environment Variable: MASBR_STORAGE_TYPE Default: None masbr_storage_local_folder \u00a4 Set local path to save the backup files. Required only when MASBR_STORAGE_TYPE=local Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None masbr_storage_cloud_rclone_file \u00a4 Set the path of rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_FILE Default: None masbr_storage_cloud_rclone_name \u00a4 Set the configuration name defined in rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_NAME Default: None masbr_storage_cloud_bucket \u00a4 Set the object storage bucket name for saving the backup files Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_BUCKET Default: None masbr_slack_enabled \u00a4 Set true or false to indicate whether this role will send Slack notification messages of the backup and restore progress. Optional Environment Variable: MASBR_SLACK_ENABLED Default: false masbr_slack_level \u00a4 Set failure , info or verbose to indicate this role to send Slack notification messages in which backup and resore phases: Slack level Backup/Restore phases failure Failed , PartiallyFailed info Completed , Failed , PartiallyFailed verbose InProgress , Completed , Failed , PartiallyFailed Optional Environment Variable: MASBR_SLACK_LEVEL Default: info masbr_slack_token \u00a4 The Slack integration token. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_TOKEN Default: None masbr_slack_channel \u00a4 The Slack channel to send the notification messages to. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_CHANNEL Default: None masbr_slack_user \u00a4 The sender of the Slack notification message. Optional Environment Variable: MASBR_SLACK_USER Default: MASBR Role Variables - Backup \u00a4 masbr_backup_schedule \u00a4 Set Cron expression to create a scheduled backup. If not set a value for this varialbe, this role will create an on-demand backup. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None Role Variables - Restore \u00a4 masbr_restore_from_version \u00a4 Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) Required only when MAS_BR_ACTION=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None Example Playbook \u00a4 Backup \u00a4 Backup MAS Core namespace resources, note that this does not include backup of any data in MongoDb, see the backup action in the mongodb role. - hosts: localhost any_errors_fatal: true vars: masbr_action: backup mas_instance_id: main masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_backup_restore Restore \u00a4 Restore MAS Core namespace resources, note that this does not include backup of any data in MongoDb, see the restore action in the mongodb role. - hosts: localhost any_errors_fatal: true vars: masbr_action: restore masbr_restore_from_version: 20240621021316 mas_instance_id: main masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_backup_restore License \u00a4 EPL-2.0","title":"suite_backup_restore"},{"location":"roles/suite_backup_restore/#backup-and-restore-mas-core","text":"","title":"Backup and Restore MAS Core"},{"location":"roles/suite_backup_restore/#overview","text":"This role supports backing up and restoring MAS Core namespace resources; supports creating on-demand or scheduled backup jobs for taking full or incremental backups, and optionally creating Kubernetes jobs for running the backup/restore process. Important A backup can only be restored to an instance with the same MAS instance ID.","title":"Overview"},{"location":"roles/suite_backup_restore/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/suite_backup_restore/#masbr_action","text":"Set backup or restore to indicate the role to create a backup or restore job. Required Environment Variable: MAS_BR_ACTION Default: None","title":"masbr_action"},{"location":"roles/suite_backup_restore/#mas_instance_id","text":"Defines the MAS instance ID for the backup or restore action. Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_backup_restore/#masbr_confirm_cluster","text":"Set true or false to indicate the role whether to confirm the currently connected cluster before running the backup or restore job. Optional Environment Variable: MASBR_CONFIRM_CLUSTER Default: false","title":"masbr_confirm_cluster"},{"location":"roles/suite_backup_restore/#masbr_copy_timeout_sec","text":"Set the transfer files timeout in seconds. Optional Environment Variable: MASBR_COPY_TIMEOUT_SEC Default: 43200 (12 hours)","title":"masbr_copy_timeout_sec"},{"location":"roles/suite_backup_restore/#masbr_job_timezone","text":"Set the time zone for creating scheduled backup job. If not set a value for this variable, this role will use UTC time zone when creating a CronJob for running scheduled backup job. Optional Environment Variable: MASBR_JOB_TIMEZONE Default: None","title":"masbr_job_timezone"},{"location":"roles/suite_backup_restore/#masbr_storage_type","text":"Set local or cloud to indicate this role to save the backup files to local file system or cloud object storage. Required Environment Variable: MASBR_STORAGE_TYPE Default: None","title":"masbr_storage_type"},{"location":"roles/suite_backup_restore/#masbr_storage_local_folder","text":"Set local path to save the backup files. Required only when MASBR_STORAGE_TYPE=local Environment Variable: MASBR_STORAGE_LOCAL_FOLDER Default: None","title":"masbr_storage_local_folder"},{"location":"roles/suite_backup_restore/#masbr_storage_cloud_rclone_file","text":"Set the path of rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_FILE Default: None","title":"masbr_storage_cloud_rclone_file"},{"location":"roles/suite_backup_restore/#masbr_storage_cloud_rclone_name","text":"Set the configuration name defined in rclone.conf file. Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_RCLONE_NAME Default: None","title":"masbr_storage_cloud_rclone_name"},{"location":"roles/suite_backup_restore/#masbr_storage_cloud_bucket","text":"Set the object storage bucket name for saving the backup files Required only when MASBR_STORAGE_TYPE=cloud Environment Variable: MASBR_STORAGE_CLOUD_BUCKET Default: None","title":"masbr_storage_cloud_bucket"},{"location":"roles/suite_backup_restore/#masbr_slack_enabled","text":"Set true or false to indicate whether this role will send Slack notification messages of the backup and restore progress. Optional Environment Variable: MASBR_SLACK_ENABLED Default: false","title":"masbr_slack_enabled"},{"location":"roles/suite_backup_restore/#masbr_slack_level","text":"Set failure , info or verbose to indicate this role to send Slack notification messages in which backup and resore phases: Slack level Backup/Restore phases failure Failed , PartiallyFailed info Completed , Failed , PartiallyFailed verbose InProgress , Completed , Failed , PartiallyFailed Optional Environment Variable: MASBR_SLACK_LEVEL Default: info","title":"masbr_slack_level"},{"location":"roles/suite_backup_restore/#masbr_slack_token","text":"The Slack integration token. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_TOKEN Default: None","title":"masbr_slack_token"},{"location":"roles/suite_backup_restore/#masbr_slack_channel","text":"The Slack channel to send the notification messages to. Required only when MASBR_SLACK_ENABLED=true Environment Variable: MASBR_SLACK_CHANNEL Default: None","title":"masbr_slack_channel"},{"location":"roles/suite_backup_restore/#masbr_slack_user","text":"The sender of the Slack notification message. Optional Environment Variable: MASBR_SLACK_USER Default: MASBR","title":"masbr_slack_user"},{"location":"roles/suite_backup_restore/#role-variables-backup","text":"","title":"Role Variables - Backup"},{"location":"roles/suite_backup_restore/#masbr_backup_schedule","text":"Set Cron expression to create a scheduled backup. If not set a value for this varialbe, this role will create an on-demand backup. Optional Environment Variable: MASBR_BACKUP_SCHEDULE Default: None","title":"masbr_backup_schedule"},{"location":"roles/suite_backup_restore/#role-variables-restore","text":"","title":"Role Variables - Restore"},{"location":"roles/suite_backup_restore/#masbr_restore_from_version","text":"Set the backup version to use in the restore, this will be in the format of a YYYMMDDHHMMSS timestamp (e.g. 20240621021316 ) Required only when MAS_BR_ACTION=restore Environment Variable: MASBR_RESTORE_FROM_VERSION Default: None","title":"masbr_restore_from_version"},{"location":"roles/suite_backup_restore/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_backup_restore/#backup","text":"Backup MAS Core namespace resources, note that this does not include backup of any data in MongoDb, see the backup action in the mongodb role. - hosts: localhost any_errors_fatal: true vars: masbr_action: backup mas_instance_id: main masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_backup_restore","title":"Backup"},{"location":"roles/suite_backup_restore/#restore","text":"Restore MAS Core namespace resources, note that this does not include backup of any data in MongoDb, see the restore action in the mongodb role. - hosts: localhost any_errors_fatal: true vars: masbr_action: restore masbr_restore_from_version: 20240621021316 mas_instance_id: main masbr_storage_type: local masbr_storage_local_folder: /tmp/masbr roles: - ibm.mas_devops.suite_backup_restore","title":"Restore"},{"location":"roles/suite_backup_restore/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_certs/","text":"suite_certs \u00a4 This role iterates through the subdirectories in $MAS_CONFIG_DIR/certs which are named as core or name of the apps like monitor , manage , iot and so on. It looks for tls.crt, tls.key and ca.crt in these subdirectories. The names of the subdirectories in $MAS_CONFIG_DIR/certs are used to construct namespace to create/identify it and also creates the TLS secret with the tls/ca certs in those namespaces. So these subdirectories should be named correctly as the app names used in namespace suffixes. Directory structure example, \u00a4 $MAS_CONFIG_DIR/certs/core/tls.crt $MAS_CONFIG_DIR/certs/core/tls.key $MAS_CONFIG_DIR/certs/core/ca.crt $MAS_CONFIG_DIR/certs/<apps>/tls.crt $MAS_CONFIG_DIR/certs/<apps>/tls.key $MAS_CONFIG_DIR/certs/<apps>/ca.crt TLS Secret \u00a4 tls.crt , tls.key and ca.crt are mandatory files in these subdirectories. They are used to create TLS secret in each applications' namespace. The role will fail if an empty app subdirectory is present or an app subdirectory missing a mandatory file Note: \u00a4 Currently the secret names for core and each app are maintained in suite_certs/defaults/main.yml . Any changes to the existing secret name or adding new apps needs to be done here. Role Variables \u00a4 mas_instance_id \u00a4 The instance ID of the Maximo Application Suite installation to verify. Required Environment Variable: MAS_INSTANCE_ID Default Value: None mas_manual_cert_mgmt \u00a4 Set this to True if you want to enable manual certificate management mode. Environment Variable: MAS_MANUAL_CERT_MGMT Default Value: False mas_config_dir \u00a4 Path to the mas config directory. Required Environment Variable: MAS_CONFIG_DIR gitops \u00a4 Boolean flag to indicate whether to run role in gitops mode. True means that no openshift resources are created on the cluster. Optional Environment Variable: GITOPS Default Value: False Role Variables - CIS as DNS Provider (Optional) \u00a4 Optional variables for users using IBM Cloud Internet Services to manage DNS. This role will guarantee that your CNAMES related to MAS routes are created or updated in the informed CIS instance. dns_provider \u00a4 Set this to cis if you manage DNS using IBM Cloud Internet. If this variable is informed with a value different than cis it results in error (except blank, as it is optional). Optional Environment Variable: DNS_PROVIDER mas_workspace_id \u00a4 Workspace Id will be used as part of CNAMES definition when using cis as dns_provider. Required if dns_provider is defined and is cis Environment Variable: MAS_WORKSPACE_ID cis_crn \u00a4 CRN Key identifying the CIS in IBM Cloud. You can find that information in the page of your CIS instance. Required if dns_provider is defined and is cis Environment Variable: CIS_CRN cis_apikey \u00a4 API Key used to access the CIS in IBM CLoud. Required if dns_provider is defined and is cis Environment Variable: CIS_APIKEY cis_subdomain \u00a4 Subdomain will be used as part of CNAMES definition when using cis as dns_provider. Required if dns_provider is defined and is cis Environment Variable: CIS_SUBDOMAIN cis_proxy \u00a4 Set this to True if you want enable proxy in your CIS CNames leveraging security rules defined for this software. Optional Environment Variable: CIS_PROXY Default Value: False The directory structure for the certificates must be like below $MAS_CONFIG_DIR/certs/core/tls.crt $MAS_CONFIG_DIR/certs/core/tls.key $MAS_CONFIG_DIR/certs/core/ca.crt $MAS_CONFIG_DIR/certs/manage/tls.crt $MAS_CONFIG_DIR/certs/manage/tls.key $MAS_CONFIG_DIR/certs/manage/ca.crt $MAS_CONFIG_DIR/certs/<app>/tls.crt $MAS_CONFIG_DIR/certs/<app>/tls.key $MAS_CONFIG_DIR/certs/<app>/ca.crt the subdirectory name in the $MAS_CONFIG_DIR/certs directory is used to construct the namespace where the TLS secret will be applied to. So name the directory approriately. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_manual_cert_mgmt: True mas_config_dir: /Users/johnbarnes/Document/masconfig roles: - ibm.mas_devops.suite_certs More Detailed View of Directory Structure \u00a4 MAS_CONFIG_DIR | |---certs | | | | | |---core | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---iot | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---monitor | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---manage | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---add | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---assist | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---optimizer | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---visualinspection | | | | | |---tls.crt | | |---tls.key | | |---ca.crt License \u00a4 EPL-2.0","title":"suite_certs"},{"location":"roles/suite_certs/#suite_certs","text":"This role iterates through the subdirectories in $MAS_CONFIG_DIR/certs which are named as core or name of the apps like monitor , manage , iot and so on. It looks for tls.crt, tls.key and ca.crt in these subdirectories. The names of the subdirectories in $MAS_CONFIG_DIR/certs are used to construct namespace to create/identify it and also creates the TLS secret with the tls/ca certs in those namespaces. So these subdirectories should be named correctly as the app names used in namespace suffixes.","title":"suite_certs"},{"location":"roles/suite_certs/#directory-structure-example","text":"$MAS_CONFIG_DIR/certs/core/tls.crt $MAS_CONFIG_DIR/certs/core/tls.key $MAS_CONFIG_DIR/certs/core/ca.crt $MAS_CONFIG_DIR/certs/<apps>/tls.crt $MAS_CONFIG_DIR/certs/<apps>/tls.key $MAS_CONFIG_DIR/certs/<apps>/ca.crt","title":"Directory structure example,"},{"location":"roles/suite_certs/#tls-secret","text":"tls.crt , tls.key and ca.crt are mandatory files in these subdirectories. They are used to create TLS secret in each applications' namespace. The role will fail if an empty app subdirectory is present or an app subdirectory missing a mandatory file","title":"TLS Secret"},{"location":"roles/suite_certs/#note","text":"Currently the secret names for core and each app are maintained in suite_certs/defaults/main.yml . Any changes to the existing secret name or adding new apps needs to be done here.","title":"Note:"},{"location":"roles/suite_certs/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_certs/#mas_instance_id","text":"The instance ID of the Maximo Application Suite installation to verify. Required Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_certs/#mas_manual_cert_mgmt","text":"Set this to True if you want to enable manual certificate management mode. Environment Variable: MAS_MANUAL_CERT_MGMT Default Value: False","title":"mas_manual_cert_mgmt"},{"location":"roles/suite_certs/#mas_config_dir","text":"Path to the mas config directory. Required Environment Variable: MAS_CONFIG_DIR","title":"mas_config_dir"},{"location":"roles/suite_certs/#gitops","text":"Boolean flag to indicate whether to run role in gitops mode. True means that no openshift resources are created on the cluster. Optional Environment Variable: GITOPS Default Value: False","title":"gitops"},{"location":"roles/suite_certs/#role-variables-cis-as-dns-provider-optional","text":"Optional variables for users using IBM Cloud Internet Services to manage DNS. This role will guarantee that your CNAMES related to MAS routes are created or updated in the informed CIS instance.","title":"Role Variables - CIS as DNS Provider (Optional)"},{"location":"roles/suite_certs/#dns_provider","text":"Set this to cis if you manage DNS using IBM Cloud Internet. If this variable is informed with a value different than cis it results in error (except blank, as it is optional). Optional Environment Variable: DNS_PROVIDER","title":"dns_provider"},{"location":"roles/suite_certs/#mas_workspace_id","text":"Workspace Id will be used as part of CNAMES definition when using cis as dns_provider. Required if dns_provider is defined and is cis Environment Variable: MAS_WORKSPACE_ID","title":"mas_workspace_id"},{"location":"roles/suite_certs/#cis_crn","text":"CRN Key identifying the CIS in IBM Cloud. You can find that information in the page of your CIS instance. Required if dns_provider is defined and is cis Environment Variable: CIS_CRN","title":"cis_crn"},{"location":"roles/suite_certs/#cis_apikey","text":"API Key used to access the CIS in IBM CLoud. Required if dns_provider is defined and is cis Environment Variable: CIS_APIKEY","title":"cis_apikey"},{"location":"roles/suite_certs/#cis_subdomain","text":"Subdomain will be used as part of CNAMES definition when using cis as dns_provider. Required if dns_provider is defined and is cis Environment Variable: CIS_SUBDOMAIN","title":"cis_subdomain"},{"location":"roles/suite_certs/#cis_proxy","text":"Set this to True if you want enable proxy in your CIS CNames leveraging security rules defined for this software. Optional Environment Variable: CIS_PROXY Default Value: False The directory structure for the certificates must be like below $MAS_CONFIG_DIR/certs/core/tls.crt $MAS_CONFIG_DIR/certs/core/tls.key $MAS_CONFIG_DIR/certs/core/ca.crt $MAS_CONFIG_DIR/certs/manage/tls.crt $MAS_CONFIG_DIR/certs/manage/tls.key $MAS_CONFIG_DIR/certs/manage/ca.crt $MAS_CONFIG_DIR/certs/<app>/tls.crt $MAS_CONFIG_DIR/certs/<app>/tls.key $MAS_CONFIG_DIR/certs/<app>/ca.crt the subdirectory name in the $MAS_CONFIG_DIR/certs directory is used to construct the namespace where the TLS secret will be applied to. So name the directory approriately.","title":"cis_proxy"},{"location":"roles/suite_certs/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_manual_cert_mgmt: True mas_config_dir: /Users/johnbarnes/Document/masconfig roles: - ibm.mas_devops.suite_certs","title":"Example Playbook"},{"location":"roles/suite_certs/#more-detailed-view-of-directory-structure","text":"MAS_CONFIG_DIR | |---certs | | | | | |---core | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---iot | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---monitor | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---manage | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---add | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---assist | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---optimizer | | | | | |---tls.crt | | |---tls.key | | |---ca.crt | |---visualinspection | | | | | |---tls.crt | | |---tls.key | | |---ca.crt","title":"More Detailed View of Directory Structure"},{"location":"roles/suite_certs/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_config/","text":"suite_config \u00a4 TODO: Summarize role Role Variables \u00a4 TODO: Finish documentation Example Playbook \u00a4 TODO: Add example License \u00a4 EPL-2.0","title":"suite_config"},{"location":"roles/suite_config/#suite_config","text":"TODO: Summarize role","title":"suite_config"},{"location":"roles/suite_config/#role-variables","text":"TODO: Finish documentation","title":"Role Variables"},{"location":"roles/suite_config/#example-playbook","text":"TODO: Add example","title":"Example Playbook"},{"location":"roles/suite_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_db2_setup_for_manage/","text":"suite_db2_setup_for_manage \u00a4 This role shouldn't need to exist, it should be part of the Manage operator, but is not so we have to do it as a seperate step in the install flow for now. The role will perform some initial setup on the Db2 instance that is needed to prepare it for use with the Manage application and supports both CP4D version 3.5 and 4.0. The role will copy a bash script (setupdb.sh) into the Db2 pod and execute it inside the container, this script will perform a number of configuration changes to the database as well as configuring the tablespaces for Maximo Manage because the operator is not yet able to do this itself. Role Variables \u00a4 db2_instance_name \u00a4 The name of the db2 instance to execute the setup in. Required Environment Variable: DB2_INSTANCE_NAME Default Value: None db2_namespace \u00a4 The namespace where the Db2 instance is running. Optional Environment Variable: DB2_NAMESPACE Default Value: db2u db2_username \u00a4 The username that will be used to connect to the database specified by db2_dbname . Optional Environment Variable: None Default Value: db2inst1 db2_dbname \u00a4 The name of the database in the instance to connect to when executing the setup script. Optional Environment Variable: None Default Value: BLUDB db2_schema \u00a4 The name of the Manage schema where the hack should be targeted in. Optional Environment Variable: None Default Value: maximo db2_tablespace_data_size \u00a4 The size of the tablespace data in the database. Optional Environment Variable: DB2_TABLESPACE_DATA_SIZE Default Value: 5000 M db2_tablespace_index_size \u00a4 The size of the tablespace indexes in the database. Optional Environment Variable: DB2_TABLESPACE_INDEX_SIZE Default Value: 5000 M db2_config_version \u00a4 Version of the enhanced DB2 parameters, currently support 1.0.0 Required Environment Variable: DB2_CONFIG_VERSION Default: 1.0.0 enforce_db2_config \u00a4 Flag to indicate restart the DB2 instance or not, the enhanced DB2 parameters required restart DB2 instance, this will cause downtime, should execute during customer maintenance window or newly created DB2 instance if set to True Required Environment Variable: ENFORCE_DB2_CONFIG Default: True Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: db2_instancename: mydb2 db2_namespace: db2u db2_config_version: \"1.0.0\" # It will cause downtime if set to true, please be careful. enforce_db2_config: true roles: - ibm.mas_devops.suite_db2_setup_for_manage License \u00a4 EPL-2.0","title":"suite_db2_setup_for_manage"},{"location":"roles/suite_db2_setup_for_manage/#suite_db2_setup_for_manage","text":"This role shouldn't need to exist, it should be part of the Manage operator, but is not so we have to do it as a seperate step in the install flow for now. The role will perform some initial setup on the Db2 instance that is needed to prepare it for use with the Manage application and supports both CP4D version 3.5 and 4.0. The role will copy a bash script (setupdb.sh) into the Db2 pod and execute it inside the container, this script will perform a number of configuration changes to the database as well as configuring the tablespaces for Maximo Manage because the operator is not yet able to do this itself.","title":"suite_db2_setup_for_manage"},{"location":"roles/suite_db2_setup_for_manage/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_db2_setup_for_manage/#db2_instance_name","text":"The name of the db2 instance to execute the setup in. Required Environment Variable: DB2_INSTANCE_NAME Default Value: None","title":"db2_instance_name"},{"location":"roles/suite_db2_setup_for_manage/#db2_namespace","text":"The namespace where the Db2 instance is running. Optional Environment Variable: DB2_NAMESPACE Default Value: db2u","title":"db2_namespace"},{"location":"roles/suite_db2_setup_for_manage/#db2_username","text":"The username that will be used to connect to the database specified by db2_dbname . Optional Environment Variable: None Default Value: db2inst1","title":"db2_username"},{"location":"roles/suite_db2_setup_for_manage/#db2_dbname","text":"The name of the database in the instance to connect to when executing the setup script. Optional Environment Variable: None Default Value: BLUDB","title":"db2_dbname"},{"location":"roles/suite_db2_setup_for_manage/#db2_schema","text":"The name of the Manage schema where the hack should be targeted in. Optional Environment Variable: None Default Value: maximo","title":"db2_schema"},{"location":"roles/suite_db2_setup_for_manage/#db2_tablespace_data_size","text":"The size of the tablespace data in the database. Optional Environment Variable: DB2_TABLESPACE_DATA_SIZE Default Value: 5000 M","title":"db2_tablespace_data_size"},{"location":"roles/suite_db2_setup_for_manage/#db2_tablespace_index_size","text":"The size of the tablespace indexes in the database. Optional Environment Variable: DB2_TABLESPACE_INDEX_SIZE Default Value: 5000 M","title":"db2_tablespace_index_size"},{"location":"roles/suite_db2_setup_for_manage/#db2_config_version","text":"Version of the enhanced DB2 parameters, currently support 1.0.0 Required Environment Variable: DB2_CONFIG_VERSION Default: 1.0.0","title":"db2_config_version"},{"location":"roles/suite_db2_setup_for_manage/#enforce_db2_config","text":"Flag to indicate restart the DB2 instance or not, the enhanced DB2 parameters required restart DB2 instance, this will cause downtime, should execute during customer maintenance window or newly created DB2 instance if set to True Required Environment Variable: ENFORCE_DB2_CONFIG Default: True","title":"enforce_db2_config"},{"location":"roles/suite_db2_setup_for_manage/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: db2_instancename: mydb2 db2_namespace: db2u db2_config_version: \"1.0.0\" # It will cause downtime if set to true, please be careful. enforce_db2_config: true roles: - ibm.mas_devops.suite_db2_setup_for_manage","title":"Example Playbook"},{"location":"roles/suite_db2_setup_for_manage/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_dns/","text":"suite_dns \u00a4 This role will manage MAS and DNS provider integration. IBM Cloud Internet Services is the only supported DNS provider currently. It will also create a secure route (https://cp4d. ) to the CP4D web client using the custom domain used in this role. Note : this role will take no action when mas_manual_cert_mgmt is set to True DNS management \u00a4 There are two different ways this role controls DNS entries in the provider: Top Level DNS entries \u00a4 This mode will create the entries directly under your DNS zone. Use this when the DNS zone matches the MAS domain exactly. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mymas.mycompany.com then you will be creating top-level DNS entries for MAS, e.g. admin , home , & api . Subdomain DNS entries \u00a4 This mode will create DNS entries in the zone under a subdomain. Use this when your DNS zone will be used for more than just one MAS instance. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mycompany.com then you will be creating subdomain DNS entries for MAS, e.g. admin.mymas , home.mymas , & api.mymas . CIS and Cloudflare integrations support both mode of DNS management. A single optional variable is required to enable subdomain DNS management, in the examples above you would set these to mymas : cis_subdomain cloudflare_subdomain Let's Encrypt Integration \u00a4 Both the CIS and Cloudflare options also enable integration with Let's Encrypt for automatic certificate management via IBM Certificate Manager. Each will create a new ClusterIssuer which can be used when installing Maximo Application Suite: Cloudflare Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cloudflare-le-prod IBM Cloud Internet Services Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cis-le-prod If you want to use Let's Encrypt certificates in your MAS installation you will need to configure the mas_cluster_issuer variable in the suite_install role, setting it to the name of the ClusterIssuer as documented above. Note There are issues with how cert-manager works with LetsEncrypt staging servers. It creates a secret for the certificate that doesn't contain the LetsEncrypt CA, but the staging service does not use a well known cert so we end up with MAS unable to trust the certificates generated by LetsEncrypt staging. At present there is no workaround for this, so do not use the LetsEncrypt staging certificate issuer. Role Variables - General \u00a4 dns_provider \u00a4 Required Environment Variable: DNS_PROVIDER Default: None mas_instance_id: \u00a4 Required Environment Variable: MAS_INSTANCE_ID Default: None mas_workspace_id: \u00a4 Required Environment Variable: MAS_WORKSPACE_ID Default: None mas_domain \u00a4 Required Environment Variable: MAS_DOMAIN Default: None ocp_ingress \u00a4 Optional Environment Variable: OCP_INGRESS Default: None cert_manager_namespace \u00a4 Namespace where Certificate Manager is installed. Optional Environment Variable: CERT_MANAGER_NAMESPACE Default: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default: None Role Variables - Cloudflare DNS Integration \u00a4 cloudflare_email \u00a4 Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_EMAIL Default: None cloudflare_apitoken \u00a4 To generate an API token follow the Cloudflare documentation . Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_APITOKEN Default: None cloudflare_zone \u00a4 This is your domain name that is managed by Cloudflare (e.g. mydomain.com ). Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_ZONE Default: None cloudflare_subdomain \u00a4 Set this to the name of the subdomain under your Cloudflare domain where you would like to manage the MAS DNS entires if you don't want MAS to use the top-level domain itself. In other words cloudflare_subdomain.cloudflare_zone should equal mas_domain . Optional Environment Variable: CLOUDFLARE_SUBDOMAIN Default: None Role Variables - IBM Cloud Internet Services DNS Integration \u00a4 Note When using CIS integration, some resources will be installed in the cluster such as RBACs, API Services and CIS Webhook deployments. In OCP 4.12+, to avoid CIS webhook deployment failure at start up, this role will grant anyuid permission to cert-manager-webhook-ibm-cis service account so it can fully access the cert-manager-webhook-ibm-cis deployment pod as a workaround: oc adm policy add-scc-to-user anyuid -z cert-manager-webhook-ibm-cis -n ibm-common-services cis_email \u00a4 Required if dns_provider is set to cis . This is the e-mail that will be used in the Cluster Issuer resource, created by this role, to connect with the certificate manager (i.e. Let's Encrypt). Environment Variable: CIS_EMAIL Default: None cis_apikey \u00a4 Required if dns_provider is set to cis Environment Variable: CIS_APIKEY Default: None cis_crn \u00a4 Required if dns_provider is set to cis Environment Variable: CIS_CRN Default: None cis_subdomain \u00a4 Optional Environment Variable: CIS_SUBDOMAIN Default: None cis_enhanced_security \u00a4 Set this to true to enable the enhanced IBM CIS DNS integration security - which includes: - Enabling WAF firewall disabling rules that affect MAS application functionalities - Enabling Proxy for DNS entries - Using an expanded list of DNS entries - Ensuring there are no wildcard DNS entry in CIS - Creating Edge Certificates in CIS instance See https://cloud.ibm.com/docs/cis?topic=cis-manage-your-ibm-cis-for-optimal-security for more details. Optional Environment Variable: CIS_ENHANCED_SECURITY Default: false Role Variables - Enhanced IBM CIS DNS Integration Secruity \u00a4 See the \"cis_enhanced_security\" variable above for details. cis_waf \u00a4 Optional Environment Variable: CIS_WAF Default: true cis_proxy \u00a4 Optional Environment Variable: CIS_PROXY Default: false cis_service_name \u00a4 Set this to override the default CIS service name that would otherwise be created as {ClusterName}-cis-{mas_instance_id} (where Cluster Name is derived automatically from the cluster) Optional Environment Variable: CIS_SERVICE_NAME update_dns \u00a4 Set this to false if you want to not update DNS entries if they already exist Optional Environment Variable: UPDATE_DNS_ENTRIES Default: true delete_wildcards \u00a4 Set this to true to force deletion of wildcard dns entries in cis Optional Environment Variable: DELETE_WILDCARDS Default: false override_edge_certs \u00a4 Set this to false to not override and delete any existing edge certificates in cis instance when creating new edge certificates Optional Environment Variable: OVERRIDE_EDGE_CERTS Default: true output_dir \u00a4 Location to output the edge-routes-{mas_instance_id}.txt Optional Environment Variable: OUTPUT_DIR Default: . (which will set the directory file in ibm/mas_devops) Role Variables - AWS Route 53 \u00a4 Prerequisites \u00a4 To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Note In some cases, the Route 53 zone may not resolve the certificate challenges generated by IBM Certificate Manager, which could cause a problem while issuing the public certificates via Let's Encrypt. In this case, a manual workaround might be needed in cert-manager-controller pod to enable recursive nameservers. For more details on how to apply this workaround, refer to this documentation . This workaround is not automated, as most of the times this is not needed, but when it is, it requires the cert-manager-controller pod to be stopped, thus we want to avoid making this behavior as standard approach. route53_hosted_zone_name \u00a4 AWS Route53 Hosted Zone name. Required. Environment Variable: ROUTE53_HOSTED_ZONE_NAME Default Value: None route53_hosted_zone_region \u00a4 AWS Route53 Hosted Zone region. Required. Environment Variable: ROUTE53_HOSTED_ZONE_REGION Default Value: Same value as defined in AWS_REGION , or if none defined, then us-east-2 is the defaulted region. route53_subdomain \u00a4 If a subdomain is defined, this will be used to create the corresponding CNAME entries in the targeted Route53 hosted zone instance. Therefore, the Route53 subdomain + the Route53 hosted zone name defined, when combined, needs to match with the chosen MAS Domain, otherwise the DNS records won't be able to get resolved. Example: MAS Top Level Domain: my-mas-instance.mycompany.com AWS Route53 hosted zone name: mycompany.com AWS Route53 subdomain: my-mas-instance Optional. Environment Variable: ROUTE53_SUBDOMAIN Default Value: None route53_email \u00a4 AWS Route53 contact e-mail. Will be set in the cluster issuer created in order to receive alerts. Optional. Environment Variable: ROUTE53_EMAIL Default Value: None. Role Variables - CloudPak for Data \u00a4 cpd_instance_namespace \u00a4 Namespace where the Cloud Pak for Data is installed and the cpd route exists. If set, then this role will also attempt to configure public certificates to the CPD route using the DNS provider defined. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default Value: None. cpd_prod_issuer_name \u00a4 Define the certificate issuer responsible for generating the public certificate for your CPD route. If not set, then it will use same issuer set for MAS instance. Optional Environment Variable: CPD_PROD_ISSUER_NAME Default Value: Same certificate issuer used for MAS instance. cpd_custom_domain \u00a4 Define the custom domain for your CPD route. If not set, then it will use same domain set for MAS instance. Optional Environment Variable: CPD_CUSTOM_DOMAIN Default Value: cp4d.{{ mas_domain }} . Example Playbook - CIS or Cloudflare \u00a4 --- - hosts: localhost any_errors_fatal: true vars: dns_provider: cis OR cloudflare mas_instance_id: inst1 mas_domain: mydomain.com cis_crn: xxx cis_apikey: xxx cis_email: xxx roles: - ibm.mas_devops.suite_dns Example Playbook - AWS Route 53 \u00a4 --- - hosts: localhost any_errors_fatal: true vars: dns_provider: route53 mas_instance_id: inst1 mas_domain: inst1.mydomain.com aws_access_key_id: xxx aws_secret_access_key: xxx route53_hosted_zone_name: mydomain.com route53_hosted_zone_region: us-east-2 route53_subdomain: inst1 route53_email: anyemail@test.com roles: - ibm.mas_devops.suite_dns License \u00a4 EPL-2.0","title":"suite_dns"},{"location":"roles/suite_dns/#suite_dns","text":"This role will manage MAS and DNS provider integration. IBM Cloud Internet Services is the only supported DNS provider currently. It will also create a secure route (https://cp4d. ) to the CP4D web client using the custom domain used in this role. Note : this role will take no action when mas_manual_cert_mgmt is set to True","title":"suite_dns"},{"location":"roles/suite_dns/#dns-management","text":"There are two different ways this role controls DNS entries in the provider:","title":"DNS management"},{"location":"roles/suite_dns/#top-level-dns-entries","text":"This mode will create the entries directly under your DNS zone. Use this when the DNS zone matches the MAS domain exactly. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mymas.mycompany.com then you will be creating top-level DNS entries for MAS, e.g. admin , home , & api .","title":"Top Level DNS entries"},{"location":"roles/suite_dns/#subdomain-dns-entries","text":"This mode will create DNS entries in the zone under a subdomain. Use this when your DNS zone will be used for more than just one MAS instance. If your MAS installation will be using the domain mymas.mycompany.com and you have a DNS zone for mycompany.com then you will be creating subdomain DNS entries for MAS, e.g. admin.mymas , home.mymas , & api.mymas . CIS and Cloudflare integrations support both mode of DNS management. A single optional variable is required to enable subdomain DNS management, in the examples above you would set these to mymas : cis_subdomain cloudflare_subdomain","title":"Subdomain DNS entries"},{"location":"roles/suite_dns/#lets-encrypt-integration","text":"Both the CIS and Cloudflare options also enable integration with Let's Encrypt for automatic certificate management via IBM Certificate Manager. Each will create a new ClusterIssuer which can be used when installing Maximo Application Suite: Cloudflare Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cloudflare-le-prod IBM Cloud Internet Services Let's Encrypt ClusterIssuer: {{ mas_instance_id }}-cis-le-prod If you want to use Let's Encrypt certificates in your MAS installation you will need to configure the mas_cluster_issuer variable in the suite_install role, setting it to the name of the ClusterIssuer as documented above. Note There are issues with how cert-manager works with LetsEncrypt staging servers. It creates a secret for the certificate that doesn't contain the LetsEncrypt CA, but the staging service does not use a well known cert so we end up with MAS unable to trust the certificates generated by LetsEncrypt staging. At present there is no workaround for this, so do not use the LetsEncrypt staging certificate issuer.","title":"Let's Encrypt Integration"},{"location":"roles/suite_dns/#role-variables-general","text":"","title":"Role Variables - General"},{"location":"roles/suite_dns/#dns_provider","text":"Required Environment Variable: DNS_PROVIDER Default: None","title":"dns_provider"},{"location":"roles/suite_dns/#mas_instance_id","text":"Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id:"},{"location":"roles/suite_dns/#mas_workspace_id","text":"Required Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id:"},{"location":"roles/suite_dns/#mas_domain","text":"Required Environment Variable: MAS_DOMAIN Default: None","title":"mas_domain"},{"location":"roles/suite_dns/#ocp_ingress","text":"Optional Environment Variable: OCP_INGRESS Default: None","title":"ocp_ingress"},{"location":"roles/suite_dns/#cert_manager_namespace","text":"Namespace where Certificate Manager is installed. Optional Environment Variable: CERT_MANAGER_NAMESPACE Default: None","title":"cert_manager_namespace"},{"location":"roles/suite_dns/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default: None","title":"custom_labels"},{"location":"roles/suite_dns/#role-variables-cloudflare-dns-integration","text":"","title":"Role Variables - Cloudflare DNS Integration"},{"location":"roles/suite_dns/#cloudflare_email","text":"Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_EMAIL Default: None","title":"cloudflare_email"},{"location":"roles/suite_dns/#cloudflare_apitoken","text":"To generate an API token follow the Cloudflare documentation . Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_APITOKEN Default: None","title":"cloudflare_apitoken"},{"location":"roles/suite_dns/#cloudflare_zone","text":"This is your domain name that is managed by Cloudflare (e.g. mydomain.com ). Required if dns_provider is set to cloudflare Environment Variable: CLOUDFLARE_ZONE Default: None","title":"cloudflare_zone"},{"location":"roles/suite_dns/#cloudflare_subdomain","text":"Set this to the name of the subdomain under your Cloudflare domain where you would like to manage the MAS DNS entires if you don't want MAS to use the top-level domain itself. In other words cloudflare_subdomain.cloudflare_zone should equal mas_domain . Optional Environment Variable: CLOUDFLARE_SUBDOMAIN Default: None","title":"cloudflare_subdomain"},{"location":"roles/suite_dns/#role-variables-ibm-cloud-internet-services-dns-integration","text":"Note When using CIS integration, some resources will be installed in the cluster such as RBACs, API Services and CIS Webhook deployments. In OCP 4.12+, to avoid CIS webhook deployment failure at start up, this role will grant anyuid permission to cert-manager-webhook-ibm-cis service account so it can fully access the cert-manager-webhook-ibm-cis deployment pod as a workaround: oc adm policy add-scc-to-user anyuid -z cert-manager-webhook-ibm-cis -n ibm-common-services","title":"Role Variables - IBM Cloud Internet Services DNS Integration"},{"location":"roles/suite_dns/#cis_email","text":"Required if dns_provider is set to cis . This is the e-mail that will be used in the Cluster Issuer resource, created by this role, to connect with the certificate manager (i.e. Let's Encrypt). Environment Variable: CIS_EMAIL Default: None","title":"cis_email"},{"location":"roles/suite_dns/#cis_apikey","text":"Required if dns_provider is set to cis Environment Variable: CIS_APIKEY Default: None","title":"cis_apikey"},{"location":"roles/suite_dns/#cis_crn","text":"Required if dns_provider is set to cis Environment Variable: CIS_CRN Default: None","title":"cis_crn"},{"location":"roles/suite_dns/#cis_subdomain","text":"Optional Environment Variable: CIS_SUBDOMAIN Default: None","title":"cis_subdomain"},{"location":"roles/suite_dns/#cis_enhanced_security","text":"Set this to true to enable the enhanced IBM CIS DNS integration security - which includes: - Enabling WAF firewall disabling rules that affect MAS application functionalities - Enabling Proxy for DNS entries - Using an expanded list of DNS entries - Ensuring there are no wildcard DNS entry in CIS - Creating Edge Certificates in CIS instance See https://cloud.ibm.com/docs/cis?topic=cis-manage-your-ibm-cis-for-optimal-security for more details. Optional Environment Variable: CIS_ENHANCED_SECURITY Default: false","title":"cis_enhanced_security"},{"location":"roles/suite_dns/#role-variables-enhanced-ibm-cis-dns-integration-secruity","text":"See the \"cis_enhanced_security\" variable above for details.","title":"Role Variables - Enhanced IBM CIS DNS Integration Secruity"},{"location":"roles/suite_dns/#cis_waf","text":"Optional Environment Variable: CIS_WAF Default: true","title":"cis_waf"},{"location":"roles/suite_dns/#cis_proxy","text":"Optional Environment Variable: CIS_PROXY Default: false","title":"cis_proxy"},{"location":"roles/suite_dns/#cis_service_name","text":"Set this to override the default CIS service name that would otherwise be created as {ClusterName}-cis-{mas_instance_id} (where Cluster Name is derived automatically from the cluster) Optional Environment Variable: CIS_SERVICE_NAME","title":"cis_service_name"},{"location":"roles/suite_dns/#update_dns","text":"Set this to false if you want to not update DNS entries if they already exist Optional Environment Variable: UPDATE_DNS_ENTRIES Default: true","title":"update_dns"},{"location":"roles/suite_dns/#delete_wildcards","text":"Set this to true to force deletion of wildcard dns entries in cis Optional Environment Variable: DELETE_WILDCARDS Default: false","title":"delete_wildcards"},{"location":"roles/suite_dns/#override_edge_certs","text":"Set this to false to not override and delete any existing edge certificates in cis instance when creating new edge certificates Optional Environment Variable: OVERRIDE_EDGE_CERTS Default: true","title":"override_edge_certs"},{"location":"roles/suite_dns/#output_dir","text":"Location to output the edge-routes-{mas_instance_id}.txt Optional Environment Variable: OUTPUT_DIR Default: . (which will set the directory file in ibm/mas_devops)","title":"output_dir"},{"location":"roles/suite_dns/#role-variables-aws-route-53","text":"","title":"Role Variables - AWS Route 53"},{"location":"roles/suite_dns/#prerequisites","text":"To run this role successfully you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Note In some cases, the Route 53 zone may not resolve the certificate challenges generated by IBM Certificate Manager, which could cause a problem while issuing the public certificates via Let's Encrypt. In this case, a manual workaround might be needed in cert-manager-controller pod to enable recursive nameservers. For more details on how to apply this workaround, refer to this documentation . This workaround is not automated, as most of the times this is not needed, but when it is, it requires the cert-manager-controller pod to be stopped, thus we want to avoid making this behavior as standard approach.","title":"Prerequisites"},{"location":"roles/suite_dns/#route53_hosted_zone_name","text":"AWS Route53 Hosted Zone name. Required. Environment Variable: ROUTE53_HOSTED_ZONE_NAME Default Value: None","title":"route53_hosted_zone_name"},{"location":"roles/suite_dns/#route53_hosted_zone_region","text":"AWS Route53 Hosted Zone region. Required. Environment Variable: ROUTE53_HOSTED_ZONE_REGION Default Value: Same value as defined in AWS_REGION , or if none defined, then us-east-2 is the defaulted region.","title":"route53_hosted_zone_region"},{"location":"roles/suite_dns/#route53_subdomain","text":"If a subdomain is defined, this will be used to create the corresponding CNAME entries in the targeted Route53 hosted zone instance. Therefore, the Route53 subdomain + the Route53 hosted zone name defined, when combined, needs to match with the chosen MAS Domain, otherwise the DNS records won't be able to get resolved. Example: MAS Top Level Domain: my-mas-instance.mycompany.com AWS Route53 hosted zone name: mycompany.com AWS Route53 subdomain: my-mas-instance Optional. Environment Variable: ROUTE53_SUBDOMAIN Default Value: None","title":"route53_subdomain"},{"location":"roles/suite_dns/#route53_email","text":"AWS Route53 contact e-mail. Will be set in the cluster issuer created in order to receive alerts. Optional. Environment Variable: ROUTE53_EMAIL Default Value: None.","title":"route53_email"},{"location":"roles/suite_dns/#role-variables-cloudpak-for-data","text":"","title":"Role Variables - CloudPak for Data"},{"location":"roles/suite_dns/#cpd_instance_namespace","text":"Namespace where the Cloud Pak for Data is installed and the cpd route exists. If set, then this role will also attempt to configure public certificates to the CPD route using the DNS provider defined. Optional Environment Variable: CPD_INSTANCE_NAMESPACE Default Value: None.","title":"cpd_instance_namespace"},{"location":"roles/suite_dns/#cpd_prod_issuer_name","text":"Define the certificate issuer responsible for generating the public certificate for your CPD route. If not set, then it will use same issuer set for MAS instance. Optional Environment Variable: CPD_PROD_ISSUER_NAME Default Value: Same certificate issuer used for MAS instance.","title":"cpd_prod_issuer_name"},{"location":"roles/suite_dns/#cpd_custom_domain","text":"Define the custom domain for your CPD route. If not set, then it will use same domain set for MAS instance. Optional Environment Variable: CPD_CUSTOM_DOMAIN Default Value: cp4d.{{ mas_domain }} .","title":"cpd_custom_domain"},{"location":"roles/suite_dns/#example-playbook-cis-or-cloudflare","text":"--- - hosts: localhost any_errors_fatal: true vars: dns_provider: cis OR cloudflare mas_instance_id: inst1 mas_domain: mydomain.com cis_crn: xxx cis_apikey: xxx cis_email: xxx roles: - ibm.mas_devops.suite_dns","title":"Example Playbook - CIS or Cloudflare"},{"location":"roles/suite_dns/#example-playbook-aws-route-53","text":"--- - hosts: localhost any_errors_fatal: true vars: dns_provider: route53 mas_instance_id: inst1 mas_domain: inst1.mydomain.com aws_access_key_id: xxx aws_secret_access_key: xxx route53_hosted_zone_name: mydomain.com route53_hosted_zone_region: us-east-2 route53_subdomain: inst1 route53_email: anyemail@test.com roles: - ibm.mas_devops.suite_dns","title":"Example Playbook - AWS Route 53"},{"location":"roles/suite_dns/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_install/","text":"suite_install \u00a4 This role install Maximo Application Suite. It internally resolve the namespace based on the mas_instance_id as mas-{mas_instance_id}-core . Role Variables - Basic Install \u00a4 mas_catalog_source \u00a4 Defines the catalog to be used to install MAS. You can set it to ibm-operator-catalog for both release as well as for development install mas_channel \u00a4 Defines which channel of MAS to subscribe to Role Variables - Basic Configuration \u00a4 mas_domain \u00a4 Optional fact, if not provided the role will use the default cluster subdomain mas_instance_id \u00a4 Defines the instance id to be used for MAS installation mas_entitlement_key \u00a4 API Key for entitled registry. This password will be used to create the image pull secret. Set to with IBM entitlement key when installing release or use your artifactory apikey for dev. mas_config_dir \u00a4 Directory containing configuration files ( *.yaml and *.yml ) to be applied to the MAS installation. Intended for creating the various MAS custom resources to configure the suite post-install, but can be used to apply any kubernetes resource you need to customize any aspect of your cluster. Role Variables - Advanced Configuration \u00a4 certManager.namespace \u00a4 The namespace containing the cert-manager to be used by MAS mas_annotations \u00a4 Provide a list of comma-separated key=value pairs which will be applied as labels on all resources created. This variable takes a comma separated list of annotations. For example, to deploy your suite in non production mode, set this to mas.ibm.com/operationalMode=nonproduction or set MAS_ANNOTATIONS environment variable as export MAS_ANNOTATIONS=mas.ibm.com/operationalMode=nonproduction Optional Environment Variable: MAS_ANNOTATIONS Default: None mas_img_pull_policy \u00a4 Sets spec.settings.imagePullPolicy , controlling the pod image pull policies in the suite ( Always , IfNotPresent , Never ). When not set the built-in operator default image pull policy will be used. Optional Environment Variable: MAS_IMG_PULL_POLICY Default: None custom_labels \u00a4 Provide a list of comma-separated key=value pairs which will be applied as labels on all resources created. Optional Environment Variable: CUSTOM_LABELS Default: None mas_manual_cert_mgmt \u00a4 Boolean variable that, when set to True, enable manual certificate management. Optional Environment Variable: MAS_MANUAL_CERT_MGMT Default: False mas_trust_default_cas \u00a4 Boolean variable that defines whether default Certificate Authorities are included in MAS trust stores. This only has an effect with IBM Maximo Application Suite version 8.11 and above Optional Environment Variable: MAS_TRUST_DEFAULT_CAS Default: True mas_pod_templates_dir \u00a4 Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration files named ibm-mas-suite.yml , ibm-mas-coreidp.yml and ibm-data-dictionary-assetdatadictionary.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the Suite spec under a top level podTemplates element, e.g. podTemplates: {object} . For ibm-data-dictionary the podTemplates will be inserted into the Suite spec under settings->dataDictionary->podTemplates . The ibm-mas-suite operator will then pass this on to the AssetDataDictionary CR when available. For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None enable_IPv6 \u00a4 Boolean variable that indicates whether it is to install in an IPv6-enabled environment. If it is true, the suite CR will have the SingleStack for ipFamilyPolicy and [\"IPv6\"] for ipFamilies. These ipFamily properties will be populated to all the services. This is currently available only in internal fyre clusters at the RTP site for testing purpose. Optional Environment Variable: ENABLE_IPv6 Default: False mas_special_characters \u00a4 Set this to true to permit special characters in user IDs and usernames. The suite configuration record (CR) will include a property named userDataValidation with the option allowSpecialChars configured. Optional Environment Variable: MAS_SPECIAL_CHARACTERS Default: None eck_enable_logstash \u00a4 When set to true will result in the creation of filebeat-output Secret in the MAS Core namespace which will reconfigure all pods to send their logs to an instance of Logstash installed by the eck role instead of sending them to the pod log. Optional Environment Variable: ECK_ENABLE_LOGSTASH Default: false mas_enable_walkme \u00a4 Boolean variable that indicates whether to enable guided tour. Optional Environment Variable: MAS_ENABLE_WALKME Default: true Role Variables - Superuser Account \u00a4 The MAS Superuser account username and password can be customized during the install by setting both of these variable. mas_superuser_username \u00a4 Optional Environment Variable: MAS_SUPERUSER_USERNAME Default: None mas_superuser_password \u00a4 Optional Environment Variable: MAS_SUPERUSER_PASSWORD Default: None Role Variables - Developer Mode \u00a4 artifactory_username \u00a4 Required when using this role with development builds on Artifactory artifactory_token \u00a4 Required when using this role with development builds on Artifactory mas_icr_cp \u00a4 Defines the entitled registry from the images should be pulled from. Set this to cp.icr.io/cp when installing release version of MAS, docker-na-public.artifactory.swg-devops.com/wiotp-docker-local for dev, unless when on FYRE in which case use docker-na-proxy-svl.artifactory.swg-devops.com/wiotp-docker-local or docker-na-proxy-rtp.artifactory.swg-devops.com/wiotp-docker-local as appropriate. mas_icr_cpopen \u00a4 Defines the registry for non-entitled images, such as operators. Set this to icr.io/cpopen when installing release version of MAS or docker-na-public.artifactory.swg-devops.com/wiotp-docker-local/cpopen for dev (or corresponding FYRE proxies as appropriate). mas_entitlement_username \u00a4 Username for the IBM entitled registry. This username will be used to create the image pull secret. Set to cp when installing release or use your w3Id when using Artifactory. Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_config_dir: \"/home/david/masconfig\" mas_entitlement_key: \"{{ lookup('env', 'IBM_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify License \u00a4 EPL-2.0","title":"suite_install"},{"location":"roles/suite_install/#suite_install","text":"This role install Maximo Application Suite. It internally resolve the namespace based on the mas_instance_id as mas-{mas_instance_id}-core .","title":"suite_install"},{"location":"roles/suite_install/#role-variables-basic-install","text":"","title":"Role Variables - Basic Install"},{"location":"roles/suite_install/#mas_catalog_source","text":"Defines the catalog to be used to install MAS. You can set it to ibm-operator-catalog for both release as well as for development install","title":"mas_catalog_source"},{"location":"roles/suite_install/#mas_channel","text":"Defines which channel of MAS to subscribe to","title":"mas_channel"},{"location":"roles/suite_install/#role-variables-basic-configuration","text":"","title":"Role Variables - Basic Configuration"},{"location":"roles/suite_install/#mas_domain","text":"Optional fact, if not provided the role will use the default cluster subdomain","title":"mas_domain"},{"location":"roles/suite_install/#mas_instance_id","text":"Defines the instance id to be used for MAS installation","title":"mas_instance_id"},{"location":"roles/suite_install/#mas_entitlement_key","text":"API Key for entitled registry. This password will be used to create the image pull secret. Set to with IBM entitlement key when installing release or use your artifactory apikey for dev.","title":"mas_entitlement_key"},{"location":"roles/suite_install/#mas_config_dir","text":"Directory containing configuration files ( *.yaml and *.yml ) to be applied to the MAS installation. Intended for creating the various MAS custom resources to configure the suite post-install, but can be used to apply any kubernetes resource you need to customize any aspect of your cluster.","title":"mas_config_dir"},{"location":"roles/suite_install/#role-variables-advanced-configuration","text":"","title":"Role Variables - Advanced Configuration"},{"location":"roles/suite_install/#certmanagernamespace","text":"The namespace containing the cert-manager to be used by MAS","title":"certManager.namespace"},{"location":"roles/suite_install/#mas_annotations","text":"Provide a list of comma-separated key=value pairs which will be applied as labels on all resources created. This variable takes a comma separated list of annotations. For example, to deploy your suite in non production mode, set this to mas.ibm.com/operationalMode=nonproduction or set MAS_ANNOTATIONS environment variable as export MAS_ANNOTATIONS=mas.ibm.com/operationalMode=nonproduction Optional Environment Variable: MAS_ANNOTATIONS Default: None","title":"mas_annotations"},{"location":"roles/suite_install/#mas_img_pull_policy","text":"Sets spec.settings.imagePullPolicy , controlling the pod image pull policies in the suite ( Always , IfNotPresent , Never ). When not set the built-in operator default image pull policy will be used. Optional Environment Variable: MAS_IMG_PULL_POLICY Default: None","title":"mas_img_pull_policy"},{"location":"roles/suite_install/#custom_labels","text":"Provide a list of comma-separated key=value pairs which will be applied as labels on all resources created. Optional Environment Variable: CUSTOM_LABELS Default: None","title":"custom_labels"},{"location":"roles/suite_install/#mas_manual_cert_mgmt","text":"Boolean variable that, when set to True, enable manual certificate management. Optional Environment Variable: MAS_MANUAL_CERT_MGMT Default: False","title":"mas_manual_cert_mgmt"},{"location":"roles/suite_install/#mas_trust_default_cas","text":"Boolean variable that defines whether default Certificate Authorities are included in MAS trust stores. This only has an effect with IBM Maximo Application Suite version 8.11 and above Optional Environment Variable: MAS_TRUST_DEFAULT_CAS Default: True","title":"mas_trust_default_cas"},{"location":"roles/suite_install/#mas_pod_templates_dir","text":"Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration files named ibm-mas-suite.yml , ibm-mas-coreidp.yml and ibm-data-dictionary-assetdatadictionary.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the Suite spec under a top level podTemplates element, e.g. podTemplates: {object} . For ibm-data-dictionary the podTemplates will be inserted into the Suite spec under settings->dataDictionary->podTemplates . The ibm-mas-suite operator will then pass this on to the AssetDataDictionary CR when available. For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None","title":"mas_pod_templates_dir"},{"location":"roles/suite_install/#enable_ipv6","text":"Boolean variable that indicates whether it is to install in an IPv6-enabled environment. If it is true, the suite CR will have the SingleStack for ipFamilyPolicy and [\"IPv6\"] for ipFamilies. These ipFamily properties will be populated to all the services. This is currently available only in internal fyre clusters at the RTP site for testing purpose. Optional Environment Variable: ENABLE_IPv6 Default: False","title":"enable_IPv6"},{"location":"roles/suite_install/#mas_special_characters","text":"Set this to true to permit special characters in user IDs and usernames. The suite configuration record (CR) will include a property named userDataValidation with the option allowSpecialChars configured. Optional Environment Variable: MAS_SPECIAL_CHARACTERS Default: None","title":"mas_special_characters"},{"location":"roles/suite_install/#eck_enable_logstash","text":"When set to true will result in the creation of filebeat-output Secret in the MAS Core namespace which will reconfigure all pods to send their logs to an instance of Logstash installed by the eck role instead of sending them to the pod log. Optional Environment Variable: ECK_ENABLE_LOGSTASH Default: false","title":"eck_enable_logstash"},{"location":"roles/suite_install/#mas_enable_walkme","text":"Boolean variable that indicates whether to enable guided tour. Optional Environment Variable: MAS_ENABLE_WALKME Default: true","title":"mas_enable_walkme"},{"location":"roles/suite_install/#role-variables-superuser-account","text":"The MAS Superuser account username and password can be customized during the install by setting both of these variable.","title":"Role Variables - Superuser Account"},{"location":"roles/suite_install/#mas_superuser_username","text":"Optional Environment Variable: MAS_SUPERUSER_USERNAME Default: None","title":"mas_superuser_username"},{"location":"roles/suite_install/#mas_superuser_password","text":"Optional Environment Variable: MAS_SUPERUSER_PASSWORD Default: None","title":"mas_superuser_password"},{"location":"roles/suite_install/#role-variables-developer-mode","text":"","title":"Role Variables - Developer Mode"},{"location":"roles/suite_install/#artifactory_username","text":"Required when using this role with development builds on Artifactory","title":"artifactory_username"},{"location":"roles/suite_install/#artifactory_token","text":"Required when using this role with development builds on Artifactory","title":"artifactory_token"},{"location":"roles/suite_install/#mas_icr_cp","text":"Defines the entitled registry from the images should be pulled from. Set this to cp.icr.io/cp when installing release version of MAS, docker-na-public.artifactory.swg-devops.com/wiotp-docker-local for dev, unless when on FYRE in which case use docker-na-proxy-svl.artifactory.swg-devops.com/wiotp-docker-local or docker-na-proxy-rtp.artifactory.swg-devops.com/wiotp-docker-local as appropriate.","title":"mas_icr_cp"},{"location":"roles/suite_install/#mas_icr_cpopen","text":"Defines the registry for non-entitled images, such as operators. Set this to icr.io/cpopen when installing release version of MAS or docker-na-public.artifactory.swg-devops.com/wiotp-docker-local/cpopen for dev (or corresponding FYRE proxies as appropriate).","title":"mas_icr_cpopen"},{"location":"roles/suite_install/#mas_entitlement_username","text":"Username for the IBM entitled registry. This username will be used to create the image pull secret. Set to cp when installing release or use your w3Id when using Artifactory.","title":"mas_entitlement_username"},{"location":"roles/suite_install/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_config_dir: \"/home/david/masconfig\" mas_entitlement_key: \"{{ lookup('env', 'IBM_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify","title":"Example Playbook"},{"location":"roles/suite_install/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_attachments_config/","text":"suite_manage_attachments_config \u00a4 This role extends support for configuring IBM Cloud Object Storage or Persistent Volume/File Storages for Manage application attachments. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior configuring attachments features. By default, Manage attachments configuration uses filestorage provider; it corresponds to your cluster's default file storage system to persist the files. Alternatively, you can provide an existing IBM Cloud Object Storage by using ibm provider, or even provision a new instance by using cos role. Finally, an existing AWS S3 service can also be provided via aws provider by this same role (see details in the Role Varilables below) Role Variables \u00a4 mas_manage_attachments_provider \u00a4 Required. Defines the storage provider type to be used to store Manage application's attachments. Available options are: filestorage (default option): Configures cluster's file storage system for Manage attachments. ibm : Configures IBM Cloud Object Storage as storage system for Manage attachments. aws : Configures Amazon S3 buckets as storage system for Manage attachments. Note: If using ibm or aws as attachments provider, the cos_bucket role will be executed to setup a new or existing targeted COS bucket to be used to store Manage attachments, therefore make sure you set the expected variables to customize your COS bucket for Manage attachments, i.e. COS_APIKEY and COS_INSTANCE_NAME . Note about S3: To run this role successfully for AWS s3 buckets, you must have already installed the AWS CLI .Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Environment Variable: MAS_MANAGE_ATTACHMENTS_PROVIDER Default Value: filestorage mas_manage_attachment_configuration_mode \u00a4 Required. Defines how attachment properties will be configured in Manage. Possible values are cr and db . When cr is selected, attachment properties will be entered in ManageWorkspace CR for each bundle, under bundleProperties key. For this mode, manage_workspace_cr_name must be informed; When db is selected, attachment properties will be updated directly in the database via SQL updates. For this mode, db2_instance_name , db2_namespace and db2_dbname must be informed. Environment Variable: MAS_MANAGE_ATTACHMENT_CONFIGURATION_MODE Default Value: db mas_instance_id \u00a4 Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \u00a4 Required. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None manage_workspace_cr_name \u00a4 Optional. Required when mas_manage_attachment_configuration_mode is set as cr . Name of the ManageWorkspace Custom Resource that will be targeted to configure the new PVC definitions. Environment Variable: MANAGE_WORKSPACE_CR_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID db2_instance_name \u00a4 Optional. Required when mas_manage_attachment_configuration_mode is set as db . The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. Note : in order to obtain the value for this variable, go to the namespace where db2 is installed and look for the pod where label=engine . Select/describe the pod representing your database, and look for the value of label app . That is your db2 instance name. Environment Variable: DB2_INSTANCE_NAME # e.g. db2u-iot or db2wh-1658148844550964 Default Value: None db2_namespace \u00a4 Optional. Required when mas_manage_attachment_configuration_mode is set as db . The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Environment Variable: DB2_NAMESPACE # e.g. db2u Default Value: db2u db2_dbname \u00a4 Optional. Required when mas_manage_attachment_configuration_mode is set as db . Name of the database within the instance. Environment Variable: DB2_DBNAME Default: BLUDB Example Playbook \u00a4 The following sample can be used to configure COS for an existing Manage application instance's attachments via ManageWorkspace CR update (note cr as configuration mode + ma_instance_id and mas_workspace_id , which will be used to infer the CR name): - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: cr mas_instance_id: masinst1 mas_workspace_id: masdev cos_instance_name: cos-masinst1 cos_bucket_name: manage-attachments-bucket ibmcloud_apikey: xxxx mas_manage_attachments_provider: ibm roles: - ibm.mas_devops.suite_manage_attachments_config The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance's attachments via SQL updates in database (note db as configuration mode + db2_instance_name ): - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: db mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_instance_name: cos-masinst1 cos_bucket_name: manage-attachments-bucket ibmcloud_apikey: xxxx mas_manage_attachments_provider: ibm roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_attachments_config The following sample playbook can be used to deploy Manage with default persistent storage for Manage attachments (PVC mount path /DOCLINKS ), and configure Manage system properties with the corresponding attachments settings; via SQL updates in database: - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: db mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_manage_attachments_provider: filestorage roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_attachments_config The following sample can be used to configure AWS S3 buckets for an existing Manage application instance's attachments. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage aws_bucket_name: manage-attachments-bucket mas_manage_attachments_provider: aws roles: - ibm.mas_devops.suite_manage_attachments_config License \u00a4 EPL-2.0","title":"suite_manage_attachments_config"},{"location":"roles/suite_manage_attachments_config/#suite_manage_attachments_config","text":"This role extends support for configuring IBM Cloud Object Storage or Persistent Volume/File Storages for Manage application attachments. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior configuring attachments features. By default, Manage attachments configuration uses filestorage provider; it corresponds to your cluster's default file storage system to persist the files. Alternatively, you can provide an existing IBM Cloud Object Storage by using ibm provider, or even provision a new instance by using cos role. Finally, an existing AWS S3 service can also be provided via aws provider by this same role (see details in the Role Varilables below)","title":"suite_manage_attachments_config"},{"location":"roles/suite_manage_attachments_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_attachments_config/#mas_manage_attachments_provider","text":"Required. Defines the storage provider type to be used to store Manage application's attachments. Available options are: filestorage (default option): Configures cluster's file storage system for Manage attachments. ibm : Configures IBM Cloud Object Storage as storage system for Manage attachments. aws : Configures Amazon S3 buckets as storage system for Manage attachments. Note: If using ibm or aws as attachments provider, the cos_bucket role will be executed to setup a new or existing targeted COS bucket to be used to store Manage attachments, therefore make sure you set the expected variables to customize your COS bucket for Manage attachments, i.e. COS_APIKEY and COS_INSTANCE_NAME . Note about S3: To run this role successfully for AWS s3 buckets, you must have already installed the AWS CLI .Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Environment Variable: MAS_MANAGE_ATTACHMENTS_PROVIDER Default Value: filestorage","title":"mas_manage_attachments_provider"},{"location":"roles/suite_manage_attachments_config/#mas_manage_attachment_configuration_mode","text":"Required. Defines how attachment properties will be configured in Manage. Possible values are cr and db . When cr is selected, attachment properties will be entered in ManageWorkspace CR for each bundle, under bundleProperties key. For this mode, manage_workspace_cr_name must be informed; When db is selected, attachment properties will be updated directly in the database via SQL updates. For this mode, db2_instance_name , db2_namespace and db2_dbname must be informed. Environment Variable: MAS_MANAGE_ATTACHMENT_CONFIGURATION_MODE Default Value: db","title":"mas_manage_attachment_configuration_mode"},{"location":"roles/suite_manage_attachments_config/#mas_instance_id","text":"Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_attachments_config/#mas_workspace_id","text":"Required. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/suite_manage_attachments_config/#manage_workspace_cr_name","text":"Optional. Required when mas_manage_attachment_configuration_mode is set as cr . Name of the ManageWorkspace Custom Resource that will be targeted to configure the new PVC definitions. Environment Variable: MANAGE_WORKSPACE_CR_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID","title":"manage_workspace_cr_name"},{"location":"roles/suite_manage_attachments_config/#db2_instance_name","text":"Optional. Required when mas_manage_attachment_configuration_mode is set as db . The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. Note : in order to obtain the value for this variable, go to the namespace where db2 is installed and look for the pod where label=engine . Select/describe the pod representing your database, and look for the value of label app . That is your db2 instance name. Environment Variable: DB2_INSTANCE_NAME # e.g. db2u-iot or db2wh-1658148844550964 Default Value: None","title":"db2_instance_name"},{"location":"roles/suite_manage_attachments_config/#db2_namespace","text":"Optional. Required when mas_manage_attachment_configuration_mode is set as db . The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Environment Variable: DB2_NAMESPACE # e.g. db2u Default Value: db2u","title":"db2_namespace"},{"location":"roles/suite_manage_attachments_config/#db2_dbname","text":"Optional. Required when mas_manage_attachment_configuration_mode is set as db . Name of the database within the instance. Environment Variable: DB2_DBNAME Default: BLUDB","title":"db2_dbname"},{"location":"roles/suite_manage_attachments_config/#example-playbook","text":"The following sample can be used to configure COS for an existing Manage application instance's attachments via ManageWorkspace CR update (note cr as configuration mode + ma_instance_id and mas_workspace_id , which will be used to infer the CR name): - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: cr mas_instance_id: masinst1 mas_workspace_id: masdev cos_instance_name: cos-masinst1 cos_bucket_name: manage-attachments-bucket ibmcloud_apikey: xxxx mas_manage_attachments_provider: ibm roles: - ibm.mas_devops.suite_manage_attachments_config The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance's attachments via SQL updates in database (note db as configuration mode + db2_instance_name ): - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: db mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_instance_name: cos-masinst1 cos_bucket_name: manage-attachments-bucket ibmcloud_apikey: xxxx mas_manage_attachments_provider: ibm roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_attachments_config The following sample playbook can be used to deploy Manage with default persistent storage for Manage attachments (PVC mount path /DOCLINKS ), and configure Manage system properties with the corresponding attachments settings; via SQL updates in database: - hosts: localhost any_errors_fatal: true vars: mas_manage_attachment_configuration_mode: db mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_manage_attachments_provider: filestorage roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_attachments_config The following sample can be used to configure AWS S3 buckets for an existing Manage application instance's attachments. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage aws_bucket_name: manage-attachments-bucket mas_manage_attachments_provider: aws roles: - ibm.mas_devops.suite_manage_attachments_config","title":"Example Playbook"},{"location":"roles/suite_manage_attachments_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_bim_config/","text":"suite_manage_bim_config \u00a4 This role extends support for configuring existing PVC mounted path for BIM (Building Information Models) in Manage application. In order for this task to run successfully your Manage application must have been configured with a proper persistent volume and mounted path. You can run suite_app_config with mas_app_settings_persistent_volumes_flag: true while installing mas_app_id: manage to have a default persistent storage configured as part of Manage deployment that can be used in this role to setup BIM. For more details on how to configure persistent storage for Manage refer to Configuring persistent volume claims . Role Variables \u00a4 mas_app_settings_bim_mount_path \u00a4 Required. Defines the persistent volume mount path to be used while configuring Manage BIM folders. If you used suite_app_config role to configure the persistent volumes while deploying Manage application, the default BIM persistent volume mount path will be the same. Environment Variable: MAS_APP_SETTINGS_BIM_MOUNT_PATH Default Value: /bim . mas_instance_id \u00a4 Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None db2_instance_name \u00a4 Required. The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the BIM system properties. Environment Variable: DB2_INSTANCE_NAME # e.g. db2u-manage Default Value: None db2_namespace \u00a4 Optional. The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the with the BIM system properties. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Environment Variable: DB2_NAMESPACE # e.g. db2u Default Value: db2u db2_dbname \u00a4 Name of the database within the instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB Example Playbook \u00a4 The following sample can be used to configure BIM for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 db2_instance_name: db2w-manage mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.suite_manage_bim_config The following sample playbook can be used to deploy Manage with default persistent storage for BIM (PVC mount path /bim ), and configure Manage system properties with the corresponding BIM settings. - hosts: localhost any_errors_fatal: true vars: mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_bim_config","title":"suite_manage_bim_config"},{"location":"roles/suite_manage_bim_config/#suite_manage_bim_config","text":"This role extends support for configuring existing PVC mounted path for BIM (Building Information Models) in Manage application. In order for this task to run successfully your Manage application must have been configured with a proper persistent volume and mounted path. You can run suite_app_config with mas_app_settings_persistent_volumes_flag: true while installing mas_app_id: manage to have a default persistent storage configured as part of Manage deployment that can be used in this role to setup BIM. For more details on how to configure persistent storage for Manage refer to Configuring persistent volume claims .","title":"suite_manage_bim_config"},{"location":"roles/suite_manage_bim_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_bim_config/#mas_app_settings_bim_mount_path","text":"Required. Defines the persistent volume mount path to be used while configuring Manage BIM folders. If you used suite_app_config role to configure the persistent volumes while deploying Manage application, the default BIM persistent volume mount path will be the same. Environment Variable: MAS_APP_SETTINGS_BIM_MOUNT_PATH Default Value: /bim .","title":"mas_app_settings_bim_mount_path"},{"location":"roles/suite_manage_bim_config/#mas_instance_id","text":"Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_bim_config/#db2_instance_name","text":"Required. The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the BIM system properties. Environment Variable: DB2_INSTANCE_NAME # e.g. db2u-manage Default Value: None","title":"db2_instance_name"},{"location":"roles/suite_manage_bim_config/#db2_namespace","text":"Optional. The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the with the BIM system properties. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Environment Variable: DB2_NAMESPACE # e.g. db2u Default Value: db2u","title":"db2_namespace"},{"location":"roles/suite_manage_bim_config/#db2_dbname","text":"Name of the database within the instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB","title":"db2_dbname"},{"location":"roles/suite_manage_bim_config/#example-playbook","text":"The following sample can be used to configure BIM for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 db2_instance_name: db2w-manage mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.suite_manage_bim_config The following sample playbook can be used to deploy Manage with default persistent storage for BIM (PVC mount path /bim ), and configure Manage system properties with the corresponding BIM settings. - hosts: localhost any_errors_fatal: true vars: mas_app_id: manage mas_app_channel: 8.4.x mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage mas_app_settings_persistent_volumes_flag: true mas_app_settings_bim_mount_path: /bim roles: - ibm.mas_devops.db2 - ibm.mas_devops.suite_db2_setup_for_manage - ibm.mas_devops.suite_config - ibm.mas_devops.suite_app_install - ibm.mas_devops.suite_app_config - ibm.mas_devops.suite_manage_bim_config","title":"Example Playbook"},{"location":"roles/suite_manage_birt_report_config/","text":"suite_manage_birt_report_config \u00a4 This role extends support for configuring Birt Report in Manage application as a separate and dedicated report bundle server workload. The following Manage properties will be added to every and each Manage server bundle: mxe.report.birt.viewerurl = https://{{ mas_workspace_id }}-{{ manage_report_bundle_server_name }}.manage.{{ mas_domain }} mxe.report.birt.disablequeuemanager = 0 (if bundle type = report ) or 1 (if bundle type != report ) The goal for this role is to setup the specific Manage Report route to be the endpoint for the generated reports in Manage (which will forward the report workload to the dedicated report type bundle pod). Role Variables \u00a4 mas_instance_id \u00a4 Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \u00a4 Required. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None manage_workspace_cr_name \u00a4 Optional. Name of the ManageWorkspace Custom Resource that will be targeted to configure the new PVC definitions. Environment Variable: MANAGE_WORKSPACE_CR_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID manage_report_bundle_server_name \u00a4 Optional. Name of the Manage report bundle server. It will be used to configure the Manage's report bundle server and its corresponding route. Not needed if the report bundle server is already configured. Environment Variable: MANAGE_REPORT_BUNDLE_SERVER_NAME Default Value: rpt Example Playbook \u00a4 The following sample can be used to configure BIRT report for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: main roles: - ibm.mas_devops.suite_manage_birt_report_config Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ```bash export MAS_INSTANCE_ID=masinst1 export MAS_WORKSPACE_ID=main export MANAGE_REPORT_BUNDLE_SERVER_NAME=report ROLE_NAME='suite_manage_birt_report_config' ansible-playbook playbooks/run_role.yml License \u00a4 EPL-2.0","title":"suite_manage_birt_report_config"},{"location":"roles/suite_manage_birt_report_config/#suite_manage_birt_report_config","text":"This role extends support for configuring Birt Report in Manage application as a separate and dedicated report bundle server workload. The following Manage properties will be added to every and each Manage server bundle: mxe.report.birt.viewerurl = https://{{ mas_workspace_id }}-{{ manage_report_bundle_server_name }}.manage.{{ mas_domain }} mxe.report.birt.disablequeuemanager = 0 (if bundle type = report ) or 1 (if bundle type != report ) The goal for this role is to setup the specific Manage Report route to be the endpoint for the generated reports in Manage (which will forward the report workload to the dedicated report type bundle pod).","title":"suite_manage_birt_report_config"},{"location":"roles/suite_manage_birt_report_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_birt_report_config/#mas_instance_id","text":"Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_birt_report_config/#mas_workspace_id","text":"Required. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/suite_manage_birt_report_config/#manage_workspace_cr_name","text":"Optional. Name of the ManageWorkspace Custom Resource that will be targeted to configure the new PVC definitions. Environment Variable: MANAGE_WORKSPACE_CR_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID","title":"manage_workspace_cr_name"},{"location":"roles/suite_manage_birt_report_config/#manage_report_bundle_server_name","text":"Optional. Name of the Manage report bundle server. It will be used to configure the Manage's report bundle server and its corresponding route. Not needed if the report bundle server is already configured. Environment Variable: MANAGE_REPORT_BUNDLE_SERVER_NAME Default Value: rpt","title":"manage_report_bundle_server_name"},{"location":"roles/suite_manage_birt_report_config/#example-playbook","text":"The following sample can be used to configure BIRT report for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: main roles: - ibm.mas_devops.suite_manage_birt_report_config","title":"Example Playbook"},{"location":"roles/suite_manage_birt_report_config/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ```bash export MAS_INSTANCE_ID=masinst1 export MAS_WORKSPACE_ID=main export MANAGE_REPORT_BUNDLE_SERVER_NAME=report ROLE_NAME='suite_manage_birt_report_config' ansible-playbook playbooks/run_role.yml","title":"Run Role Playbook"},{"location":"roles/suite_manage_birt_report_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_customer_files_config/","text":"suite_manage_customer_files_config \u00a4 This role extends support for configuring S3 / Cloud Object Storage to store Manage application customer files. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior configuring customer files features. You can run cos role to provision an IBM Cloud Object Storage or you can provide existing IBM Cloud Object Storage or AWS S3 information to use it as storage for Manage application customer files. As part of this role, three defaulted buckets will be created: \"{{ mas_instance_id }}-{{ mas_workspace_id }}-custfiles\" \"{{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesbackup\" \"{{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesrecovery\" These buckets will be used to store Manage's customer documents and files, and also will be used on the backup and recovery process. Role Variables \u00a4 cos_type \u00a4 Required. Defines the storage provider type to be used to store Manage application's customer files. Currently available options are: ibm : Configures IBM Cloud Object Storage as storage system for Manage attachments. aws : Configures Amazon S3 buckets as storage system for Manage attachments. Note: If using ibm or aws as attachments provider, the cos_bucket role will be executed to setup a new or existing targeted COS bucket to be used to store Manage attachments, therefore make sure you set the expected variables to customize your COS bucket for Manage attachments. To run this role successfully for AWS S3 buckets, you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Environment Variable: COS_TYPE Default Value: None. custfiles_bucketname \u00a4 Optional. The main customer files bucket name. Environment Variable: MANAGE_CUSTFILES_BUCKET_NAME Default Value: {{ mas_instance_id }}-{{ mas_workspace_id }}-custfiles custfiles_bucketname_backup \u00a4 Optional. The customer files bucket name used for backup. Environment Variable: MANAGE_CUSTFILES_BACKUP_BUCKET_NAME Default Value: {{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesbackup custfiles_bucketname_recovery \u00a4 Optional. The customer files bucket name used for recovery. Environment Variable: MANAGE_CUSTFILES_RECOVERY_BUCKET_NAME Default Value: {{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesrecovery mas_instance_id \u00a4 Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \u00a4 Required. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None manage_workspace_cr_name \u00a4 Optional. Name of the ManageWorkspace Custom Resource that will be targeted to configure the new PVC definitions. Environment Variable: MANAGE_WORKSPACE_CR_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID Example Playbook \u00a4 The following sample can be used to configure COS for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx roles: - ibm.mas_devops.suite_manage_customer_files_config The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_customer_files_config The following sample can be used to configure AWS S3 buckets for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: aws roles: - ibm.mas_devops.suite_manage_customer_files_config License \u00a4 EPL-2.0","title":"suite_manage_customer_files_config"},{"location":"roles/suite_manage_customer_files_config/#suite_manage_customer_files_config","text":"This role extends support for configuring S3 / Cloud Object Storage to store Manage application customer files. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior configuring customer files features. You can run cos role to provision an IBM Cloud Object Storage or you can provide existing IBM Cloud Object Storage or AWS S3 information to use it as storage for Manage application customer files. As part of this role, three defaulted buckets will be created: \"{{ mas_instance_id }}-{{ mas_workspace_id }}-custfiles\" \"{{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesbackup\" \"{{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesrecovery\" These buckets will be used to store Manage's customer documents and files, and also will be used on the backup and recovery process.","title":"suite_manage_customer_files_config"},{"location":"roles/suite_manage_customer_files_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_customer_files_config/#cos_type","text":"Required. Defines the storage provider type to be used to store Manage application's customer files. Currently available options are: ibm : Configures IBM Cloud Object Storage as storage system for Manage attachments. aws : Configures Amazon S3 buckets as storage system for Manage attachments. Note: If using ibm or aws as attachments provider, the cos_bucket role will be executed to setup a new or existing targeted COS bucket to be used to store Manage attachments, therefore make sure you set the expected variables to customize your COS bucket for Manage attachments. To run this role successfully for AWS S3 buckets, you must have already installed the AWS CLI . Also, you need to have AWS user credentials configured via aws configure command or simply export AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables with your corresponding AWS username credentials prior running this role. Environment Variable: COS_TYPE Default Value: None.","title":"cos_type"},{"location":"roles/suite_manage_customer_files_config/#custfiles_bucketname","text":"Optional. The main customer files bucket name. Environment Variable: MANAGE_CUSTFILES_BUCKET_NAME Default Value: {{ mas_instance_id }}-{{ mas_workspace_id }}-custfiles","title":"custfiles_bucketname"},{"location":"roles/suite_manage_customer_files_config/#custfiles_bucketname_backup","text":"Optional. The customer files bucket name used for backup. Environment Variable: MANAGE_CUSTFILES_BACKUP_BUCKET_NAME Default Value: {{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesbackup","title":"custfiles_bucketname_backup"},{"location":"roles/suite_manage_customer_files_config/#custfiles_bucketname_recovery","text":"Optional. The customer files bucket name used for recovery. Environment Variable: MANAGE_CUSTFILES_RECOVERY_BUCKET_NAME Default Value: {{ mas_instance_id }}-{{ mas_workspace_id }}-custfilesrecovery","title":"custfiles_bucketname_recovery"},{"location":"roles/suite_manage_customer_files_config/#mas_instance_id","text":"Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_customer_files_config/#mas_workspace_id","text":"Required. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/suite_manage_customer_files_config/#manage_workspace_cr_name","text":"Optional. Name of the ManageWorkspace Custom Resource that will be targeted to configure the new PVC definitions. Environment Variable: MANAGE_WORKSPACE_CR_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID","title":"manage_workspace_cr_name"},{"location":"roles/suite_manage_customer_files_config/#example-playbook","text":"The following sample can be used to configure COS for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx roles: - ibm.mas_devops.suite_manage_customer_files_config The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm cos_instance_name: cos-masinst1 ibmcloud_apikey: xxxx roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_customer_files_config The following sample can be used to configure AWS S3 buckets for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: aws roles: - ibm.mas_devops.suite_manage_customer_files_config","title":"Example Playbook"},{"location":"roles/suite_manage_customer_files_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_imagestitching_config/","text":"suite_manage_imagestitching_config \u00a4 This role configures the manage workspace to autodeploy the image stitching application. The role will only make changes if the Civil component has been installed. It will configure a PVC, patch the ManageWorkspace CR with the name of the PVC and also patch PV specifications based on the PVC. It also sets two key system properties required by the image stitching application: 'mci.imagestitching.apiurl' and 'imagestitching.dataInputPath'. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running in order to set system properties. Role Variables \u00a4 stitching_pvcname \u00a4 Required. The postfix for the PVC created for image stitching. The ManageWorkspace CR will prepend 'mas_instance_id-mas_workspace_id-' to generate the actual name of the PVC Environment Variable: IMAGESTITCHING_PVCNAME Default Value: manage-imagestitching stitching_storage_class \u00a4 Optional : The storage class used for the PVC. If not specified the default value will be found by discovery. The storage class must support ReadWriteMany(RWX) access mode. Environment Variable: IMAGESTITCHING_STORAGE_CLASS Default Value: None stitching_storage_size \u00a4 Required : The size of the persistent volume claim. Environment Variable: IMAGESTITCHING_STORAGE_SIZE Default Value: 20 Gi stitching_storage_mode \u00a4 Required : The access mode for the PVC. Environment Variable: IMAGESTITCHING_STORAGE_MODE Default Value: ReadWriteMany stitching_storage_mountpath \u00a4 Required : The mount path of the Persistent Volume. Environment Variable: IMAGESTITCHING_STORAGE_MOUNTPATH Default Value: imagestitching mas_instance_id \u00a4 Required :. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \u00a4 Required :. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None mas_domain \u00a4 Required : The domain name for the Manage cluster Environment Variable: MAS_DOMAIN Default Value: Discovered from Suite CR Example Playbook \u00a4 The following sample will configure image stitching for an existing Manage application instance via ManageWorkspace CR update: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: civil mas_workspace_id: masdev mas_domain: civil.ibmmasdev.com stitching_pvcname: manage-imagestitching stitching_storage_class: nfs-client stitching_storage_size: 20Gi stitching_storage_mode: ReadWriteMany stitching_storage_mountpath: imagestitching roles: - ibm.mas_devops.suite_manage_imagestitching_config License \u00a4 EPL-2.0","title":"suite_manage_imagestitching_config"},{"location":"roles/suite_manage_imagestitching_config/#suite_manage_imagestitching_config","text":"This role configures the manage workspace to autodeploy the image stitching application. The role will only make changes if the Civil component has been installed. It will configure a PVC, patch the ManageWorkspace CR with the name of the PVC and also patch PV specifications based on the PVC. It also sets two key system properties required by the image stitching application: 'mci.imagestitching.apiurl' and 'imagestitching.dataInputPath'. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running in order to set system properties.","title":"suite_manage_imagestitching_config"},{"location":"roles/suite_manage_imagestitching_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_imagestitching_config/#stitching_pvcname","text":"Required. The postfix for the PVC created for image stitching. The ManageWorkspace CR will prepend 'mas_instance_id-mas_workspace_id-' to generate the actual name of the PVC Environment Variable: IMAGESTITCHING_PVCNAME Default Value: manage-imagestitching","title":"stitching_pvcname"},{"location":"roles/suite_manage_imagestitching_config/#stitching_storage_class","text":"Optional : The storage class used for the PVC. If not specified the default value will be found by discovery. The storage class must support ReadWriteMany(RWX) access mode. Environment Variable: IMAGESTITCHING_STORAGE_CLASS Default Value: None","title":"stitching_storage_class"},{"location":"roles/suite_manage_imagestitching_config/#stitching_storage_size","text":"Required : The size of the persistent volume claim. Environment Variable: IMAGESTITCHING_STORAGE_SIZE Default Value: 20 Gi","title":"stitching_storage_size"},{"location":"roles/suite_manage_imagestitching_config/#stitching_storage_mode","text":"Required : The access mode for the PVC. Environment Variable: IMAGESTITCHING_STORAGE_MODE Default Value: ReadWriteMany","title":"stitching_storage_mode"},{"location":"roles/suite_manage_imagestitching_config/#stitching_storage_mountpath","text":"Required : The mount path of the Persistent Volume. Environment Variable: IMAGESTITCHING_STORAGE_MOUNTPATH Default Value: imagestitching","title":"stitching_storage_mountpath"},{"location":"roles/suite_manage_imagestitching_config/#mas_instance_id","text":"Required :. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_imagestitching_config/#mas_workspace_id","text":"Required :. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/suite_manage_imagestitching_config/#mas_domain","text":"Required : The domain name for the Manage cluster Environment Variable: MAS_DOMAIN Default Value: Discovered from Suite CR","title":"mas_domain"},{"location":"roles/suite_manage_imagestitching_config/#example-playbook","text":"The following sample will configure image stitching for an existing Manage application instance via ManageWorkspace CR update: - hosts: localhost any_errors_fatal: true vars: mas_instance_id: civil mas_workspace_id: masdev mas_domain: civil.ibmmasdev.com stitching_pvcname: manage-imagestitching stitching_storage_class: nfs-client stitching_storage_size: 20Gi stitching_storage_mode: ReadWriteMany stitching_storage_mountpath: imagestitching roles: - ibm.mas_devops.suite_manage_imagestitching_config","title":"Example Playbook"},{"location":"roles/suite_manage_imagestitching_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_import_certs_config/","text":"suite_manage_import_certs_config \u00a4 This role extends support for importing certificates into Manage application's workspace. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior importing new certificates. You can run this as standalone role, providing a local path for a file that contains the Manage certificates definition ( manage_certificates_file_path_local variable). Or you can invoke this role inside another playbook/role, passing the Manage certificates content as a list variable ( manage_certificates ) and an alias prefix as a string variable ( manage_certificates_alias_prefix ). The certificate alias name will be concatenated with the alias prefix plus auto incremented accordingly to the number of certificates provided i.e If you provide a list with 3 certificates, and define manage_certificates_alias_prefix: myaliasprefixpart , then the alias name will be myaliasprefixpart1; myaliasprefixpart2; myaliasprefixpart3 Role Variables \u00a4 mas_instance_id \u00a4 Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \u00a4 Required. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None manage_workspace_cr_name \u00a4 Optional. Name of the ManageWorkspace Custom Resource that will be targeted to import the new certificates. Environment Variable: MANAGE_WORKSPACE_CR_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID manage_certificates_file_path_local \u00a4 Required if running as standalone role. This defines a local path pointing the certificates definition from a custom file. Sample file definition can be found in files/manage-certs-sample.yml . Environment Variable: MANAGE_CERTIFICATES_FILE_PATH_LOCAL Default Value: None Example Playbook \u00a4 The following sample can be used to import Manage certificates for an existing Manage instance, using a local path pointing the certificates definition from a custom file. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev manage_certificates_file_path_local: /my-path/manage-certs.yml roles: - ibm.mas_devops.suite_manage_import_certs_config The following sample can be used to import Manage certificates for an existing Manage instance, passing the certificates and prefix from a variable. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev manage_certificates: ['-----BEGIN CERTIFICATE----- << your-cert-content >> -----END CERTIFICATE-----'] manage_certificates_alias_prefix: \"myaliasprefixpart\" roles: - ibm.mas_devops.suite_manage_import_certs_config Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ```bash export MAS_INSTANCE_ID=masinst1 export MAS_WORKSPACE_ID=masdev export MANAGE_CERTIFICATES_FILE_PATH_LOCAL=/my-path/manage-certs.yml ROLE_NAME='suite_manage_import_certs_config' ansible-playbook playbooks/run_role.yml License \u00a4 EPL-2.0","title":"suite_manage_import_certs_config"},{"location":"roles/suite_manage_import_certs_config/#suite_manage_import_certs_config","text":"This role extends support for importing certificates into Manage application's workspace. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior importing new certificates. You can run this as standalone role, providing a local path for a file that contains the Manage certificates definition ( manage_certificates_file_path_local variable). Or you can invoke this role inside another playbook/role, passing the Manage certificates content as a list variable ( manage_certificates ) and an alias prefix as a string variable ( manage_certificates_alias_prefix ). The certificate alias name will be concatenated with the alias prefix plus auto incremented accordingly to the number of certificates provided i.e If you provide a list with 3 certificates, and define manage_certificates_alias_prefix: myaliasprefixpart , then the alias name will be myaliasprefixpart1; myaliasprefixpart2; myaliasprefixpart3","title":"suite_manage_import_certs_config"},{"location":"roles/suite_manage_import_certs_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_import_certs_config/#mas_instance_id","text":"Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_import_certs_config/#mas_workspace_id","text":"Required. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/suite_manage_import_certs_config/#manage_workspace_cr_name","text":"Optional. Name of the ManageWorkspace Custom Resource that will be targeted to import the new certificates. Environment Variable: MANAGE_WORKSPACE_CR_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID","title":"manage_workspace_cr_name"},{"location":"roles/suite_manage_import_certs_config/#manage_certificates_file_path_local","text":"Required if running as standalone role. This defines a local path pointing the certificates definition from a custom file. Sample file definition can be found in files/manage-certs-sample.yml . Environment Variable: MANAGE_CERTIFICATES_FILE_PATH_LOCAL Default Value: None","title":"manage_certificates_file_path_local"},{"location":"roles/suite_manage_import_certs_config/#example-playbook","text":"The following sample can be used to import Manage certificates for an existing Manage instance, using a local path pointing the certificates definition from a custom file. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev manage_certificates_file_path_local: /my-path/manage-certs.yml roles: - ibm.mas_devops.suite_manage_import_certs_config The following sample can be used to import Manage certificates for an existing Manage instance, passing the certificates and prefix from a variable. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev manage_certificates: ['-----BEGIN CERTIFICATE----- << your-cert-content >> -----END CERTIFICATE-----'] manage_certificates_alias_prefix: \"myaliasprefixpart\" roles: - ibm.mas_devops.suite_manage_import_certs_config","title":"Example Playbook"},{"location":"roles/suite_manage_import_certs_config/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ```bash export MAS_INSTANCE_ID=masinst1 export MAS_WORKSPACE_ID=masdev export MANAGE_CERTIFICATES_FILE_PATH_LOCAL=/my-path/manage-certs.yml ROLE_NAME='suite_manage_import_certs_config' ansible-playbook playbooks/run_role.yml","title":"Run Role Playbook"},{"location":"roles/suite_manage_import_certs_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_load_dbc_scripts/","text":"suite_manage_load_dbc_scripts \u00a4 This role allows to load and execute one or more ad-hoc DBC script files into Manage/Health server. Only dbc format files will be accepted. The role will assert if each script executed successfully and fail in case of errors while locating the DBC scripts or executing them against the Manage/Health server. Role Variables \u00a4 mas_instance_id \u00a4 Required. Defines the instance id that was used for the MAS installation. It is used to lookup the Manage/Health namespace. - Environment Variable: MAS_INSTANCE_ID - Default Value: None mas_app_id \u00a4 Required. - Environment Variable: MAS_APP_ID - One of [ health , manage ] - Default Value: None dbc_script_path_local \u00a4 Optional. Defines the local path/folder where the DBC script files should be located in order to be loaded onto the Manage/Health server. Environment Variable: DBC_SCRIPT_PATH_LOCAL Default Value: suite_manage_load_dbc_scripts/files Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" dbc_script_path_local: \"{{ lookup('env', 'DBC_SCRIPT_PATH_LOCAL') }}\" roles: - ibm.mas_devops.suite_manage_load_dbc_scripts License \u00a4 EPL-2.0","title":"suite_manage_load_dbc_scripts"},{"location":"roles/suite_manage_load_dbc_scripts/#suite_manage_load_dbc_scripts","text":"This role allows to load and execute one or more ad-hoc DBC script files into Manage/Health server. Only dbc format files will be accepted. The role will assert if each script executed successfully and fail in case of errors while locating the DBC scripts or executing them against the Manage/Health server.","title":"suite_manage_load_dbc_scripts"},{"location":"roles/suite_manage_load_dbc_scripts/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_load_dbc_scripts/#mas_instance_id","text":"Required. Defines the instance id that was used for the MAS installation. It is used to lookup the Manage/Health namespace. - Environment Variable: MAS_INSTANCE_ID - Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_load_dbc_scripts/#mas_app_id","text":"Required. - Environment Variable: MAS_APP_ID - One of [ health , manage ] - Default Value: None","title":"mas_app_id"},{"location":"roles/suite_manage_load_dbc_scripts/#dbc_script_path_local","text":"Optional. Defines the local path/folder where the DBC script files should be located in order to be loaded onto the Manage/Health server. Environment Variable: DBC_SCRIPT_PATH_LOCAL Default Value: suite_manage_load_dbc_scripts/files","title":"dbc_script_path_local"},{"location":"roles/suite_manage_load_dbc_scripts/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" dbc_script_path_local: \"{{ lookup('env', 'DBC_SCRIPT_PATH_LOCAL') }}\" roles: - ibm.mas_devops.suite_manage_load_dbc_scripts","title":"Example Playbook"},{"location":"roles/suite_manage_load_dbc_scripts/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_logging_config/","text":"suite_manage_logging_config \u00a4 This role extends support for configuring IBM Cloud Object Storage to storage Manage application server logs. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior configuring logging features. The default for Manage logging configuration is to use IBM Cloud Object Storage as persistent storage for Manage logging. You can run cos role to provision an IBM Cloud Object Storage or you can provide existing IBM Cloud Object Storage information to use it as storage for Manage application logs. In addition, you can also define a AWS S3 bucket as storage system for Manage logs. Role Variables \u00a4 cos_type \u00a4 Required. Defines the storage provider type to be used to store Manage application's logs. Available options are: ibm : Configures IBM Cloud Object Storage as storage system for Manage logging. aws : Configures AWS S3 buckets as storage system for Manage logging. When running this role, the cos_bucket role will be executed underneath the covers to setup a new or existing targeted IBM Cloud object or AWS S3 storage bucket to be used to store Manage logs, therefore make sure you set the expected variables to customize your Object Storage bucket accordingly to the desired provider. Environment Variable: COS_TYPE Default Value: None. mas_instance_id \u00a4 Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \u00a4 Required. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None db2_instance_name \u00a4 Required. The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. Environment Variable: DB2_INSTANCE_NAME # e.g. db2u-iot or db2wh-1658148844550964 Default Value: None db2_namespace \u00a4 Optional. The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Environment Variable: DB2_NAMESPACE # e.g. db2u Default Value: db2u db2_dbname \u00a4 Name of the database within the instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB Example Playbook \u00a4 The following sample can be used to configure COS for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm db2_instance_name: db2w-manage cos_instance_name: cos-masinst1 cos_bucket_name: manage-logs-bucket ibmcloud_apikey: xxxx roles: - ibm.mas_devops.suite_manage_logging_config The following sample can be used to configure AWS S3 for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: aws cos_bucket_action: create aws_bucket_name: manage-logs-bucket aws_region: us-east-2 aws_bucket_versioning_flag: True aws_bucket_encryption: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}' db2_instance_name: db2w-manage roles: - ibm.mas_devops.suite_manage_logging_config The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_type: ibm cos_instance_name: cos-masinst1 cos_bucket_name: manage-logs-bucket ibmcloud_apikey: xxxx roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_logging_config License \u00a4 EPL-2.0","title":"suite_manage_logging_config"},{"location":"roles/suite_manage_logging_config/#suite_manage_logging_config","text":"This role extends support for configuring IBM Cloud Object Storage to storage Manage application server logs. Note: This role should be executed after Manage application is deployed and activated as it needs Manage up and running prior configuring logging features. The default for Manage logging configuration is to use IBM Cloud Object Storage as persistent storage for Manage logging. You can run cos role to provision an IBM Cloud Object Storage or you can provide existing IBM Cloud Object Storage information to use it as storage for Manage application logs. In addition, you can also define a AWS S3 bucket as storage system for Manage logs.","title":"suite_manage_logging_config"},{"location":"roles/suite_manage_logging_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_logging_config/#cos_type","text":"Required. Defines the storage provider type to be used to store Manage application's logs. Available options are: ibm : Configures IBM Cloud Object Storage as storage system for Manage logging. aws : Configures AWS S3 buckets as storage system for Manage logging. When running this role, the cos_bucket role will be executed underneath the covers to setup a new or existing targeted IBM Cloud object or AWS S3 storage bucket to be used to store Manage logs, therefore make sure you set the expected variables to customize your Object Storage bucket accordingly to the desired provider. Environment Variable: COS_TYPE Default Value: None.","title":"cos_type"},{"location":"roles/suite_manage_logging_config/#mas_instance_id","text":"Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_logging_config/#mas_workspace_id","text":"Required. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/suite_manage_logging_config/#db2_instance_name","text":"Required. The DB2 Warehouse instance name that stores your Manage application tables and data. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. Environment Variable: DB2_INSTANCE_NAME # e.g. db2u-iot or db2wh-1658148844550964 Default Value: None","title":"db2_instance_name"},{"location":"roles/suite_manage_logging_config/#db2_namespace","text":"Optional. The namespace in your cluster that hosts the DB2 Warehouse instance name. This will be used to lookup for Manage application database and update it with the IBM Object Storage configuration. If you do not provide it, the role will try to find the Db2 Warehouse in db2u namespace. Environment Variable: DB2_NAMESPACE # e.g. db2u Default Value: db2u","title":"db2_namespace"},{"location":"roles/suite_manage_logging_config/#db2_dbname","text":"Name of the database within the instance. Optional Environment Variable: DB2_DBNAME Default: BLUDB","title":"db2_dbname"},{"location":"roles/suite_manage_logging_config/#example-playbook","text":"The following sample can be used to configure COS for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: ibm db2_instance_name: db2w-manage cos_instance_name: cos-masinst1 cos_bucket_name: manage-logs-bucket ibmcloud_apikey: xxxx roles: - ibm.mas_devops.suite_manage_logging_config The following sample can be used to configure AWS S3 for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev cos_type: aws cos_bucket_action: create aws_bucket_name: manage-logs-bucket aws_region: us-east-2 aws_bucket_versioning_flag: True aws_bucket_encryption: '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}' db2_instance_name: db2w-manage roles: - ibm.mas_devops.suite_manage_logging_config The following sample playbook can be used to provision COS in IBM Cloud and configure COS for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev db2_instance_name: db2w-manage cos_type: ibm cos_instance_name: cos-masinst1 cos_bucket_name: manage-logs-bucket ibmcloud_apikey: xxxx roles: - ibm.mas_devops.cos - ibm.mas_devops.suite_manage_logging_config","title":"Example Playbook"},{"location":"roles/suite_manage_logging_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_manage_pvc_config/","text":"suite_manage_pvc_config \u00a4 This role extends support for configuring persistent volume claims for Manage application. Note This role should be executed after Manage application is deployed and activated because it needs Manage up and running prior to configuring the additional persistent volume claims. The are two options to setup new Manage PVCS: Exporting Manage PVCs variables Loading Manage PVCs variables from a file Role Variables \u00a4 mas_instance_id \u00a4 Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \u00a4 Required. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None manage_workspace_cr_name \u00a4 Optional. Name of the ManageWorkspace Custom Resource that will be targeted to configure the new PVC definitions. Environment Variable: MANAGE_WORKSPACE_CR_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID mas_app_settings_custom_persistent_volume_pvc_name \u00a4 Optional. Name of the Persistent Volume Claim to be configured. Environment Variable: MAS_APP_SETTINGS_CUSTOM_PVC_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID-cust-files-pvc mas_app_settings_custom_persistent_volume_pv_name \u00a4 Optional. Name of the volume that will be created to store data from the PVC. Make sure your storage class provider supports the name you define. If not sure about what to set, you can leave it unset then a random name will the defined. Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_NAME Default Value: None. mas_app_settings_custom_persistent_volume_pvc_size \u00a4 Optional. Size of the Persistent Volume Claim. Environment Variable: MAS_APP_SETTINGS_CUSTOM_PVC_SIZE Default Value: 100Gi mas_app_settings_custom_persistent_volume_mount_path \u00a4 Optional. Mouth path of the Persistent Volume in the Manage server container. Environment Variable: MAS_APP_SETTINGS_CUSTOM_MOUNT_PATH Default Value: /MeaGlobalDirs mas_app_settings_custom_persistent_volume_sc_name \u00a4 Optional. Persistent Volume Claim Storage Class. If not set, it will be automatically defined accordingly to your cluster's available storage classes. Both ReadWriteMany(RWX) and ReadWriteOnce(RWO) are supported. Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_STORAGE_CLASS Default Value: None. mas_app_settings_custom_persistent_volume_file_path \u00a4 Optional. Alternatively, this defines a local path pointing the persistent volume definition from a custom file. Sample file definition can be found in files/manage-persistent-volumes-sample.yml . Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_FILE_PATH Default Value: None. Example Playbook \u00a4 The following sample can be used to configure new PVCs for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev mas_app_settings_custom_persistent_volume_sc_name: \"ibmc-file-gold-gid\" mas_app_settings_custom_persistent_volume_pvc_name: \"my-manage-pvc\" mas_app_settings_custom_persistent_volume_pvc_size: \"20Gi\" mas_app_settings_custom_persistent_volume_mount_path: \"/MyOwnFolder\" roles: - ibm.mas_devops.suite_manage_pvc_config The following sample can be used to configure new PVCs for an existing Manage application instance from a custom file definition. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev mas_app_settings_custom_persistent_volume_file_path: \"/my-path/manage-pv.yml\" roles: - ibm.mas_devops.suite_manage_pvc_config Run Role Playbook \u00a4 After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ```bash export MAS_APP_SETTINGS_CUSTOM_PV_STORAGE_CLASS=ibmc-file-silver-gid export MAS_APP_SETTINGS_CUSTOM_PVC_NAME=my-manage-pvc export MAS_APP_SETTINGS_CUSTOM_PVC_SIZE=20Gi export MAS_APP_SETTINGS_CUSTOM_MOUNT_PATH=/MyOwnDir export MAS_APP_SETTINGS_CUSTOM_PV_FILE_PATH=/my-path/manage-pv.yml ROLE_NAME='suite_manage_pvc_config' ansible-playbook playbooks/run_role.yml License \u00a4 EPL-2.0","title":"suite_manage_pvc_config"},{"location":"roles/suite_manage_pvc_config/#suite_manage_pvc_config","text":"This role extends support for configuring persistent volume claims for Manage application. Note This role should be executed after Manage application is deployed and activated because it needs Manage up and running prior to configuring the additional persistent volume claims. The are two options to setup new Manage PVCS: Exporting Manage PVCs variables Loading Manage PVCs variables from a file","title":"suite_manage_pvc_config"},{"location":"roles/suite_manage_pvc_config/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_manage_pvc_config/#mas_instance_id","text":"Required. The instance ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_manage_pvc_config/#mas_workspace_id","text":"Required. The workspace ID of Maximo Application Suite. This will be used to lookup for Manage application resources. Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/suite_manage_pvc_config/#manage_workspace_cr_name","text":"Optional. Name of the ManageWorkspace Custom Resource that will be targeted to configure the new PVC definitions. Environment Variable: MANAGE_WORKSPACE_CR_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID","title":"manage_workspace_cr_name"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_pvc_name","text":"Optional. Name of the Persistent Volume Claim to be configured. Environment Variable: MAS_APP_SETTINGS_CUSTOM_PVC_NAME Default Value: $MAS_INSTANCE_ID-$MAS_WORKSPACE_ID-cust-files-pvc","title":"mas_app_settings_custom_persistent_volume_pvc_name"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_pv_name","text":"Optional. Name of the volume that will be created to store data from the PVC. Make sure your storage class provider supports the name you define. If not sure about what to set, you can leave it unset then a random name will the defined. Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_NAME Default Value: None.","title":"mas_app_settings_custom_persistent_volume_pv_name"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_pvc_size","text":"Optional. Size of the Persistent Volume Claim. Environment Variable: MAS_APP_SETTINGS_CUSTOM_PVC_SIZE Default Value: 100Gi","title":"mas_app_settings_custom_persistent_volume_pvc_size"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_mount_path","text":"Optional. Mouth path of the Persistent Volume in the Manage server container. Environment Variable: MAS_APP_SETTINGS_CUSTOM_MOUNT_PATH Default Value: /MeaGlobalDirs","title":"mas_app_settings_custom_persistent_volume_mount_path"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_sc_name","text":"Optional. Persistent Volume Claim Storage Class. If not set, it will be automatically defined accordingly to your cluster's available storage classes. Both ReadWriteMany(RWX) and ReadWriteOnce(RWO) are supported. Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_STORAGE_CLASS Default Value: None.","title":"mas_app_settings_custom_persistent_volume_sc_name"},{"location":"roles/suite_manage_pvc_config/#mas_app_settings_custom_persistent_volume_file_path","text":"Optional. Alternatively, this defines a local path pointing the persistent volume definition from a custom file. Sample file definition can be found in files/manage-persistent-volumes-sample.yml . Environment Variable: MAS_APP_SETTINGS_CUSTOM_PV_FILE_PATH Default Value: None.","title":"mas_app_settings_custom_persistent_volume_file_path"},{"location":"roles/suite_manage_pvc_config/#example-playbook","text":"The following sample can be used to configure new PVCs for an existing Manage application instance. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev mas_app_settings_custom_persistent_volume_sc_name: \"ibmc-file-gold-gid\" mas_app_settings_custom_persistent_volume_pvc_name: \"my-manage-pvc\" mas_app_settings_custom_persistent_volume_pvc_size: \"20Gi\" mas_app_settings_custom_persistent_volume_mount_path: \"/MyOwnFolder\" roles: - ibm.mas_devops.suite_manage_pvc_config The following sample can be used to configure new PVCs for an existing Manage application instance from a custom file definition. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_workspace_id: masdev mas_app_settings_custom_persistent_volume_file_path: \"/my-path/manage-pv.yml\" roles: - ibm.mas_devops.suite_manage_pvc_config","title":"Example Playbook"},{"location":"roles/suite_manage_pvc_config/#run-role-playbook","text":"After installing the Ansible Collection you can easily run the role standalone using the run_role playbook provided. ```bash export MAS_APP_SETTINGS_CUSTOM_PV_STORAGE_CLASS=ibmc-file-silver-gid export MAS_APP_SETTINGS_CUSTOM_PVC_NAME=my-manage-pvc export MAS_APP_SETTINGS_CUSTOM_PVC_SIZE=20Gi export MAS_APP_SETTINGS_CUSTOM_MOUNT_PATH=/MyOwnDir export MAS_APP_SETTINGS_CUSTOM_PV_FILE_PATH=/my-path/manage-pv.yml ROLE_NAME='suite_manage_pvc_config' ansible-playbook playbooks/run_role.yml","title":"Run Role Playbook"},{"location":"roles/suite_manage_pvc_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_rollback/","text":"suite_rollback \u00a4 This role is to roll back Maximo Application Suite to an earlier version. Rollback is possible only in 8.11 and later. From 8.11 onwards, every version comes with a set of supported versions to which Suite can be rolled back. For example, you can roll back Maximo Application Suite from 8.11.x to 8.11.0. This role validates given MAS installation is ready for the core platform to be rolled back to a specific MAS core version, and (as long as dry run mode is not enabled) will execute the rollback. It will validate that the specified version is compatible to rollback from the current version. It will validate that the core is already running at the targetted version. It will rollback the MAS core platform to the desired version (as long as dry run is not enabled). It will validate that the core platform has been successfully reconciled at the rolled back version. It will not validate that all core services successfully deploy after the reconcile (but we will be working on this limitation). Role Variables \u00a4 mas_instance_id \u00a4 The ID of the MAS instance to rollback. Required Environment Variable: MAS_INSTANCE_ID Default: None rollback_mas_core \u00a4 When set to true will ensure that the role performs rollback operation. optional Environment Variable: ROLLBACK_MAS_CORE Default: True verify_core_version \u00a4 When set to true will ensure that the role checks the current MAS core version matches with specified version. optional Environment Variable: VERIFY_CORE_VERSION Default: False mas_core_version \u00a4 The version of the MAS core that you want to rollback to or to validate current version. It is required when any of the ROLLBACK_MAS_CORE and VERIFY_CORE_VERSION variables is set to true . Required Environment Variable: MAS_CORE_VERSION Default: None mas_rollback_dryrun \u00a4 When set to true will ensure that the role only preforms rollback validation checks and does not make any changes to the target installation. Optional Environment Variable: MAS_ROLLBACK_DRYRUN Default: False Example Playbook \u00a4 Automatic Target Selection \u00a4 Running this playbook will rollback MAS core to the specified version. If you run this playbook when you are already on the same version it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_core_version: 8.11.0 mas_rollback_dryrun: False roles: - ibm.mas_devops.suite_rollback Verify MAS core version \u00a4 Running this playbook will attempt to verify the current version of MAS core matches with the specified version. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_core_version: 8.11.0 mas_upgrade_dryrun: False rollback_mas_core: False verify_core_version: True roles: - ibm.mas_devops.suite_rollback","title":"suite_rollback"},{"location":"roles/suite_rollback/#suite_rollback","text":"This role is to roll back Maximo Application Suite to an earlier version. Rollback is possible only in 8.11 and later. From 8.11 onwards, every version comes with a set of supported versions to which Suite can be rolled back. For example, you can roll back Maximo Application Suite from 8.11.x to 8.11.0. This role validates given MAS installation is ready for the core platform to be rolled back to a specific MAS core version, and (as long as dry run mode is not enabled) will execute the rollback. It will validate that the specified version is compatible to rollback from the current version. It will validate that the core is already running at the targetted version. It will rollback the MAS core platform to the desired version (as long as dry run is not enabled). It will validate that the core platform has been successfully reconciled at the rolled back version. It will not validate that all core services successfully deploy after the reconcile (but we will be working on this limitation).","title":"suite_rollback"},{"location":"roles/suite_rollback/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_rollback/#mas_instance_id","text":"The ID of the MAS instance to rollback. Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_rollback/#rollback_mas_core","text":"When set to true will ensure that the role performs rollback operation. optional Environment Variable: ROLLBACK_MAS_CORE Default: True","title":"rollback_mas_core"},{"location":"roles/suite_rollback/#verify_core_version","text":"When set to true will ensure that the role checks the current MAS core version matches with specified version. optional Environment Variable: VERIFY_CORE_VERSION Default: False","title":"verify_core_version"},{"location":"roles/suite_rollback/#mas_core_version","text":"The version of the MAS core that you want to rollback to or to validate current version. It is required when any of the ROLLBACK_MAS_CORE and VERIFY_CORE_VERSION variables is set to true . Required Environment Variable: MAS_CORE_VERSION Default: None","title":"mas_core_version"},{"location":"roles/suite_rollback/#mas_rollback_dryrun","text":"When set to true will ensure that the role only preforms rollback validation checks and does not make any changes to the target installation. Optional Environment Variable: MAS_ROLLBACK_DRYRUN Default: False","title":"mas_rollback_dryrun"},{"location":"roles/suite_rollback/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_rollback/#automatic-target-selection","text":"Running this playbook will rollback MAS core to the specified version. If you run this playbook when you are already on the same version it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_core_version: 8.11.0 mas_rollback_dryrun: False roles: - ibm.mas_devops.suite_rollback","title":"Automatic Target Selection"},{"location":"roles/suite_rollback/#verify-mas-core-version","text":"Running this playbook will attempt to verify the current version of MAS core matches with the specified version. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_core_version: 8.11.0 mas_upgrade_dryrun: False rollback_mas_core: False verify_core_version: True roles: - ibm.mas_devops.suite_rollback","title":"Verify MAS core version"},{"location":"roles/suite_uninstall/","text":"suite_uninstall \u00a4 This role removes Maximo Application Suite Core Platform. Note that it does not remove any data from MongoDb, and does not remove any applications from the MAS install, generally it should be used after suite_app_uninstall to remove all installed Maximo Applicaton Suite applications. Role Variables \u00a4 mas_instance_id \u00a4 Defines the MAS instance to be removed from the cluster Required Environment Variable: MAS_INSTANCE_ID Default: None mas_wipe_mongo_data \u00a4 Defines whether Mongo databases should be deleted along with MAS uninstall Optional Environment Variable: MAS_WIPE_MONGO_DATA Default: False Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" roles: - ibm.mas_devops.suite_uninstall License \u00a4 EPL-2.0","title":"suite_uninstall"},{"location":"roles/suite_uninstall/#suite_uninstall","text":"This role removes Maximo Application Suite Core Platform. Note that it does not remove any data from MongoDb, and does not remove any applications from the MAS install, generally it should be used after suite_app_uninstall to remove all installed Maximo Applicaton Suite applications.","title":"suite_uninstall"},{"location":"roles/suite_uninstall/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_uninstall/#mas_instance_id","text":"Defines the MAS instance to be removed from the cluster Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_uninstall/#mas_wipe_mongo_data","text":"Defines whether Mongo databases should be deleted along with MAS uninstall Optional Environment Variable: MAS_WIPE_MONGO_DATA Default: False","title":"mas_wipe_mongo_data"},{"location":"roles/suite_uninstall/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" roles: - ibm.mas_devops.suite_uninstall","title":"Example Playbook"},{"location":"roles/suite_uninstall/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_upgrade/","text":"suite_upgrade \u00a4 This role validates if a given MAS installation is ready for the core platform to be upgraded to a specific subscription channel, and (as long as dry run mode is not enabled) will execute the upgrade. It will validate that the current subscription channel is able to be upgraded to the target channel. It will validate that all installed applications have already been upgraded to versions compatible with the new version of the Core Platform. It will upgrade the MAS core platform to the desired channel (as long as dry run is not enabled). It will validate that the core platform has been successfully reconciled at the upgraded version. It will not validate that all core services successfully deploy after the reconcile (but we will be working on this limitation). Role Variables \u00a4 mas_instance_id \u00a4 The ID of the MAS instance to upgrade. Required Environment Variable: MAS_INSTANCE_ID Default: None mas_channel \u00a4 The name of the MAS subscription channel that you want to upgrade to, if not provided the correct version to upgrade to will be automatically selected based on the current version of MAS installed. Optional Environment Variable: MAS_CHANNEL Default: None mas_upgrade_dryrun \u00a4 When set to true will ensure that the role only preforms upgrade validation checks and does not make any changes to the target installation. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: False skip_compatibility_check \u00a4 When set to true will skip compatibility check before the upgrade install. By default, compatibility check will be performed to validate the specific target mas_channel is valid or not based on the existing mas version. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: False Example Playbook \u00a4 Automatic Target Selection \u00a4 Running this playbook will upgrade MAS to the next release. If you run this playbook when you are already on the latest release then it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade_check Explicit Upgrade Target \u00a4 Running this playbook will attempt to upgrade MAS to the specified release. If the specified release cannot be upgraded to from the installed version of MAS then no action will be taken. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_channel: 8.8.x mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade_check","title":"suite_upgrade"},{"location":"roles/suite_upgrade/#suite_upgrade","text":"This role validates if a given MAS installation is ready for the core platform to be upgraded to a specific subscription channel, and (as long as dry run mode is not enabled) will execute the upgrade. It will validate that the current subscription channel is able to be upgraded to the target channel. It will validate that all installed applications have already been upgraded to versions compatible with the new version of the Core Platform. It will upgrade the MAS core platform to the desired channel (as long as dry run is not enabled). It will validate that the core platform has been successfully reconciled at the upgraded version. It will not validate that all core services successfully deploy after the reconcile (but we will be working on this limitation).","title":"suite_upgrade"},{"location":"roles/suite_upgrade/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_upgrade/#mas_instance_id","text":"The ID of the MAS instance to upgrade. Required Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/suite_upgrade/#mas_channel","text":"The name of the MAS subscription channel that you want to upgrade to, if not provided the correct version to upgrade to will be automatically selected based on the current version of MAS installed. Optional Environment Variable: MAS_CHANNEL Default: None","title":"mas_channel"},{"location":"roles/suite_upgrade/#mas_upgrade_dryrun","text":"When set to true will ensure that the role only preforms upgrade validation checks and does not make any changes to the target installation. Optional Environment Variable: MAS_UPGRADE_DRYRUN Default: False","title":"mas_upgrade_dryrun"},{"location":"roles/suite_upgrade/#skip_compatibility_check","text":"When set to true will skip compatibility check before the upgrade install. By default, compatibility check will be performed to validate the specific target mas_channel is valid or not based on the existing mas version. Optional Environment Variable: SKIP_COMPATIBILITY_CHECK Default: False","title":"skip_compatibility_check"},{"location":"roles/suite_upgrade/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/suite_upgrade/#automatic-target-selection","text":"Running this playbook will upgrade MAS to the next release. If you run this playbook when you are already on the latest release then it will take no action. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade_check","title":"Automatic Target Selection"},{"location":"roles/suite_upgrade/#explicit-upgrade-target","text":"Running this playbook will attempt to upgrade MAS to the specified release. If the specified release cannot be upgraded to from the installed version of MAS then no action will be taken. - hosts: localhost any_errors_fatal: true vars: mas_instance_id: instance1 mas_channel: 8.8.x mas_upgrade_dryrun: False roles: - ibm.mas_devops.suite_upgrade_check","title":"Explicit Upgrade Target"},{"location":"roles/suite_verify/","text":"suite_verify \u00a4 Verify a MAS installation is ready to use. This role will also print out the Admin Dashboard URL and the username and password of the superuser. If you want to disable these credentials being written to the output set the mas_hide_superuser_credentials to True . Role Variables \u00a4 mas_instance_id \u00a4 Required. The instance ID of the Maximo Application Suite installation to verify. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_hide_superuser_credentials \u00a4 Set this to True if you want to disable the display of the superuser credentials as part of the verify. When this is enabled the debug will only identify the name of the secret containing the credentials rather than displaying the actual values. Environment Variable: MAS_HIDE_SUPERUSER_CREDENTIALS Default Value: False Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_hide_superuser_credentials: True roles: - ibm.mas_devops.suite_verify License \u00a4 EPL-2.0","title":"suite_verify"},{"location":"roles/suite_verify/#suite_verify","text":"Verify a MAS installation is ready to use. This role will also print out the Admin Dashboard URL and the username and password of the superuser. If you want to disable these credentials being written to the output set the mas_hide_superuser_credentials to True .","title":"suite_verify"},{"location":"roles/suite_verify/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_verify/#mas_instance_id","text":"Required. The instance ID of the Maximo Application Suite installation to verify. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_verify/#mas_hide_superuser_credentials","text":"Set this to True if you want to disable the display of the superuser credentials as part of the verify. When this is enabled the debug will only identify the name of the secret containing the credentials rather than displaying the actual values. Environment Variable: MAS_HIDE_SUPERUSER_CREDENTIALS Default Value: False","title":"mas_hide_superuser_credentials"},{"location":"roles/suite_verify/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_hide_superuser_credentials: True roles: - ibm.mas_devops.suite_verify","title":"Example Playbook"},{"location":"roles/suite_verify/#license","text":"EPL-2.0","title":"License"},{"location":"roles/turbonomic/","text":"Turbonomic \u00a4 Installs kubeturbo from any available CatalogSource, and automatically configures it to connect to a defined Turbonomic server. Note The Turbonomic Kubernetes Operator does not support disconnected installation. The kubeturbo deployment will be created using a tag rather than a digest, which prevents the use of an ImageContentSourcePolicy to configure a mirror registry for this image. Role Variables - KubeTurbo Configuration \u00a4 kubeturbo_namespace \u00a4 Set the namespace where the KubeTurbo operator will be installed. Optional Environment Variable: KUBETURBO_NAMESPACE Default: kubeturbo Role Variables - Turbonomic Server Configuration \u00a4 turbonomic_target_name \u00a4 The cluster name is required to install Kubeturbo. The agent component is deployed onto target Kubernetes and OpenShift cluster which then send data to the Turbonomic ARM server. Required Environment Variable: TURBONOMIC_TARGET_NAME Default: None turbonomic_server_url \u00a4 The route is required to access the Turbonomics instance. Kubeturbo communicates with the Turbo Server using the supplied turbonomic route as the Turbonomic Server endpoint while configuring kubeturbo. Required Environment Variable: TURBONOMIC_SERVER_URL Default: None turbonomic_server_version \u00a4 The version of the Turbonomic server you are connecting to. Optional Environment Variable: TURBONOMIC_SERVER_VERSION Default: None turbonomic_username \u00a4 The username to authenticate with the Turbonomic server. Required Environment Variable: TURBONOMIC_USERNAME Default: None turbonomic_password \u00a4 The password to authenticate with the Turbonomic server. Required Environment Variable: TURBONOMIC_PASSWORD Default: None Example Playbook \u00a4 - hosts: localhost any_errors_fatal: true vars: turbonomic_server_url: https://myturbonomicserver.com turbonomic_server_version: \"8.9.4\" turbonomic_username: user turbonomic_password: passw0rd turbonomic_target_name: myocp roles: - ibm.mas_devops.turbonomic License \u00a4 EPL-2.0","title":"turbonomic"},{"location":"roles/turbonomic/#turbonomic","text":"Installs kubeturbo from any available CatalogSource, and automatically configures it to connect to a defined Turbonomic server. Note The Turbonomic Kubernetes Operator does not support disconnected installation. The kubeturbo deployment will be created using a tag rather than a digest, which prevents the use of an ImageContentSourcePolicy to configure a mirror registry for this image.","title":"Turbonomic"},{"location":"roles/turbonomic/#role-variables-kubeturbo-configuration","text":"","title":"Role Variables - KubeTurbo Configuration"},{"location":"roles/turbonomic/#kubeturbo_namespace","text":"Set the namespace where the KubeTurbo operator will be installed. Optional Environment Variable: KUBETURBO_NAMESPACE Default: kubeturbo","title":"kubeturbo_namespace"},{"location":"roles/turbonomic/#role-variables-turbonomic-server-configuration","text":"","title":"Role Variables - Turbonomic Server Configuration"},{"location":"roles/turbonomic/#turbonomic_target_name","text":"The cluster name is required to install Kubeturbo. The agent component is deployed onto target Kubernetes and OpenShift cluster which then send data to the Turbonomic ARM server. Required Environment Variable: TURBONOMIC_TARGET_NAME Default: None","title":"turbonomic_target_name"},{"location":"roles/turbonomic/#turbonomic_server_url","text":"The route is required to access the Turbonomics instance. Kubeturbo communicates with the Turbo Server using the supplied turbonomic route as the Turbonomic Server endpoint while configuring kubeturbo. Required Environment Variable: TURBONOMIC_SERVER_URL Default: None","title":"turbonomic_server_url"},{"location":"roles/turbonomic/#turbonomic_server_version","text":"The version of the Turbonomic server you are connecting to. Optional Environment Variable: TURBONOMIC_SERVER_VERSION Default: None","title":"turbonomic_server_version"},{"location":"roles/turbonomic/#turbonomic_username","text":"The username to authenticate with the Turbonomic server. Required Environment Variable: TURBONOMIC_USERNAME Default: None","title":"turbonomic_username"},{"location":"roles/turbonomic/#turbonomic_password","text":"The password to authenticate with the Turbonomic server. Required Environment Variable: TURBONOMIC_PASSWORD Default: None","title":"turbonomic_password"},{"location":"roles/turbonomic/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: turbonomic_server_url: https://myturbonomicserver.com turbonomic_server_version: \"8.9.4\" turbonomic_username: user turbonomic_password: passw0rd turbonomic_target_name: myocp roles: - ibm.mas_devops.turbonomic","title":"Example Playbook"},{"location":"roles/turbonomic/#license","text":"EPL-2.0","title":"License"},{"location":"roles/uds/","text":"uds \u00a4 Installs IBM User Data Services as part of IBM Foundational Services in the ibm-common-services namespace. If mas_instance_id and the others associated parameters are provided then the role will also generate a configuration file that can be directly applied to IBM Maximo Application Suite. Role Variables - Installation \u00a4 uds_action \u00a4 Inform the role whether to perform an install or uninstall of IBM User Data Services or the Slim User Data Services. Supported values are install , uninstall , install-suds or uninstall-suds Optional Environment Variable: UDS_ACTION Default: install cluster_name \u00a4 Required only for ROSA cluster. This variable is required to extract the UDS certificates. For other clusters this variable is not used. Environment Variable: CLUSTER_NAME Default Value: None uds_storage_class \u00a4 Required. Storage class where UDS will be installed. On IBM Cloud RedHat Openshift Kubernetes Service (ROKS) ibmc-block-bronze is the recommended value. The storage class must support ReadWriteOnce(RWO) access mode. Environment Variable: UDS_STORAGE_CLASS Default Value: None uds_event_scheduler_frequency \u00a4 Defines the frequency that BAS will collect event data. The value can be set following a cron tab format. Environment Variable: UDS_EVENT_SCHEDULER_FREQUENCY Default Value: @daily cluster ingres tls secret name \u00a4 Specify the name of the cluster's ingres tls secret which contains the default router certificate. Optional Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default Role Variables - BASCfg Generation \u00a4 mas_instance_id \u00a4 The instance ID of Maximo Application Suite that the BasCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \u00a4 Local directory to save the generated BasCfg resource definition. This can be used to manually configure a MAS instance to connect to BAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None mas_segment_key \u00a4 Override the built-in segment key used by MAS when communicating with User Data Services. This variable is only used for the generation of the BASCfg template, and in 99% of use cases you will not need to set this. Optional Environment Variable: MAS_SEGMENT_KEY Default Value: None uds_contact.email \u00a4 Sets the Contact e-mail address used by the MAS instance's UDS configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: UDS_CONTACT_EMAIL Default Value: None uds_contact.first_name \u00a4 Sets the Contact first name used by the MAS instance's UDS configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: UDS_CONTACT_FIRSTNAME Default Value: None uds_contact.last_name \u00a4 Sets the Contact last name used by the MAS instance's UDS configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: UDS_CONTACT_LASTNAME Default Value: None uds_endpoint_url \u00a4 Sets the UDS endpoint url used by the MAS instance's UDS configuration. Optional, used to instruct the role to set up MAS for an existing UDS instance. Environment Variable: UDS_ENDPOINT_URL Default Value: None uds_tls_crt \u00a4 Sets the UDS TLS CA or Server Certificate used by the MAS instance's UDS configuration. Optional, used to instruct the role to set up MAS for an existing UDS instance. Environment Variable: UDS_TLS_CERT Default Value: None uds_tls_crt_local_file_path \u00a4 The path on the local system to a file containing the TLS CA certificate of the AnalyticsProxy to be used when the Maximo Application Suite is registered with UDS. This variable is only used if uds_tls_crt has not been set. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: UDS_TLS_CERT_LOCAL_FILE Default Value: None uds_api_key \u00a4 Sets the UDS api key used by the MAS instance's UDS configuration. Optional, used to instruct the role to set up MAS for an existing UDS instance. Environment Variable: UDS_API_KEY Default Value: None custom_labels \u00a4 List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None mas_pod_templates_dir \u00a4 Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-bascfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the BasCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None include_cluster_ingress_cert_chain \u00a4 Optional. When set to True , includes the complete certificates chain in the generated MAS configuration, when a trusted certificate authority is found in your cluster's ingress. Optional Environment Variable: INCLUDE_CLUSTER_INGRESS_CERT_CHAIN Default: False Example Playbook \u00a4 Install in-cluster and generate MAS configuration \u00a4 - hosts: localhost any_errors_fatal: true vars: uds_storage_class: ibmc-block-bronze mas_instance_id: masinst1 mas_config_dir: ~/masconfig uds_contact: email: 'john@email.com' first_name: 'john' last_name: 'winter' roles: - ibm.mas_devops.uds Generate MAS configuration for existing installation \u00a4 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig uds_endpoint_url: \"https://xxx\" uds_api_key: \"xxx\" uds_tls_crt_local_file_path: \"/path/to/uds.crt\" uds_contact: email: 'john@email.com' first_name: 'john' last_name: 'winter' roles: - ibm.mas_devops.uds License \u00a4 EPL-2.0","title":"uds"},{"location":"roles/uds/#uds","text":"Installs IBM User Data Services as part of IBM Foundational Services in the ibm-common-services namespace. If mas_instance_id and the others associated parameters are provided then the role will also generate a configuration file that can be directly applied to IBM Maximo Application Suite.","title":"uds"},{"location":"roles/uds/#role-variables-installation","text":"","title":"Role Variables - Installation"},{"location":"roles/uds/#uds_action","text":"Inform the role whether to perform an install or uninstall of IBM User Data Services or the Slim User Data Services. Supported values are install , uninstall , install-suds or uninstall-suds Optional Environment Variable: UDS_ACTION Default: install","title":"uds_action"},{"location":"roles/uds/#cluster_name","text":"Required only for ROSA cluster. This variable is required to extract the UDS certificates. For other clusters this variable is not used. Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/uds/#uds_storage_class","text":"Required. Storage class where UDS will be installed. On IBM Cloud RedHat Openshift Kubernetes Service (ROKS) ibmc-block-bronze is the recommended value. The storage class must support ReadWriteOnce(RWO) access mode. Environment Variable: UDS_STORAGE_CLASS Default Value: None","title":"uds_storage_class"},{"location":"roles/uds/#uds_event_scheduler_frequency","text":"Defines the frequency that BAS will collect event data. The value can be set following a cron tab format. Environment Variable: UDS_EVENT_SCHEDULER_FREQUENCY Default Value: @daily","title":"uds_event_scheduler_frequency"},{"location":"roles/uds/#cluster-ingres-tls-secret-name","text":"Specify the name of the cluster's ingres tls secret which contains the default router certificate. Optional Environment Variable: OCP_INGRESS_TLS_SECRET_NAME Default Value: router-certs-default","title":"cluster ingres tls secret name"},{"location":"roles/uds/#role-variables-bascfg-generation","text":"","title":"Role Variables - BASCfg Generation"},{"location":"roles/uds/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the BasCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/uds/#mas_config_dir","text":"Local directory to save the generated BasCfg resource definition. This can be used to manually configure a MAS instance to connect to BAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a BasCfg template. Optional Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/uds/#mas_segment_key","text":"Override the built-in segment key used by MAS when communicating with User Data Services. This variable is only used for the generation of the BASCfg template, and in 99% of use cases you will not need to set this. Optional Environment Variable: MAS_SEGMENT_KEY Default Value: None","title":"mas_segment_key"},{"location":"roles/uds/#uds_contactemail","text":"Sets the Contact e-mail address used by the MAS instance's UDS configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: UDS_CONTACT_EMAIL Default Value: None","title":"uds_contact.email"},{"location":"roles/uds/#uds_contactfirst_name","text":"Sets the Contact first name used by the MAS instance's UDS configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: UDS_CONTACT_FIRSTNAME Default Value: None","title":"uds_contact.first_name"},{"location":"roles/uds/#uds_contactlast_name","text":"Sets the Contact last name used by the MAS instance's UDS configuration. Required when mas_instance_id and mas_config_dir are set Environment Variable: UDS_CONTACT_LASTNAME Default Value: None","title":"uds_contact.last_name"},{"location":"roles/uds/#uds_endpoint_url","text":"Sets the UDS endpoint url used by the MAS instance's UDS configuration. Optional, used to instruct the role to set up MAS for an existing UDS instance. Environment Variable: UDS_ENDPOINT_URL Default Value: None","title":"uds_endpoint_url"},{"location":"roles/uds/#uds_tls_crt","text":"Sets the UDS TLS CA or Server Certificate used by the MAS instance's UDS configuration. Optional, used to instruct the role to set up MAS for an existing UDS instance. Environment Variable: UDS_TLS_CERT Default Value: None","title":"uds_tls_crt"},{"location":"roles/uds/#uds_tls_crt_local_file_path","text":"The path on the local system to a file containing the TLS CA certificate of the AnalyticsProxy to be used when the Maximo Application Suite is registered with UDS. This variable is only used if uds_tls_crt has not been set. Optional, used to instruct the role to set up MAS for an existing SLS instance. Environment Variable: UDS_TLS_CERT_LOCAL_FILE Default Value: None","title":"uds_tls_crt_local_file_path"},{"location":"roles/uds/#uds_api_key","text":"Sets the UDS api key used by the MAS instance's UDS configuration. Optional, used to instruct the role to set up MAS for an existing UDS instance. Environment Variable: UDS_API_KEY Default Value: None","title":"uds_api_key"},{"location":"roles/uds/#custom_labels","text":"List of comma separated key=value pairs for setting custom labels on instance specific resources. Optional Environment Variable: CUSTOM_LABELS Default Value: None","title":"custom_labels"},{"location":"roles/uds/#mas_pod_templates_dir","text":"Provide the directory where supported pod templates configuration files are defined. This role will look for a configuration file named ibm-mas-bascfg.yml in the named directory. The content of the configuration file should be the yaml block that you wish to be inserted into the BasCfg spec under a top level podTemplates element, e.g. podTemplates: {object} . For examples refer to the BestEfforts reference configuration in the MAS CLI , for full documentation of the supported options refer to the Customizing Pod Templates in the product documentation. Optional Environment Variable: MAS_POD_TEMPLATES_DIR Default: None","title":"mas_pod_templates_dir"},{"location":"roles/uds/#include_cluster_ingress_cert_chain","text":"Optional. When set to True , includes the complete certificates chain in the generated MAS configuration, when a trusted certificate authority is found in your cluster's ingress. Optional Environment Variable: INCLUDE_CLUSTER_INGRESS_CERT_CHAIN Default: False","title":"include_cluster_ingress_cert_chain"},{"location":"roles/uds/#example-playbook","text":"","title":"Example Playbook"},{"location":"roles/uds/#install-in-cluster-and-generate-mas-configuration","text":"- hosts: localhost any_errors_fatal: true vars: uds_storage_class: ibmc-block-bronze mas_instance_id: masinst1 mas_config_dir: ~/masconfig uds_contact: email: 'john@email.com' first_name: 'john' last_name: 'winter' roles: - ibm.mas_devops.uds","title":"Install in-cluster and generate MAS configuration"},{"location":"roles/uds/#generate-mas-configuration-for-existing-installation","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_config_dir: ~/masconfig uds_endpoint_url: \"https://xxx\" uds_api_key: \"xxx\" uds_tls_crt_local_file_path: \"/path/to/uds.crt\" uds_contact: email: 'john@email.com' first_name: 'john' last_name: 'winter' roles: - ibm.mas_devops.uds","title":"Generate MAS configuration for existing installation"},{"location":"roles/uds/#license","text":"EPL-2.0","title":"License"}]}