---
# This is a horrible hack, but it's what CP4D tells us to do :(
#
# This should be executed as part of cluster prepararion if you want to use CP4D v4.
#
# It will reboot all worker nodes, causing disruption to the entire cluster and everything
# running on it we do not include this as part of the normal flow because, well it shouldn't
# be necessary to reboot worker nodes to install containerized software.  Hopefully this is just
# an example of poor documentation and there's a simple alternative that we can implement to
# remove this role.
#
# https://cloud.ibm.com/docs/openshift?topic=openshift-registry#cluster_global_pull_secret


# 0. Fail if required params are not provided
# -----------------------------------------------------------------------------
- name: "Fail if cpd_registry_password has not been provided"
  when: cpd_registry_password is not defined or cpd_registry_password == ""
  fail:
    msg: "cpd_registry_password property has not been set"

# 0. Fail if we are trying to do this in an unsupported cluster type
# -----------------------------------------------------------------------------
- name: "Fail if cluster_type is not set to roks"
  when: cluster_type is not defined or cluster_type != "roks"
  fail:
    msg: "cluster_type property has not been set, only 'roks' is supported in this role"

- name: "Debug information"
  debug:
    msg:
      - "CPD registry ................. {{ cpd_registry }}"
      - "CPD username ................. {{ cpd_registry_user }}"
      - "Cluster Name ................. {{ cluster_name }}"
      - "Cluster Type ................. {{ cluster_type }}"


# 1. Update cp4d entitlement key and cpd registry server in cluster's pull-secret
# ---------------------------------------------------------------------------------------------------------------------

# 1.1 Generate the new secret
- name: Set new secret content
  vars:
    entitledAuthStr: "{{ cpd_registry_user }}:{{ cpd_registry_password }}"
    entitledAuth: "{{ entitledAuthStr | b64encode }}"
    content:
      - "{\"auths\":{\"{{ cpd_registry }}\":{\"username\":\"{{ cpd_registry_user }}\",\"password\":\"{{ cpd_registry_password }}\",\"email\":\"{{ cpd_registry_user }}\",\"auth\":\"{{ entitledAuth }}\"}"
      - "}"
      - "}"
  set_fact:
    new_secret: "{{ content | join('') }}"

# 1.2 Find the existing secret, and we are going to modify it rather than replace
- name: Retrieve existing pull-secret content
  community.kubernetes.k8s_info:
    api: v1
    kind: Secret
    name: pull-secret
    namespace: openshift-config
  register: pullsecret

- name: Get the original cred secrets
  set_fact:
    original_secret: "{{ item.data }}"
  with_items: "{{ pullsecret.resources }}"
  no_log: true

- name: Get the dockerconfigjson info
  set_fact:
    secret_string: '{{ original_secret[".dockerconfigjson"] | b64decode | from_json }}'

# 1.3 Append our new credentials to the secret
- name: Combine new secret content
  set_fact:
    new_secret_string: '{{ secret_string | combine( new_secret, recursive=True) }}'

# 1.4. Overwrite the secret
- name: "Update new pull-secret"
  community.kubernetes.k8s:
    definition:
      apiVersion: v1
      kind: Secret
      type: kubernetes.io/dockerconfigjson
      metadata:
        name: pull-secret
        namespace: openshift-config
      data:
        .dockerconfigjson: "{{ new_secret_string | to_json | b64encode }}"
  register: secretUpdateResult

- name: Debug change state
  debug:
    msg: "Default Pull Secret Changed ....... {{ secretUpdateResult.changed }}"


# 2. Reload the cluster worker nodes **if the pull-secret was changed**
# ---------------------------------------------------------------------------------------------------------------------

# 2.1 Get cluster info
- name: "Get cluster worker node list"
  when: secretUpdateResult.changed
  shell: ibmcloud oc worker ls -c {{ cluster_name }} -q | awk '{print $1}'
  register: cluster_lookup
  failed_when: "cluster_lookup.rc > 1"

# 2.2 Issue reload command to the cluster's worker nodes
- name: "Reload all worker nodes"
  vars:
    workers: "{{ item }}"
  when:
    - secretUpdateResult.changed
    - item | length > 0
  shell: ibmcloud oc worker reload -c {{ cluster_name }} -w {{ item }} -f
  with_items: "{{ cluster_lookup.stdout_lines }}"

# 2.3 Wait for cluster status to enter "pending"
- name: "Wait until the ROKS cluster status is 'pending' (30 minute timeout)"
  when: secretUpdateResult.changed
  shell: ibmcloud oc cluster get --cluster {{ cluster_name }} --output json
  register: roks_cluster_completion
  until:
    - roks_cluster_completion.rc == 0
    - (roks_cluster_completion.stdout | from_json).state == 'pending'
  retries: 30
  delay: 60

# 2.4 Wait for cluster status to return to "normal"
- name: "Wait until the ROKS cluster status is 'normal' again (1 hour timeout)"
  when: secretUpdateResult.changed
  shell: ibmcloud oc cluster get --cluster {{ cluster_name }} --output json
  register: roks_cluster_completion
  until:
    - roks_cluster_completion.rc == 0
    - (roks_cluster_completion.stdout | from_json).state == 'normal'
  retries: 60
  delay: 60
