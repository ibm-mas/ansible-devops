---
# 1. Wait for ccs-cr to be created
# -----------------------------------------------------------------------------
- name: "wait-ccs : Wait for ccs-cr to be created"
  kubernetes.core.k8s_info:
    api_version: ccs.cpd.ibm.com/v1beta1
    name: ccs-cr
    namespace: "{{ cpd_instance_namespace }}"
    kind: CCS
  register: ccscr_output
  until:
    - ccscr_output.resources is defined
    - ccscr_output.resources | length > 0
  retries: 60 # approx 30 minutes before we give up
  delay: 30 # seconds

# check if ccs cr is already patched
- set_fact:
    is_ccs_already_patched: "{{ ccscr_output.resources[0].spec.couchdb_resources is defined and ccscr_output.resources[0].spec.blockStorageClass is defined and ccscr_output.resources[0].spec.imagePullSecret is defined }}"

- debug:
    msg:
      - "CCS CR already patched? .............. {{ is_ccs_already_patched }}"
      - "CCS Block Storage Class .............. {{ ccscr_output.resources[0].spec.blockStorageClass | default('<undefined>', true) }}"

# 2. Apply the patch per recommendation from CP4D team
# https://github.ibm.com/NGP-TWC/ml-planning/issues/32683
# https://medium.com/@dany.drouin/scaling-watson-knowledge-catalog-on-cloud-pak-for-data-11623f41f7df
# -----------------------------------------------------------------------------
# only run following block if is_ccs_already_patched == False
- block:
    - name: "wait-ccs : Patch ccs-cr to increase resource limits"
      kubernetes.core.k8s:
        api_version: ccs.cpd.ibm.com/v1beta1
        name: ccs-cr
        namespace: "{{ cpd_instance_namespace }}"
        kind: CCS
        definition:
          spec:
            imagePullSecret: "{{ ibm_entitlement_key_secret }}"
            blockStorageClass: "{{ cpd_service_block_storage_class }}"
            fileStorageClass: "{{ cpd_service_storage_class }}"
            couchdb_resources:
              limits:
                cpu: "16"
                memory: 16Gi
              requests:
                cpu: "3"
                memory: 256Mi
            couchdb_search_resources:
              limits:
                cpu: "4"
                memory: 6Gi
              requests:
                cpu: 250m
                memory: 256Mi
        apply: true
        server_side_apply:
          field_manager: ansible
          force_conflicts: true

    # 3. Delete ccs-operator pod to force the reconcile from the beginning after ccs-cr is patched.
    # -----------------------------------------------------------------------------
    - name: "wait-ccs : Scale down ccs-operator"
      kubernetes.core.k8s:
        api_version: apps/v1
        name: ibm-cpd-ccs-operator
        namespace: "{{ cpd_operators_namespace }}"
        kind: Deployment
        definition:
          spec:
            replicas: 0
        apply: true
        server_side_apply:
          field_manager: ansible
          force_conflicts: true

    - name: "wait-ccs : Scale up ccs-operator"
      kubernetes.core.k8s:
        api_version: apps/v1
        name: ibm-cpd-ccs-operator
        namespace: "{{ cpd_operators_namespace }}"
        kind: Deployment
        definition:
          spec:
            replicas: 1
        apply: true
        server_side_apply:
          field_manager: ansible
          force_conflicts: true

    # 4. Wait for ccs operator ...
    # -----------------------------------------------------------------------------
    - name: "wait-ccs : Wait for ccs-operator to be ready again (60s delay)"
      kubernetes.core.k8s_info:
        api_version: apps/v1
        name: ibm-cpd-ccs-operator
        namespace: "{{ cpd_operators_namespace }}"
        kind: Deployment
      register: ccs_operator_lookup
      until: ccs_operator_lookup.resources[0].status.availableReplicas is defined
      retries: 20 # Approximately 20 minutes before we give up
      delay: 60 # 1 minute

  when: not is_ccs_already_patched

- include_tasks: "tasks/wait/wait-elasticsearch.yml"
  when:
    - cpd_48_or_higher # elastic search operator was just introduced with cpd 4.8
    - not skip_ibm_entitlement_injection # eventually we hope to be able to skip patching the elastic search cr with image pull secret, but not for now

# 5. Wait for CouchDB Stateful Set to be ready
# -----------------------------------------------------------------------------
# There have been issues with CouchDB not starting due to Persistent Storage,
# This task restarts any failing pods
- include_tasks: "tasks/wait/wait-couchdb.yml"
  when:
    - cpd_48

# 6. Wait for CCS CR to be ready
# -----------------------------------------------------------------------------
# Note: We can't fail early when we see Failed status, as the operator will
# report failed multiple times during initial reconcile.
# We give it an hour to have made it past upgrading elasticsearch if still
# failing check to see if elasticsearch needs rescuing by deleting and
# recreating.
# regardless of if the elasticsearch rescue is performed we wait another 4 hours
# to let the process complete (in the always section)
- block:
  - name: "wait-ccs : Wait for ccsStatus 'Completed' (5m interval)"
    kubernetes.core.k8s_info:
      api_version: "ccs.cpd.ibm.com/v1beta1"
      kind: CCS
      name: "ccs-cr"
      namespace: "{{ cpd_instance_namespace }}"
    register: ccs_cr_lookup
    until:
      - ccs_cr_lookup.resources is defined
      - ccs_cr_lookup.resources | length == 1
      - ccs_cr_lookup.resources[0].status is defined
      - ccs_cr_lookup.resources[0].status.ccsStatus is defined
      - ccs_cr_lookup.resources[0].status.ccsStatus == "Completed"
    retries: 12 # 1 hour
    delay: 300 # Every 5 minutes

  rescue:
  - name: "get the elasticsearch statefulset"
    kubernetes.core.k8s_info:
      api_version: apps/v1
      namespace: "{{ cpd_instance_namespace }}"
      kind: StatefulSet
      label_selectors: ["ibm-es-master=True"]
    register: statefulset_output

  - name: "check if elasticsearch rescue is appropriate"
    set_fact:
       recover_elasticsearch: true
    when:
      - statefulset_output.resources is defined
      - statefulset_output.resources | length == 1
      - statefulset_output.resources[0].status is defined
      - statefulset_output.resources[0].status.availableReplicas is defined
      - statefulset_output.resources[0].status.availableReplicas == 0

  - name: "Pause for 5 minutes before removing elasticsearchcluster and associated persistent volumes"
    pause:
      minutes: 5

  - name: "reset rescue decision"
    set_fact:
       recover_elasticsearch: false

  - name: "confirm elasticsearch rescue is still appropriate"
    set_fact:
       recover_elasticsearch: true
    when:
      - statefulset_output.resources is defined
      - statefulset_output.resources | length == 1
      - statefulset_output.resources[0].status is defined
      - statefulset_output.resources[0].status.availableReplicas is defined
      - statefulset_output.resources[0].status.availableReplicas == 0

  - name: "delete the elasticsearch cluster"
    k8s:
      api_version: elasticsearch.opencontent.ibm.com/v1
      kind: ElasticsearchCluster
      state: absent
      namespace: "{{ cpd_instance_namespace }}"
      name: "{{ statefulset_output.resources[0].metadata.ownerReferences[0].name }}"
    when:
      - recover_elasticsearch == true

  - name: "Delete the elasticsearch PVCs"
    k8s:
      api_version:  v1
      kind: PersistentVolumeClaim
      namespace: "{{ cpd_instance_namespace }}"
      label_selectors: ["app.kubernetes.io/component={{ statefulset_output.resources[0].metadata.name}}"]
      state: absent
    when:
      - recover_elasticsearch == true

  always:

  - name: "wait-ccs : Wait for ccsStatus 'Completed' (5m interval)"
    kubernetes.core.k8s_info:
      api_version: "ccs.cpd.ibm.com/v1beta1"
      kind: CCS
      name: "ccs-cr"
      namespace: "{{ cpd_instance_namespace }}"
    register: ccs_cr_lookup
    until:
      - ccs_cr_lookup.resources is defined
      - ccs_cr_lookup.resources | length == 1
      - ccs_cr_lookup.resources[0].status is defined
      - ccs_cr_lookup.resources[0].status.ccsStatus is defined
      - ccs_cr_lookup.resources[0].status.ccsStatus == "Completed"
    retries: 50 # Just over 4 hours
    delay: 300 # Every 5 minutes

- name: "wait-ccs : Check that the CCS ccsStatus is 'Completed'"
  assert:
    that: ccs_cr_lookup.resources[0].status.ccsStatus == "Completed"
    fail_msg: "CCS install failed (ccsStatus)"
